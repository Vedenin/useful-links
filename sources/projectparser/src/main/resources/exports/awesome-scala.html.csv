"category"	"name"	"url"	"description"	"star"	"stackOverflow"	"stackOverflowUrl"	"license"	"licenseUrl"	"site"	"userGuide"	"newStars"	"newWatchs"	"newForks"	"pageText"	"newStackOverflow"	"github"	"allText"	"isExist"
"Database"	"Activate ★ 290 ⧗ 1"	"https://github.com/fwbrasil/activate"	"Pluggable object persistence in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"292"	"31"	"47"	"GitHub - fwbrasil/activate: Pluggable persistence in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 31 Star 292 Fork 47 fwbrasil/activate Code Issues 36 Pull requests 5 Pulse Graphs Pluggable persistence in Scala http://activate-framework.org 957 commits 5 branches 49 releases 13 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.3.2 gh-pages master v1.2.2 v1.2.3 Nothing to show v1.7 v1.6.2 v1.6.1 v1.6 v1.5.4 v1.5.3 v1.5.2 v1.5.1 v1.5 v1.5-RC2 v1.5-RC1 v1.5-M6 v1.5-M5 v1.5-M4 v1.5-M3 v1.5-M2 v1.5-M1 v1.4.4 v1.4.3 v1.4.2 v1.4.1 v1.4 v1.4-RC2 v1.4-RC1 v1.3.5 v1.3.4 v1.3.3 v1.3.1 v1.3 v1.3-RC1 v1.2.2 v1.2.1 v1.2 v1.2-RC6 v1.2-RC5 v1.2-RC4 v1.2-RC3 v1.2-RC2 v1.1 v1.1-RC2 v1.1-RC1 v1.0 v1.0-RC4 v1.0-RC3 v1.0-RC2 v1.0-RC1 v0.9 v0.8 v0.6 Nothing to show New pull request Latest commit 6fd207a Oct 18, 2015 fwbrasil Merge pull request #183 from cailhuiris/master … MoyEntity -> MyEntity, Permalink Failed to load latest commit information. activate-cassandra-async/src/main/scala/net/fwbrasil/activate/storage/cassandra refactoring entity hierarchy Nov 30, 2013 activate-core/src/main fix #144 Sep 20, 2014 activate-docs MoyEntity -> MyEntity, Oct 18, 2015 activate-finagle-mysql/src/main/scala/net/fwbrasil/activate/storage/relational/async finagle mysql - fix pool config Jun 13, 2014 activate-graph/src/main/scala/net/fwbrasil/activate/storage/graph refactoring entity hierarchy Nov 29, 2013 activate-jackson/src/main/scala/net/fwbrasil/activate/json/jackson refactoring entity hierarchy Nov 29, 2013 activate-jdbc-async/src/main/scala/net/fwbrasil/activate/storage/relational/async fix eager initialization for async queries Jun 21, 2014 activate-jdbc/src/main/scala/net/fwbrasil/activate/storage/relational fix #154 Sep 19, 2014 activate-lift/src/main/scala/net/fwbrasil/activate/lift refactoring entity hierarchy Nov 29, 2013 activate-mongo-async/src/main/scala/net/fwbrasil/activate/storage/mongo/async reactivemongo 0.10.5.akka23-SNAPSHOT Jun 25, 2014 activate-mongo/src/main/scala/net/fwbrasil/activate/storage/mongo fix #93 Jan 30, 2014 activate-play/src/main/scala/net/fwbrasil/activate/play Update to Play 2.3.2 Jul 29, 2014 activate-prevalent/src/main/scala/net/fwbrasil/activate/storage/prevalent prevalent storage - fix class loading for sbt Feb 9, 2014 activate-prevayler/src/main/scala refactoring Jan 28, 2014 activate-slick/src/main/scala/net/fwbrasil/activate/slick wip - scala 2.11 support Sep 16, 2014 activate-spray-json/src/main/scala/net/fwbrasil/activate/json/spray wip - scala 2.11 support Sep 17, 2014 activate-test remove unused import Jan 23, 2015 project update sbt version Sep 21, 2014 src/main/ls ls version Apr 27, 2012 .gitignore minor Sep 8, 2012 .travis.yml update travis config (scala version) Sep 21, 2014 CHANGELOG.md 1.6 version Jun 13, 2014 LICENSE-LGPL Changing to LGPL Jun 7, 2011 README.md Added Gitter badge Jan 14, 2015 README.md Activate Persistence Framework Documentation (index): Get started Persistence Context Entity Transaction Validation Query Test support Migration Multiple VMs Mass statement Play framework Lift framework Spray Json Architecture Introduction Activate is a framework to persist objects in Scala. It is a STM (Software Transactional Memory) durable, with pluggable persistence. Its core is the RadonSTM, which provides a powerful mechanism for controlling transactions in memory, analogous to the transactions of databases, to do optimistic concurrency control. The durability of transactions (persistence) is pluggable and can use persistence in different paradigms such as relational (JDBC), prevalence (Prevayler) and non-relational (MongoDB). Benefits The main benefits of the framework are: Atomic, consistent, isolated and durable transactions. You can use entities without worrying about concurrency issues. Entities are always consistent in memory and in the persistence layer. For example, if rollback occurs, entities in memory stay consistent. Transaction propagation control, including nested transactions. Entities are lazy loaded and initialized automatically when needed. Queries are type-safe and consistent, even with objects created in the current transaction. Therefore, an entity created in the same transaction may be returned in a query. The available memory is used efficiently, minimizing the conversation with the storage and maximizing performance. Build Use sbt 0.11.2 to build Activate. Use the command ""eclipse"" to generate the eclipse project. To run tests, you have to provide the databases instances for the contexts defined on the net.fwbrasil.activate.ActivateTest.contexts method of the activate-tests project. License The code is licensed under LGPL. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/fwbrasil/activate"	"Pluggable object persistence in Scala."	"true"
"Database"	"Casbah"	"http://mongodb.github.io/casbah/"	"() - Officially supported Scala driver for MongoDB"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Casbah Learn Downloads Community Docs Blog Casbah A Scala toolkit for MongoDB Latest documentation Introduction Casbah is a Scala toolkit for MongoDB. Quick Start The recommended way to get started is with a dependency management system. Select the version and dependency management system below and the snippet can be copied and pasted into your build. Alternatively, head over to our documentation to learn more about getting started with Scala and MongoDB. Casbah 3.1.1 3.0.0 2.8.2 Download  <dependencies>     <dependency>         <groupId>org.mongodb</groupId>         <artifactId>casbah</artifactId>         <version>3.1.1</version>     </dependency> </dependencies>         libraryDependencies += ""org.mongodb"" %% ""casbah"" % ""3.1.1""    <dependencies>     <dependency>         <groupId>org.mongodb</groupId>         <artifactId>casbah</artifactId>         <version>3.0.0</version>     </dependency> </dependencies>         libraryDependencies += ""org.mongodb"" %% ""casbah"" % ""3.0.0""    <dependencies>     <dependency>         <groupId>org.mongodb</groupId>         <artifactId>casbah</artifactId>         <version>2.8.2</version>     </dependency> </dependencies>         libraryDependencies += ""org.mongodb"" %% ""casbah"" % ""2.8.2""   Releases Release Documentation 3.1.1 Reference | API 3.0.0 Reference | API 2.8.2 Reference | API MongoDB University M101J: MongoDB for Java Developers Learn everything you need to know to get started building a MongoDB-based app. From basic installation, JSON, schema design, querying, insertion of data, indexing and working with the Java driver. Learn More Copyright © 2008-2015 MongoDB, Inc. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc."	"null"	"null"	"() - Officially supported Scala driver for MongoDB"	"true"
"Database"	"repo"	"https://github.com/mongodb/casbah"	"() - Officially supported Scala driver for MongoDB"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"498"	"52"	"119"	"GitHub - mongodb/casbah: Officially supported Scala Driver for MongoDB Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 498 Fork 119 mongodb/casbah Code Pull requests 2 Pulse Graphs Officially supported Scala Driver for MongoDB http://api.mongodb.org/scala/casbah 1,092 commits 14 branches 67 releases Fetching contributors Scala 95.9% Shell 3.6% JavaScript 0.5% Scala Shell JavaScript Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.7.x 2.7 2.8.x SCALA-89 gh-pages light-dsl master release-2.0 release-2.1_scala2.8.0 release-2.1_scala2.9 release-2.1 release-2.3 release-2.3+ wip-abstract_typed-collections Nothing to show r3.1.1 r3.1.0 r3.1.0-rc0 r3.0.0-M2 r3.0.0-M1 r2.4.1 r2.4.0 r2.3.0 r2.3.0-RC1 r2.3.0-M1 r2.1.5.0_2.9.0-1 r2.1.5.0_2.8.x r2.1.5-1_2.9.x r2.1.5-1_2.8.1 r2.1.5-1_2.8.0 r2.1.5-1_2.8.x r2.1.2 r2.1.1 r2.1.0 r2.0.3 r2.0.2 r2.0.1 r2.0 r2.0rc3 r2.0rc2 r2.0rc1 r2.0rc0 r2.0b3 r2.0b3p1 r2.0b2 r2.0b1 r1.0.8.1 r1.0.8 r1.0.7.5 r1.0.7.4 r1.0.7.3 r1.0.7 r1.0.6 r1.0.5 r1.0.2 r1.0.1 r1.0 r1.0rc4 r1.0rc3 r1.0rc1 2.8.2 2.8.1 2.8.0 2.8.0-RC2 2.8.0-RC1 2.8.0-RC0 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.7.0-RC2 2.7.0-RC1 2.7.0-RC0 2.6.5 2.6.4 2.6.3 2.6.2 2.6.1 2.6.0 2.5.0 Nothing to show New pull request Latest commit 92a1772 Jun 6, 2016 rozza Docs: Clarify the impact of CASBAH-169 in the changelog Permalink Failed to load latest commit information. casbah-commons/src Added getAs[Any] for null values regression test Feb 9, 2016 casbah-core/src Added ReadConcern to the implicits Dec 7, 2015 casbah-gridfs/src Scalastyle and lint fixes Sep 29, 2015 casbah-query/src for consistency: change behaviour regarding present-but-null Feb 3, 2016 docs Docs: Clarify the impact of CASBAH-169 in the changelog Jun 6, 2016 examples/src/test/scala Updated the build Sep 29, 2015 project Update build to 3.1.2-SNAPSHOT Mar 1, 2016 .gitignore Updated the Documentation Site Sep 28, 2015 .travis.yml Updated travis.yml Mar 1, 2016 AUTHORS Added $currentDate op to query DSL Jun 25, 2015 CONTRIBUTING.rst Updated jira links May 20, 2014 LICENSE - Initial import Feb 12, 2010 README.rst Update README.rst Apr 6, 2016 build.sbt Updates to the build added assembly for a single jar May 23, 2013 sbt Updated Scala versions supported Sep 28, 2015 README.rst Casbah Info: Scala toolkit for MongoDB (com.mongodb.casbah). See the mongo site for more information. See github for the latest source. Author: Brendan W. McAdams Maintainer: Ross Lawley <ross@mongodb.com> About Casbah is an interface for MongoDB designed to provide more flexible access from both Java and Scala. The core focus is on providing a Scala oriented wrapper interface around the Java mongo driver. For the Scala side, contains series of wrappers and DSL-like functionality for utilizing MongoDB from within Scala. This currently utilises the very Java-oriented Mongo Java driver, and attempts to provide more scala-like functionality on top of it. This has been tested with MongoDB 1.2.x+ and 2.x of the Mongo java driver. We are constantly adding new functionality, and maintain a detailed Casbah documentation hub. Please address any questions or problems to the Casbah Mailing List on Google Groups. For more information about Casbah see the API Docs or Tutorial. Project Artifacts Casbah is separated out into several artifacts: casbah-commons Provides utilities to improve working with Scala and MongoDB together without dependencies on anything but the MongoDB Java Driver and ScalaJ-Collection. This includes Scala Collections 2.8 compatible wrappers for DBList and DBObject as well as type conversion facilities to simplify the use of Scala types with MongoDB (and register your own custom types) casbah-query The Query DSL which provides an internal Scala DSL for querying MongoDB using native, MongoDB syntax operators. This only depends upon Commons and can be used standalone without the rest of Casbah. casbah-core This is the wrappers for interacting directly with MongoDB providing more Scala-like interactions. It depends upon both Commons and Query as well as ScalaTime for use of JodaTime (which we prefer over JDK date but you are welcome to use JDK Dates). casbah-gridfs This provides enhancement wrappers to GridFS including loan pattern support. It is dependent on Core (and by transitive property, Commons & Query as well) but is not included in Core - you must explicitly load if it you want to use GridFS. Support / Feedback For issues with, questions about, or feedback for Casbah, please look into our support channels. Please do not email any of the Casbah developers directly with issues or questions - you're more likely to get an answer on the casbah mailing list or the mongodb-user list on Google Groups. Bugs / Feature Requests Think you’ve found a bug? Want to see a new feature in Casbah? Please open a case in our issue management tool, JIRA: Create an account and login. Navigate to the CASBAH project. Click Create Issue - Please provide as much information as possible about the issue type and how to reproduce it. Bug reports in JIRA for all driver projects (i.e. CASBAH, JAVA, CSHARP) and the Core Server (i.e. SERVER) project are public. Security Vulnerabilities If you’ve identified a security vulnerability in a driver or any other MongoDB project, please report it according to the instructions here. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/mongodb/casbah"	"() - Officially supported Scala driver for MongoDB"	"true"
"Database"	"CouchDB-Scala"	"https://github.com/beloglazov/couchdb-scala"	"Purely functional Scala client for CouchDB"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"52"	"9"	"13"	"GitHub - beloglazov/couchdb-scala: A purely functional Scala client for CouchDB Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 9 Star 52 Fork 13 beloglazov/couchdb-scala Code Issues 7 Pull requests 1 Pulse Graphs A purely functional Scala client for CouchDB 227 commits 6 branches 7 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags features/maven-and-gitter-badges fix/code-style fix/test-app-does-not-output-anything fix/update-readme-for-0.7.0 fix/upickle-viewwithkeys master Nothing to show v0.7.2 v0.7.1 v0.7.0 v0.6.0 v0.5.2 v0.5.1 v0.5.0 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. cla code-style examples/src/main/scala/com/ibm/couchdb/examples project src .gitignore .travis.yml LICENSE README.md build.sbt cloudant-supported-features.md couchdb-supported-features.md scalastyle-config.xml README.md CouchDB-Scala This is a purely functional Scala client for CouchDB. The design goals are compositionality, expressiveness, type-safety, and ease of use. It's based on these awesome libraries: Scalaz, Http4s, uPickle, and Monocle. Getting started Add the following dependency to your SBT config: libraryDependencies += ""com.ibm"" %% ""couchdb-scala"" % ""0.7.2"" Tutorial This Scala client tries to stay as close to the native CouchDB API as possible, while adding type-safety and automatic serialization/deserialization of Scala objects to and from JSON using uPickle. The best way to get up to speed with the client is to first obtain a good understanding of how CouchDB works and its native API. Some good resources to learn CouchDB are: CouchDB: The Definitive Guide CouchDB Documentation To get started, add the following import to your Scala file (or just start the SBT console with sbt console, which automatically adds the required imports): import com.ibm.couchdb._ Then, you need to create a client instance by passing in the IP address or host name of the CouchDB server, port number, and optionally the scheme (which defaults to http): val couch = CouchDb(""127.0.0.1"", 5984) Through this object, you get access to the following CouchDB API sections: Server: server-level operations Databases: operations on databases Design: operations for creating and managing design documents Documents: creating, modifying, and querying documents and attachments Query: querying views, shows, and lists Server API The server API section provides only 3 operations: getting the server info, which is equivalent to making a GET request to the / resource of the CouchDB server; generating a UUID; and generating a sequence of UUIDs. For example, to make a server info request using the client instance created above: couch.server.info.run The couch.server property refers to an instance of the Server class, which represents the server API section. Then, calling the info method generates a scalaz.concurrent.Task, which describes an action of making a GET request to the server. At this point, an actual request is not yet made. Instead, Task encapsulates a description of a computation, which can be executed later. This allows us to control side-effects and keep the functions pure. Tim Perrett has written a very nice blog post with more background and documentation on Scalaz's Task. The return type of couch.server.info is Task[Res.ServerInfo], which means that when this task is executed, it may return a ServerInfo object or fail. To execute a Task, we need to call the run method, which triggers the actual GET request to server, whose response is then automatically parsed and mapped onto the ServerInfo case class that contains a few fields describing the server instance like the CouchDB version, etc. Ideally, instead of executing a Task and causing side-effects in the middle of a program, we should delay the execution as much as possible to keep the core application logic pure. Rather then executing Tasks to obtain the query result, we can perform action on the query results and compose Tasks in a functional way using higher-order functions like map and flatMap, or for-comprehensions. We will see more examples of this later. In further code snippets, I will omit calls to the run method assuming that the point where effectful computations are executed is externalized. The other operations of the server API can be performed in a similar way. To generate a UUID, you just need to call couch.server.mkUuid, which returns Task[String]. To generate n UUIDs, call couch.server.mkUuids(n), which returns Task[Seq[String]] representing a task of generating a sequence of n UUIDs. For more usage examples, please refer to ServerSpec. Databases API The databases API implements more useful functionality like creating, deleting, and getting information about databases. To create a database: couch.dbs.create(""awesome-database"") The couch.dbs property refers to an instance of the Databases class, which represents the databases API section. A call to the create method returns a Task[Res.Ok], which represents a request returning an instance of the Res.Ok case class if it succeeds, or a failure object if it fails. Failure handling is done using methods on Task, part of which are covered in Tim Perrett's blog post. In two words the actual result of a Task execution is Throwable \/ A, which is either an exception or the desired type A. In the case or dbs.create, the desired result is of type Res.Ok, which is a case class representing a response from the server in case of a succeeded request. Other methods provided by the databases API are dbs.delete(""awesome-database"") to delete a database, dbs.get(""awesome-database"") to get information about a database returned as an instance of DbInfo case class that includes such fields as data size, number of documents in the database, etc. For some examples of using the databases API, please refer to DatabasesSpec. Design API While the API sections described earlier operate at the level above databases, the Design, Documents, and Query APIs are applied within the context of a single database. Therefore, to obtain instances of these interfaces, the context needs to be specialized by specifying the name of a database of interest: val db = couch.db(""awesome-database"", TypeMapping.empty) This method call returns an instance of the CouchDbApi case class representing the context of a single database, through which we can get access to the Design, Documents, and Query APIs. The db method takes 2 arguments: the database name and an instance of TypeMapping. We will discuss TypeMapping later, for now we can just pass an empty mapping using TypeMapping.empty. Through CouchDbApi we can obtain an instance of the Design class representing the Design API section for our database: db.design The Design API allows us to create, retrieve, update, delete, and manage attachments to design documents stored in the current database (you can get the name of the database from an instance of CouchDbApi using db.name). Let's take a look at an example of a design document with a single view. First, assume we have a collection of people each corresponding to an object of a case class Person with a name and age fields: case class Person(name: String, age: Int) Let's define a view with just a map function that emits person names as keys and ages as values. To do that, we are going to use a CouchView case class: val ageView = CouchView(map =     """"""     |function(doc) {     |   emit(doc.doc.name, doc.doc.age);     |}     """""".stripMargin) Basically, we define our map function in plain JavaScript and assign it to the map field of a CouchView object. This function maps each document to a pair of the person's name as the key and age as the value. Notice, that we need to use doc.doc to get to the fields of the person object for reasons that will become clear later. To define a view that contains a reduce operation, specify the relevant Javascript function to the reduce attribute of the CouchView case class constructor like so: val totalAgeView = CouchView(map =     """"""     |function(doc) {     |   emit(doc._id, doc.doc.age);     |}     """""".stripMargin,     reduce =     """"""     |function(key, values, rereduce) {     |   return sum(values);     |}     """""".stripMargin) We can now create an instance of our design document using the defined ageView and totalAgeView: val designDoc = CouchDesign(     name  = ""test-design"",     views = Map(""age-view"" -> ageView, ""total-age-view"" -> totalAgeView)) CouchDesign supports other fields like shows and lists, but for this simple example we only specify the design name and views as a Map from view names to CouchView objects. Proper management of complex design documents is a separate topic (e.g., JavaScript functions can be stored in separate .js files and loaded dynamically). We can finally proceed to submitting the defined design document to our database: db.design.create(designDoc) This method call returns an object of type Task[Res.DocOk]. The DocOk case class represents a response from the server to a succeeded request involving creating, modifying, and deleting documents. Compared with Res.Ok, it includes 2 extra fields: id (the ID of the created/updated/deleted document) and rev (the revision of the created/updated/deleted document). In the case of design documents, the ID is composed of the design name prefixed with _design/. In other words, designDoc will get the _design/test-design ID. Each revision is a unique 32-character UUID string. We can now retrieve the design document from the database by name or by ID: db.design.get(""test-design"") db.design.getById(""_design/test-design"") Once the returned Tasks are executed, each of these calls returns an instance of CouchDesign corresponding to our design document with some extra fields, e.g., _id, _rev, _attachments, etc. To update a design document, we must first retrieve it from the database to know the current revision and avoid conflicts, make changes to the content, and submit the updated version. Let's say we want to add another view, which emits ages as keys and names as values assigned to a nameView variable, then our updated view Map is: val updatedViews = Map(     ""age-view""  -> ageView,     ""name-view"" -> nameView) We can now submit the changes to the database as follows: for {     initial <- db.design.get(""test-design"")     docOk   <- db.design.update(initial.copy(views = updatedViews)) } yield docOk Here, we use a for-comprehension to chain 2 monadic actions. If both actions succeed, we get a Res.DocOk object as a result containing the new revision of the design document stored in the _rev field. The Design API supports a few other operations, to see their usage examples please refer to DesignSpec. Documents API The Documents API implements operations for creating, querying, modifying, and deleting documents and their attachments. At this stage, it's time to discuss how Scala objects are represented in CouchDB and what TypeMapping is used for. One of the design goals of CouchDB-Scala is to make it as easy as possible to store and retrieve documents by automating the process of serialization and deserialization to and from JSON. This functionality is based on uPickle, which uses macros to automatically generate readers and writers for case classes. However, it also allows implementing custom readers and writers for your domain classes if they are not case classes. For example, these can be Thrift / Scrooge generated entities or your custom classes. CouchDB automatically adds several fields to every document containing metadata about the document, such as _id, _rev, _attachments, _conflicts, etc. To take advantage of uPickle's support for case classes, a decision was made to have a case class called CouchDoc[D] that has all the metadata fields generated by CouchDB and also includes 2 special fields: doc for storing an instance of your domain class D, and kind for storing a string representation of the document type that can be used for filtering in views, shows, and lists (we use kind instead of type here, as type is a reserved keyword in Scala). In other words, if your domain model is represented by a set of case classes, the serialization and deserialization will be handled completely transparently for you. TypeMapping is used for defining a mapping from you domain model classes to a string representation of the corresponding document type. Continuing the previous example with the Person case class, we can define a TypeMapping, for example, as follows: val typeMapping = TypeMapping(classOf[Person] -> ""Person"") Here, we are specifying a mapping from the class name Person to a document kind as a string. The TypeMapping factory maps classes to their canonical names to preserve uniqueness. Whenever a document is submitted to the database, the kind field is automatically populated based on the specified mapping. If the type mapping is not specified (as we did above by using TypeMapping.empty), the kind field is ignored. We can now provide the newly defined TypeMapping to create a fully specified database context: val db = couch.db(""awesome-database"", typeMapping) Similarly to the other API sections, we can use the database context to get an instance of the Documents class representing the Documents API section: db.docs Let's define some data: val alice = Person(""Alice"", 25) val bob   = Person(""Bob"", 30) val carl  = Person(""Carl"", 20) We can now store these objects in the database as follows: db.docs.create(alice) This method assigns a UUID generated with server.mkUuid that we've seen above to the document being stored. Another option is to specify our own document ID if it's known to be unique: db.docs.create(bob, ""bob"") As another alternative, we can create multiple documents with auto-generated UUIDs at once using a batch request: db.docs.createMany(Seq(alice, bob, carl)) We can retrieve a document from the database by ID: db.docs.get[Person](""bob"") Here, we have to be explicit about the expected object type to allow uPickle to do its magic, that's why we specify the type parameter to the get method. This method returns Task[CouchDoc[Person]], which basically means that we are getting back a task that after executing successfully will give us an instance of CouchDoc[Person]. This object will contain an instance of Person in the doc field equivalent to the original Person(""Bob"", 30). You can also retrieve a set of documents by IDs using: db.docs.getMany.includeDocs[Person].withIds(Seq(""id1"", ""id1"")).build.query A call to getMany returns an instance of GetManyDocumentsQueryBuilder, which is a class allowing you to build a query in a type-safe way. Under the hood, it makes a request to the /{db}/_all_docs endpoint. As you can see from the linked documentation on this endpoint, it has many optional parameters. The GetManyDocumentsQueryBuilder class provides a fluent interface for constructing queries to this endpoint. For example, to limit the number of documents to the maximum of 10 and return them in the descending order: db.docs.getMany.limit(10).descending.includeDocs[Person].withIds(Seq(""id1"", ""id2"")).build.query This creates an instance of Task[CouchDocs[String, CouchDocRev, Person]], which looks complicated but just represents a task that returns basically a sequence of documents. The queryIncludeDocs method serves as a way to complete the query construction process, which also sets the include_docs option to include the full content of the documents mapped to Person objects on arrival. It's also possible to execute a query without including the document content using db.docs.getMany.build.query, which is equivalent to keeping the include_docs set to its default false value. This query will only return metadata on the matching documents. In this case, we don't need to specify the type parameter as no mapping is required since the document content is not retrieved. To retrieve all documents in the database of a given type without specifying ids, you could use one of the following approaches: val allPeople1 = db.docs.getMany.byTypeUsingTemporaryView[Person].build.query The first approach, byTypeUsingTemporaryView[T], uses a temporary view under the hood for type based filtering. While convenient for development purposes, it is inefficient and should not be used in production. For efficiency you should instead use byType[K, V](view_name), or the simpler byType[V](view_name), which require that you first create a type filtering permanent view, and then pass its name as argument to one of these methods. Because a permanent view is used, these approaches are more efficient and are thus the recommended approach for type based document filtering. Note in byType[K, V](view_name) the parameters K and V represent the key and value types of the permanent view. The document's kind attribute must be the first key of such a view, as in the type filter view function example shown below. function(doc) {     emit([doc.kind, doc._id], doc._id); } The above function could then be used as follows: val allPeople2 = db.docs.getMany.byType[(String, String), String](your_view_name).build.query In the simpler byType[V](view_name), K is implicitly assumed to be of type Tuple of two strings (String, String). Note the document's kind attribute must be the first key of such a view, as in the type filter view function example defined above. The above function could then be used as follows: val allPeople3 = db.docs.getMany.byType[String](your_view_name).build.query There is a similar query builder for retrieving single documents GetDocumentQueryBuilder that makes GET requests to the /{db}/{docid} endpoint. This query builder can accessed through db.docs.get. There are other operations provided by the Documents API, such as updating documents, deleting documents, adding attachments, retrieving attachments, etc. For more usage examples, please refer to DocumentsSpec. Query API The Query API provides an interface for querying views, shows, and lists. Let's say we want to query our age-view defined earlier. To do that, we first obtain an instance of ViewQueryBuilder as follows: val ageView = db.query.view[String, Int](""test-design"", ""age-view"").get val totalAgeView = db.query.view[String, Int](""test-design"", ""total-age-view"").get We need to specify 2 type parameters to the view method representing the types of the key and value emitted by the view. In the case of age-view and total-age-view, it's String for the key (person name) and Int for the value (person age). We can now use the ageView query builder to retrieve all the documents from the view: ageView.build.query This method call returns an instance of Task[CouchKeyVals[String, Int]]. Since we haven't specified the include_docs option, this query only retrieves a sequence of document IDs, keys, and values emitted by the view's map function. This method makes a call to the /{db}/_design/{ddoc}/_view/{view} endpoint, and the builder supports all the relevant options. Similarly, to query the total age of Persons in the document using the totalAgeView builder we can do: totalAgeView.reduce[Int].build.query The type parameter T specified to queryWithReduce[T], in this case Int, is the expected return type of the view's reduce function. We can also make more complex queries. Let's say we want to get 10 people starting from the name Bob and include the document content: ageView.startKey(""Bob"").limit(10).includeDocs[Person].build.query This returns an instance of Task[CouchDocs[String, Int, Person]], which once executed results in a sequence of objects encapsulating the metadata about the documents (id, key, value, offset, total_rows) and the corresponding Person objects. Please follow the definitions of case classes in CouchModel to fully understand the structure of the returned objects. It's also possible to only get the documents from a view that match the specified keys. For example, we can use that to get only documents of Alice and Carl: ageView.build.query(Seq(""Alice"", ""Carl"")) This return an instance of Task[CouchKeyVals[String, Int]]. For other usage examples of the view Query API, please refer to QueryViewSpec. The APIs for querying shows and lists are structured similarly to view querying and follow the official CouchDB specification. Please refer to QueryShowSpec and QueryListSpec for more details and examples. Authentication At the moment, the client supports only the basic authentication method. To use it, just pass your username and password to the CouchDb factory: val couch = CouchDb(""127.0.0.1"", 6984, https = true, ""username"", ""password"") Please note that enabling HTTPS is recommended to avoid sending your credentials in plain text. The default CouchDB HTTPS port is 6984. Complete example Here is a basic example of an application that stores a set of case class instances in a database, retrieves them back, and prints out afterwards: object Basic extends App {    // Define a simple case class to represent our data model   case class Person(name: String, age: Int)    // Define a type mapping used to transform class names into the doc kind   val typeMapping = TypeMapping(classOf[Person] -> ""Person"")    // Define some sample data   val alice = Person(""Alice"", 25)   val bob   = Person(""Bob"", 30)   val carl  = Person(""Carl"", 20)    // Create a CouchDB client instance   val couch  = CouchDb(""127.0.0.1"", 5984)   // Define a database name   val dbName = ""couchdb-scala-basic-example""   // Get an instance of the DB API by name and type mapping   val db     = couch.db(dbName, typeMapping)    val actions = for {   // Delete the database or ignore the error if it doesn't exist     _ <- couch.dbs.delete(dbName).ignoreError     // Create a new database     _ <- couch.dbs.create(dbName)     // Insert documents into the database     _ <- db.docs.createMany(Seq(alice, bob, carl))     // Retrieve all documents from the database and unserialize to Person     docs <- db.docs.getMany.includeDocs[Person].build.query   } yield docs.getDocsData    // Execute the actions and process the result   actions.attemptRun match {     // In case of an error (left side of Either), print it     case -\/(e) => println(e)     // In case of a success (right side of Either), print each object     case \/-(a) => a.map(println(_))   }  } You can run this example from the project directory using sbt: sbt ""run-main com.ibm.couchdb.examples.Basic"" Mailing list Please feel free to join our mailing list, we welcome all questions and suggestions: https://groups.google.com/forum/#!forum/couchdb-scala Contributing We welcome contributions, but request you follow these guidelines. Please raise any bug reports on the project's issue tracker. In order for us to accept pull-requests, the contributor must first complete a Contributor License Agreement (CLA). This clarifies the intellectual property license granted with any contribution. It is for your protection as a Contributor as well as the protection of IBM and its customers; it does not change your rights to use your own Contributions for any other purpose. You can download the CLAs here: individual corporate If you are an IBMer, please contact us directly as the contribution process is slightly different. Contributors Anton Beloglazov (@beloglazov) Ermyas Abebe (@ermyas) Copyright and license © Copyright 2015 IBM Corporation, Google Inc. Distributed under the Apache 2.0 license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/beloglazov/couchdb-scala"	"Purely functional Scala client for CouchDB"	"true"
"Database"	"doobie ★ 426 ⧗ 0"	"https://github.com/tpolecat/doobie"	"Pure functional JDBC layer for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"514"	"46"	"60"	"GitHub - tpolecat/doobie: principled database access for scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 46 Star 514 Fork 60 tpolecat/doobie Code Issues 54 Pull requests 3 Pulse Graphs principled database access for scala 745 commits 9 branches 13 releases Fetching contributors Scala 97.5% Shell 2.5% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: series/0.3.x Switch branches/tags Branches Tags doc log master preprocesor series/0.2.x series/0.3.x xa-config xa-config2 xa-config3 Nothing to show v0.3.0 v0.3.0-M1 v0.2.4 v0.2.4-M1 v0.2.3 v0.2.3-RC4 v0.2.3-RC3 v0.2.3-RC2 v0.2.3-RC1 v0.2.2 v0.2.1 v0.2.0 v0.1 Nothing to show New pull request Latest commit 592ca3a Jul 8, 2016 tpolecat committed on GitHub Merge pull request #300 from tpolecat/modules … Put all sub-projects in the same place. Permalink Failed to load latest commit information. modules reorg Jul 8, 2016 project scalastyle May 2, 2016 .gitignore copy non/imp setup Jul 18, 2015 .travis.yml scalastyle May 3, 2016 CHANGELOG.md doc Jul 7, 2016 LICENSE Update LICENSE Jan 25, 2015 PUBLISHING.md publishing.md Jul 7, 2016 README.md doc Jul 8, 2016 build.sbt reorg Jul 8, 2016 sbt update sbt launcher Dec 6, 2015 scalastyle-config.xml scalastyle May 3, 2016 version.sbt Setting version to 0.3.1-SNAPSHOT Jul 7, 2016 world.sql text ~> varchar Jan 16, 2015 README.md doobie doobie is a pure functional JDBC layer for Scala. It is not an ORM, nor is it a relational algebra; it just provides a principled way to construct programs (and higher-level libraries) that use JDBC. doobie introduces very few new abstractions; if you are familiar with basic scalaz typeclasses like Functor and Monad you should have no trouble here. For common use cases doobie provides a minimal but expressive high-level API: import doobie.imports._, scalaz.effect.IO  val xa = DriverManagerTransactor[IO](   ""org.postgresql.Driver"", ""jdbc:postgresql:world"", ""postgres"", """" )  case class Country(code: String, name: String, population: Long)  def find(n: String): ConnectionIO[Option[Country]] =    sql""select code, name, population from country where name = $n"".query[Country].option  // And then  scala> find(""France"").transact(xa).unsafePerformIO res0: Option[Country] = Some(Country(FRA,France,59225700)) doobie is a Typelevel project. This means we embrace pure, typeful, functional programming, and provide a safe and friendly environment for teaching, learning, and contributing as described in the Typelevel Code of Conduct. Quick Start Supported releases and dependencies are shown below. doobie status jdk scala scalaz scalaz-stream shapeless 0.3.0 stable 1.8+ 2.10, 2.11, 2.12 7.2 0.8 2.3 0.2.4 stable 1.7+ 2.10, 2.11 7.1 0.8 2.2 0.2.3 eol 1.6+ 2.10, 2.11 7.1 0.7 2.2 Note that doobie is pre-1.0 software and is still undergoing active development. New versions are not binary compatible with prior versions, although in most cases user code will be source compatible. Nontrivial breaking changes will be introduced through a deprecation cycle of at least one minor (0.x) release. To use doobie you need to add the following to your build.sbt. libraryDependencies += ""org.tpolecat"" %% ""doobie-core"" % ""0.3.0"" // or any supported release above If you are using Scala 2.10 you must also add the paradise compiler plugin. addCompilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.0.1"" cross CrossVersion.full) It is likely that you will want one or more add-on libraries. doobie provides the following, which have the same version as doobie-core and are released together. doobie-contrib-h2 for H2-specific type mappings. doobie-contrib-hikari for HikariCP connection pooling. doobie-contrib-postgresql for PostgreSQL-specific type mappings. doobie-contrib-specs2 for specs2 support for typechecking queries. See the book of doobie for more information on these add-ons. Documentation and Support See the changelog for an overview of changes in this and previous versions. Behold the book of doobie ← start here The scaladoc will be handy once you get your feet wet. There is also the source. If you're here you know where to look. Check the examples. If you have comments or run into trouble, please file an issue. Find tpolecat on the FreeNode #scala channel, or join the Gitter Channel. Presentations, Blog Posts, etc. Listed newest first. If you have given a presentation or have written a blog post that includes doobie, let me know and I'll add it to this list. The Functional Web Stack by Gary Coady - Dublin Scala Users Group, April 2016 End to End and On The Level by Dave Gurnell - Typelevel Summit, Philadelphia, March 2016 Programs as Values: JDBC Programming with doobie by Rob Norris - Scala by the Bay, 2015 - slides Typechecking SQL in Slick and doobie by Richard Dallaway DB to JSON with a Microservice by Da Terry - code Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/tpolecat/doobie"	"Pure functional JDBC layer for Scala."	"true"
"Database"	"Elastic4s ★ 587 ⧗ 0"	"https://github.com/sksamuel/elastic4s"	"A scala DSL / reactive client for Elasticsearch"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"649"	"57"	"222"	"GitHub - sksamuel/elastic4s: Non blocking, type safe DSL and Scala client for Elasticsearch Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 57 Star 649 Fork 222 sksamuel/elastic4s Code Issues 36 Pull requests 2 Pulse Graphs Non blocking, type safe DSL and Scala client for Elasticsearch 1,778 commits 14 branches 66 releases 94 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.0.betadev master release/0.90.x release/1.0.x release/1.1.x release/1.2.x release/1.3.x release/1.4.x release/1.5.x release/1.6.x release/1.7.x release/2.0.x release/2.1.x scoverage Nothing to show v2.3.0 v2.2.1 v2.2.0 v1.1.1.0 v1.1.0.0 v1.0.2.0 v.1.0.1.2 2.1.2 2.1.1 2.0.0 1.15.12 1.7.5 1.7.4 1.7.3 1.7.1 1.7.0 1.6.6 1.6.5 1.6.4 1.6.3 1.6.2 1.6.1 1.6.0 1.5.17 1.5.16 1.5.15 1.5.14 1.5.13 1.5.11 1.5.10 1.5.9 1.5.8 1.5.7 1.5.6 1.5.5 1.5.4 1.5.3 1.5.2 1.5.1 1.5.0 1.4.14 1.4.13 1.4.12 1.4.11 1.4.10 1.4.9 1.4.8 1.4.7.0 1.4.7 1.4.5 1.4.4 1.4.3 1.4.2 1.4.0 1.4.0.Beta1 1.3.4 1.2.3.0 1.2.1.3 1.2.1.0 1.2.0.0 1.1.2.0 1.1.1.2 1.0.3.0 1.0.1 0.98.13.1 0.90.13.2 Nothing to show New pull request Latest commit 7ae09bb Jul 4, 2016 sksamuel committed on GitHub Merge pull request #589 from mariussoutier/patch-1 … Update ElasticSearch to 2.3.3 Permalink Failed to load latest commit information. elastic4s-circe/src Add test for Indexable May 6, 2016 elastic4s-core-tests/src/test Merge pull request #575 from Honeyfy/inner_hits-and-token_count-exten… Jun 20, 2016 elastic4s-core/src Return a failed future from the client when the java client raise an … Jun 28, 2016 elastic4s-examples/src/main/scala/com/sksamuel/elastic4s/examples fix: spell anaylzers -> analyzers Nov 27, 2015 elastic4s-jackson/src Added json4s module and implicit conversions to Hitas Jul 25, 2015 elastic4s-json4s/src/main/scala/com/sksamuel/elastic4s/json4s Added json4s module and implicit conversions to Hitas Jul 25, 2015 elastic4s-streams/src Replace implicit ActorSystem with ActorRefFactory (in ReactiveElastic) May 6, 2016 elastic4s-testkit/src Fixed race-condition when ensuring index existed May 17, 2016 guide Add: setSource and setExtraSource on SearchRequestBuilder May 17, 2016 project Update ElasticSearch to 2.3.3 Jul 4, 2016 .coveralls.yml Updated readme for sources. Jun 30, 2013 .editorconfig Add editorconfig configuration Mar 19, 2014 .gitignore Release 1.6.5 Jul 10, 2015 .sbtopts Added sbt opts to boost mem in tests Feb 3, 2016 .travis.yml Trying travis on non-container infra Feb 8, 2016 LICENSE renamed license Aug 9, 2013 README.md Add Canal+ as user. Jun 20, 2016 build.sbt Build HitAs and Indexable derivation with circe May 6, 2016 version.sbt Setting version to 2.3.1-SNAPSHOT May 13, 2016 README.md elastic4s - Elasticsearch Scala Client Elastic4s is mostly a wrapper around the standard Elasticsearch Java client with the intention of creating a concise, idiomatic, reactive, type safe DSL for applications in Scala that use Elasticsearch. The Java client, which can of course be used directly in Scala, is more verbose due to Java's nature. Scala lets us do better. Elastic4s's DSL allows you to to construct your requests programatically, with syntatic and semantic errors manifested at compile time, and uses standard Scala futures to enable you to easily integrate into your existing asynchronous workflow. The aim of the DSL is that requests are written in an SQL-like way, while staying true to the Java API or Rest API. Elastic4s supports Scala collections so you don't have to do tedious conversions from your Scala domain classes into Java collections. It also allows you to index documents directly without having to extract and set fields manually - eg from a case class, a JSON document, or a Map (or a custom source). Due to its type safe nature, it is easy to see what operations are available for any request type, because your IDE can use type information to show what methods are available. Key points Type safe concise DSL Integrates with standard Scala futures Uses Scala collections library Leverages the built-in Java client Provides reactive-streams implementation Release The latest release is 2.3.0 which is compatible with Elasticsearch 2.3.x. There are releases for both Scala 2.10 and Scala 2.11. For releases that are compatible with earlier versions of Elasticsearch, search maven central. For more information read Using Elastic4s in your project. Elastic4s Release Target Elasticsearch version 2.3.0 2.3.X 2.2.1 2.2.X 2.1.2 2.1.X 2.0.1 2.0.X 1.7.5 1.7.X 1.6.6 1.6.X 1.5.17 1.5.X 1.4.14 1.4.x 1.3.3 1.3.x 1.2.3.0 1.2.x 1.1.2.0 1.1.x 1.0.3.0 1.0.x 0.90.13.2 0.90.x Changelog 2.1.1 #484 Fixed bug in es-streams throwing ClassCastException #483 Added overloaded doc method to update to accept indexables 2.1.0 Optimize was renamed to ForceMerge. The existing optimize method are deprecated and forceMerge(indexes*) has been added in its place. #395 Added pipeline aggregation definitions #458 Added parameters to clear cache operation #475 Fixed breaking change in terms query rewrite was removed from Elasticsearch's matchXXX queries so has been removed in the dsl Added GeoCentroid aggregation Added terminateAfter to search definition SearchType.SCAN is now deprecated in Elasticsearch count is deprecated in Elasticsearch and should be replaced with a search with size 0 2.0.1 #473 Added missing ""filter"" clause from bool query #475 Fixed breaking change in terms query #474 Added ""item"" likes to more like this query 2.0.0 Major upgrade to Elasticsearch 2.0.0 including breaking changes. Please raise a PR if I've missed any breaking changes. In elasticsearch 2.0.0 one of the major changes has been filters have become queries. So in elastic4s this means all methods xxxFilter are now xxxQuery, eg hasChildrenFilter is now hasChildrenQuery. Some options that existed only on filters like cache and cache key are now removed. Fuzzy like this query has been removed (this was removed in elasticsearch itself) Script dsl has changed. To create a script to pass into a method, you use script(script) or script(name, script) with further parameters set using the builder pattern. DynamicTemplate dsl template name <name> has been removed. Now you supply the full field definition in the dsl method, as such template(field(""price_*"", DoubleType)) Index_analyzer has been removed in elasticsearch. Use analyzer and then override the analyzer for search with search_analyzer MoreLikeThis was removed from elasticsearch in favour of a moreLikeThisQuery on a search request. moreLikeThisQuery has changed camel case (capital L), also now requires the 'like' text as the 2nd method, eg moreLikeThisQuery(""field"").text(""a"") (both can take varargs as well). Search requests now return a richer response type. Previously it returned the java type. The richer type has java style methods so your code will continue to compile, but with deprecation warnings. The sorting DSL has changed in that the previous infix style methods are deprecated. So field sort x becomes fieldSort(x) etc. Or and And filters have been removed completely (not changed into queries like other filters). Use a bool query with must clauses for and's and should clauses for or's. Highlight dsl has changed slightly, highlight field x is now deprecated in favour of highlight(x) Delete mapping has been removed (this is removed in elasticsearch itself) IndexStatus api has been removed (this was removed in elasticsearch itself) Template has been renamed dynamic template (to better match the terminology in elasticsesarch) Field and mapping syntax has changed slightly. The implicit ""fieldname"" as StringType ... has been deprecated in favour of field(""fieldname"", StringType) or stringField(), longField, etc In es-streams the ResponseListener has changed to accept a BulkItemResult instead of a BulkItemResponse Multiget now returns a rich scala wrapper in the form of MultiGetResult. The richer type has java style methods so your code will continue to compile, but with deprecation warnings. GetSegments returns a scala wrapper in the form of GetSegmentsResult IndexStats returns a scala wrapper in the form IndexStatsResult 1.7.0 Works with Elasticsearch 1.7.x Removed sync client (deprecated since 1.3.0) 1.6.6 Fix for race condition in elastic-streams subscriber 1.6.5 Added sourceAsUpsert to allow Indexable as upsert in update queries Added geohash aggregation Added geohash cell filter Added cluster state api Added support for unmapped_type property Added block until index/type exists testkit helpers Added raw query to count dsl Added show typeclass for count Added InFilter and IndicesFilter Added shorter syntax for field types, eg stringField(name) vs field name <name> typed StringType 1.6.4 Added reactive streams implementation for elastic4s. Support explicit field types in the update dsl Added missing options to restore snapshot dsl Added show typeclass for percolate register 1.5.17 Added clear scroll api Added missing options to restore snapshot dsl 1.6.3 Added clear scroll api Added show typeclass for multisearch Allow update dsl to use explicit field values 1.5.16 Added HitAs as a replacement for the Reader typeclass Added indices option to mapping, count and search dsl Added docValuesFormat to timestamp mapping 1.6.2 Added new methods to testkit Introduced simplier syntax for sorts Added HitAs as a replacement for the Reader typeclass Fixed validate query for block queries Added show typeclasses for search, create index, into into, validate, count, and percolate to allow easy debugging of the json of requests. 1.5.15 Added matched_fields and highlight filter to highlighter 1.6.1 Added IterableSearch for iterating over a scroll Enhanced multiget dsl to include routing, version and field options Added rich result for GetAliasResponse Added context queries to suggestions Breaking change: Changed syntax of suggestions to be clearer and allow for type safe results Allow setting analyzer by name on matchphraseprefix Added singleMethodSyntax variant, eg indexInto(index) rather than index into index Added re-write to validate Added filter support to alias (previously only the java client filters were supported) Added cluster settings api Added field stats api Addd docValuesFormat to timestamp mapping Added matched_fields and highlight filter to highlighter Supported stopwords_list in filter Reworked testkit to allow more configuration over the creating of the test clients Dependencies Starting from version 1.5.13 the main artifact has been renamed to elastic4s-core_2.x. Please update your build scripts. There is now an elastic4s-testkit_2.x which brings in a couple of useful methods for waiting until the node/cluster is in some expected state. Very useful when trying unit tests. If you previously used the Jackson support for DocumentSource or Indexables then you need to add a new dependency elastic4s-jackson_2.x. This will allow you to do import ElasticJackson.Implicits._ which puts a Jackson based Indexable into scope, which allows any class to be indexed automagically without the need to manually create maps or json objects. Similarly, if you are using response.hitsAs[T], then the same import brings in a Reader that will convert any type to a case class. Introduction The basic usage of the Scala driver is that you create an instance of ElasticClient and then invoke the various execute methods with the requests you want to perform. The execute methods are asynchronous and will return a standard Scala Future[T] where T is the response type appropriate for your request type. For example a search request will return a response of type SearchResponse which contains the results of the search. Requests, such as inserting a document, searching, creating an index, etc, are created using the DSL syntax that is similar in style to SQL queries. For example to create a search request, you would do: search in ""index/type"" query ""findthistext"" The response objects are, for the most part, the exact same type the Java API returns. This is because there is mostly no reason to wrap these as they are fairly easy to use in Scala. All the DSL keywords are located in the ElasticDsl trait which needs to be imported or extended. An example is worth 1000 characters so here is a quick example of how to create a local node with a client and index a one field document. Then we will search for that document using a simple text query. import com.sksamuel.elastic4s.ElasticClient import com.sksamuel.elastic4s.ElasticDsl._  object Test extends App {    val client = ElasticClient.local    // await is a helper method to make this operation synchronous instead of async   // You would normally avoid doing this in a real program as it will block your thread   client.execute { index into ""bands"" / ""artists"" fields ""name""->""coldplay"" }.await    // we need to wait until the index operation has been flushed by the server.   // this is an important point - when the index future completes, that doesn't mean that the doc   // is necessarily searchable. It simply means the server has processed your request and the doc is   // queued to be flushed to the indexes. Elasticsearch is eventually consistent.   // For this demo, we'll simply wait for 2 seconds (default refresh interval is 1 second).   Thread.sleep(2000)    // now we can search for the document we indexed earlier   val resp = client.execute { search in ""bands"" / ""artists"" query ""coldplay"" }.await   println(resp)  } For more in depth examples keep reading. Syntax Here is a list of the common requests and the syntax used to create them. For more details on each request click through to the readme page. For options that are not yet documented, refer to the Elasticsearch documentation as the DSL closely mirrors the standard Java API / REST API. Operation Syntax Add Alias add alias ""<alias>"" on ""<index>"" Cancel Tasks cancelTasks(nodeIds) Clear index cache clear cache <name> Close index closeIndex(<name>) Count count from <indexes> types <types> <queryblock> Cluster health get cluster health Cluster stats get cluster stats Create Index createIndex(<name>) mappings { mappings block> } [settings] Create Repository createRepository(<repo>) type(<type>) settings <settings> Create Snapshot createSnapshot(<name>) in <repo> ... Create Template createTemplate(<name>) pattern <pattern> mappings {...} [settings] Delete by id delete id <id> from <index/type> [settings] Delete index deleteIndex(<index>) [settings] Delete Snapshot deleteSnapshot(<name>).in(<repo>) ... Delete Template deleteTemplate(<name>) Explain explain id <id> in <index/type> query { <queryblock> } Field stats field stats <indexes> Flush Index flush index <name> Get get id <id> from <index/type> [settings] Get Alias getAlias(<name>).on(<index>) Get Mapping getMapping(<index> / <type>) Get Segments getSegments(<indexes>) Get Snapshot getSnapshot <name> from <repo> Get Template getTemplate(<name>) Index index into <index/type> fields { <fieldblock> } [settings] Index exists indexExists(<name>) Index Status indexStatus(<index>) List Tasks listTasks(nodeIds) More like this morelike id <id> in <index/type> { fields <fieldsblock> } [settings] Multiget multiget ( get id 1 from index, get id 2 from index, ... ) Multisearch multi ( search ..., search ..., ...) Open index openIndex(<name>) Force Merge forceMerge(<indexes>*) [settings] Percolate Doc percolateIn(<index>) doc <fieldsblock> Put mapping putMapping(<index> / <type>) as { mappings block } Recover Index recoverIndex(<name>) Refresh index refreshIndex(<name>) Register Query register id <id> into <index> query { <queryblock> } Remove Alias removeAlias(<alias>).on(<index>) Restore Snapshot restore snapshot <name> from <repo> ... Search search in <index/type> query ... postFilter ... sort ... Search scroll searchScroll(<scrollId>) Type Exists typesExists(<types>) in <index> Update update id <id> in <index/type> script <script> [settings] Validate validateIn(<index/type>) query <queryblock> Please also note some java interoperability notes. Client A locally configured node and client can be created simply by invoking local on the ElasticClient object: import com.sksamuel.elastic4s.ElasticClient val client = ElasticClient.local To specify settings for the local node you can pass in a settings object like this: val settings = Settings.settingsBuilder()       .put(""http.enabled"", false)       .put(""path.home"", ""/var/elastic/"") val client = ElasticClient.local(settings.build) To connect to a remote elastic cluster then you need to use the remote() call specifying the hostnames and ports. Please note that this is the port for the TCP interface (normally 9300) and NOT the port you connect with when using HTTP (normally 9200). // single node val client = ElasticClient.remote(""host1"", 9300) For multiple nodes it's better to use the elasticsearch client uri connection string. This is in the format ""elasticsearch://host:port,host:port,..."" (Note, no parameters can be added). For example: val uri = ElasticsearchClientUri(""elasticsearch://foo:1234,boo:9876"") val client = ElasticClient.remote(uri) If you need to pass settings to the client, then you need to invoke remote() with a settings object. For example to specify the cluster name (if you changed the default then you must specify the cluster name). import org.elasticsearch.common.settings.Settings val settings = Settings.settingsBuilder().put(""cluster.name"", ""myClusterName"").build() val client = ElasticClient.remote(settings, ElasticsearchClientUri(""elasticsearch://somehost:9300"")) If you already have a handle to a Node in the Java API then you can create a client from it easily: val node = ... // node from the java API somewhere val client = ElasticClient.fromNode(node) Create Index All documents in Elasticsearch are stored in an index. We do not need to tell Elasticsearch in advance what an index will look like (eg what fields it will contain) as Elasticsearch will adapt the index dynamically as more documents are added, but we must at least create the index first. To create an index called ""places"" that is fully dynamic we can simply use: client.execute { create index ""places"" } We can optionally set the number of shards and / or replicas client.execute { create index ""places"" shards 3 replicas 2 } Sometimes we want to specify the properties of the fields in the index in advance. This allows us to manually set the type of the field (where Elasticsearch might infer something else) or set the analyzer used, or multiple other options To do this we add mappings: import com.sksamuel.elastic4s.mappings.FieldType._ import com.sksamuel.elastic4s.StopAnalyzer  client.execute {     create index ""places"" mappings (         ""cities"" as (             ""id"" typed IntegerType,             ""name"" typed StringType boost 4,             ""content"" typed StringType analyzer StopAnalyzer         )     ) } Then Elasticsearch is configured with those mappings for those fields only. It is still fully dynamic and other fields will be created as needed with default options. Only the fields specified will be ""fixed"". More examples on the create index syntax can be found here. Analyzers Analyzers control how Elasticsearch parses the fields for indexing. For example, you might decide that you want whitespace to be important, so that ""band of brothers"" is indexed as a single ""word"" rather than the default which is to split on whitespace. There are many advanced options available in analayzers. Elasticsearch also allows us to create custom analyzers. For more details read about the DSL support for analyzers. Indexing To index a document we need to specify the index and type and optionally we can set an id. If we don't include an id then elasticsearch will generate one for us. We must also include at least one field. Fields are specified as standard tuples. client.execute {   index into ""places"" / ""cities"" id ""uk"" fields (     ""name"" -> ""London"",     ""country"" -> ""United Kingdom"",     ""continent"" -> ""Europe"",     ""status"" -> ""Awesome""   ) } There are many additional options we can set such as routing, version, parent, timestamp and op type. See official documentation for additional options, all of which exist in the DSL as keywords that reflect their name in the official API. Indexing from Classes Sometimes it is useful to index directly from your domain model, and not have to create maps of fields inline. For this elastic4s provides the Indexable typeclass. Simply provide an implicit instance of Indexable[T] in scope for any class T that you wish to index, and then you can use source t on the index request. For example: // a simple example of a domain model case class Character(name: String, location: String)  // how you turn the type into json is up to you implicit object CharacterIndexable extends Indexable[Character] {   override def json(t: Character): String = s"""""" { ""name"" : ""${t.name}"", ""location"" : ""${t.location}"" } """""" }  // now the index request reads much cleaner val jonsnow = Character(""jon snow"", ""the wall"") client.execute {   index into ""gameofthrones"" / ""characters"" source jonsnow } Some people prefer to write typeclasses manually for the types they need to support. Other people like to just have it done automagically. For those people, elastic4s provides a Jackson based implementation of Indexable[Any] that will convert anything to Json. To use this, you need to add the jackson extension to the build. The next step is to import the implicit into scope with import ElasticJackson.Implicits._ where ever you want to use the source method. With that implicit in scope, you can now pass any type you like to source and Jackson will marshall it to json for you. Another way that existed prior to the Indexable typeclass was the DocumentSource or DocumentMap abstractions. For these, you provide an instance of DocumentSource that returns a Json String, or an instance of DocumentMap that provides a Map[String, Any]. case class Character(name: String, location: String)  case class CharacterSource(c: Character) extends DocumentSource {   def json : String = s"""""" { ""name"" : ""${c.name}"", ""location"" : ""${c.location}"" } """""" }  val jonsnow = Character(""jon snow"", ""the wall"") client.execute {   index into ""music"" / ""bands"" doc CharacterSource(jonsnow) } There isn't much difference, but the typeclass approach (the former) is considered more idomatic scala. More details on the document traits page. Beautiful! Searching Searching is naturally the most involved operation. There are many ways to do searching in elastic search and that is reflected in the higher complexity of the query DSL. To do a simple text search, where the query is parsed from a single string search in ""places"" / ""cities"" query ""London"" That is actually an example of a SimpleStringQueryDefinition. The string is implicitly converted to that type of query. It is the same as specifying the query type directly: search in ""places"" / ""cities"" query simpleStringQuery(""London"") The simple string example is the only time we don't need to specify the query type. We can search for everything by not specifying a query at all. search in ""places"" / ""cities"" We might want to limit the number of results and / or set the offset. search in ""places"" / ""cities"" query ""paris"" start 5 limit 10 We can search against certain fields only: search in ""places"" / ""cities"" query termQuery(""country"", ""France"") Or by a prefix: search in ""places"" / ""cities"" query prefixQuery(""country"", ""France"") Or by a regular expression (slow, but handy sometimes!): search in ""places"" / ""cities"" query regexQuery(""country"", ""France"") There are many other types, such as range for numeric fields, wildcards, distance, geo shapes, matching. Read more about search syntax here. Read about multisearch here. Read about suggestions here. Search Conversion By default Elasticsearch search responses contain an array of SearchHit instances which contain things like the id, index, type, version, etc as well as the document source as a string or map. Elastic4s provides a means to convert these back to meaningful domain types quite easily using the HitAs[T] typeclass. Provide an implementation of this typeclass, as an in scope implicit, for whatever type you wish to marshall search responses into, and then you can call as[T] on the response. A full example: case class Character(name: String, location: String)  implicit object CharacterHitAs extends HitAs[Character] {   override def as(hit: RichSearchHit): Character = {     Character(hit.sourceAsMap(""name"").toString, hit.sourceAsMap(""location"").toString)   } }  val resp = client.execute {   search in ""gameofthrones"" / ""characters"" query ""kings landing"" }.await // don't block in real code  // .as[Character] will look for an implicit HitAs[Character] in scope // and then convert all the hits into Characters for us. val characters :Seq[Character] = resp.as[Character]  This is basically the inverse of the Indexable typeclass. And just like Indexable, there is a general purpose Jackson HitAs[Any] implementation for those who wish to have some sugar. To use this, you need to add the jackson extension to the build. The next step is to import the implicit into scope with import ElasticJackson.Implicits._ where ever you want to use the as[T] methods. As a bonus feature of the Jackson implementation, if your domain object has fields called _timestamp, _id, _type, _index, or _version then those special fields will be automatically populated as well. Highlighting Elasticsearch can annotate results to show which part of the results matched the queries by using highlighting. Just think when you're in google and you see the snippets underneath your results - that's what highlighting does. We can use this very easily, just add a highlighting definition to your search request, where you set the field or fields to be highlighted. Viz: search in ""music"" / ""bios"" query ""kate bush"" highlighting (   highlight field ""body"" fragmentSize 20 ) All very straightforward. There are many options you can use to tweak the results. In the example above I have simply set the snippets to be taken from the field called ""body"" and to have max length 20. You can set the number of fragments to return, seperate queries to generate them and other things. See the elasticsearch page on highlighting for more info. Get Sometimes we don't want to search and want to retrieve a document directly from the index by id. In this example we are retrieving the document with id 'coldplay' from the bands/rock index and type. client.execute {  get id ""coldplay"" from ""bands"" / ""rock"" } We can get multiple documents at once too. Notice the following multiget wrapping block. client.execute {   multiget(     get id ""coldplay"" from ""bands/rock"",     get id ""keane"" from ""bands/rock""   ) } See more get examples and usage of multiget here Deleting In the rare case that we become tired of a band we might want to remove them. Naturally we wouldn't want to remove Chris Martin and boys so we're going to remove U2 instead. We think they're a little past their best (controversial). This operation assumes the id of the document is ""u2"". client.execute {   delete id ""u2"" from ""bands/rock"" } We can take this a step further by deleting by a query rather than id. In this sense the delete is very similar to an SQL delete statement. In this example we're deleting all bands where their type is rap. client.execute {     delete from index ""bands"" types ""rock"" where termQuery(""type"", ""rap"") } See more about delete on the delete page Updates We can update existing documents without having to do a full index, by updating a partial set of fields. client.execute {   update 25 in ""scifi/starwars"" docAsUpsert (     ""character"" -> ""chewie"",     ""race"" -> ""wookie""   ) } Read more about updates and see more examples. More like this If you want to return documents that are ""similar"" to a current document we can do that very easily with the more like this query. client.execute {   morelike id 4 from ""beers/lager"" percentTermsToMatch 0.5 } For all the options see here. Bulk Operations Elasticsearch is fast. Roundtrips are not. Sometimes we want to wrestle every last inch of performance and a useful way to do this is to batch up requests. Elastic has guessed our wishes and created the bulk API. To do this we simply wrap index, delete and update requests using the bulk keyword and pass to the execute method in the client. client.execute {   bulk (     index into ""bands/rock"" fields ""name""->""coldplay"",     index into ""bands/rock"" fields ""name""->""kings of leon"",     index into ""bands/pop"" fields (       ""name""->""elton john"",       ""best_album""->""tumbleweed connection""     )   ) } A single HTTP or TCP request is now needed for 4 operations. In addition Elasticsearch can now optimize the requests, by combinging inserts or using aggressive caching. The example above uses simple documents just for clarity of reading; the usual optional settings can still be used. See more information on the bulk page. Json Output It can be useful to see the json output of requests in case you wish to tinker with the request in a REST client or your browser. It can be much easier to tweak a complicated query when you have the instant feedback of the HTTP interface. Elastic4s makes it easy to get this json where possible. Simply call .show on a request to get back a json string. Eg: val req = search in ""music"" / ""bands"" query ""coldplay"" ... println(req.show) // would output json client.execute { req } // now executes that request Not all requests have a json body. For example get-by-id is modelled purely by http query parameters, there is no json body to output. And some requests don't convert to json in the Java client so aren't yet supported by the show typeclass. Also, for clarity, it should be pointed out that the client doesn't send JSON to the server, it uses a binary protocol. So the provided json format should be treated as a debugging tool only. The requests that support .show are search, multi, create index, index into, validate, percolate, count. Synchronous Operations All operations are normally asynchronous. Sometimes though you might want to block - for example when doing snapshots or when creating the initial index. You can call .await on any operation to block until the result is ready. This is especially useful when testing. val resp = client.execute { index into ""bands/rock"" fields (""name""->""coldplay"", ""debut""->""parachutes"") }.await resp.isInstanceOf[IndexResponse] // true Helpers Helpers provide higher level APIs to work with Elasticsearch. Reindexing data Use the reindex helper to reindex data from source index to target index. client.reindex(   sourceIndex = ""sourceIndex"",   targetIndex = ""targetIndex"",   chunkSize = 500,   scroll = ""5m"") DSL Completeness As it stands the Scala DSL covers all of the common operations - index, create, delete, delete by query, search, validate, percolate, update, explain, get, and bulk operations. There is good support for the various settings for each of these - more so than the Java client provides in the sense that more settings are provided in a type safe manner. However there are settings and operations (mostly admin / cluster related) that the DSL does not yet cover (pull requests welcome!). In these cases it is necessary to drop back to the Java API. This can be done by calling .java on the client object to get the underlying java elastic client, or .admin to get the admin based client, eg, the following request is a Java API request. client.admin.cluster.prepareHealth.setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet This way you can still access everything the normal Java client covers in the cases where the Scala DSL is missing a construct, or where there is no need to provide a DSL. Elastic Reactive Streams Elastic4s has an implementation of the reactive streams api for both publishing and subscribing that is built using Akka. To use this, you need to add a dependency on the elastic4s-streams module. There are two things you can do with the reactive streams implementation. You can create an elastic subscriber, and have that stream data from some publisher into elasticsearch. Or you can create an elastic publisher and have documents streamed out to subscribers. Integrate First you have to add an additional dependeny to your build.sbt libraryDependencies += ""com.sksamuel.elastic4s"" %% ""elastic4s-streams"" % ""1.7.4"" Import the new API with import com.sksamuel.elastic4s.streams.ReactiveElastic._ Publisher An elastic publisher can be created for any arbitrary query you wish, and then using the efficient search scroll API, the entire dataset that matches your query is streamed out to subscribers. And make sure you have an Akka Actor System in implicit scope implicit val system = ActorSystem() Then create a publisher from the client using any query you want. You must specify the scroll parameter, as the publisher uses the scroll API. val publisher = client.publisher(search in ""myindex"" query ""sometext"" scroll ""1m"") Now you can add subscribers to this publisher. They can of course be any type that adheres to the reactive-streams api, so you could stream out to a mongo database, or a filesystem, or whatever custom type you want. publisher.subscribe(someSubscriber) If you just want to stream out an entire index then you can use the overloaded form: val publisher = client.publisher(""index1"", keepAlive = ""1m"") Subscription An elastic subcriber can be created that will stream a request to elasticsearch for each item produced by a publisher. The subscriber can create index, update, or delete requests, so is a good way to synchronize datasets. import ReactiveElastic._ And make sure you have an Akka Actor System in implicit scope. implicit val system = ActorSystem() Then create a subscriber, specifying the following parameters: A type parameter that is the type of object that the publisher will provide How many documents should be included per index batch (10-100 is usually good) How many concurrent batches should be in flight (usually around the number of cores) An optional ResponseListener that will be notified for each item that was successfully acknowledged by the es cluster An optional function that will be called once the subscriber has received all data. Defaults to a no-op An optional function to call if the subscriber encouters an error. Defaults to a no-op. In addition there should be a further implicit in scope of type RequestBuilder[T] that will accept objects of T (the type produced by your publisher) and build an index, update, or delete request suitable for dispatchin to elasticsearch. implicit val builder = new RequestBuilder[SomeType] {   import ElasticDsl._   // the request returned doesn't have to be an index - it can be anything supported by the bulk api   def request(t: T): BulkCompatibleDefinition =  index into ""index"" / ""type"" fields .... } Then the subscriber can be created, and attached to a publisher: val subscriber = client.subscriber[SomeType](batchSize, concurrentBatches, () => println ""all done"") publisher.subscribe(subscriber) Building and Testing This project is built with SBT. So to build sbt compile  And to test sbt test  Integration tests run on a local elastic that is created and torn down as part of the tests inside your standard temp folder. There is no need to configure anything externally. Used By Barclays Bank HSBC Shazaam Graphflow Hotel Urbano Immobilien Scout HMRC Canal+ Raise a PR to add your company here Contributions Contributions to elastic4s are always welcome. Good ways to contribute include: Raising bugs and feature requests Fixing bugs and enhancing the DSL Improving the performance of elastic4s Adding to the documentation License This software is licensed under the Apache 2 license, quoted below.  Copyright 2013-2016 Stephen Samuel  Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sksamuel/elastic4s"	"A scala DSL / reactive client for Elasticsearch"	"true"
"Database"	"MapperDao ★ 8 ⧗ 32"	"https://github.com/kostaskougios/mapperdao"	"An ORM library for oracle, mysql, mssql, and postgresql"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"10"	"1"	"0"	"GitHub - kostaskougios/mapperdao: A Scala ORM library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 10 Fork 0 kostaskougios/mapperdao Code Issues 2 Pull requests 0 Pulse Graphs A Scala ORM library 1,985 commits 5 branches 26 releases Fetching contributors Scala 99.3% Other 0.7% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0.0-rc17 batchrefactoring master queryphase wiki Nothing to show 1.0.0.rc25 1.0.0.rc25-correct 1.0.0.rc16 1.0.0.rc13 1.0.0.rc11 1.0.0.rc1 1.0.0-rc15 1.0.0-rc6 1.0.0-rc5 1.0.0-rc4 1.0.0-rc2 1.0.0-beta 0.9.2 0.9.0 0.8.4@701 0.8.4 0.8.3 0.8.2 0.8.0 0.7.0 0.6.2 0.6.1 0.6.0 0.5.3 0.5.2 0.5.1 Nothing to show New pull request Latest commit 02f211e Apr 25, 2016 kostaskougios Merge pull request #7 from MiroslavCsonka/patch-1 … Scala syntax highlight for code example Permalink Failed to load latest commit information. documentation migration from googlecode to github Apr 5, 2015 src faster compilation for tests May 16, 2015 wiki migration from googlecode to github Apr 5, 2015 .gitignore migration from googlecode to github Apr 5, 2015 README types, oracle Aug 7, 2012 Readme.md Scala syntax highlight for code example Apr 24, 2016 deploy attempt to compile to 2.11 Apr 21, 2014 pom.xml updated scala to 2.11.2 Aug 25, 2014 tests cleanup of compilation warnings Jun 2, 2014 Readme.md Summary MapperDao is an ORM library for the scala language and the following databases: oracle postgresql mysql derby sql server h2 It allows ...(more) News 05/04/2015 : moved the project to github. 25/08/2014 : updated the tutorial 08/06/2014 : 1.0.1 for scala 2.10 & 2.11 is available, a maintenance release. The artifactId now complies with sbt rules reg. scala version. Also minor clean up of the exposed API and code. 21/04/2014 : 1.0.0.2.11 is now released for scala 2.11 . 20/04/2014 : 1.0.0.2.10 is now released for scala 2.10 . 18/01/2014 : 1.0.0.2.10.3-SNAPSHOT with immutable query DSL, better aliasing and immutable builder ...(more) Quick Links mapperdao tutorial (pdf) Documentation Examples Sbt/Maven Configuration Discussions group F.A.Q. Example import java.util.Properties import org.apache.commons.dbcp.BasicDataSourceFactory  // create a datasource using apache dbcp val properties = new Properties properties.load(getClass.getResourceAsStream(""/jdbc.test.properties"")) val dataSource = BasicDataSourceFactory.createDataSource(properties)  // create the mapperdao instance, connect to an oracle database and register our 2 entities import com.googlecode.mapperdao.utils.Setup val (jdbc,mapperDao,queryDao,txManager) = Setup.oracle(dataSource,List(PersonEntity,CompanyEntity))  // domain model classes (immutable) class Person(val name: String, val company: Company) class Company(val name: String)  // mappings (using default table and column naming convention) object PersonEntity extends Entity[Int,SurrogateIntId, Person] {     val id = key(""id"") autogenerated (_.id)     val name = column(""name"") to (_.name)     val company = manytoone(CompanyEntity) to (_.company)      def constructor(implicit m) = new Person(name, company) with Stored {         val id: Int = PersonEntity.id     } }  object CompanyEntity extends Entity[Int,SurrogateIntId, Company] {     val id = key(""id"") autogenerated (_.id)     val name = column(""name"") to (_.name)      def constructor(implicit m) = new Company(name) with Stored {         val id: Int = CompanyEntity.id     } }  val tx = Transaction.get(txManager, Propagation.Nested, Isolation.ReadCommited, -1)   // insert a person import mapperDao._ val person = new Person(""Kostas"", new Company(""Coders limited""))  val inserted = tx { () => insert(PersonEntity, person) } // inserts person, company, in 1 transaction  // print the autogenerated id and the person name println(s""${inserted.id} ${inserted.name}""))  // now update the company for this person val company2 = insert(CompanyEntity, Company(""Scala Inc"")) val modified = new Person(inserted.name, company2) val updated = update(PersonEntity, inserted, modified) // no transaction here, but we could do the operation transactionally  // and select it from the database val selected = select(PersonEntity, updated.id).get  // finally, delete the row mapperDao.delete(PersonEntity, selected)  // run some queries val pe=PersonEntity //alias val people=query(select from pe) // get all // people is a list of Person with IntId  // fetch only page 2 of all people val people=query(QueryConfig.pagination(2, 10),select from pe) // people is a list of Person with IntId  Roadmap sqlite driver optimistic locking sum, avg, min, max and for column mappings and groupby in mappings of statistical entities Please visit the Wiki for setup instructions & usage documentation MapperDao would like to thank YourKit is kindly supporting this open source project with its full-featured Java Profiler. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications. Take a look at YourKit's leading software products: YourKit Java Profiler and YourKit .NET Profiler. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/kostaskougios/mapperdao"	"An ORM library for oracle, mysql, mssql, and postgresql"	"true"
"Database"	"Memcontinuationed ★ 49 ⧗ 22"	"https://github.com/Atry/memcontinuationed"	"Memcached client for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"49"	"6"	"4"	"GitHub - Atry/memcontinuationed: Memcached client for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 49 Fork 4 Atry/memcontinuationed Code Issues 2 Pull requests 1 Pulse Graphs Memcached client for Scala 55 commits 1 branch 2 releases Fetching contributors Scala 99.8% Shell 0.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 0.3.1 0.3.0 Nothing to show New pull request Latest commit 2738a36 Feb 27, 2015 Atry Fix indention Permalink Failed to load latest commit information. project src .gitignore .travis.yml Update .travis.yml Sep 11, 2014 LICENSE NOTICE README.md build.sbt README.md Memcontinuationed Memcontinuationed is an asynchronous memcached client for Scala. Memcontinuationed is the fastest memcached client on JVM, much faster than spymemcached or Whalin's client. Why is Memcontinuationed so fast? Reason 1. Better threading model Memcontinuationed never blocks any threads. On the other hand, spymemcached does not block the IO thread but it does block the user's thread. All Memcontinuationed API are @suspendable, which mean these methods can be invoked by a thread, and return to another thread. Reason 2. Optimization for huge number of IOPS Memcontinuationed can merge multiply get or gets requests into one request. On the other hand, spymemcached sends all requests immediately, never waiting for previous response. The spymemcached way consumes more CPU and more TCP overheads than Memcontinuationed. Even worse, the spymemcached way is not compatible with some memcached server. Note: Because Memcontinuationed queues requests until all the previous response have been received, you may need to create a connection pool of com.dongxiguo.memcontinuationed.Memcontinuationed to maximize the IOPS. A sample to use Memcontinuationed import com.dongxiguo.memcontinuationed.Memcontinuationed import com.dongxiguo.memcontinuationed.StorageAccessor import java.io._ import java.net._ import java.nio.channels.AsynchronousChannelGroup import java.util.concurrent.Executors import scala.util.continuations.reset import scala.util.control.Exception.Catcher  object Sample {    def main(args: Array[String]) {     val threadPool = Executors.newCachedThreadPool()     val channelGroup = AsynchronousChannelGroup.withThreadPool(threadPool)      // The locator determines where the memcached server is.     // You may want to implement ketama hashing here.     def locator(accessor: StorageAccessor[_]) = {       new InetSocketAddress(""localhost"", 1978)     }      val memcontinuationed = new Memcontinuationed(channelGroup, locator)      // The error handler     implicit def catcher:Catcher[Unit] = {       case e: Exception =>         scala.Console.err.print(e)         sys.exit(-1)     }      reset {       memcontinuationed.set(MyKey(""hello""), ""Hello, World!"")       val result = memcontinuationed.require(MyKey(""hello""))       assert(result == ""Hello, World!"")       println(result)       sys.exit()     }   } }  /**  * `MyKey` specifies how to serialize the data of a key/value pair.  */ case class MyKey(override val key: String) extends StorageAccessor[String] {    override def encode(output: OutputStream, data: String, flags: Int) {     output.write(data.getBytes(""UTF-8""))   }    override def decode(input: InputStream, flags: Int): String = {     val result = new Array[Byte](input.available)     input.read(result)     new String(result, ""UTF-8"")   } } There is something you need to know: get, set, and most of other methods in Memcontinuationed are @suspendable. You must invoke them in reset or in another @suspendable function you defined. get, set, and most of other methods in Memcontinuationed accept an implicit parameter Catcher. You must use Catcher to handle exceptions from @suspendable functions, instead of try/catch. MyKey is the key you passed to server, which is a custom StorageAccessor. You should implement your own StorageAccessor for each type of data. If your value's format is Protocol Buffers, you can use com.dongxiguo.memcontinuationed.ProtobufAccessor as your custom key's super class. Build configuration Add these lines to your build.sbt if you use Sbt: libraryDependencies += ""com.dongxiguo"" %% ""memcontinuationed"" % ""0.3.2""  libraryDependencies <++= scalaBinaryVersion { bv =>   bv match {     case ""2.10"" => {       Seq()     }     case _ => {       Seq(""org.scala-lang.plugins"" % s""scala-continuations-library_$bv"" % ""1.0.1"")     }   } }  libraryDependencies <+= scalaVersion { sv =>   if (sv.startsWith(""2.10."")) {     compilerPlugin(""org.scala-lang.plugins"" % ""continuations"" % sv)   } else {     compilerPlugin(""org.scala-lang.plugins"" % s""scala-continuations-plugin_$sv"" % ""1.0.1"")   } }  scalacOptions += ""-P:continuations:enable"" Requirement Memcontinuationed requires Scala 2.10.x or 2.11.x, and JRE 7. Links The API documentation Coding examples Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Atry/memcontinuationed"	"Memcached client for Scala."	"true"
"Database"	"Morpheus ★ 12 ⧗ 2"	"https://github.com/websudos/morpheus"	"Reactive type safe Scala Driver for MySQL/Postgres."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"15"	"2"	"0"	"GitHub - outworkers/morpheus: Reactive type-safe Scala driver for SQL databases Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 15 Fork 0 outworkers/morpheus Code Issues 0 Pull requests 0 Pulse Graphs Reactive type-safe Scala driver for SQL databases http://outworkers.github.io/morpheus 214 commits 5 branches 4 releases Fetching contributors Scala 99.5% Shell 0.5% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop feature/MPH-66 feature/deps_bump gh-pages master Nothing to show v0.2.5 v0.2.4 org=com.websudos,name=morpheus,version=0.3.1 org=com.websudos,name=morpheus,version=0.2.7 Nothing to show New pull request Latest commit e2d89cb Jun 21, 2016 alexflav23 committed on GitHub Merge pull request #14 from outworkers/release/sbt_build … Moving to build.sbt. Upgrading deps. Permalink Failed to load latest commit information. build docs morpheus-dsl/src morpheus-mysql/src morpheus-postgres/src/main/scala/com/websudos/morpheus/postgres morpheus-spark/src/main/scala/com/websudos/morpheus/spark morpheus-testkit/src/main/scala/com/websudos/morpheus/testkit project .gitignore .travis.yml CONTRIBUTING.md LICENSE.txt README.md build.sbt changelog.md Fixing documentation and travis CI support. Adding enum columns and b… Sep 4, 2015 morpheus.graffle scalastyle-config.xml sonar-project.properties Improving coverage and adding missing tests. Aug 17, 2014 README.md morpheus To stay up-to-date with our latest releases and news, follow us on Twitter: @websudos. Named after the Greek God of Dreams, morpheus is a reactive type-safe Scala DSL for MySQL, Postgres, MSSQL, MariaDB, Oracle and Sybase. We choose this name as it is the dream DSL for any Scala/SQL user, finessed to perfection up to the Websudos quality standard you've gotten used to. Also, morpheus stands for morphing. With the single switch of an import, morpheus will perform a full feature swap from MySQL to OracleSQL for example. It doesn't have a query compiler, instead it entirely mimics the functionality the database has. We've taken it up ourselves to produce the highest quality database integration tooling for all Scala users, currently hitting that bar for Cassandra and Neo4J. But why stop there? Using morpheus The current version is: val morpheusVersion = . Morpheus is actively and avidly developed. It is not yet production ready, so trial at your own risk. The stable release is always available on Maven Central and will be indicated by the badge at the top of this readme. The Maven Central badge is pointing at the latest version Intermediary releases are available through our managed Bintray repository available at https://dl.bintray.com/websudos/oss-releases/. The latest version available on our Bintray repository is indicated by the Bintray badge at the top of this readme. Table of contents Design philosophy Integrating Morpheus Supported databases and documentation MySQL MariaDB Postgres Oracle(Morpheus Enterprise) MS SQL(Morpheus Enterprise) Support Copyright Design philosophy You're probably wondering how Morpheus fairs compared to the more established players in the Scala SQL market and why we set out to do something new in the first place. To sum it up, we believe Slick is an excellent tool but we do not believe you should learn about our abstractions to get things done. A DSL should auto-magically encode the same syntax and the logic as the tool it's designed for. Instead of learning about primitives and rules we thought of to abstract away discrepancies between the various SQL implementations, Morpheus features a unique approach, what we call the auto-magical flip. Although at this point in time only MySQL is supported, Morpheus is designed to give you an ""all-you-can-eat"" buffet through a single import. As follows: import com.websudos.morpheus.mysql._. And done, you can now define tables, query and so on. Say you have something like this: Recipes.select.distinctRow.where(_.name eqs ""test""). DISTINCTROW doesn't exist in the Postgres SELECT statement syntax, but it's a standard thing as far as MySQL is concerned. Here's how Morpheus operates: If you change the top level import to: com.websudos.morpheus.postgres._ and you try to compile the same distinctRow query. But there will be none. The method will simply not exist. Morpheus has now auto-magically performed a full feature swap, changed communication protocol and all underlying settings, and all you get now is Postgres features. How? Quite a lot of fun magic under the hood, have a look throughout our decently documented codebase for more information. The beauty of it is that you don't have to care. Slick makes it easy to move from one SQL database to the other with less code changes, but if you're well set on a database you already know and love, it may be counter productive to have to learn about a framework when you could use Morpheus and all you need is IDE auto-completes to get lightning fast development productivity. Oh, and did we mention it's entirely asynchronous and reactive? No JDBC. Integrating Morpheus back to top Morpheus is designed to give you an all-you-can eat buffet through a single import, so all you really have to do is to pick the module corresponding to the database you want to use. At this point in time only MySQL is supported. If you are using MySQL, you would simply use the following: libraryDependencies ++= Seq(   ""com.websudos""  %% ""morpheus-mysql""                % morpheusVersion ) And then you can: import com.websudos.morpheus.mysql._, which will give you the full set of MySQL methods and features without any overlaps or unsupported operations. Morpheus guarantees you can almost never write an invalid SQL query unless you try really really hard. Available modules The full list of available modules is: libraryDependencies ++= Seq(   ""com.websudos""  %% ""morpheus-dsl""                  % morpheusVersion,   ""com.websudos""  %% ""morpheus-mysql""                % morpheusVersion,   ""com.websudos""  %% ""morpheus-postgres""             % morpheusVersion ) Contributors back to top Morpheus was developed by us from scratch in an attempt to evolve the SQL tooling in the Scala ecosystem to the new level and bring in fully reactive database access while preserving the complete SQL syntax you are used to. Flavian Alexandru @alexflav23 Benjamin Edwards @benjumanji Commercial support back to top We, the people behind phantom run a software development house specialised in Scala and NoSQL. If you are after enterprise grade training or support for using phantom, Outworkers is here to help! We offer a comprehensive range of elite Scala development services, including but not limited to: Software development Remote Scala contractors for hire Advanced Scala and Morpheus training We are big fans of open source and we will open source every project we can! To read more about our OSS efforts, click here. morpheus-enterprise Morpheus Enterprise is an upgraded version of Morpheus which includes several extremely powerful features that will not be available in the open source version, including and not limited to: Full online access to a complete set of phantom tutorials, accompanied by our direct support. Advanced support for integrating morpheus with Apache Spark and especially SparkSQL. A very powerful schema management framework called morpheus-migrations which allows you not only to completely automate schema management and do it all in Scala, but also to let phantom automatically handle most use cases for you. morpheus-autotables, an advanced macro based framework which will auto-generate, auto-manage, and auto-migrate all your queries from case classes. Full support for Oracle, Oracle Exadata and SQL Server Copyright back to top Copyright (c) 2012 - 2015 outworkers. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/websudos/morpheus"	"Reactive type safe Scala Driver for MySQL/Postgres."	"true"
"Database"	"Phantom ★ 406 ⧗ 0"	"https://github.com/websudos/phantom"	"Reactive type safe Scala driver for Apache Cassandra."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"440"	"43"	"105"	"GitHub - outworkers/phantom: Reactive type-safe Scala driver for Cassandra/Datastax Enterprise Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 43 Star 440 Fork 105 outworkers/phantom Code Issues 14 Pull requests 1 Wiki Pulse Graphs Reactive type-safe Scala driver for Cassandra/Datastax Enterprise http://outworkers.github.io/phantom/ 727 commits 9 branches 51 releases Fetching contributors Scala 99.7% Other 0.3% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop feature/builder feature/keyspace_serializer feature/logging feature/small_fixes feature/travis_build gh-pages master release/1.27.0_1 Nothing to show v1.114.1 v1.25.1 v1.25.0 v1.23.1 v1.21.5 v1.21.3 v1.21.2 v1.21.1 v1.21.0 v1.20.0 v1.19.0 v1.18.1 v1.18.0 v1.17.7 v1.17.5 v1.17.3 v1.17.0 v1.16.0 v1.15.0 v1.14.2 v1.14.1 v1.13.0 v1.12.3 v1.12.2 v1.11.0 v1.10.1 v1.1.0 v0.1.5-SNAPSHOT org=com.websudos,name=phantom,version=1.27.1 org=com.websudos,name=phantom,version=1.26.7 org=com.websudos,name=phantom,version=1.26.5 org=com.websudos,name=phantom,version=1.26.4 org=com.websudos,name=phantom,version=1.26.3 org=com.websudos,name=phantom,version=1.26.2 org=com.websudos,name=phantom,version=1.26.0 org=com.websudos,name=phantom,version=1.25.4 org=com.websudos,name=phantom,version=1.9.10 org=com.websudos,name=phantom,version=1.9.9 org=com.websudos,name=phantom,version=1.9.8 org=com.websudos,name=phantom,version=1.9.7 org=com.websudos,name=phantom,version=1.9.6 1.22.1 1.8.12 1.4.0 1.0.0 0.8.0 0.7.0 0.6.0 0.4.0 0.3.0 0.2.0 Nothing to show New pull request Latest commit ac5cf68 Jun 29, 2016 alexflav23 committed on GitHub Merge pull request #514 from outworkers/release/1.27.0 … Release/1.27.0 Permalink Failed to load latest commit information. build Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-connectors/src/main/scala/com/websudos/phantom/connectors Resolving conflict.s Jun 29, 2016 phantom-container-test Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-dsl/src Resolving conflict.s Jun 29, 2016 phantom-example/src Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-finagle/src Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-jdk8/src Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-reactivestreams/src Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-sbt/src/main/scala-2.10/com/websudos/phantom/sbt Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 phantom-thrift/src Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 project Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 .gitignore Release/1.13.0: Added ordering clauses support + operators. Nov 2, 2015 .jvmopts Release/1.13.0: Added ordering clauses support + operators. Nov 2, 2015 .travis.yml Release/1.27.0: Removing ZK, updating deps. Adding DB methods. Jun 29, 2016 CONTRIBUTING.md Release/1.5.0: Official Scala 2.11 support in phantom. Jan 4, 2015 LICENSE.txt Fixing casing of keyword. Apr 4, 2016 README.md Update README.md May 19, 2016 build.sbt Resolving conflict.s Jun 29, 2016 scalastyle-config.xml Updating license reference May 6, 2016 README.md phantom Reactive type-safe Scala DSL for Cassandra To stay up-to-date with our latest releases and news, follow us on Twitter: @outworkers_uk. If you use phantom, please consider adding your company to our list of adopters. phantom is and will always be freeware, but the more adopters our projects have, the more people from our company will actively work to make them better. Using phantom Scala 2.10 and 2.11 releases We publish phantom in 2 formats, stable releases and bleeding edge. The stable release is always available on Maven Central and will be indicated by the badge at the top of this readme. The Maven Central badge is pointing at the latest version Intermediary releases are available through our managed Bintray repository available at https://dl.bintray.com/websudos/oss-releases/. The latest version available on our Bintray repository is indicated by the Bintray badge at the top of this readme. Latest versions The latest versions are available here. The badges automatically update when a new version is released. Latest stable version: (Maven Central) Bleeding edge: (Websudos OSS releases on Bintray) Tutorials on phantom and Cassandra For ease of use and far better management of documentation, we have decided to export the README.md to a proper Wiki page, now available here. The following are the current resources available for learning phantom, outside of tests which are very useful in highlighting all the possible features in phantom and how to use them. This is a list of resources to help you learn phantom and Cassandra: Datastax Introduction to Cassandra. The official Scala API docs for phantom The main Wiki The StackOverflow phantom-dsl tag, which we always monitor! Anything tagged phantom on our blog is a phantom tutorial: phantom tutorials A series on Cassandra: Getting rid of the SQL mentality A series on Cassandra: Indexes and keys A series on Cassandra: Advanced features A series on phantom: Getting started with phantom The Play! Phantom Activator template Thiago's Cassandra + Phantom demo repository Issues and questions back to top We love Cassandra to bits and use it in every bit of our stack. phantom makes it super trivial for Scala users to embrace Cassandra. Cassandra is highly scalable and it is by far the most powerful database technology available, open source or otherwise. Phantom is built on top of the Datastax Java Driver, which does most of the heavy lifting. We are very happy to help implement missing features in phantom, answer questions about phantom, and occasionally help you out with Cassandra questions! Please use GitHub for any issues or bug reports. Adopters This is a list of companies that have embraced phantom as part of their technology stack and using it in production environments. CreditSuisse ING Wincor Nixdorf Paddy Power Mobli Pellucid Analytics Equens outworkers VictorOps Socrata Sphonic Anomaly42 Tecsisa Tuplejump FiloDB - the fast analytics database built on Cassandra and Spark Chartboost License and copyright Phantom is freeware software and uses a proprietary license that in plain English says the following: Phantom is the intellectual property of Outworkers, it is not provided under an OSS license. You can use phantom in commercial products or otherwise, so long as you use one of the official versions available on Bintray or Maven Central. You are not allowed to distribute altered copies of phantom in binary form. You cannot offer paid for training on phantom unless you are a direct partner to Outworkers and you have a written intellectual property agreement in place with us. If you simply have a Build.scala or build.sbt dependency on phantom, you have nothing to worry about. All paid for features are published and sold separately as phantom-pro, everything that is currently available for free will remain so forever. If you would like our help with any new content or initiatives, we'd love to hear about it! Contributors back to top Phantom was developed at outworkers as an in-house project. All Cassandra integration at outworkers goes through phantom, and nowadays it's safe to say most Scala/Cassandra users in the world rely on phantom. Flavian Alexandru (@alexflav23) - maintainer Bartosz Jankiewicz (@bjankie1) Benjamin Edwards (@benjumanji) Kevin Wright (@kevinwright) Eugene Zhulenev (@ezhulenev) Michal Matloka (@mmatloka) Thiago Pereira (@thiagoandrade6) Juan José Vázquez (@juanjovazquez) Viktor Taranenko (@viktortnk) Stephen Samuel (@sksamuel) Evan Chan (@evanfchan) Jens Halm (@jenshalm) Donovan Levinson (@levinson) Copyright back to top Special thanks to Viktor Taranenko from WhiskLabs, who gave us the original idea. Copyright © 2013 - 2016 outworkers. Contributing to phantom back to top Contributions are most welcome! Use GitHub for issues and pull requests and we will happily help out in any way we can! Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/websudos/phantom"	"Reactive type safe Scala driver for Apache Cassandra."	"true"
"Database"	"PostgreSQL and MySQL async ★ 787 ⧗ 0"	"https://github.com/mauricio/postgresql-async"	"Async database drivers to talk to PostgreSQL and MySQL in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"832"	"93"	"122"	"GitHub - mauricio/postgresql-async: Async, Netty based, database drivers for PostgreSQL and MySQL written in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 93 Star 832 Fork 122 mauricio/postgresql-async Code Issues 23 Pull requests 3 Pulse Graphs Async, Netty based, database drivers for PostgreSQL and MySQL written in Scala 434 commits 2 branches 20 releases Fetching contributors Scala 99.1% Other 0.9% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags closing-prepared-statements master Nothing to show v0.2.20 v0.2.19 v0.2.18 v0.2.17 v0.2.16 v0.2.15 v0.2.14 v0.2.13 v0.2.12 v0.2.11 v0.2.10 v0.2.9 v0.2.7 v0.2.6 v0.2.5 v0.2.4 v0.2.3 v0.2.2 v0.1.0 0.2.8 Nothing to show New pull request Latest commit cc58769 Jul 8, 2016 mauricio committed on GitHub Merge pull request #178 from SattaiLanfear/urlparse-updates … Reworked URLParser to process more URLs.  Added MySQL URLParser Permalink Failed to load latest commit information. db-async-common/src Merge pull request #178 from SattaiLanfear/urlparse-updates Jul 8, 2016 mysql-async Merge pull request #178 from SattaiLanfear/urlparse-updates Jul 8, 2016 postgresql-async Merge pull request #178 from SattaiLanfear/urlparse-updates Jul 8, 2016 project Starting next development cycle Jun 10, 2016 script Add SSL support.. Mar 7, 2016 .gitignore Upgrading some of the dependent libraries Mar 5, 2016 .travis.yml Updating SBT and JDK versions Aug 4, 2015 CHANGELOG.md Updated readme and changelog Mar 17, 2016 LICENSE.txt Rename LICENCE.txt to LICENSE.txt Mar 15, 2016 Procfile Upgrading some of the dependent libraries Mar 5, 2016 README.markdown Starting next development cycle Jun 10, 2016 Vagrantfile Connects to MySQL even if the auth protocol details are not provided,… Jul 26, 2014 bootstrap.sh Connects to MySQL even if the auth protocol details are not provided,… Jul 26, 2014 README.markdown postgresql-async & mysql-async - async, Netty based, database drivers for MySQL and PostgreSQL written in Scala 2.10 and 2.11 Abstractions and integrations Include them as dependencies Database connections and encodings Prepared statements gotcha What are the design goals? What is missing? How can you help? Main public interface Connection QueryResult ResultSet Prepared statements Transactions Example usage (for PostgreSQL, but it looks almost the same on MySQL) LISTEN/NOTIFY support (PostgreSQL only) Contributing Licence postgresql-async & mysql-async - async, Netty based, database drivers for MySQL and PostgreSQL written in Scala 2.10 and 2.11 The main goal for this project is to implement simple, async, performant and reliable database drivers for PostgreSQL and MySQL in Scala. This is not supposed to be a JDBC replacement, these drivers aim to cover the common process of send a statement, get a response that you usually see in applications out there. So it's unlikely there will be support for updating result sets live or stuff like that. This project always returns JodaTime when dealing with date types and not the java.util.Date class. If you want information specific to the drivers, check the PostgreSQL README and the MySQL README. You can view the project's CHANGELOG here. Abstractions and integrations Activate Framework - full ORM solution for persisting objects using a software transactional memory (STM) layer; ScalikeJDBC-Async - provides an abstraction layer on top of the driver allowing you to write less SQL and make use of a nice high level database access API; mod-mysql-postgresql - vert.x module that integrates the driver into a vert.x application; dbmapper - enables SQL queries with automatic mapping from the database table to the Scala class and a mechanism to create a Table Date Gateway model with very little boiler plate code; Quill - A compile-time language integrated query library for Scala. Include them as dependencies And if you're in a hurry, you can include them in your build like this, if you're using PostgreSQL: ""com.github.mauricio"" %% ""postgresql-async"" % ""0.2.20"" Or Maven: <dependency>   <groupId>com.github.mauricio</groupId>   <artifactId>postgresql-async_2.11</artifactId>   <version>0.2.20</version> </dependency> And if you're into MySQL: ""com.github.mauricio"" %% ""mysql-async"" % ""0.2.20"" Or Maven: <dependency>   <groupId>com.github.mauricio</groupId>   <artifactId>mysql-async_2.11</artifactId>   <version>0.2.20</version> </dependency> Database connections and encodings READ THIS NOW Both clients will let you set the database encoding for something else. Unless you are 1000% sure of what you are doing, DO NOT change the default encoding (currently, UTF-8). Some people assume the connection encoding is the database or columns encoding but IT IS NOT, this is just the connection encoding that is used between client and servers doing communication. When you change the encoding of the connection you are not affecting your database's encoding and your columns WILL NOT be stored with the connection encoding. If the connection and database/column encoding is different, your database will automatically translate from the connection encoding to the correct encoding and all your data will be safely stored at your database/column encoding. This is as long as you are using the correct string types, BLOB columns will not be translated since they're supposed to hold a stream of bytes. So, just don't touch it, create your tables and columns with the correct encoding and be happy. Prepared statements gotcha If you have used JDBC before, you might have heard that prepared statements are the best thing on earth when talking to databases. This isn't exactly true all the time (as you can see on this presentation by @tenderlove) and there is a memory cost in keeping prepared statements. Prepared statements are tied to a connection, they are not database-wide, so, if you generate your queries dynamically all the time you might eventually blow up your connection memory and your database memory. Why? Because when you create a prepared statement, locally, the connection keeps the prepared statement description in memory. This can be the returned columns information, input parameters information, query text, query identifier that will be used to execute the query and other flags. This also causes a data structure to be created at your server for every connection. So, prepared statements are awesome, but are not free. Use them judiciously. What are the design goals? fast, fast and faster small memory footprint avoid copying data as much as possible (we're always trying to use wrap and slice on buffers) easy to use, call a method, get a future or a callback and be happy never, ever, block all features covered by tests next to zero dependencies (it currently depends on Netty, JodaTime and SFL4J only) What is missing? more authentication mechanisms benchmarks more tests (run the jacoco:cover sbt task and see where you can improve) timeout handler for initial handshare and queries How can you help? checkout the source code find bugs, find places where performance can be improved check the What is missing piece check the issues page for bugs or new features send a pull request with specs Main public interface Connection Represents a connection to the database. This is the root object you will be using in your application. You will find three classes that implement this trait, PostgreSQLConnection, MySQLConnection and ConnectionPool. The difference between them is that ConnectionPool is, as the name implies, a pool of connections and you need to give it an connection factory so it can create connections and manage them. To create both you will need a Configuration object with your database details. You can create one manually or create one from a JDBC or Heroku database URL using the URLParser object. QueryResult It's the output of running a statement against the database (either using sendQuery or sendPreparedStatement). This object will contain the amount of rows, status message and the possible ResultSet (Option[ResultSet]) if the query returns any rows. ResultSet It's an IndexedSeq[Array[Any]], every item is a row returned by the database. The database types are returned as Scala objects that fit the original type, so smallint becomes a Short, numeric becomes BigDecimal, varchar becomes String and so on. You can find the whole default transformation list at the project specific documentation. Prepared statements Databases support prepared or precompiled statements. These statements allow the database to precompile the query on the first execution and reuse this compiled representation on future executions, this makes it faster and also allows for safer data escaping when dealing with user provided data. To execute a prepared statement you grab a connection and: val connection : Connection = ... val future = connection.sendPreparedStatement( ""SELECT * FROM products WHERE products.name = ?"", Array( ""Dominion"" ) ) The ? (question mark) in the query is a parameter placeholder, it allows you to set a value in that place in the query without having to escape stuff yourself. The driver itself will make sure this parameter is delivered to the database in a safe way so you don't have to worry about SQL injection attacks. The basic numbers, Joda Time date, time, timestamp objects, strings and arrays of these objects are all valid values as prepared statement parameters and they will be encoded to their respective database types. Remember that not all databases are created equal, so not every type will work or might work in unexpected ways. For instance, MySQL doesn't have array types, so, if you send an array or collection to MySQL it won't work. Remember that parameters are positional the order they show up at query should be the same as the one in the array or sequence given to the method call. Transactions Both drivers support transactions at the database level, the isolation level is the default for your database/connection, to change the isolation level just call your database's command to set the isolation level for what you want. Here's an example of how transactions work:   val future = connection.inTransaction {     c =>     c.sendPreparedStatement(this.insert)      .flatMap( r => c.sendPreparedStatement(this.insert))   } The inTransaction method allows you to execute a collection of statements in a single transactions, just use the connection object you will receive in your block and send your statements to it. Given each statement causes a new future to be returned, you need to flatMap the calls to be able to get a Future[T] instead of Future[Future[...]] back. If all futures succeed, the transaction is committed normally, if any of them fail, a rollback is issued to the database. You should not reuse a database connection that has rolled back a transaction, just close it and create a new connection to continue using it. Example usage (for PostgreSQL, but it looks almost the same on MySQL) You can find a small Play 2 app using it here and a blog post about it here. In short, what you would usually do is: import com.github.mauricio.async.db.postgresql.PostgreSQLConnection import com.github.mauricio.async.db.postgresql.util.URLParser import com.github.mauricio.async.db.util.ExecutorServiceUtils.CachedExecutionContext import com.github.mauricio.async.db.{RowData, QueryResult, Connection} import scala.concurrent.duration._ import scala.concurrent.{Await, Future}  object BasicExample {    def main(args: Array[String]) {      val configuration = URLParser.parse(""jdbc:postgresql://localhost:5233/my_database?user=postgres&password=somepassword"")     val connection: Connection = new PostgreSQLConnection(configuration)      Await.result(connection.connect, 5 seconds)      val future: Future[QueryResult] = connection.sendQuery(""SELECT 0"")      val mapResult: Future[Any] = future.map(queryResult => queryResult.rows match {       case Some(resultSet) => {         val row : RowData = resultSet.head         row(0)       }       case None => -1     }     )      val result = Await.result( mapResult, 5 seconds )      println(result)      connection.disconnect    }  } First, create a PostgreSQLConnection, connect it to the database, execute queries (this object is not thread safe, so you must execute only one query at a time) and work with the futures it returns. Once you are done, call disconnect and the connection is closed. You can also use the ConnectionPool provided by the driver to simplify working with database connections in your app. Check the blog post above for more details and the project's ScalaDocs. LISTEN/NOTIFY support (PostgreSQL only) LISTEN/NOTIFY is a PostgreSQL-specific feature for database-wide publish-subscribe scenarios. You can listen to database notifications as such:   val connection: Connection = ...    connection.sendQuery(""LISTEN my_channel"")   connection.registerNotifyListener {     message =>     println(s""channel: ${message.channel}, payload: ${message.payload}"")   } Contributing Contributing to the project is simple, fork it on Github, hack on what you're insterested in seeing done or at the bug you want to fix and send a pull request back. If you thing the change is too big or requires architectural changes please create an issue before you start working on it so we can discuss what you're trying to do. You should be easily able to build this project in your favorite IDE since it's built by SBT using a plugin that generates your IDE's project files. You can use sbt-idea for IntelliJ Idea and sbteclipse for Eclipse integration. Check our list of contributors! Licence This project is freely available under the Apache 2 licence, fork, fix and send back :) Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/mauricio/postgresql-async"	"Async database drivers to talk to PostgreSQL and MySQL in Scala."	"true"
"Database"	"Quill ★ 501 ⧗ 0"	"https://github.com/getquill/quill"	"Compile-time Language Integrated Query for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"584"	"36"	"60"	"GitHub - getquill/quill: Compile-time Language Integrated Queries for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 36 Star 584 Fork 60 getquill/quill Code Issues 104 Pull requests 2 Pulse Graphs Compile-time Language Integrated Queries for Scala http://getquill.io 1,026 commits 5 branches 15 releases 19 contributors Scala 99.5% Shell 0.5% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags fix-382 gh-pages io-monad master returning Nothing to show website v0.7.0 v0.6.0 v0.5.0 v0.4.1 v0.4.0 v0.3.1 v0.3.0 v0.2.1 v0.2.0 v0.1.3 v0.1.2 v0.1.1 v0.1.0 v0.0.2 Nothing to show New pull request Latest commit f79918c Jul 15, 2016 fwbrasil committed on GitHub Merge pull request #449 from getquill/sqlite … sqlite support Permalink Failed to load latest commit information. .github add `Workaround` session to the github issue template Jul 10, 2016 build sqlite support Jul 14, 2016 project fix codecov report May 5, 2016 quill-async/src make type aliases for `run` results public Jul 10, 2016 quill-cassandra/src make liftable values work for the cassandra module. Jul 5, 2016 quill-core/src fail compilation if query is defined outside a `quote` Jul 9, 2016 quill-finagle-mysql/src make type aliases for `run` results public Jul 10, 2016 quill-jdbc/src sqlite support Jul 15, 2016 quill-sql/src sqlite support Jul 15, 2016 .gitignore sqlite support Jul 15, 2016 .pullapprove.yml don't reset pullapprove count on push Mar 20, 2016 .travis.yml fully automate release process Jul 9, 2016 CASSANDRA.md introduce contexts Jul 4, 2016 CHANGELOG.md 0.7.0 release Jul 2, 2016 CODE_OF_CONDUCT.md add the Code of Conduct Mar 20, 2016 CONTRIBUTING.md Add note about docker-compose linux installation Jun 20, 2016 LICENSE.txt Update LICENSE.txt Nov 30, 2015 README.md sqlite support Jul 15, 2016 SLICK.md Update SLICK.md May 25, 2016 build.sbt sqlite support Jul 15, 2016 codecov.yml reconfigure test coverage settings May 8, 2016 docker-compose.yml update versions and use patched cqlsh Jul 2, 2016 example.gif fix broken links Mar 3, 2016 quill.png reduce logo size May 23, 2016 version.sbt Setting version to 0.7.1-SNAPSHOT Jul 3, 2016 README.md IMPORTANT: This is the documentation for the latest SNAPSHOT version. Please refer to the website at http://getquill.io for the lastest release's documentation. Compile-time Language Integrated Query for Scala Quill provides a Quoted Domain Specific Language (QDSL) to express queries in Scala and execute them in a target language. The library's core is designed to support multiple target languages, currently featuring specializations for Structured Query Language (SQL) and Cassandra Query Language (CQL). Boilerplate-free mapping: The database schema is mapped using simple case classes. Quoted DSL: Queries are defined inside a quote block. Quill parses each quoted block of code (quotation) at compile time and translates them to an internal Abstract Syntax Tree (AST) Compile-time query generation: The ctx.run call reads the quotation's AST and translates it to the target language at compile time, emitting the query string as a compilation message. As the query string is known at compile time, the runtime overhead is very low and similar to using the database driver directly. Compile-time query validation: If configured, the query is verified against the database at compile time and the compilation fails if it is not valid. The query validation does not alter the database state. Quotation Introduction The QDSL allows the user to write plain Scala code, leveraging scala's syntax and type system. Quotations are created using the quote method and can contain any excerpt of code that uses supported operations. To create quotations, first create a context instance. Please see the context section for more details on the different context available. For this documentation, a special type of context that acts as a mirror is used: import io.getquill._  val ctx = new SqlMirrorContext[Literal] The context instance provides all types and methods to deal quotations: import ctx._ A quotation can be a simple value: val pi = quote(3.14159) And be used within another quotation: case class Circle(radius: Float)  val areas = quote {   query[Circle].map(c => pi * c.radius * c.radius) } Quotations can also contain high-order functions and inline values: val area = quote {   (c: Circle) => {     val r2 = c.radius * c.radius     pi * r2   } } val areas = quote {   query[Circle].map(c => area(c)) } Quill's normalization engine applies reduction steps before translating the quotation to the target language. The correspondent normalized quotation for both versions of the areas query is: val areas = quote {   query[Circle].map(c => 3.14159 * c.radius * c.radius) } Scala doesn't have support for high-order functions with type parameters. Quill supports anonymous classes with an apply method for this purpose: val existsAny = quote {   new {     def apply[T](xs: Query[T])(p: T => Boolean) =         xs.filter(p(_)).nonEmpty   } }  val q = quote {   query[Circle].filter { c1 =>     existsAny(query[Circle])(c2 => c2.radius > c1.radius)   } } Compile-time quotations Quotations are both compile-time and runtime values. Quill uses a type refinement to store the quotation's AST as an annotation available at compile-time and the q.ast method exposes the AST as runtime value. It is important to avoid giving explicit types to quotations when possible. For instance, this quotation can't be read at compile-time as the type refinement is lost: // Avoid type widening (Quoted[Query[Circle]]), or else the quotation will be dynamic. val q: Quoted[Query[Circle]] = quote {   query[Circle].filter(c => c.radius > 10) }  ctx.run(q) // Dynamic query Quill falls back to runtime normalization and query generation if the quotation's AST can be read at compile-time. Please refer to dynamic queries for more information Bindings Quotations are designed to be self-contained, without references to runtime values outside their scope. There are two mechanisms to explicitly bind runtime values to a quotation execution. Lifted values A runtime value can be lifted to a quotation through the method lift: def biggerThan(i: Float) = quote {   query[Circle].filter(r => r.radius > lift(i)) } ctx.run(biggerThan(10)) // SELECT r.radius FROM Circle r WHERE r.radius > ? Parametrized quotations A quotation can be defined as a function: val biggerThan = quote {   (i: Int) =>     query[Circle].filter(r => r.radius > i) } And a runtime value can be specified when running it: ctx.run(biggerThan)(10) // SELECT r.radius FROM Circle r WHERE r.radius > ? Schema The database schema is represented by case classes. By default, quill uses the class and field names as the database identifiers: case class Circle(radius: Float)  val q = quote {   query[Circle].filter(c => c.radius > 1) }  ctx.run(q) // SELECT c.radius FROM Circle c WHERE c.radius > 1 Alternatively, the identifiers can be customized: val circles = quote {   query[Circle].schema(_.entity(""circle_table"").columns(_.radius -> ""radius_column"")) }  val q = quote {   circles.filter(c => c.radius > 1) }  ctx.run(q) // SELECT c.radius_column FROM circle_table c WHERE c.radius_column > 1 If multiple tables require custom identifiers, it is good practice to define a schema object with all table queries to be reused across multiple queries: case class Circle(radius: Int) case class Rectangle(length: Int, width: Int) object schema {   val circles = quote {     query[Circle].schema(         _.entity(""circle_table"")         .columns(_.radius -> ""radius_column""))   }   val rectangles = quote {     query[Rectangle].schema(         _.entity(""rectangle_table"")         .columns(           _.length -> ""length_column"",           _.width -> ""width_column""))   } } It is possible to define a column that is a key generated by the database. It will be ignored during insertions and returned as the result. Note that it accepts only values that can be read as Long. case class Product(id: Long, description: String, sku: Long)  val q = quote {   query[Product].schema(_.generated(_.id)).insert }  ctx.run(q) // INSERT INTO Product (description,sku) VALUES (?, ?) Queries The overall abstraction of quill queries is use database tables as if they were in-memory collections. Scala for-comprehensions provide syntatic sugar to deal with this kind of monadic operations: case class Person(id: Int, name: String, age: Int) case class Contact(personId: Int, phone: String)  val q = quote {   for {     p <- query[Person] if(p.id == 999)     c <- query[Contact] if(c.personId == p.id)   } yield {     (p.name, c.phone)   } }  ctx.run(q) // SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id) Quill normalizes the quotation and translates the monadic joins to applicative joins, generating a database-friendly query that avoids nested queries. Any of the following features can be used together with the others and/or within a for-comprehension: filter val q = quote {   query[Person].filter(p => p.age > 18) }  ctx.run(q) // SELECT p.id, p.name, p.age FROM Person p WHERE p.age > 18 map val q = quote {   query[Person].map(p => p.name) }  ctx.run(q) // SELECT p.name FROM Person p flatMap val q = quote {   query[Person].filter(p => p.age > 18).flatMap(p => query[Contact].filter(c => c.personId == p.id)) }  ctx.run(q) // SELECT c.personId, c.phone FROM Person p, Contact c WHERE (p.age > 18) AND (c.personId = p.id) sortBy val q1 = quote {   query[Person].sortBy(p => p.age) }  ctx.run(q1) // SELECT p.id, p.name, p.age FROM Person p ORDER BY p.age ASC NULLS FIRST  val q2 = quote {   query[Person].sortBy(p => p.age)(Ord.descNullsLast) }  ctx.run(q2) // SELECT p.id, p.name, p.age FROM Person p ORDER BY p.age DESC NULLS LAST  val q3 = quote {   query[Person].sortBy(p => (p.name, p.age))(Ord(Ord.asc, Ord.desc)) }  ctx.run(q3) // SELECT p.id, p.name, p.age FROM Person p ORDER BY p.name ASC, p.age DESC drop/take val q = quote {   query[Person].drop(2).take(1) }  ctx.run(q) // SELECT x.id, x.name, x.age FROM Person x LIMIT 1 OFFSET 2 groupBy val q = quote {   query[Person].groupBy(p => p.age).map {     case (age, people) =>       (age, people.size)   } }  ctx.run(q) // SELECT p.age, COUNT(*) FROM Person p GROUP BY p.age union val q = quote {   query[Person].filter(p => p.age > 18).union(query[Person].filter(p => p.age > 60)) }  ctx.run(q) // SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18 // UNION SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x unionAll/++ val q = quote {   query[Person].filter(p => p.age > 18).unionAll(query[Person].filter(p => p.age > 60)) }  ctx.run(q) // SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18 // UNION ALL SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x  val q2 = quote {   query[Person].filter(p => p.age > 18) ++ query[Person].filter(p => p.age > 60) }  ctx.run(q2) // SELECT x.id, x.name, x.age FROM (SELECT id, name, age FROM Person p WHERE p.age > 18 // UNION ALL SELECT id, name, age FROM Person p1 WHERE p1.age > 60) x aggregation val r = quote {   query[Person].map(p => p.age) }  ctx.run(r.min) // SELECT MIN(p.age) FROM Person p ctx.run(r.max) // SELECT MAX(p.age) FROM Person p ctx.run(r.avg) // SELECT AVG(p.age) FROM Person p ctx.run(r.sum) // SELECT SUM(p.age) FROM Person p ctx.run(r.size) // SELECT COUNT(p.age) FROM Person p isEmpty/nonEmpty val q = quote {   query[Person].filter{ p1 =>     query[Person].filter(p2 => p2.id != p1.id && p2.age == p1.age).isEmpty   } }  ctx.run(q) // SELECT p1.id, p1.name, p1.age FROM Person p1 WHERE // NOT EXISTS (SELECT * FROM Person p2 WHERE (p2.id <> p1.id) AND (p2.age = p1.age))  val q2 = quote {   query[Person].filter{ p1 =>     query[Person].filter(p2 => p2.id != p1.id && p2.age == p1.age).nonEmpty   } }  ctx.run(q2) // SELECT p1.id, p1.name, p1.age FROM Person p1 WHERE // EXISTS (SELECT * FROM Person p2 WHERE (p2.id <> p1.id) AND (p2.age = p1.age)) contains val q = quote {   query[Person].filter(p => Set(1, 2).contains(p.id)) }  ctx.run(q) // SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (1, 2)  val q1 = quote { (ids: Set[Int]) =>   query[Person].filter(p => ids.contains(p.id)) }  ctx.run(q1) // SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (?)  val peopleWithContacts = quote {   query[Person].filter(p => query[Contact].filter(c => c.personId == p.id).nonEmpty) } val q2 = quote {   query[Person].filter(p => peopleWithContacts.contains(p.id)) }  ctx.run(q2) // SELECT p.id, p.name, p.age FROM Person p WHERE p.id IN (SELECT p1.* FROM Person p1 WHERE EXISTS (SELECT c.* FROM Contact c WHERE c.personId = p1.id)) distinct val q = quote {   query[Person].map(p => p.age).distinct }  ctx.run(q) // SELECT DISTINCT p.age FROM Person p joins In addition to applicative joins Quill also supports explicit joins (both inner and left/right/full outer joins). val q = quote {   query[Person].join(query[Contact]).on((p, c) => c.personId == p.id) }  ctx.run(q) // SELECT p.id, p.name, p.age, c.personId, c.phone• // FROM Person p INNER JOIN Contact c ON c.personId = p.id  val q = quote {   query[Person].leftJoin(query[Contact]).on((p, c) => c.personId == p.id) }  ctx.run(q) // SELECT p.id, p.name, p.age, c.personId, c.phone• // FROM Person p LEFT JOIN Contact c ON c.personId = p.id  The example joins above cover the simple case. What do you do when a query requires joining more than 2 tables? With Quill the following multi-join queries are equivalent, choose according to preference: case class Employer(id: Int, personId: Int, name: String)  val qFlat = quote {   for{     (p,e) <- query[Person].join(query[Employer]).on(_.id == _.personId)        c  <- query[Contact].leftJoin(_.personId == p.id)   } yield(p, e, c) }  val qNested = quote {   for{     ((p,e),c) <-       query[Person].join(query[Employer]).on(_.id == _.personId)       .leftJoin(query[Contact]).on(         _._1.id == _.personId       )   } yield(p, e, c) }  ctx.run(qFlat) ctx.run(qNested) // SELECT p.id, p.name, p.age, e.id, e.personId, e.name, c.id, c.phone• // FROM Person p INNER JOIN Employer e ON p.id = e.personId LEFT JOIN Contact c ON c.personId = p.id Query probing Query probing validates queries against the database at compile time, failing the compilation if it is not valid. The query validation does not alter the database state. This feature is disabled by default. To enable it, mix the QueryProbing trait to the database configuration: lazy val ctx = new MyContext(""configKey"") with QueryProbing  The context must be created in a separate compilation unit in order to be loaded at compile time. Please use this guide that explains how to create a separate compilation unit for macros, that also serves to the purpose of defining a query-probing-capable context. context could be used instead of macros as the name of the separate compilation unit. The configurations correspondent to the config key must be available at compile time. You can achieve it by adding this line to your project settings: unmanagedClasspath in Compile += baseDirectory.value / ""src"" / ""main"" / ""resources""  If your project doesn't have a standard layout, e.g. a play project, you should configure the path to point to the folder that contains your config file. Actions Database actions are defined using quotations as well. These actions don't have a collection-like API but rather a custom DSL to express inserts, deletes and updates. Note: Actions take either a List (in which case the query is batched) or a single value. insert val a = quote(query[Contact].insert)  ctx.run(a)(List(Contact(999, ""+1510488988""))) // INSERT INTO Contact (personId,phone) VALUES (?, ?) ctx.run(a)(Contact(999, ""+1510488988"")) // insert single item It is also possible to insert specific columns: val a = quote {   (personId: Int, phone: String) =>     query[Contact].insert(_.personId -> personId, _.phone -> phone) }  ctx.run(a)(List((999, ""+1510488988""))) // INSERT INTO Contact (personId,phone) VALUES (?, ?) Or column queries: val a = quote {   (id: Int) =>     query[Person].insert(_.id -> id, _.age -> query[Person].map(p => p.age).max) }  ctx.run(a)(List(999)) // INSERT INTO Person (id,age) VALUES (?, (SELECT MAX(p.age) FROM Person p)) update val a = quote {   query[Person].filter(_.id == 999).update }  ctx.run(a)(List(Person(999, ""John"", 22))) // UPDATE Person SET id = ?, name = ?, age = ? WHERE id = 999 ctx.run(a)(Person(999, ""John"", 22)) // update single item Using specific columns: val a = quote {   (id: Int, age: Int) =>     query[Person].filter(p => p.id == id).update(_.age -> age) }  ctx.run(a)(List((999, 18))) // UPDATE Person SET age = ? WHERE id = ? Using columns as part of the update: val a = quote {   (id: Int) =>     query[Person].filter(p => p.id == id).update(p => p.age -> (p.age + 1)) }  ctx.run(a)(List(999)) // UPDATE Person SET age = (age + 1) WHERE id = ? Using column a query: val a = quote {   (id: Int) =>     query[Person].filter(p => p.id == id).update(_.age -> query[Person].map(p => p.age).max) }  ctx.run(a)(List(999)) // UPDATE Person SET age = (SELECT MAX(p.age) FROM Person p) WHERE id = ? delete val a = quote {   query[Person].filter(p => p.name == """").delete }  ctx.run(a) // DELETE FROM Person WHERE name = '' Implicit query Quill provides implicit conversions from case class companion objects to query[T] through an additional trait: val ctx = new SqlMirrorContext with ImplicitQuery  import ctx._  val q = quote {   for {     p <- Person if(p.id == 999)     c <- Contact if(c.personId == p.id)   } yield {     (p.name, c.phone)   } }  ctx.run(q) // SELECT p.name, c.phone FROM Person p, Contact c WHERE (p.id = 999) AND (c.personId = p.id) Note the usage of Person and Contact instead of query[Person] and query[Contact]. SQL-specific operations Some operations are sql-specific and not provided with the generic quotation mechanism. The sql contexts provide implicit classes for this kind of operation: val ctx = new SqlMirrorContext import ctx._ like val q = quote {   query[Person].filter(p => p.name like ""%John%"") } ctx.run(q) // SELECT p.id, p.name, p.age FROM Person p WHERE p.name like '%John%' Cassandra-specific operations The cassandra context also provides a few additional operations: val ctx = new CassandraMirrorContext import ctx._ allowFiltering val q = quote {   query[Person].filter(p => p.age > 10).allowFiltering } ctx.run(q) // SELECT id, name, age FROM Person WHERE age > 10 ALLOW FILTERING ifNotExists val q = quote {   query[Person].insert(_.age -> 10, _.name -> ""John"").ifNotExists } ctx.run(q) // INSERT INTO Person (age,name) VALUES (10, 'John') IF NOT EXISTS ifExists val q = quote {   query[Person].filter(p => p.name == ""John"").delete.ifExists } ctx.run(q) // DELETE FROM Person WHERE name = 'John' IF EXISTS usingTimestamp val q1 = quote {   query[Person].insert(_.age -> 10, _.name -> ""John"").usingTimestamp(99) } ctx.run(q1) // INSERT INTO Person (age,name) VALUES (10, 'John') USING TIMESTAMP 99  val q2 = quote {   query[Person].usingTimestamp(99).update(_.age -> 10) } ctx.run(q2) // UPDATE Person USING TIMESTAMP 99 SET age = 10 usingTtl val q1 = quote {   query[Person].insert(_.age -> 10, _.name -> ""John"").usingTtl(11) } ctx.run(q1) // INSERT INTO Person (age,name) VALUES (10, 'John') USING TTL 11  val q2 = quote {   query[Person].usingTtl(11).update(_.age -> 10) } ctx.run(q2) // UPDATE Person USING TTL 11 SET age = 10  val q3 = quote {   query[Person].usingTtl(11).filter(_.name == ""John"").delete } ctx.run(q3)   // DELETE FROM Person USING TTL 11 WHERE name = 'John' using val q1 = quote {   query[Person].insert(_.age -> 10, _.name -> ""John"").using(ts = 99, ttl = 11) } ctx.run(q1) // INSERT INTO Person (age,name) VALUES (10, 'John') USING TIMESTAMP 99 AND TTL 11  val q2 = quote {   query[Person].using(ts = 99, ttl = 11).update(_.age -> 10) } ctx.run(q2) // UPDATE Person USING TIMESTAMP 99 AND TTL 11 SET age = 10  val q3 = quote {   query[Person].using(ts = 99, ttl = 11).filter(_.name == ""John"").delete } ctx.run(q3) // DELETE FROM Person USING TIMESTAMP 99 AND TTL 11 WHERE name = 'John' ifCond val q1 = quote {   query[Person].update(_.age -> 10).ifCond(_.name == ""John"") } ctx.run(q1) // UPDATE Person SET age = 10 IF name = 'John'  val q2 = quote {   query[Person].filter(_.name == ""John"").delete.ifCond(_.age == 10) } ctx.run(q2) // DELETE FROM Person WHERE name = 'John' IF age = 10 delete column val q = quote {   query[Person].map(p => p.age).delete } ctx.run(q) // DELETE p.age FROM Person Dynamic queries Quill's default operation mode is compile-time, but there are queries that have their structure defined only at runtime. Quill automatically falls back to runtime normalization and query generation if the query's structure is not static. Example: val ctx = new SqlMirrorContext  import ctx._  sealed trait QueryType case object Minor extends QueryType case object Senior extends QueryType  def people(t: QueryType): Quoted[Query[Person]] =   t match {     case Minor => quote {       query[Person].filter(p => p.age < 18)     }     case Senior => quote {       query[Person].filter(p => p.age > 65)     }   }  ctx.run(people(Minor)) // SELECT p.id, p.name, p.age FROM Person p WHERE p.age < 18  ctx.run(people(Senior)) // SELECT p.id, p.name, p.age FROM Person p WHERE p.age > 65 Extending quill Infix Infix is a very flexible mechanism to use non-supported features without having to use plain queries in the target language. It allows insertion of arbitrary strings within quotations. For instance, quill doesn't support the FOR UPDATE SQL feature. It can still be used through infix and implicit classes: implicit class ForUpdate[T](q: Query[T]) {   def forUpdate = quote(infix""$q FOR UPDATE"".as[Query[T]]) }  val a = quote {   query[Person].filter(p => p.age < 18).forUpdate }  ctx.run(a) // SELECT p.id, p.name, p.age FROM (SELECT * FROM Person p WHERE p.age < 18 FOR UPDATE) p The forUpdate quotation can be reused for multiple queries. The same approach can be used for RETURNING ID: implicit class ReturningId[T](a: Action[T]) {   def returningId = quote(infix""$a RETURNING ID"".as[Action[T]]) }  val a = quote {   query[Person].insert(_.name -> ""John"", _.age -> 21).returningId }  ctx.run(a) // INSERT INTO Person (name,age) VALUES ('John', 21) RETURNING ID A custom database function can also be used through infix: val myFunction = quote {   (i: Int) => infix""MY_FUNCTION($i)"".as[Int] }  val q = quote {   query[Person].map(p => myFunction(p.age)) }  ctx.run(q) // SELECT MY_FUNCTION(p.age) FROM Person p Custom encoding Quill uses Encoders to encode query inputs and Decoders to read values returned by queries. The library provides a few built-in encodings and two mechanisms to define custom encodings: mapped encoding and raw encoding. Mapped Encoding If the correspondent database type is already supported, use mappedEncoding. In this example, String is already supported by Quill and the UUID encoding from/to String is defined through mapped encoding: import java.util.UUID  implicit val encodeUUID = mappedEncoding[UUID, String](_.toString) implicit val decodeUUID = mappedEncoding[String, UUID](UUID.fromString(_)) Raw Encoding If the database type is not supported by Quill, it is possible to provide ""raw"" encoders and decoders: trait UUIDEncodingExample {   val jdbcContext: JdbcContext[PostgresDialect, Literal] // your context should go here    import jdbcContext._    implicit val uuidDecoder: Decoder[UUID] =     decoder[UUID] {       row => index =>         UUID.fromString(row.getObject(index).toString) // database-specific implementation     }   implicit val uuidEncoder: Encoder[UUID] =     encoder[UUID] {       row => (idx, uuid) =>         row.setObject(idx, uuid, java.sql.Types.OTHER) // database-specific implementation     } } Wrapped types Quill also supports encoding of ""wrapped types"". Just extend the WrappedValue trait and Quill will automatically encode the underlying primitive type. case class UserId(value: Int) extends AnyVal with WrappedValue[Int] case class User(id: UserId, name: String)  val q = quote {   (id: UserId) => for {     u <- query[User] if u.id == id   } yield u } ctx.run(q)(UserId(1))  // SELECT u.id, u.name FROM User u WHERE (u.id = 1) Contexts Contexts represent the database and provide an execution interface for queries. Mirror context Quill provides mirror context for test purposes. Instead of running the query, mirror context return a structure with the information that would be used to run the query. There are three mirror context instances: io.getquill.MirrorContext: Mirrors the quotation AST io.getquill.SqlMirrorContext: Mirrors the SQL query io.getquill.CassandraMirrorContext: Mirrors the CQL query SQL Contexts Contexts represent the database and provide an execution interface for queries. Example: lazy val ctx = new JdbcContext[MySQLDialect, SnakeCase](""db"") Dialect The SQL dialect to be used by the context is defined by the first type parameter. Some context types are specific to a database and thus not require it. Quill has three built-in dialects: io.getquill.context.sql.idiom.H2Dialect io.getquill.context.sql.idiom.MySQLDialect io.getquill.context.sql.idiom.PostgresDialect io.getquill.context.sql.idiom.SqliteDialect Naming strategy The second type parameter defines the naming strategy to be used when translating identifiers (table and column names) to SQL. strategy example io.getquill.naming.Literal some_ident -> some_ident io.getquill.naming.Escape some_ident -> ""some_ident"" io.getquill.naming.UpperCase some_ident -> SOME_IDENT io.getquill.naming.LowerCase SOME_IDENT -> some_ident io.getquill.naming.SnakeCase someIdent -> some_ident io.getquill.naming.CamelCase some_ident -> someIdent io.getquill.naming.MysqlEscape some_ident -> `some_ident` io.getquill.naming.PostgresEscape $some_ident -> $some_ident Multiple transformations can be defined using mixin. For instance, the naming strategy SnakeCase with UpperCase produces the following transformation: someIdent -> SOME_IDENT The transformations are applied from left to right. Configuration The string passed to the context is used as the key to obtain configurations using the typesafe config library. Additionally, any member of a context can be overriden. Example: lazy val ctx = new JdbcContext[MySQLDialect, SnakeCase](""db"") {   override def dataContext = ??? // create the datasource manually }   quill-jdbc Quill uses HikariCP for connection pooling. Please refer to HikariCP's documentation for a detailed explanation of the available configurations. Note that there are dataContext configurations, that go under dataContext, like user and password, but some pool settings may go under the root config, like connectionTimeout. Transactions The JdbcContext provides thread-local transaction support: ctx.transaction {   ctx.run(query[Person].delete)   // other transactional code }  The body of transaction can contain calls to other methods and multiple run calls, since the transaction is propagated through a thread-local. MySQL sbt dependencies libraryDependencies ++= Seq(   ""mysql"" % ""mysql-connector-java"" % ""5.1.38"",   ""io.getquill"" %% ""quill-jdbc"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new JdbcContext[MySQLDialect, SnakeCase](""db"") application.properties ctx.dataContextClassName=com.mysql.jdbc.jdbc2.optional.MysqlDataSource ctx.dataContext.url=jdbc:mysql://host/database ctx.dataContext.user=root ctx.dataContext.password=root ctx.dataContext.cachePrepStmts=true ctx.dataContext.prepStmtCacheSize=250 ctx.dataContext.prepStmtCacheSqlLimit=2048 ctx.connectionTimeout=30000  Postgres sbt dependencies libraryDependencies ++= Seq(   ""org.postgresql"" % ""postgresql"" % ""9.4.1208"",   ""io.getquill"" %% ""quill-jdbc"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new JdbcContext[PostgresDialect, SnakeCase](""db"") application.properties ctx.dataContextClassName=org.postgresql.ds.PGSimpleDataContext ctx.dataContext.user=root ctx.dataContext.password=root ctx.dataContext.databaseName=database ctx.dataContext.portNumber=5432 ctx.dataContext.serverName=host ctx.connectionTimeout=30000  Sqlite sbt dependencies libraryDependencies ++= Seq(   ""org.xerial"" % ""sqlite-jdbc"" % ""3.8.11.2"",   ""io.getquill"" %% ""quill-jdbc"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new JdbcContext[SqliteDialect, SnakeCase](""db"") application.properties ctx.driverClassName=org.sqlite.JDBC ctx.jdbcUrl=""jdbc:sqlite:/path/to/db/file.db""  quill-async Transactions The async module provides transaction support based on a custom implicit execution context: ctx.transaction { implicit ec =>   ctx.run(query[Person].delete)   // other transactional code }  The body of transaction can contain calls to other methods and multiple run calls, but the transactional code must be done using the provided implicit execution context. For instance: def deletePerson(name: String)(implicit ec: ExecutionContext) =    ctx.run(query[Person].filter(_.name == lift(name)).delete)  ctx.transaction { implicit ec =>   deletePerson(""John"") }  Depending on how the main execution context is imported, it is possible to produce an ambigous implicit resolution. A way to solve this problem is shadowing the multiple implicits by using the same name: import scala.concurrent.ExecutionContext.Implicits.{ global => ec }  def deletePerson(name: String)(implicit ec: ExecutionContext) =    ctx.run(query[Person].filter(_.name == lift(name)).delete)  ctx.transaction { implicit ec =>   deletePerson(""John"") }  Note that the global execution context is renamed to ec. MySQL Async sbt dependencies libraryDependencies ++= Seq(   ""io.getquill"" %% ""quill-async"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new MysqlAsyncContext[SnakeCase](""db"") application.properties ctx.host=host ctx.port=3306 ctx.user=root ctx.password=root ctx.database=database ctx.poolMaxQueueSize=4 ctx.poolMaxObjects=4 ctx.poolMaxIdle=999999999 ctx.poolValidationInterval=100  Postgres Async sbt dependencies libraryDependencies ++= Seq(   ""io.getquill"" %% ""quill-async"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new PostgresAsyncContext[SnakeCase](""db"") application.properties ctx.host=host ctx.port=5432 ctx.user=root ctx.password=root ctx.database=database ctx.poolMaxQueueSize=4 ctx.poolMaxObjects=4 ctx.poolMaxIdle=999999999 ctx.poolValidationInterval=100  quill-finagle-mysql Transactions The finagle context provides transaction support through a Local value. See twitter util's scaladoc for more details. ctx.transaction {   ctx.run(query[Person].delete)   // other transactional code }  The body of transaction can contain calls to other methods and multiple run calls, since the transaction is automatically propagated through the Local value. sbt dependencies libraryDependencies ++= Seq(   ""io.getquill"" %% ""quill-finagle-mysql"" % ""0.7.1-SNAPSHOT"" )  context definition lazy val ctx = new FinagleMysqlContext[SnakeCase](""db"") application.properties ctx.dest=localhost:3306 ctx.user=root ctx.password=root ctx.database=database ctx.pool.watermark.low=0 ctx.pool.watermark.high=10 ctx.pool.idleTime=5 # seconds ctx.pool.bufferSize=0 ctx.pool.maxWaiters=2147483647  Cassandra Contexts sbt dependencies libraryDependencies ++= Seq(   ""io.getquill"" %% ""quill-cassandra"" % ""0.7.1-SNAPSHOT"" )  synchronous context lazy val ctx = new CassandraSyncContext[SnakeCase](""db"") asynchronous context lazy val ctx = new CassandraAsyncContext[SnakeCase](""db"") stream context lazy val ctx = new CassandraStreamContext[SnakeCase](""db"") The configurations are set using runtime reflection on the Cluster.builder instance. It is possible to set nested structures like queryOptions.consistencyLevel, use enum values like LOCAL_QUORUM, and set multiple parameters like in credentials. application.properties ctx.keyspace=quill_test ctx.preparedStatementCacheSize=1000 ctx.session.contactPoint=127.0.0.1 ctx.session.queryOptions.consistencyLevel=LOCAL_QUORUM ctx.session.withoutMetrics=true ctx.session.withoutJMXReporting=false ctx.session.credentials.0=root ctx.session.credentials.1=pass ctx.session.maxSchemaAgreementWaitSeconds=1 ctx.session.addressTranslater=com.datastax.driver.core.policies.IdentityTranslater  Additional resources Templates In order to quickly start with Quill, we have setup some template projects: Play Framework with Quill JDBC Slick comparison Please refer to SLICK.md for a detailed comparison between Quill and Slick. Cassandra libraries comparison Please refer to CASSANDRA.md for a detailed comparison between Quill and other main alternatives for interaction with Cassandra in Scala. Code of Conduct Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. See CODE_OF_CONDUCT.md for details. License See the LICENSE file for details. Maintainers @fwbrasil @godenji @gustavoamigo @jilen @lvicentesanchez You can notify all maintainers using the handle @getquill/maintainers. Acknowledgments The project was created having Philip Wadler's talk ""A practical theory of language-integrated query"" as its initial inspiration. The development was heavily influenced by the following papers: A Practical Theory of Language-Integrated Query Everything old is new again: Quoted Domain Specific Languages The Flatter, the Better Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/getquill/quill"	"Compile-time Language Integrated Query for Scala"	"true"
"Database"	"ReactiveCouchbase"	"http://reactivecouchbase.org/"	"Reactive Scala Driver for Couchbase. Also includes a Play plug-in. An official plug-in is also in development."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2"	"4"	"2"	"GitHub - ReactiveCouchbase/ReactiveCouchbase-es: Event Store based on ReactiveCouchbase Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 2 Fork 2 ReactiveCouchbase/ReactiveCouchbase-es Code Issues 1 Pull requests 0 Pulse Graphs Event Store based on ReactiveCouchbase http://reactivecouchbase.org/ 16 commits 2 branches 2 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags akka-persistence master Nothing to show 0.3 0.1 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. eventstore/src/main/scala/org/reactivecouchase/eventstore project .gitignore LICENSE.txt NOTICE.txt README.md README.md ReactiveCouchbase Event Store The Couchbase Event store provide a very simple way to store application events. It uses ReactiveCoucbase to work and can be nicely used from within a Play 2/Akka application. You can find more informations about ReactiveCouchbase-core and ReactiveCouchbase-play on their github pages. You can use ReactiveCouchbase-es that way : case class CartCreated(customerId: Long, message: String)  object CartCreated {   val cartCreatedFormat = Json.format[CartCreated] }  object EventSourcingBoostrap {    lazy val driver = ReactiveCouchbaseDriver()    lazy val couchbaseES = CouchbaseEventSourcing( ActorSystem(""couchbase-es-1""), driver.bucket(""es"") )     .registerFormatter(CartCreated.cartCreatedFormat)    def bootstrap() = {     couchbaseES.replayAll()   } }  object Cart {   val cartProcessor = EventSourcingBoostrap.couchbaseES                         .processorOf(Props(new CartProcessor with EventStored))   def createCartForUser(user: User) {     cartProcessor ! Message.create( CartCreated(user.id, ""Useful message"") )   } }  class CartProcessor extends Actor {   // Application state is here !!!   var numberOfCreatedCart = 0   def receive = {     case msg: CartCreated => {       numberOfCreatedCart = numberOfCreatedCart + 1       println( s""[CartProcessor] live carts ${counter} - Last message (${msg.message})"" )     }     case _ =>   } }  val user1 = User( ... ) val user2 = User( ... ) val user3 = User( ... )  EventSourcingBoostrap.bootstrap() // prints nothing if bucket is empty  Cart.createCartForUser( user1 ) // prints : [CartProcessor] live carts 1 - Last message (Useful message) Cart.createCartForUser( user2 ) // prints : [CartProcessor] live carts 2 - Last message (Useful message) Cart.createCartForUser( user3 ) // prints : [CartProcessor] live carts 3 - Last message (Useful message)  EventSourcingBoostrap.bootstrap() // prints : [CartProcessor] live carts 4 - Last message (Useful message) // prints : [CartProcessor] live carts 5 - Last message (Useful message) // prints : [CartProcessor] live carts 6 - Last message (Useful message)  Cart.createCartForUser( user1 ) // prints : [CartProcessor] live carts 7 - Last message (Useful message) Cart.createCartForUser( user2 ) // prints : [CartProcessor] live carts 8 - Last message (Useful message) Cart.createCartForUser( user3 ) // prints : [CartProcessor] live carts 9 - Last message (Useful message)  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ReactiveCouchbase/ReactiveCouchbase-es"	"Reactive Scala Driver for Couchbase. Also includes a Play plug-in. An official plug-in is also in development."	"true"
"Database"	"ReactiveMongo ★ 643 ⧗ 0"	"https://github.com/ReactiveMongo/ReactiveMongo"	"Reactive Scala Driver for MongoDB."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"659"	"83"	"177"	"GitHub - ReactiveMongo/ReactiveMongo: Non-blocking, Reactive MongoDB Driver for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 83 Star 659 Fork 177 ReactiveMongo/ReactiveMongo Code Issues 41 Pull requests 6 Wiki Pulse Graphs Non-blocking, Reactive MongoDB Driver for Scala http://reactivemongo.org 778 commits 14 branches 19 releases Fetching contributors Scala 98.7% Other 1.3% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.10.x 0.10.5.x.akka22 0.10.5.x.akka23 0.11.x bson-refactoring bson2 commands-refactoring gh-pages master readpreferences scala-future scalafuture-leak-fix travis-snapshots travis Nothing to show v0.10.0 0.11.14 0.11.13 0.11.12 0.11.11 0.11.10 0.11.9 0.11.7 0.11.6 0.11.5 0.11.4 0.11.3 0.11.2 0.11.1 0.11.0 0.11.0-M1 0.9 0.8.1 0.8 Nothing to show New pull request Latest commit 70ac46c Jul 15, 2016 cchantep committed on GitHub Merge pull request #528 from cchantep/cursor-refact … Minor cursor and logging refactoring; JMX support Permalink Failed to load latest commit information. .github GitHub template for PR Apr 16, 2016 .travis_scripts Minor cursor and logging refactoring; JMX support; Close #529; Close #… Jul 15, 2016 bson Copy-on-read for the BSONBinary handler Jun 26, 2016 driver Minor cursor and logging refactoring; JMX support; Close #529; Close #… Jul 15, 2016 iteratees/src Network latency testing Mar 7, 2016 jmx/src Minor cursor and logging refactoring; JMX support; Close #529; Close #… Jul 15, 2016 macros/src NodeSet tests, connection and monitor logging Jun 19, 2016 project Minor cursor and logging refactoring; JMX support; Close #529; Close #… Jul 15, 2016 .gitignore ignore .plugins.sbt (for local plugins), update to sbt-0.12.4, sbt-id… Sep 6, 2013 .travis.yml Minor cursor and logging refactoring; JMX support; Close #529; Close #… Jul 15, 2016 CONTRIBUTING.md Scalariform integration Jul 3, 2015 LICENSE.txt License Aug 29, 2012 NOTICE.txt Update license info Mar 3, 2013 README.md NodeSet tests, connection and monitor logging Jun 19, 2016 README.md ReactiveMongo ReactiveMongo is a scala driver that provides fully non-blocking and asynchronous I/O operations. Usage In your project/Build.scala: libraryDependencies ++= Seq(   ""org.reactivemongo"" %% ""reactivemongo"" % ""VERSION"" ) Build manually To benefit from the latest improvements and fixes, you may want to compile ReactiveMongo from source. You will need a Git client and SBT. From the shell, first checkout the source: $ git clone git@github.com:ReactiveMongo/ReactiveMongo.git  Then go to the ReactiveMongo directory and launch the SBT build console: $ cd ReactiveMongo $ sbt > +publish-local  Running tests: In order to execute the unit and integration tests, SBT can be used as follows. sbt test-only -- exclude mongo2  When running against MongoDB 2.6, the command must replace exclude mongo2 with exclude not_mongo26. The test environement must be able to handle the maximum number of incoming connection for the MongoDB instance. This must be checked, and eventually updated, using ulimit -n. Travis: Learn More Complete documentation and tutorials Search or create issues Get help Contribute Samples: These sample applications are kept up to date with the latest driver version. They are built upon Play 2.3. ReactiveMongo Tailable Cursor, WebSocket and Play 2 Full Web Application featuring basic CRUD operations and GridFS streaming Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ReactiveMongo/ReactiveMongo"	"Reactive Scala Driver for MongoDB."	"true"
"Database"	"ReactiveNeo ★ 45 ⧗ 7"	"https://github.com/websudos/reactiveneo"	"Reactive type-safe Scala driver for Neo4J."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"47"	"6"	"6"	"GitHub - outworkers/reactiveneo: Reactive type-safe Scala driver for Neo4J Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 47 Fork 6 outworkers/reactiveneo Code Issues 3 Pull requests 0 Pulse Graphs Reactive type-safe Scala driver for Neo4J http://websudos.github.io/reactiveneo 32 commits 5 branches 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop feature/NEO-3_embedded_neo4j feature/NEO-22_count gh-pages master Nothing to show v0.3.1 Nothing to show New pull request Latest commit 8ddb41d Aug 13, 2015 alexflav23 Merge pull request #18 from websudos/release/filename_fix … Fixing filename clash on Windows machines and releasing 0.3.1 Permalink Failed to load latest commit information. project Fixing filename clash on Windows machines and releasing 0.3.1 Aug 13, 2015 reactiveneo-dsl/src Fixing filename clash on Windows machines and releasing 0.3.1 Aug 13, 2015 reactiveneo-testing/src Upgrading coveralls to autoplugin version. Removing some deprecation … Jul 21, 2015 .gitignore Release/NEO-5: Initial DSL Sep 21, 2014 .travis.yml Adding support for Scala 2.10 and 2.11 May 10, 2015 CONTRIBUTING.md Upgrading util library and improving docs. Nov 13, 2014 LICENSE Update LICENSE Dec 25, 2014 README.md Fixing documentation. Jul 26, 2015 scalastyle-config.xml Release/NEO-5: Initial DSL Sep 21, 2014 tests.sc NEO-106 result parsing Oct 18, 2014 README.md reactiveneo Reactive type-safe Scala DSL for Neo4j Table of contents Getting it Graph modelling Nodes Relationships Indexes Querying The library enforces strong type checks that imposes some restrictions on query format. Every node and relationship used in the query needs to be defined and named. E.g. this kind of query is not supported: MATCH (wallstreet { title:'Wall Street' })<-[r:ACTED_IN]-(actor) RETURN r  Instead you will need to use proper labels for nodes to produce the following query: MATCH (wallstreet:Movie { title:'Wall Street' })<-[r:ACTED_IN]-(actor:Actor) RETURN r  Getting it libraryDependencies ++= Seq(   ""com.websudos"" %% ""reactiveneo-dsl"" % ""0.3.0"",   ""com.websudos"" %% ""reactiveneo-testing"" % ""0.3.0"" ) Graph modelling Back to top Nodes Back to top Domain model class case class Person(name: String, age: Int)  Reactiveneo node definition import com.websudos.reactiveneo.dsl._  class PersonNode extends Node[PersonNode, Person] {    object name extends StringAttribute with Index    object age extends IntegerAttribute    def fromNode(data: QueryRecord): Person = {     Person(name[String](data), age[Int](data))     }  }  Relationships Back to top Reactiveneo relationship definition import com.websudos.reactiveneo.dsl._  class PersonRelation extends Relationship[PersonRelation, Person] {    object name extends StringAttribute with Index    object age extends IntegerAttribute    def fromNode(data: QueryRecord): Person = {     Person(name[String](data), age[Int](data))     }  }  Indexes Back to top Querying Back to top Connection Prerequisite to making Neo4j requests is REST endpoint definition. This is achived using RestConnection class. scala> implicit val service = RestConnection(""localhost"", 7474) service: RestConnection  Making requests In this example all nodes of Person type are returned. scala> val personNodes = Person().returns(case p ~~ _ => p).execute personNodes: Future[Seq[Person]]  The strange construct in the returns function is execution of extractor in the pattern. Pattern defines set of objects that participate in the query. The objects are nodes and relationships. You can also query for specific attributes of a node. scala> val personNames = Person().returns(case p ~~ _ => p.name).execute personNames: Future[Seq[String]]  A query that involves attributes matching. scala> val personNodes = Person(_.name := ""Tom"").returns(case p ~~ _ => p).execute personNodes: Future[Seq[Person]]  Query for a person that has a relationship to another person scala> val personNodes = (Person() :->: Person())                          .returns(case p1 ~~ _ => p).execute personNodes: Future[Seq[Person]]  Query for a person that has a relationship to another person with given name scala> val personNodes = (Person() :->: Person(_.name := ""James""))                          .returns(case p ~~ _ => p).execute personNodes: Future[Seq[Person]]  Query for a person that has a relationship to another person scala> val personNodes = (Person() :<-: WorkRelationship() :->: Person())                          .returns(case p1 ~~ r ~~ p2 ~~ _ => p1).execute personNodes: Future[Seq[Person]]  Query for a person that has a relationship to another person with given name scala> val personNodes = (Person() :-: WorkRelationship(_.company := ""ABC"") :->: Person(_.name := ""John""))                          .returns(case p1 ~~ _ => p1).execute personNodes: Future[Seq[Person]]  An arbitrary Cypher query Cypher is a rich language and whenever you need to use it directly escaping the abstraction layer it's still possible with ReactiveNeo. Use the same REST connection object with an arbitrary Cypher query. scala> val query = ""MATCH (n:Person) RETURN n"" query: String implicit val parser: Reads[Person] = ((__ \ ""name"").read[String] and (__ \ ""age"").read[Int])(Person)  parser: Reads[Person] val result = service.makeRequest[Person](query).execute result: Future[Seq[Person]]  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/websudos/reactiveneo"	"Reactive type-safe Scala driver for Neo4J."	"true"
"Database"	"rediscala ★ 530 ⧗ 0"	"https://github.com/etaty/rediscala"	"Non-blocking, Reactive Redis driver for Scala (with Sentinel support)"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"555"	"39"	"81"	"GitHub - etaty/rediscala: Non-blocking, Reactive Redis driver for Scala (with Sentinel support) Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 39 Star 555 Fork 81 etaty/rediscala Code Issues 28 Pull requests 5 Pulse Graphs Non-blocking, Reactive Redis driver for Scala (with Sentinel support) 353 commits 18 branches 12 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bytestring-deserializer feature/zremrangebylex features/benchmark features/fast-parse-number-fix features/fast-parse-number features/more-bench features/slow-hgetall fix/brpoplpush fix/redis-parse-double-inf fix/sentinels-test fix/slow-multibulk gh-pages master npeters-pool-connection-status npeters-sentinel-pool-slave round-robin-pool stratiator-scripting-conversions travis Nothing to show 1.6.0 1.5.0 1.4.2 1.4.1 1.4.0 1.3.1 1.3 1.2 1.1 1.0 0.6-SNAPSHOT 0.5-SNAPSHOT Nothing to show New pull request Latest commit f1c1101 Apr 1, 2016 etaty Merge pull request #134 from ddworak/master … Fix failing future in RoundRobinPoolRequest Permalink Failed to load latest commit information. benchmark/src/main/scala/redis make Redis.dispatcher configurable and config executeContext to use the Jan 17, 2016 project fix publish scaladoc with ghpages Jan 3, 2016 src fix failing future in RoundRobinPoolRequest Apr 1, 2016 .gitignore Merge branch 'pool-connection-status' of https://github.com/npeters/r… Sep 14, 2015 .travis.yml travis update + fix tests: use awaitAssert Sep 14, 2015 LICENSE.txt Apache License 2.0 Aug 3, 2013 README.md make Redis.dispatcher configurable and config executeContext to use the Jan 17, 2016 version.sbt 1.7.0-SNAPSHOT Jan 17, 2016 README.md rediscala A Redis client for Scala (2.10+) and (AKKA 2.2+) with non-blocking and asynchronous I/O operations. Reactive : Redis requests/replies are wrapped in Futures. Typesafe : Redis types are mapped to Scala types. Fast : Rediscala uses redis pipelining. Blocking redis commands are moved into their own connection. A worker actor handles I/O operations (I/O bounds), another handles decoding of Redis replies (CPU bounds). Set up your project dependencies If you use SBT, you just have to edit build.sbt and add the following: From version 1.3.1: use akka 2.3 released for scala 2.10 & 2.11 // new repo on maven.org libraryDependencies += ""com.github.etaty"" %% ""rediscala"" % ""1.6.0""   // old repo on bintray (1.5.0 and inferior version) resolvers += ""rediscala"" at ""http://dl.bintray.com/etaty/maven"" libraryDependencies += ""com.etaty.rediscala"" %% ""rediscala"" % ""1.5.0"" For older rediscala versions (<= 1.3): use akka 2.2 released for scala 2.10 only use github ""repo"" resolvers += ""rediscala"" at ""https://raw.github.com/etaty/rediscala-mvn/master/releases/""  libraryDependencies += ""com.etaty.rediscala"" %% ""rediscala"" % ""1.3"" Connect to the database import redis.RedisClient import scala.concurrent.Await import scala.concurrent.duration._ import scala.concurrent.ExecutionContext.Implicits.global  object Main extends App {   implicit val akkaSystem = akka.actor.ActorSystem()    val redis = RedisClient()    val futurePong = redis.ping()   println(""Ping sent!"")   futurePong.map(pong => {     println(s""Redis replied with a $pong"")   })   Await.result(futurePong, 5 seconds)    akkaSystem.shutdown() } Basic Example https://github.com/etaty/rediscala-demo You can fork with : git clone git@github.com:etaty/rediscala-demo.git then run it, with sbt run Redis Commands All commands are supported : Keys (scaladoc) Strings (scaladoc) Hashes (scaladoc) Lists non-blocking (scaladoc) blocking (scaladoc) Sets (scaladoc) Sorted Sets (scaladoc) Pub/Sub (scaladoc) Transactions (scaladoc) Connection (scaladoc) Scripting (scaladoc) Server (scaladoc) HyperLogLog (scaladoc) Blocking commands RedisBlockingClient is the instance allowing access to blocking commands : blpop brpop brpopplush   redisBlocking.blpop(Seq(""workList"", ""otherKeyWithWork""), 5 seconds).map(result => {     result.map({       case (key, work) => println(s""list $key has work : ${work.utf8String}"")     })   }) Full example: ExampleRediscalaBlocking You can fork with: git clone git@github.com:etaty/rediscala-demo.git then run it, with sbt run Transactions The idea behind transactions in Rediscala is to start a transaction outside of a redis connection. We use the TransactionBuilder to store call to redis commands (and for each command we give back a future). When exec is called, TransactionBuilder will build and send all the commands together to the server. Then the futures will be completed. By doing that we can use a normal connection with pipelining, and avoiding to trap a command from outside, in the transaction...   val redisTransaction = redis.transaction() // new TransactionBuilder   redisTransaction.watch(""key"")   val set = redisTransaction.set(""key"", ""abcValue"")   val decr = redisTransaction.decr(""key"")   val get = redisTransaction.get(""key"")   redisTransaction.exec() Full example: ExampleTransaction You can fork with : git clone git@github.com:etaty/rediscala-demo.git then run it, with sbt run TransactionsSpec will reveal even more gems of the API. Pub/Sub You can use a case class with callbacks RedisPubSub or extend the actor RedisSubscriberActor as shown in the example below object ExamplePubSub extends App {   implicit val akkaSystem = akka.actor.ActorSystem()    val redis = RedisClient()    // publish after 2 seconds every 2 or 5 seconds   akkaSystem.scheduler.schedule(2 seconds, 2 seconds)(redis.publish(""time"", System.currentTimeMillis()))   akkaSystem.scheduler.schedule(2 seconds, 5 seconds)(redis.publish(""pattern.match"", ""pattern value""))   // shutdown Akka in 20 seconds   akkaSystem.scheduler.scheduleOnce(20 seconds)(akkaSystem.shutdown())    val channels = Seq(""time"")   val patterns = Seq(""pattern.*"")   // create SubscribeActor instance   akkaSystem.actorOf(Props(classOf[SubscribeActor], channels, patterns).withDispatcher(""rediscala.rediscala-client-worker-dispatcher""))  }  class SubscribeActor(channels: Seq[String] = Nil, patterns: Seq[String] = Nil) extends RedisSubscriberActor(channels, patterns) {   override val address: InetSocketAddress = new InetSocketAddress(""localhost"", 6379)    def onMessage(message: Message) {     println(s""message received: $message"")   }    def onPMessage(pmessage: PMessage) {     println(s""pattern message received: $pmessage"")   } } Full example: ExamplePubSub You can fork with : git clone git@github.com:etaty/rediscala-demo.git then run it, with sbt run RedisPubSubSpec will reveal even more gems of the API. Scripting RedisScript is a helper, you can put your LUA script inside and it will compute the hash. You can use it with evalshaOrEval which run your script even if it wasn't already loaded.   val redis = RedisClient()    val redisScript = RedisScript(""return 'rediscala'"")    val r = redis.evalshaOrEval(redisScript).map({     case b: Bulk => println(b.toString())   })   Await.result(r, 5 seconds) Full example: ExampleScripting Redis Sentinel SentinelClient connect to a redis sentinel server. SentinelMonitoredRedisClient connect to a sentinel server to find the master addresse then start a connection. In case the master change your RedisClient connection will automatically connect to the new master server. If you are using a blocking client, you can use SentinelMonitoredRedisBlockingClient Pool RedisClientPool connect to a pool of redis servers. Redis commands are dispatched to redis connection in a round robin way. Master Slave RedisClientMasterSlaves connect to a master and a pool of slaves. The write commands are sent to the master, while the read commands are sent to the slaves in the RedisClientPool Config Which Dispatcher to Use By default, the actors in this project will use the dispatcher rediscala.rediscala-client-worker-dispatcher. If you want to use another dispatcher, just config the implicit value of redisDispatcher: implicit val redisDispatcher = RedisDispatcher(""akka.actor.default-dispatcher"") ByteStringSerializer ByteStringDeserializer ByteStringFormatter ByteStringSerializer ByteStringDeserializer ByteStringFormatter case class DumbClass(s1: String, s2: String)  object DumbClass {   implicit val byteStringFormatter = new ByteStringFormatter[DumbClass] {     def serialize(data: DumbClass): ByteString = {       //...     }      def deserialize(bs: ByteString): DumbClass = {       //...     }   } } //...    val dumb = DumbClass(""s1"", ""s2"")    val r = for {     set <- redis.set(""dumbKey"", dumb)     getDumbOpt <- redis.get[DumbClass](""dumbKey"")   } yield {     getDumbOpt.map(getDumb => {       assert(getDumb == dumb)       println(getDumb)     })   } Full example: ExampleByteStringFormatter Scaladoc Rediscala scaladoc API (latest version 1.6) Rediscala scaladoc API (version 1.5) Rediscala scaladoc API (version 1.4) Rediscala scaladoc API (version 1.3) Rediscala scaladoc API (version 1.2) Rediscala scaladoc API (version 1.1) Rediscala scaladoc API (version 1.0) Performance More than 250 000 requests/second benchmark result from scalameter sources directory The hardware used is a macbook retina (Intel Core i7, 2.6 GHz, 4 cores, 8 threads, 8GB) running the sun/oracle jvm 1.6 You can run the bench with : clone the repo git clone git@github.com:etaty/rediscala.git run sbt bench:test open the bench report rediscala/tmp/report/index.html Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/etaty/rediscala"	"Non-blocking, Reactive Redis driver for Scala (with Sentinel support)"	"true"
"Database"	"Relate ★ 84 ⧗ 2"	"https://github.com/lucidsoftware/relate"	"Lightweight, blazing-fast database access layer for Scala that abstracts the idiosyncricies of the JDBC while keeping complete control over the SQL."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"94"	"28"	"9"	"GitHub - lucidsoftware/relate: Performant database access in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 28 Star 94 Fork 9 lucidsoftware/relate Code Issues 3 Pull requests 1 Wiki Pulse Graphs Performant database access in Scala http://lucidsoftware.github.io/relate/ 212 commits 17 branches 18 releases Fetching contributors Scala 99.8% Makefile 0.2% Scala Makefile Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.1.x add-as-multimap gh-pages integration-test it-clean-up master merge-with-benchmarks parseable-type-class pauldraper-from-boolean pauldraper-travis query-timeouts release-1.0 robust-implicit-parsing seq-to-iterable support-parseable-interp v1.6.x v1.7.x Nothing to show v1.13.0 v1.12.0 v1.11.0 v1.10.0 v1.9.0 v1.8.0 v1.7.1 v1.7.0 v1.6.1 v1.6.0 v1.4 v1.3 v1.2 v1.1.1 v1.1 v1.0 1.6.1 1.5 Nothing to show New pull request Latest commit c97da3d Jul 8, 2016 msiebert committed on GitHub Update README.md Permalink Failed to load latest commit information. project Revert ""fixes regression build"" Feb 22, 2016 src .gitignore .travis.yml Move and update benchmarks from relate-benchmarks Feb 3, 2016 LICENSE README.md build.sbt makefile README.md Relate http://lucidsoftware.github.io/relate/ Relate is a lightweight, blazingly fast database access layer for Scala that abstracts the idiosyncricies of the JDBC while keeping complete control over the SQL. Examples val ids = Seq(1, 2, 3) sql""SELECT email FROM users WHERE id in ($ids)"".asMap { row =>   row.long(""id"") -> row.string(""email"") } val id = 4 val email = ""github@lucidchart.com"" sql""INSERT INTO users VALUES ($id, $email)"".execute() Continue to Documentation Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lucidsoftware/relate"	"Lightweight, blazing-fast database access layer for Scala that abstracts the idiosyncricies of the JDBC while keeping complete control over the SQL."	"true"
"Database"	"rethink-scala ★ 87 ⧗ 3"	"https://github.com/kclay/rethink-scala"	"Scala Driver for RethinkDB"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"94"	"9"	"21"	"GitHub - kclay/rethink-scala: Scala Driver for RethinkDB Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 9 Star 94 Fork 21 kclay/rethink-scala Code Issues 12 Pull requests 0 Wiki Pulse Graphs Scala Driver for RethinkDB 319 commits 11 branches 10 releases Fetching contributors Scala 87.5% Protocol Buffer 10.2% Java 2.2% Shell 0.1% Scala Protocol Buffer Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.15 async backend catchup change_feed channel_retrival lifted master streams upgrade-1.11 v2 Nothing to show v0.4.9 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4 v0.3 v0.2 Nothing to show New pull request Latest commit e5ae88f Feb 2, 2016 kclay Setting version to 0.4.10-SNAPSHOT Permalink Failed to load latest commit information. core Cleaning up api Nov 19, 2015 lib Migrate to SBT 0.13.8, Scala 2.11.7, and JVM 1.8. Perform some genera… Nov 8, 2015 lifted/src/main/scala/com/rethinkscala/lifted Cleaning up api Mar 26, 2014 project Getting ready for 0.4.9 Release Feb 2, 2016 scripts Migrate to SBT 0.13.8, Scala 2.11.7, and JVM 1.8. Perform some genera… Nov 9, 2015 .gitignore added eclipse support Apr 22, 2015 .travis.yml - Fixing travis config Aug 23, 2014 LICENSE Renaming project Jun 16, 2013 README.md Getting ready for 0.4.9 Release Feb 2, 2016 Vagrantfile - Adding return_changes support Aug 28, 2014 version.sbt Setting version to 0.4.10-SNAPSHOT Feb 2, 2016 README.md Scala Rethinkdb Driver Work in progress for RethinkDB 2.0 release. FEATURES Full Type Safety (still a work in progress, will use macros to support case class type safety, right now all queries should be typed checked against the rules of RQL, ) Mapping to and from case classes , this allows you to fetch data to an case class via .as[CaseClass] or insert data from case classes (will be translated to a Map[String,_] Lazy evaluation, all expressions are evaluated in a lazy fashion, meaning that the ast will be build up but the inner args and optargs wont be resolved until .run/.as or .ast is called for performance. Importing com.rethinkscala.Implicits._ will give you a more normal way to construct your rql so you can write normal scala code without worrying about casting in a Typed or via Expr Uses Jackson for json mapping via jackson-module-scala Guide Getting Started Type Safety TODO Complete Test Suite Fix compile warns Allow type safety for Predicate classes via macros SBT Users val main = Project(....).settings(resolvers ++= Seq(""RethinkScala Repository"" at ""http://kclay.github.io/releases""))  val rethinkscala = ""com.rethinkscala"" %% ""core"" % ""0.5.0"", val rethinkscala = ""com.rethinkscala"" %% ""core"" % ""0.5.1-SNAPSHOT"" To get started import com.rethinkscala.Blocking._ // for blocking api implicit val blockingConnection = Blocking(Version3) import com.rethinkscala.Async._ // for async api implicit val asyncConnection = Async(Version3) Examples scala> r.table(""marvel"").map(hero=> hero \ ""combatPower"" + hero \ ""combatPower"" * 2) res2: com.rethinkscala.ast.RMap = RMap(Table(marvel,None,None),Predicate1(<function1>))   scala> import com.rethinkscala._ import com.rethinkscala._  scala> val version =new Version3(""172.16.2.45"") version: com.rethinkscala.net.Version3 = Version3(172.16.2.45,28015,None,5)  scala> implicit val connection = new Connection(version) connection: com.rethinkscala.Connection = Connection(Version2(172.16.2.45,28015,None,5))  scala> val info =DB(""foo"").table(""bar"").info info: com.rethinkscala.ast.Info = Info(Table(bar,Some(false),Some(DB(foo))))  //case class DBResult(name: String, @JsonProperty(""type"") kind: String) extends Document //case class TableInfoResult(name: String, @JsonProperty(""type"") kind: String, db: DBResult) extends Document  scala> val result = info.as[TableInfoResult] result: Either[com.rethinkscala.net.RethinkError,com.rethinkscala.net.TableInfoResult] = Right(TableInfoResult(bar,TABLE,DBResult(test,DB)))  // selecting data scala> r.db(""test"").table(""foos"").create.run res1: Either[com.rethinkscala.net.RethinkError,Boolean] = Right(true)  scala> val table = r.db(""test"").table(""foos"") table: com.rethinkscala.ast.Table = Table(foos,Some(false),Some(DB(test)))  scala> val records = for(i <-1 to  5) yield SelectFoo(i) records: scala.collection.immutable.IndexedSeq[SelectFoo] = Vector(SelectFoo(1), SelectFoo(2), SelectFoo(3), SelectFoo(4), SelectFoo(5))  scala> table.insert(records).run res2: Either[com.rethinkscala.net.RethinkError,com.rethinkscala.net.InsertResult] = Right(InsertResult(5,0,0,0,None,null,0,0))  scala> val results = table.between(2,4).order(""id"").as[SelectFoo] results: Either[com.rethinkscala.net.RethinkError,Seq[SelectFoo]] = Right(Cursor(SelectFoo(2), SelectFoo(3), SelectFoo(4)))  scala> val results = table.filter(f=> f \ ""id""> 2).as[SelectFoo] results: Either[com.rethinkscala.net.RethinkError,Seq[SelectFoo]] = Right(Cursor(SelectFoo(3), SelectFoo(5), SelectFoo(4)))  scala> val results = table.filter(f=> f \ ""id""> 2).order(""id"".desc).as[SelectFoo] results: Either[com.rethinkscala.net.RethinkError,Seq[SelectFoo]] = Right(Cursor(SelectFoo(5), SelectFoo(4), SelectFoo(3)))   More found at here Be sure to check out the wiki Installation Note needs protobuf 2.5.0 installed Checkout Main repo sudo apt-get install protobuf-compiler git clone git@github.com:kclay/rethink-scala.git cd rethink-scala sbt compile  Version 0.4.9 - 12/07/15 Update to Scala 2.11.7, SBT 0.13.8, and target JVM 1.8 (thanks @crockpotveggies) Added scalaz-stream support Refactored connections to have Backends, this will allow switching connection/stream implementations Updated to support rethink API 2.0 0.4.6 - 05/12/15 Polymorphism support #23 No need to extend Document anymore #19 Breaking Change - table.getAll(indes:String,attr:Any) is now table.getAll(attr:Any).withIndex(index:String) Breaking Change - Updated table.indexCreate definition Support for Continuing a stream Added functional blocking delegate which uses Try Updated Netty.io to 4.0.27.Final Fixed issues with connection pooling where results wouldn't resolve and timeout. Breaking Change - casting an Var to map is now done with mapOf[T] Made connection setup sequence fully async Blocking._ and Async._ now has Version3 visible Added more examples/tutorial (thanks @Shegnc ) ###0.4.6 - 01/29/15 Geo Support Uuid Support Fixed restoring connection on failed quries (thanks @mick-h) Fixed Option support (thanks @mick-h) Fixed issue were null results were not handle properly (thanks @mick-h) Fixed nested document insert (thanks @maohde) Fixed exception being thrown when query returns back empty results (thanks @mick-h) 0.4.5 - 10/12/14 Fixing Nested documents for json Fixed connection pooling Minor bug fixes 0.4.4 - 8/26/214 JSON Support Start of HTTP Support Updated to support 1.13.x 0.4.3 - 9/24/13 Added zip bug fixes 0.4.2 - 08/12/13 Fixed update and filter quries 0.4 - 08/09/13 Fixed race conditions in connection pool Switched to using protobuf-java 2.4.1 due to compatibility issues with akka-cluster 2.2.0 minor bug fixes 0.3 - 07/28/13 A com.rethinkscala.Schema object can be created , this provides some helper methods to provide type-saftey for tables The map representation of the query is returned as well if cased to com.rethinkscala.net.Document and is accessable by using a a com.rethinkscala.net.DocPath Active record type update and saves are not available, importing you schema into scope provides your case classes with an save and update method, update does a full replace for now Case classes can now have their id automatically generated by providing a id:Option[String] = None Documents now have a beforeInsert and afterInsert lifecycle, afterInsert/afterInsert(generatedKey:String) Driver now supports 1.7.x which allows you to fetch the new_vals for single entry update/insert via ChangeResult.resultValue[T] minor bug fixes 0.2 - 07/04/13 Streams are now supported, this means that when using methods like .getAll and between the results will be wrapped in a com.rethinkscala.net.Cursor[R] which will act like an Seq[R] 0.1 - Initial release Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/kclay/rethink-scala"	"Scala Driver for RethinkDB"	"true"
"Database"	"Salat ★ 474 ⧗ 9"	"https://github.com/salat/salat/"	"ORM for MongoDB. A related Play-plugin is also available."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"476"	"60"	"103"	"GitHub - salat/salat: Salat is a simple serialization library for case classes. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 60 Star 476 Fork 103 salat/salat Code Issues 50 Pull requests 7 Wiki Pulse Graphs Salat is a simple serialization library for case classes. https://github.com/salat/salat 750 commits 16 branches 18 releases Fetching contributors Scala 98.0% Java 2.0% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.0.8_2.9.0-1 1.9.x-branch ALTERNATE_REALITY casbah_2.3.0-RC1 etorreborre-master gh-1-debug gh-pages master proxy-grater salat-cockaigne salat_2.10.RC3 salat_2.10 scala_2.9.x_backport scoverage topic/renovate v0.0.7-errata Nothing to show v1.9.10 v1.9.5 v1.9.5_backport v1.9.4 v1.9.3 v1.9.3_backport v1.9.2 v1.9.0 v0.0.8_2.9.x v0.0.8 v0.0.7 v0.0.5 v0.0.4 v0.0.3 v0.0.2 v0.0.1 1.9.9 1.9.1 Nothing to show New pull request Latest commit fa787e1 Jul 13, 2016 noahlz 1.9.10 release notes. Permalink Failed to load latest commit information. notes 1.9.10 release notes. Jul 13, 2016 project Issue #142 Update project file for new project coordinates and copyri… May 31, 2016 salat-core/src Issuer #137 - Fix. Also, minor refactoring of surrounding (redundant)… Oct 28, 2014 salat-util/src Add Scala 2.11 support! Updated dependencies, replace some deprecated… Apr 23, 2014 src/main/ls Prepare for 1.9.1 release Aug 29, 2012 .gitignore Added JSON support for BSONTimestamp type. Aug 28, 2012 .travis.yml Update TravisCI build configuration: nzucker@gmail.com recipient and … Jul 11, 2016 LICENSE.md Update LICENSE.md Jun 21, 2013 README.md New stable version is 1.9.10 Jul 13, 2016 README.md Salat Salat is a simple serialization library for case classes. Salat currently supports bidirectional serialization for: MongoDB's DBObject (using casbah) JSON (using JSON4S) maps Goals Simplicity. Flexibility. Consistency. Your model there and back again should just work. Get Salat Salat publishes snapshots and releases to OSS Sontatype. Stable Release Available for Scala 2.10 and 2.11. Based on Casbah 2.7.1. ""com.novus"" %% ""salat"" % ""1.9.10""  Release Notes Snapshot Available for Scala 2.10 and 2.11. Based on Casbah 2.7.1. ""com.novus"" %% ""salat"" % ""2.0.0-SNAPSHOT""  Release Notes (In Progress) Legacy support Please remove all references to repo.novus.com from your build files. After 0.0.8, Salat will be hosted exclusively by Sonatype. If you are not using sbt 0.11.2+, explicitly add OSS Sonatype to your resolvers: resolvers += ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots""  Scala 2.9.3 Based on Casbah 2.7.0. ""com.novus"" %% ""salat"" % ""1.9.7""  Scala 2.9.2 Based on Casbah 2.6.4. ""com.novus"" %% ""salat"" % ""1.9.5""  Scala 2.8.1 Based on Casbah 2.1.5-1. ""com.novus"" %% ""salat"" % ""0.0.8""  Release Notes Play 2 plugin Are you using Play framework? Make sure to see our Play support wiki page, and check out Leon Radley's plugin at leon/play-salat. Documentation See the wiki and the mailing list. What does Salat support? See Supported Types. What doesn't Salat support? We don't have the resources to support everything. Here are some things Salat doesn't do: Java compatibility non-case classes type aliases nested inner classes varags arrays multiple constructors tuples Option containing a collection (see collection support for workarounds) relationship management like a traditional ORM How does Salat work? Salat uses the Product trait implemented by case classes with the hi-fi type information found in pickled Scala signatures. Details are thin on the ground, but here's where we got started: the source code for scala.tools.scalap.scalax.rules.scalasig.ScalaSigParser SID # 10 (draft) - Storage of pickled Scala signatures in class files Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/salat/salat/"	"ORM for MongoDB. A related Play-plugin is also available."	"true"
"Database"	"Scala ActiveRecord ★ 263 ⧗ 0"	"https://github.com/aselab/scala-activerecord"	"ORM library for scala, inspired by ActiveRecord of Ruby on Rails."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"268"	"35"	"28"	"GitHub - aselab/scala-activerecord: ActiveRecord-like ORM library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 35 Star 268 Fork 28 aselab/scala-activerecord Code Issues 17 Pull requests 0 Wiki Pulse Graphs ActiveRecord-like ORM library for Scala 456 commits 3 branches 8 releases 6 contributors Scala 97.4% HTML 1.6% Other 1.0% Scala HTML Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags custom-type-support master nscala-time-support Nothing to show 0.3.2 0.3.1 0.3.0 0.2.3 0.2.2 0.2.1 0.2.0 0.1 Nothing to show New pull request Latest commit 46e1b47 Jul 15, 2016 y-yoshinoya committed on GitHub Update README.md Permalink Failed to load latest commit information. activerecord/src migration from Play Plugin to Play Module. bump Play version (2.5.x) Jul 4, 2016 bin bump sbt version Jun 24, 2016 generator/src bump scalastyle version Jun 24, 2016 macro/src/main #11 strict type check for Option values Mar 3, 2015 notes add 0.3.2 release note Jun 27, 2016 play2 Update README.md Jul 15, 2016 play2Sbt/src fixed library versions Jul 1, 2016 play2Specs/src/main/scala Play 2.4 support Jul 1, 2015 project migration from Play Plugin to Play Module. bump Play version (2.5.x) Jul 4, 2016 scalatra fixed library versions Jul 1, 2016 scalatraSbt/src fixed library versions Jul 1, 2016 specs/src fixed test:compile error Mar 3, 2015 .gitignore first commit Apr 10, 2012 CREDITS.txt Switched to Rails naming conventions Oct 21, 2012 MIT-LICENSE.txt add credit and license file Apr 11, 2012 README.md Update README.md Jul 15, 2016 scalastyle-config.xml fixed scalastyle warnings Sep 11, 2014 README.md Scala ActiveRecord scala-activerecord is an ORM library for Scala. This library is inspired by ActiveRecord of Ruby on Rails. It is designed following the CoC(Convention over Configuration), DRY(Don't Repeat Yourself) principles. Minimal example Model implementation: package models  import com.github.aselab.activerecord._ import com.github.aselab.activerecord.dsl._  case class Person(name: String, age: Int) extends ActiveRecord  object Person extends ActiveRecordCompanion[Person] Schema definition: package models  import com.github.aselab.activerecord._ import com.github.aselab.activerecord.dsl._  object Tables extends ActiveRecordTables {   val people = table[Person] } ActiveRecord model usage: import com.github.aselab.activerecord.dsl._ import models._  object App extends App {   Tables.initialize    Person(""person1"", 25).save   Person(""person2"", 18).save   Person(""person3"", 40).save   Person(""person4"", 18).save    Person.findBy(""name"", ""person1"") //=> Some(Person(""person1"", 25))   Person.findBy(""age"", 55) //=> None   Person.findAllBy(""age"", 18).toList //=> List(Person(""person2"", 18), Person(""person4"", 18))   Person.where(_.age.~ >= 20).orderBy(_.age desc).toList //=> List(Person(""person3"", 40), Person(""person1"", 25))    Tables.cleanup } Schema and query DSL is based on Squeryl. Features Auto connection management Composable query operation Callback Validation Association Documents and other resources Wiki ScalaDoc Sample project CloudBees(CI) Web framework support Play 2.x plugin Scalatra 2.4.0 plugin License MIT Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/aselab/scala-activerecord"	"ORM library for scala, inspired by ActiveRecord of Ruby on Rails."	"true"
"Database"	"scala-redis ★ 623 ⧗ 3"	"https://github.com/debasishg/scala-redis"	"A Scala library for connecting to a redis server, with clustering support"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"652"	"58"	"184"	"GitHub - debasishg/scala-redis: A scala library for connecting to a redis server, or a cluster of redis nodes using consistent hashing on the client side. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 58 Star 652 Fork 184 debasishg/scala-redis forked from acrosa/scala-redis Code Issues 37 Pull requests 10 Wiki Pulse Graphs A scala library for connecting to a redis server, or a cluster of redis nodes using consistent hashing on the client side. 276 commits 11 branches 22 releases 37 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags auth-fix fix-134 master pre-scala-2.10 redis-1.x redis-2-old redis-2 redis-akka-io scala-redis-sentinel wip-change-hgetall wip-evalsha Nothing to show v3.1 v3.0 v2.15 v2.14 v2.13 v2.11 v2.9 v2.9-pre-scala-2.10 v2.9-2.10.0 v2.8 v2.7 v2.4.2 v2.4.0 v2.2.3 v1.4_1.2.6_2.8.0.RC7 v1.3_2.8.0.Beta1 v1.2_2.8.0.Beta1 v1.2 v1.2-SNAPSHOT_2.8.0.Beta1 v1.1 v1.0.1_2.8.0.Beta1 v1.0.1 Nothing to show New pull request Pull request Compare This branch is 268 commits ahead, 26 commits behind acrosa:master. Latest commit cbeb107 Mar 5, 2016 debasishg Merge pull request #163 from pengfei-xue/master … add support for command zrangebylex Permalink Failed to load latest commit information. project Upped scala version to 2.11.7 Dec 6, 2015 src add support for command zrangebylex Mar 2, 2016 .gitignore fixes issue #18 #18 (reconnect after idle time) - implemented retry i… Jun 30, 2012 README.md Upgraded version & changed README May 10, 2015 README.md Redis Scala client Key features of the library Native Scala types Set and List responses. Transparent serialization Connection pooling Consistent Hashing on the client. Support for Clustering of Redis nodes. Information about redis Redis is a key-value database. It is similar to memcached but the dataset is not volatile, and values can be strings, exactly like in memcached, but also lists and sets with atomic operations to push/pop elements. http://redis.io Key features of Redis Fast in-memory store with asynchronous save to disk. Key value get, set, delete, etc. Atomic operations on sets and lists, union, intersection, trim, etc. Requirements sbt (get it at http://www.scala-sbt.org/) Installation Add to Build.scala or build.sbt libraryDependencies ++= Seq(     ""net.debasishg"" %% ""redisclient"" % ""3.0"" ) Usage Start your redis instance (usually redis-server will do it) $ cd scala-redis $ sbt > update > console  And you are ready to start issuing commands to the server(s) Redis 2 implements a new protocol for binary safe commands and replies Let us connect and get a key: scala> import com.redis._ import com.redis._  scala> val r = new RedisClient(""localhost"", 6379) r: com.redis.RedisClient = localhost:6379  scala> r.set(""key"", ""some value"") res3: Boolean = true  scala> r.get(""key"") res4: Option[String] = Some(some value)  Let us try out some List operations: scala> r.lpush(""list-1"", ""foo"") res0: Option[Int] = Some(1)  scala> r.rpush(""list-1"", ""bar"") res1: Option[Int] = Some(2)  scala> r.llen(""list-1"") res2: Option[Int] = Some(2)  Let us look at some serialization stuff: scala> import serialization._ import serialization._  scala> import Parse.Implicits._ import Parse.Implicits._  scala> r.hmset(""hash"", Map(""field1"" -> ""1"", ""field2"" -> 2)) res0: Boolean = true  scala> r.hmget[String,String](""hash"", ""field1"", ""field2"") res1: Option[Map[String,String]] = Some(Map(field1 -> 1, field2 -> 2))  scala> r.hmget[String,Int](""hash"", ""field1"", ""field2"") res2: Option[Map[String,Int]] = Some(Map(field1 -> 1, field2 -> 2))  scala> val x = ""debasish"".getBytes(""UTF-8"") x: Array[Byte] = Array(100, 101, 98, 97, 115, 105, 115, 104)  scala> r.set(""key"", x) res3: Boolean = true  scala> import Parse.Implicits.parseByteArray import Parse.Implicits.parseByteArray  scala> val s = r.get[Array[Byte]](""key"") s: Option[Array[Byte]] = Some([B@6e8d02)  scala> new String(s.get) res4: java.lang.String = debasish  scala> r.get[Array[Byte]](""keey"") res5: Option[Array[Byte]] = None  Using Client Pooling scala-redis is a blocking client, which serves the purpose in most of the cases since Redis is also single threaded. But there may be situations when clients need to manage multiple RedisClients to ensure thread-safe programming. scala-redis includes a Pool implementation which can be used to serve this purpose. Based on Apache Commons Pool implementation, RedisClientPool maintains a pool of instances of RedisClient, which can grow based on demand. Here's a sample usage .. val clients = new RedisClientPool(""localhost"", 6379) def lp(msgs: List[String]) = {   clients.withClient {     client => {       msgs.foreach(client.lpush(""list-l"", _))       client.llen(""list-l"")     }   } } Using a combination of pooling and futures, scala-redis can be throttled for more parallelism. This is the typical recommended strategy if you are looking forward to scale up using this redis client. Here's a sample usage .. we are doing a parallel throttle of an lpush, rpush and set operations in redis, each repeated a number of times .. If we have a pool initialized, then we can use the pool to repeat the following operations. // lpush def lp(msgs: List[String]) = {   clients.withClient {     client => {       msgs.foreach(client.lpush(""list-l"", _))       client.llen(""list-l"")     }   } }  // rpush def rp(msgs: List[String]) = {   clients.withClient {     client => {       msgs.foreach(client.rpush(""list-r"", _))       client.llen(""list-r"")     }   } }  // set def set(msgs: List[String]) = {   clients.withClient {     client => {       var i = 0       msgs.foreach { v =>         client.set(""key-%d"".format(i), v)         i += 1       }       Some(1000)     }   } } And here's the snippet that throttles our redis server with the above operations in a non blocking mode using Scala futures: val l = (0 until 5000).map(_.toString).toList val fns = List[List[String] => Option[Int]](lp, rp, set) val tasks = fns map (fn => scala.actors.Futures.future { fn(l) }) val results = tasks map (future => future.apply()) Implementing asynchronous patterns using pooling and Futures scala-redis is a blocking client for Redis. But you can develop high performance asynchronous patterns of computation using scala-redis and Futures. RedisClientPool allows you to work with multiple RedisClient instances and Futures offer a non-blocking semantics on top of this. The combination can give you good numbers for implementing common usage patterns like scatter/gather. Here's an example that you will also find in the test suite. It uses the scatter/gather technique to do loads of push across many lists in parallel. The gather phase pops from all those lists in parallel and does some compuation over them. Here's the main routine that implements the pattern: // set up Executors val system = ActorSystem(""ScatterGatherSystem"") import system.dispatcher  val timeout = 5 minutes  private[this] def flow[A](noOfRecipients: Int, opsPerClient: Int, keyPrefix: String,   fn: (Int, String) => A) = {   (1 to noOfRecipients) map {i =>     Future {       fn(opsPerClient, ""list_"" + i)     }   } }  // scatter across clients and gather them to do a sum def scatterGatherWithList(opsPerClient: Int)(implicit clients: RedisClientPool) = {   // scatter   val futurePushes = flow(100, opsPerClient, ""list_"", listPush)    // concurrent combinator: Future.sequence   val allPushes = Future.sequence(futurePushes)    // sequential combinator: flatMap   val allSum = allPushes flatMap {result =>     // gather     val futurePops = flow(100, opsPerClient, ""list_"", listPop)     val allPops = Future.sequence(futurePops)     allPops map {members => members.sum}   }   Await.result(allSum, timeout).asInstanceOf[Long] }  // scatter across clietns and gather the first future to complete def scatterGatherFirstWithList(opsPerClient: Int)(implicit clients: RedisClientPool) = {   // scatter phase: push to 100 lists in parallel   val futurePushes = flow(100, opsPerClient, ""seq_"", listPush)    // wait for the first future to complete   val firstPush = Future.firstCompletedOf(futurePushes)    // do a sum on the list whose key we got from firstPush   val firstSum = firstPush map {key =>     listPop(opsPerClient, key)   }   Await.result(firstSum, timeout).asInstanceOf[Int] } License This software is licensed under the Apache 2 license, quoted below. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/debasishg/scala-redis"	"A Scala library for connecting to a redis server, with clustering support"	"true"
"Database"	"ScalikeJDBC ★ 583 ⧗ 0"	"https://github.com/scalikejdbc/scalikejdbc"	"A tidy SQL-based DB access library for Scala developers."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"626"	"81"	"111"	"GitHub - scalikejdbc/scalikejdbc: A tidy SQL-based DB access library for Scala developers. This library naturally wraps JDBC APIs and provides you easy-to-use APIs. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 81 Star 626 Fork 111 scalikejdbc/scalikejdbc Code Issues 16 Pull requests 2 Wiki Pulse Graphs A tidy SQL-based DB access library for Scala developers. This library naturally wraps JDBC APIs and provides you easy-to-use APIs. http://scalikejdbc.org/ 1,785 commits 11 branches 115 releases 42 contributors Scala 98.0% Shell 1.1% Batchfile 0.9% Scala Shell Batchfile Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.7.x 1.8.x 2.0.x 2.1.x 2.2.x 2.3.x develop feature/asis feature/issue100 feature/optional_sqlsyntax master Nothing to show 2.4.2 2.4.1 2.4.0 2.4.0.RC2 2.4.0.RC1 2.3.5 2.3.4 2.3.3 2.3.2 2.3.1 2.3.0 2.2.9 2.2.8 2.2.7 2.2.6 2.2.5 2.2.4 2.2.3 2.2.2 2.2.1 2.2.0 2.1.4 2.1.3 2.1.2 2.1.1 2.1.0 2.1.0.RC2 2.1.0.RC1 2.0.7 2.0.6 2.0.5 2.0.4 2.0.3 2.0.2 2.0.1 2.0.0 2.0.0-beta3 2.0.0-beta2 2.0.0-beta1 2.0.0-RC3 2.0.0-RC2 1.8.2 1.8.1 1.8.0 1.7.7 1.7.6 1.7.5 1.7.4 1.7.3 1.7.2 1.7.1 1.7.0 1.6.11 1.6.10 1.6.9 1.6.8 1.6.7 1.6.6 1.6.5 1.6.4 1.6.3 1.6.2 1.6.1 1.6.0 1.5.4 1.5.3 1.5.2 1.5.1 1.5.0 1.4.8 1.4.7 1.4.6 1.4.5 1.4.4 1.4.3 1.4.2 1.4.1 1.4.0 1.3.7 1.3.6 1.3.5 1.3.4 1.3.3 1.3.2 1.3.1 1.3.0 1.2.3 1.2.2 1.2.1 1.2.0 1.1.1 1.1.0 1.0.3 1.0.2 1.0.1 1.0.0 0.6.7 0.6.6 0.6.5 0.6.4 Nothing to show New pull request Latest commit 368ad40 Jun 14, 2016 seratch Fix MiMa settings Permalink Failed to load latest commit information. notes version 2.4.2 Jun 11, 2016 project Fix MiMa settings Jun 14, 2016 sandbox SNAPSHOT Jun 11, 2016 scalikejdbc-cli Scala 2.11.8 Mar 31, 2016 scalikejdbc-config Fix #494 Allow driver absence when using scalikejdbc-config Feb 20, 2016 scalikejdbc-core Merge pull request #528 from Tolsi/daemon-thread-in-connection-pool-obj May 30, 2016 scalikejdbc-interpolation-macro Remove copyright from source code Mar 18, 2015 scalikejdbc-interpolation Convert timezone too when using ParameterBinderFactory which fixes #529 May 25, 2016 scalikejdbc-jsr310 remove unused imports Nov 17, 2015 scalikejdbc-library Remove unncessary ScalaBigIntegerConverter Jan 25, 2016 scalikejdbc-mapper-generator-core Intoroduce ParameterBinderFactory for type conversion hook Apr 21, 2016 scalikejdbc-mapper-generator Merge pull request #510 from xuwei-k/issue509-view Mar 31, 2016 scalikejdbc-syntax-support-macro Intoroduce ParameterBinderFactory for type conversion hook Apr 20, 2016 scalikejdbc-test Remove copyright from source code Mar 18, 2015 scripts Fix scripts and README May 15, 2016 tools-support/intellij/templates scalikejdbc.SQLInterpolation is deprecated Dec 31, 2014 .gitignore Added templates for Intellij IDEA Jul 8, 2013 .travis.yml Scala 2.12.0-M4 Apr 25, 2016 CONTRIBUTING.md Fix scripts and README May 15, 2016 LICENSE.txt Update copyright Feb 28, 2015 logo.png Updated logo Jun 16, 2014 readme.md Fix scripts and README May 15, 2016 team-rules.md Update team-rules.md Mar 17, 2016 travis.sh Scala 2.11.8 Mar 30, 2016 travis_before.sh should not execute ""git add"" multiple times Mar 21, 2015 readme.md ScalikeJDBC Just write SQL and get things done! ScalikeJDBC is a tidy SQL-based DB access library for Scala that naturally wraps JDBC and provides easy-to-use APIs. ScalikeJDBC is practical and production-ready. Use this library for your real projects. http://scalikejdbc.org/ Gitter Chat for Casual Q&A English: 日本語 (Japanese): Getting Started Just add ScalikeJDBC, a JDBC driver, and an slf4j implementation to your sbt build settings: libraryDependencies ++= Seq(   ""org.scalikejdbc"" %% ""scalikejdbc""        % ""2.4.+"",   ""com.h2database""  %  ""h2""                 % ""1.4.+"",   ""ch.qos.logback""  %  ""logback-classic""    % ""1.1.+"" ) If you're a Play2 user, take a look at play-support project, too: https://github.com/scalikejdbc/scalikejdbc-play-support First example After adding the above dependencies to your build.sbt, run sbt console and execute the following code: import scalikejdbc._  // initialize JDBC driver & connection pool Class.forName(""org.h2.Driver"") ConnectionPool.singleton(""jdbc:h2:mem:hello"", ""user"", ""pass"")  // ad-hoc session provider on the REPL implicit val session = AutoSession  // table creation, you can run DDL statements using #execute as with JDBC sql"""""" create table members (   id serial not null primary key,   name varchar(64),   created_at timestamp not null ) """""".execute.apply()  // insert initial data Seq(""Alice"", ""Bob"", ""Chris"") foreach { name =>   sql""insert into members (name, created_at) values (${name}, current_timestamp)"".update.apply() }  // retrieve all data as a List of Map elements val entities: List[Map[String, Any]] = sql""select * from members"".map(_.toMap).list.apply()  // define an entity object and extractor import org.joda.time._ case class Member(id: Long, name: Option[String], createdAt: DateTime) object Member extends SQLSyntaxSupport[Member] {   override val tableName = ""members""   def apply(rs: WrappedResultSet): Member = new Member(     rs.long(""id""), rs.stringOpt(""name""), rs.jodaDateTime(""created_at"")) }  // find all Members val members: List[Member] = sql""select * from members"".map(rs => Member(rs)).list.apply() How did it go? If you'd like to know more details or see more practical examples, see the full documentation at: http://scalikejdbc.org/ License Published source code and binary files have the following copyright: Copyright scalikejdbc.org Apache License, Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.html  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalikejdbc/scalikejdbc"	"A tidy SQL-based DB access library for Scala developers."	"true"
"Database"	"scala-sql"	"https://github.com/wangzaixiang/scala-sql"	"Yet another SQL-based DB access library for scala language"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"11"	"3"	"1"	"GitHub - wangzaixiang/scala-sql: scala SQL api Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 11 Fork 1 wangzaixiang/scala-sql Code Issues 0 Pull requests 0 Pulse Graphs scala SQL api 25 commits 1 branch 0 releases Fetching contributors Scala 97.2% Java 2.8% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. backup src .gitignore Readme.md build.sbt usage.md Readme.md utility scala-sql scala has published to Maven Central as com.github.wangzaixiang/scala-sql/1.0.0-beta sbt usage: libraryDependencies +=  ""com.github.wangzaixiang"" %% ""scala-sql"" % ""1.0.0-beta"" scalaVersion := ""2.11.6""  Changelog 2014-12-30 add sbt support, remove macro support feature and maybe ported back for 2.11 TODO refract executeUpdateWithGenerateKey(sql)(processGenerateKeys) to what? Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/wangzaixiang/scala-sql"	"Yet another SQL-based DB access library for scala language"	"true"
"Database"	"scredis ★ 141 ⧗ 0"	"https://github.com/Livestream/scredis"	"Non-blocking Redis client built on top of Akka IO (used by Livestream)"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"147"	"35"	"16"	"GitHub - Livestream/scredis: Non-blocking, ultra-fast Scala Redis client built on top of Akka IO, used in production at Livestream Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 35 Star 147 Fork 16 Livestream/scredis Code Issues 0 Pull requests 0 Wiki Pulse Graphs Non-blocking, ultra-fast Scala Redis client built on top of Akka IO, used in production at Livestream 146 commits 8 branches 24 releases 7 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.x.x-2.9.x 1.x.x-2.10 1.x.x-2.11 gh-pages hostname-reference master new-logging readme-deprecation Nothing to show v2.0.6 v2.0.5 v2.0.4 v2.0.3-2.11 v2.0.3-2.10 v2.0.2-2.11 v2.0.2-2.10 v2.0.1-2.11 v2.0.1-2.10 v2.0.0-RC1-2.11 v2.0.0-RC1-2.10 v2.0.0-2.11 v2.0.0-2.10 v1.1.2-2.11 v1.1.2-2.10 v1.1.2-2.9.x v1.1.1-2.10 v1.1.1-2.9.x v1.1.0-2.10 v1.1.0-2.9.x v1.0.1-2.10 v1.0.1-2.9.x v1.0.0-2.10 v1.0.0-2.9.x Nothing to show New pull request Latest commit a7a79f5 May 19, 2016 quentinms Updated readme to state deprecation (#56) Permalink Failed to load latest commit information. project Serialized pub sub requests Jul 24, 2014 src version Apr 21, 2016 .gitignore Fixing tests Jul 11, 2014 LICENSE v1.0.0 Oct 10, 2013 NOTICE v1.0.0 Oct 10, 2013 README.md Updated readme to state deprecation (#56) May 19, 2016 build.sbt version Apr 21, 2016 README.md This repository is no longer maintained. Please refer to the scredis organization's repo for an actively developed version scredis Scredis is a reactive, non-blocking and ultra-fast Scala Redis client built on top of Akka IO. It has been (and still is) extensively used in production at Livestream. Documentation Scaladoc Features Supports all Redis commands up to v2.8.13 Built on top of Akka non-blocking IO Super fast, see Benchmarks section below Automatic reconnection Automatic pipelining Transactions Pub/Sub Subscribe selectively with partial functions Tracked Subscribe and Unsubscribe commands (they return a Future as any other commands) Automatically resubscribes to previously subscribed channels/patterns upon reconnection Customizable serialization and deserialization of command inputs and outputs Fully configurable Akka dispatchers Pipelined write batch size Receive timeout TCP buffer size hints Request encoding buffer pool Concurrent requests cap (bounded memory consumption) Getting started Binaries Scredis 2.x.x is compatible with Scala 2.10 and 2.11. Binary releases are hosted on the Sonatype Central Repository. resolvers += ""Sonatype OSS Releases"" at ""https://oss.sonatype.org/content/repositories/releases/""  libraryDependencies += ""com.livestream"" %% ""scredis"" % ""2.0.6"" Snapshots are hosted on a separate repository. resolvers += ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/""  libraryDependencies += ""com.livestream"" %% ""scredis"" % ""2.0.6-SNAPSHOT"" Quick example import scredis._ import scala.util.{ Success, Failure }  // Creates a Redis instance with default configuration. // See reference.conf for the complete list of configurable parameters. val redis = Redis()  // Import internal ActorSystem's dispatcher (execution context) to register callbacks import redis.dispatcher  // Executing a non-blocking command and registering callbacks on the returned Future redis.hGetAll(""my-hash"") onComplete {   case Success(content) => println(content)   case Failure(e) => e.printStackTrace() }  // Executes a blocking command using the internal, lazily initialized BlockingClient redis.blocking.blPop(0, ""queue"")  // Subscribes to a Pub/Sub channel using the internal, lazily initialized SubscriberClient redis.subscriber.subscribe(""My Channel"") {   case message @ PubSubMessage.Message(channel, messageBytes) => println(     message.readAs[String]()   )   case PubSubMessage.Subscribe(channel, subscribedChannelsCount) => println(     s""Successfully subscribed to $channel""   ) }  // Shutdown all initialized internal clients along with the ActorSystem redis.quit() Benchmarks The following benchmarks have been performed using ScalaMeter with the SeparateJvmsExecutor, configured with Warmer.Default, Measurer.Default and Aggregator.average. The source code can be found here. Hardware MacBook Pro (15-inch, Early 2011) 2.0GHz quad-core Intel Core i7 processor with 6MB shared L3 cache 16GB of 1333MHz DDR3 memory Mac OS X 10.9.4 Java > java -version java version ""1.7.0_45"" Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)  Scala Scala 2.11.2 Scredis 2.0.0-RC1 with default configuration Redis Redis 2.8.13 running locally (on the same machine) Results [info] :::Summary of regression test results - Accepter()::: [info] Test group: Client.PING [info] - Client.PING.Test-0 measurements: [info]   - at size -> 1000000: passed [info]     (mean = 1496.30 ms, ci = <1396.51 ms, 1596.10 ms>, significance = 1.0E-10) [info]   - at size -> 2000000: passed [info]     (mean = 3106.07 ms, ci = <2849.27 ms, 3362.87 ms>, significance = 1.0E-10) [info]   - at size -> 3000000: passed [info]     (mean = 4735.93 ms, ci = <4494.92 ms, 4976.94 ms>, significance = 1.0E-10) [info] [info] Test group: Client.GET [info] - Client.GET.Test-1 measurements: [info]   - at size -> 1000000: passed [info]     (mean = 2452.47 ms, ci = <2308.81 ms, 2596.12 ms>, significance = 1.0E-10) [info]   - at size -> 2000000: passed [info]     (mean = 4880.42 ms, ci = <4629.75 ms, 5131.09 ms>, significance = 1.0E-10) [info]   - at size -> 3000000: passed [info]     (mean = 7271.20 ms, ci = <6795.45 ms, 7746.94 ms>, significance = 1.0E-10) [info] [info] Test group: Client.SET [info] - Client.SET.Test-2 measurements: [info]   - at size -> 1000000: passed [info]     (mean = 2969.00 ms, ci = <2768.45 ms, 3169.54 ms>, significance = 1.0E-10) [info]   - at size -> 2000000: passed [info]     (mean = 5912.59 ms, ci = <5665.94 ms, 6159.24 ms>, significance = 1.0E-10) [info]   - at size -> 3000000: passed [info]     (mean = 8752.69 ms, ci = <8403.07 ms, 9102.31 ms>, significance = 1.0E-10) [info] [info]  Summary: 3 tests passed, 0 tests failed.  Ping 1,000,000 requests -> 1496.30 ms = 668,315 req/s 2,000,000 requests -> 3106.07 ms = 643,900 req/s 3,000,000 requests -> 4735.93 ms = 633,455 req/s Get 1,000,000 requests -> 2452.47 ms = 407,752 req/s 2,000,000 requests -> 4880.42 ms = 409,801 req/s 3,000,000 requests -> 7271.20 ms = 412,587 req/s Set 1,000,000 requests -> 2969.00 ms = 336,814 req/s 2,000,000 requests -> 5912.59 ms = 338,261 req/s 3,000,000 requests -> 8752.69 ms = 342,752 req/s Scredis 1.x.x Binaries Scredis 1.x.x is compatible with Scala 2.9.x, 2.10 and 2.11. Binary releases are hosted on the Sonatype Central Repository. resolvers += ""Sonatype OSS Releases"" at ""https://oss.sonatype.org/content/repositories/releases/""  libraryDependencies += ""com.livestream"" %% ""scredis"" % ""1.1.2"" Snapshots are hosted on a separate repository. resolvers += ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/""  libraryDependencies += ""com.livestream"" %% ""scredis"" % ""1.1.2-SNAPSHOT"" License Copyright (c) 2013 Livestream LLC. All rights reserved. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Livestream/scredis"	"Non-blocking Redis client built on top of Akka IO (used by Livestream)"	"true"
"Database"	"Slick ★ 1499 ⧗ 0"	"https://github.com/slick/slick"	"Modern database query and access library for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1585"	"158"	"369"	"GitHub - slick/slick: Scala Language Integrated Connection Kit Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 158 Star 1,585 Fork 369 slick/slick Code Issues 331 Pull requests 70 Wiki Pulse Graphs Scala Language Integrated Connection Kit http://slick.lightbend.com 1,756 commits 131 branches 54 releases 67 contributors Scala 98.7% Python 1.1% HTML 0.2% Scala Python HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0 2.0-no-bin-compat 2.0 2.1-no-bin-compat 2.1 3.0 3.1 master scala-2.7 sqlite-upgrade stringAggr tmp/access-cast-cap tmp/action-monad tmp/agg-win-func tmp/aggregate-explicit-join tmp/aggregation-convenience tmp/async-stack-trace tmp/auto-mapped-column-types tmp/case-class-shape-docs tmp/caseClassShape tmp/charMappingBroken tmp/codegen-fix-negative-defaults tmp/column-improvements tmp/composable-optionmapper tmp/dbio-trampolining tmp/developer-docs tmp/doc-improvements-rc1 tmp/docs-theme tmp/duplicate-test-bundles tmp/dynamic-update-demo tmp/early-aggregation-lifting tmp/eval tmp/extensibleReverseEngineering tmp/fetch-drop-fusion tmp/fix-doctests tmp/fix-leaking-session tmp/fixDistinct tmp/fixDoctests tmp/fixSubstring tmp/for-update tmp/globaltypes2 tmp/hackSql tmp/hlist-queries tmp/hlistCompileSpeedup tmp/into-docs tmp/issue-159 tmp/issue-625 tmp/issue-645 tmp/issue-794 tmp/issue-1008 tmp/issue-1104 tmp/issue-1113 tmp/issue-1170 tmp/issue-1532 tmp/jenkinsTest tmp/liftedfunction-arrows tmp/literal-to-bind-safe-literal tmp/mima-for-trp tmp/mima tmp/more-rc1-docs tmp/moreGeneralAppliedInvoker tmp/nested-columns2 tmp/new-columnization tmp/new-table-definitions tmp/optional-compiler-dependency tmp/outerJoinBug tmp/parameter-logging tmp/positional-converters tmp/prevent-update-all tmp/projection-scaladoc tmp/provenshape-shape tmp/proxydatasource tmp/queryInterpreterRefactoring tmp/queryThis tmp/relational-profile tmp/release-2.1.0-M1 tmp/release-2.1.0-M2 tmp/remove-column tmp/remove-notIn tmp/remove-zero tmp/rename-jdbcdriver tmp/sbt-0.13.5 tmp/scala-2.11.0-rc4 tmp/scalatypes-in-drivers tmp/selective-phases tmp/shape-improvements-refactored tmp/should-not-typecheck tmp/si-9164 tmp/slick-extensions-compat tmp/slick-extensions-drivers tmp/switch-dbprops tmp/table-type-inference tmp/testBuildUpdate tmp/testJenkins tmp/third-party-projects tmp/tuple2hlist tmp/type-providers-cg tmp/type-providers-simple tmp/type-providers-tree-based tmp/typeMapperInitOrderMessage tmp/typeinfo-issue tmp/undo-model-api-changes tmp/unforce-macro-usage-recompilation tmp/upVersions tmp/update-build tmp/yy/cg tmp/2.1-trp topic/aggregation topic/backend-refactoring topic/codegen-phase topic/codegen-phase2 topic/docs-understanding-api topic/modular-compiler topic/nested-tuples-query2 topic/nested-tuples topic/new-ast topic/new-option-mapper topic/profile-specific-tables topic/projection-types topic/scala-2.10.0-M4 topic/simplify-ast topic/testkit topic/tuples-everywhere topic/type-providers topic/unify-packing wip/codegen wip/docs wip/fullOuterJoins wip/mongodb wip/poc-catch-eqeq wip/upgrade-guide Nothing to show rp/3.0.1-15v07p01 rp/2.1.0-15v01 rp/2.1.0-14v12 3.2.0-M1 3.1.1 3.1.0 3.1.0-RC3 3.1.0-RC2 3.1.0-RC1 3.1.0-M2 3.1.0-M1 3.0.3 3.0.2 3.0.1 3.0.0 3.0.0-RC3 3.0.0-RC2 3.0.0-RC1 3.0.0-M1 2.1.0 2.1.0-RC3 2.1.0-RC2 2.1.0-RC1 2.1.0-M2 2.1.0-M1 2.0.3 2.0.2 2.0.2-RC1 2.0.1 2.0.1-RC1 2.0.0 2.0.0-RC1 2.0.0-M3 2.0.0-M2 2.0.0-M1 1.0.1 1.0.1-RC1 1.0.0 1.0.0-RC2 1.0.0-RC1 0.11.2 0.11.1 0.11.0 0.10.0 0.10.0_Scala2.10.0-M6 0.10.0-M3 0.10.0-M2 0.10.0-M1 0.9.5 0.9.4 0.9.3 0.9.2 0.9.1 0.9.0 Nothing to show New pull request Latest commit a0eb1ee Jul 8, 2016 radsaggi committed on GitHub Improvements to teskit for simplifying tests using action-based queri… … …es (#1544)  Improvements to teskit for simplifying tests using action-based queries Added shouldYield operator so that we can now have query shouldYield aValue instead of query.map(_ shouldBe aValue) query shouldYield Set(...) instead of query.map(_.toSet shouldBe Set(...)) Similarly for Seq and List Permalink Failed to load latest commit information. common-test-resources fixes issue 1274 Jun 29, 2016 osgi-tests/src/test/scala/slick/osgi Package structure refactoring Oct 7, 2015 project Update Scala versions to 2.11.8 + 2.12.0-M5 Jul 4, 2016 reactive-streams-tests/src/test Package structure refactoring Oct 7, 2015 slick-codegen/src/main/scala/slick/codegen Merge pull request #1523 from trevorsibanda/add-o.unique Jul 6, 2016 slick-hikaricp/src/main/scala/slick/jdbc/hikaricp fixes issue 1274 Jun 29, 2016 slick-testkit/src Improvements to teskit for simplifying tests using action-based queri… Jul 8, 2016 slick/src Merge pull request #1523 from trevorsibanda/add-o.unique Jul 6, 2016 test-dbs Add Slick Extensions drivers Feb 2, 2016 .gitignore Run Reactive Streams tests with ScalaTest Oct 2, 2015 .travis.yml Merge branch 'refs/heads/3.1' into tmp/rollup-3.1-to-master Jan 5, 2016 CONTRIBUTING.md Update CONTRIBUTING.md Feb 6, 2016 LICENSE.txt Update Typesafe copyright header current year Feb 3, 2016 README.md Add Slick Extensions drivers Feb 2, 2016 circle.yml Add custom 'circle.yml' for running all the test using the oraclejdk8 Nov 20, 2015 jvmopts.travis Require Java 8 and Scala 2.11 Oct 2, 2015 scaladoc-root.txt Package structure and project layout refactoring: Feb 16, 2015 README.md Slick Slick is a modern database query and access library for Scala. It allows you to work with stored data almost as if you were using Scala collections while at the same time giving you full control over when a database access happens and which data is transferred. You can write your database queries in Scala instead of SQL, thus profiting from the static checking, compile-time safety and compositionality of Scala. Slick features an extensible query compiler which can generate code for different backends. The following database systems are directly supported for type-safe queries: Derby/JavaDB H2 HSQLDB/HyperSQL MySQL PostgreSQL SQLite Oracle 11g IBM DB2 LUW 10 Microsoft SQL Server 2008 Accessing other database systems is possible, with a reduced feature set. The manual and scaladocs for Slick can be found on the Slick web site. Licensing conditions (BSD-style) can be found in LICENSE.txt. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/slick/slick"	"Modern database query and access library for Scala."	"true"
"Database"	"Sorm ★ 209 ⧗ 0"	"https://github.com/sorm/sorm"	"A functional boilerplate-free Scala ORM."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"217"	"24"	"29"	"GitHub - sorm/sorm: A functional boilerplate-free Scala ORM Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 24 Star 217 Fork 29 sorm/sorm Code Issues 26 Pull requests 1 Pulse Graphs A functional boilerplate-free Scala ORM http://sorm-framework.org 1,406 commits 5 branches 26 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.4/dev gh-pages master pr/50 verify-error-scala-issue Nothing to show notype-scala-issue archive/not-working-macros archive/decoupled-membership 0.3.20 0.3.19 0.3.18 0.3.17 0.3.16 0.3.15 0.3.14 0.3.13 0.3.12 0.3.11 0.3.10 0.3.9 0.3.8 0.3.7 0.3.6 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 0.2.0 0.1.0 Nothing to show New pull request Latest commit 28c5fcc Jul 7, 2016 nikita-volkov Remove Oracle from tests, since Travis isn't set up to initialize it Permalink Failed to load latest commit information. src Remove Oracle from tests, since Travis isn't set up to initialize it Jul 7, 2016 .travis.yml Fix Mysql Jan 24, 2015 LICENSE no message Sep 11, 2012 README.md Release Jul 7, 2016 README_Oracle Added information about string length limitations. May 25, 2016 RELEASE_NOTES Release Jul 7, 2016 pom.xml Release Jul 7, 2016 README.md SORM SORM is an object-relational mapping framework designed to eliminate boilerplate and maximize productivity. It is absolutely abstracted from relational side, automatically creating database tables, emitting queries, inserting, updating and deleting records. This all functionality is presented to the user with a simple API around standard Scala's case classes. For more info, tutorials and documentation please visit the official site. Supported databases MySQL PostgreSQL H2 HSQLDB Oracle (experimental) Supported Scala versions 2.10, 2.11 Maven SORM is distributed in Maven Central, here's a dependency to the latest release version: <dependency>   <groupId>org.sorm-framework</groupId>   <artifactId>sorm</artifactId>   <version>0.3.20</version> </dependency>  SBT libraryDependencies += ""org.sorm-framework"" % ""sorm"" % ""0.3.20""  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sorm/sorm"	"A functional boilerplate-free Scala ORM."	"true"
"Database"	"Squeryl ★ 457 ⧗ 9"	"https://github.com/squeryl/squeryl"	"A Scala DSL for talking with databases with minimum verbosity and maximum type safety."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"462"	"36"	"118"	"GitHub - squeryl/squeryl: A Scala DSL for talking with databases with minimum verbosity and maximum type safety Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 36 Star 462 Fork 118 squeryl/squeryl Code Issues 6 Pull requests 15 Wiki Pulse Graphs A Scala DSL for talking with databases with minimum verbosity and maximum type safety http://squeryl.org 656 commits 72 branches 12 releases 30 contributors Scala 99.1% Java 0.9% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.9.4-final 0.9.4final_on2.9.1 0.9.5 2.11.0 7-mricog AST-lifting-overhaul KEMandatory4-updateById-and-partialUpdate-new-syntak Scala2.9.0-RC3 Scala2.9.0-RC4 Scala2.9.0-RC5 ast-experiment aw_IntelliJClasspath aw_Timestamp aw_close_statements aw_closeStatements aw_createFkIndex aw_fieldReferenceLinker aw_issue_14 aw_publicTableMetadata aw_stringLength aw_unique_and_indexed aw_uniqueColumn big-decimal-continued bitwalker-add-string-arrays case-statement column-attribute-declarations composite-keys constants-in-statementwriter date-format-test db-object-prefixes dw_issue_42 dw_issue_79 dw_static_methods dynamic-queries dynamic-where-clause enum-deduction enumerations equalsNull2isNull generated-key-def gh-pages ids-other-side-of-relation in_set_bug inhibit-left-outer lazy_session lifecycle-events lift-support lift2.1 master mssql multimap-conversions no_field_annotation oracle-alternate-paging-strategy pmb_pg re-parametrizable-statement refactor-of-select-element relations replacement-of-KeyedEntity-by-a-typeclass safer-ked sbt0.10.1_build scala2.9.0-RC1 scala2.10.0-M7 site-ng2 sql-case-statements stats-logging subbranch4merge switch-from2.9.1-RC1-to-2.8.1 test-case-deep-nest-customtype tupleQuery2Multimap v0.9.4 v0_9_4 versioning-and-auditing wip_inhibit_when Nothing to show 0.9.6-RC3 0.9.6-RC2 0.9.6-RC1 0.9.5-Final 0.9.5-7 0.9.5-6 0.9.5-5 0.9.5-4 0.9.5-3 0.9.5-2 0.9.5-1 0.9.4-RC7 Nothing to show New pull request Latest commit 45a69cb Oct 11, 2015 davewhittaker Merge pull request #204 from xuwei-k/patch-1 … add maven central and scaladoc badge Permalink Failed to load latest commit information. project A few changes to get travis publishing. Oct 8, 2015 src update dependencies. add travis-ci settings Apr 16, 2015 .gitignore Ignore tags file Jan 27, 2015 .travis.yml A few changes to get travis publishing. Oct 8, 2015 license.txt updated apache license Mar 2, 2010 org.squeryl.tests.cfg Ignores unsupported test Mar 11, 2015 org.squeryl.tests.cfg.full temporaty discard of CustomTypeMode Dec 5, 2011 org.squeryl.tests.cfg.travis update dependencies. add travis-ci settings Apr 16, 2015 readme.markdown add maven central and scaladoc badge Oct 11, 2015 release Changes to build process. Mar 1, 2012 readme.markdown How to build Download or clone repository: git clone git://github.com/squeryl/squeryl.git Open a shell in the project's root directory and launch SBT with ./sbt this will fetch the required version of Scala both for SBT itself and for Squeryl. The 'test' command will run the test suite against the minimalist but very complete H2 database. Type 'package' to create a jar in ./target/[scala version]. For more information Documentation can be found at The Squeryl Website and Scaladoc. Questions can be directed to the Google Group Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/squeryl/squeryl"	"A Scala DSL for talking with databases with minimum verbosity and maximum type safety."	"true"
"Database"	"Tepkin ★ 84 ⧗ 13"	"https://github.com/fehmicansaglam/tepkin"	"Reactive MongoDB Driver for Scala built on top of Akka IO and Akka Streams."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"85"	"14"	"11"	"GitHub - fehmicansaglam/tepkin: Reactive MongoDB Driver for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 85 Fork 11 fehmicansaglam/tepkin Code Issues 2 Pull requests 0 Wiki Pulse Graphs Reactive MongoDB Driver for Scala 201 commits 2 branches 4 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 0.4 0.3 0.2 0.1 Nothing to show New pull request Latest commit 94ae5ec Feb 24, 2016 fehmicansaglam Update README.md Permalink Failed to load latest commit information. bson Fix BsonNull impl in bson dsl Jul 15, 2015 examples #27 upgrade to akka 2.4.2 Feb 19, 2016 pide Change Future[Source] return values to Source May 20, 2015 project #27 upgrade to akka 2.4.2 Feb 19, 2016 tepkin #27 forgot about this test Feb 19, 2016 .editorconfig Add editorconfig Feb 16, 2015 .gitignore First implementation including only Count command Feb 12, 2015 .travis.yml Configure Travis to send notifications to gitter.im Mar 21, 2015 LICENSE Initial commit Feb 10, 2015 README.md Update README.md Feb 24, 2016 build.sbt Upgrade to akka stream 1.0 Jul 15, 2015 README.md Deprecation Notice This project has moved to https://github.com/jeroenr/tepkin Tepkin Reactive MongoDB Driver for Scala built on top of Akka IO and Akka Streams. Only MongoDB 2.6+, Scala 2.11+ is supported. Java support has been dropped. See details here: https://github.com/fehmicansaglam/tepkin/issues/22 Don't hesitate to ask questions in the Tepkin Google Group or join chat on Gitter: Contributions Tepkin is a young but very active project and absolutely needs your help. Good ways to contribute include: Raising bugs and feature requests Fixing bugs Improving the performance Adding to the documentation Please read our Scala Guide first: https://github.com/fehmicansaglam/tepkin/wiki/Scala-Guide Quick Start Setting up dependencies Latest stable Tepkin release is 0.5 and is available on Maven Central. Just add the following dependency: libraryDependencies ++= Seq(   ""net.fehmicansaglam"" %% ""tepkin"" % ""0.5"" ) Or if you want to be on the bleeding edge using snapshots, latest snapshot release is 0.6-SNAPSHOT. Add the following repository and dependency: resolvers += ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/""  libraryDependencies ++= Seq(   ""net.fehmicansaglam"" %% ""tepkin"" % ""0.6-SNAPSHOT"" ) Scala API Working with BSON DSL To construct a Bson document, you can either create BsonElements and join them with ~ or create a document directly. import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._ import net.fehmicansaglam.bson.element.BsonObjectId import org.joda.time.DateTime  // Construct a BsonDocument from BsonElements val element = ""name"" := ""Johny"" val document = element ~   (""surname"" := ""Doe"") ~   (""age"" := 28) ~   (""months"" := $array(1, 2, 3))  // Construct a BsonDocument val document = $document(   ""_id"" := BsonObjectId.generate,   ""name"" := ""Johny"",   ""surname"" := ""Doe"",   ""age"" := 28,   ""months"" := $array(1, 2, 3),   ""details"" := $document(     ""salary"" := 455.5,     ""inventory"" := $array(""a"", 3.5, 1L, true),     ""birthday"" := new DateTime(1987, 3, 5, 0, 0)   ) ) There is an implicit conversion from any BsonElement to BsonDocument for convenience. import net.fehmicansaglam.bson.BsonDocument import net.fehmicansaglam.bson.element.BsonElement import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  val element: BsonElement = ""name"" := ""fehmi"" val document: BsonDocument = ""name"" := ""fehmi"" Connecting to MongoDB To make a connection to MongoDB, use the MongoClient interface. import net.fehmicansaglam.tepkin.MongoClient  // Connect to a MongoDB node. val client = MongoClient(""mongodb://localhost"") MongoClient manages multiple connection pools to MongoDB instances and therefore is a heavy class. Most of the time you will need only one MongoClient instance per application. Use MongoDatabase and MongoCollection in order to obtain a reference to a database and a collection. // Obtain a reference to the ""tepkin"" database val db = client(""tepkin"")  // Obtain a reference to the ""example"" collection in ""tepkin"" database. val collection = db(""example"") MongoDatabase and MongoCollection are lightweight classes and may be instantiated more than once if needed. However they are both immutable and reusable. All methods in the MongoCollection class need an implicit scala.concurrent.ExecutionContext and an akka.util.Timeout. You can define a default timeout and use the client's execution context as shown below: import akka.util.Timeout import scala.concurrent.duration._  // val client = ...  import client.ec implicit val timeout: Timeout = 5.seconds Find documents import net.fehmicansaglam.bson.BsonDocument import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  val query: BsonDocument = ""name"" := ""fehmi""  val source = collection.find(query) All find methods in Tepkin return an akka.stream.scaladsl.Source[List[BsonDocument], ActorRef]. Then you can use any method in Akka Streams to process the returned stream. Insert operations Insert a single document import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  val document = (""name"" := ""fehmi"") ~ (""surname"" := ""saglam"") collection.insert(document) Insert a collection of documents import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  val documents = (1 to 100).map(i => $document(""name"" := s""fehmi$i"")) collection.insert(documents) Insert a large number of documents from a stream import akka.stream.ActorFlowMaterializer import akka.stream.scaladsl.Source import net.fehmicansaglam.bson.BsonDocument import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  import scala.collection.immutable.Iterable  implicit val mat = ActorFlowMaterializer()(client.context)  val documents: Source[List[BsonDocument], Unit] = Source {   Iterable.tabulate(100) { _ =>     (1 to 1000).map(i => $document(""name"" := s""fehmi$i"")).toList   } }  collection.insertFromSource(documents).runForeach(_ => ()) Other queries Update import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  import scala.concurrent.Future  val document = (""name"" := ""fehmi"") ~ (""surname"" := ""saglam"")  val result: Future[UpdateResult] = for {   insert <- collection.insert(document)   update <- collection.update(     query = ""name"" := ""fehmi"",     update = $set(""name"" := ""fehmi can"")   ) } yield update Find and update Update and return the old document. import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  collection.findAndUpdate(   query = Some(""name"" := ""fehmi""),   update = $set(""name"" := ""fehmi can"") ) Update and return the updated document. import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._  collection.findAndUpdate(   query = Some(""name"" := ""fehmi""),   update = $set(""name"" := ""fehmi can""),   returnNew = true ) Create index import net.fehmicansaglam.bson.BsonDsl._ import net.fehmicansaglam.bson.Implicits._ import net.fehmicansaglam.tepkin.protocol.command.Index  collection.createIndexes(Index(name = ""name_surname"", key = (""name"" := 1) ~ (""surname"" := 1))) Donations Tepkin is a free software project and will always be. I work hard to make it stable and to add new features. I am always available if you encounter a problem and file an issue on Github. If you like Tepkin and find it helpful, you might give me a gift from some of the books (Kindle) I have in my wish list: My Wish List on Amazon. Thanks! One last thing, I am available for hire. If you think you know a job that is suitable for me, especially in Europe, please contact me at fehmican dot saglam at gmail dot com. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/fehmicansaglam/tepkin"	"Reactive MongoDB Driver for Scala built on top of Akka IO and Akka Streams."	"true"
"Messaging"	"Op-Rabbit ★ 96 ⧗ 19"	"https://github.com/SpinGo/op-rabbit"	"High-level messaging library for Akka and Op-Rabbit."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"113"	"15"	"29"	"GitHub - SpinGo/op-rabbit: The Opinionated RabbitMQ Library for Scala and Akka Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 113 Fork 29 SpinGo/op-rabbit Code Issues 16 Pull requests 2 Wiki Pulse Graphs The Opinionated RabbitMQ Library for Scala and Akka 197 commits 5 branches 32 releases Fetching contributors Scala 99.6% Other 0.4% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags flowgraph master use-java-topology-recovery v1.0.x v1.2.x Nothing to show v1.3.0 v1.2.2 v1.2.1 v1.2.0 v1.1.3 v1.1.2 v1.1.1 v1.1.0 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v1.0.0-RC4 v1.0.0-RC3 v1.0.0-RC2 v1.0.0-RC1 v1.0.0-M17 v1.0.0-M16 v1.0.0-M15 v1.0.0-M14 v1.0.0-M13 v1.0.0-M12 v1.0.0-M11 v1.0.0-M10 v1.0.0-M9 v1.0.0-M7 v1.0.0-M6 v1.0.0-M5 v1.0.0-M4 v1.0.0-M3 Nothing to show New pull request Latest commit 348a3ae May 18, 2016 timcharper Merge pull request #75 from dmexe/akka-log-error … Wrong arguments for log.error in AsyncAckingRabbitConsumer Permalink Failed to load latest commit information. addons various warnings resolved Feb 29, 2016 bin helper script to preview readme Dec 14, 2015 core Wrong arguments for log.error in AsyncAckingRabbitConsumer May 19, 2016 project v1.3.0 Feb 29, 2016 .gitignore helper script to preview readme Dec 14, 2015 .travis.yml remove scala 2.10.x from travis Feb 29, 2016 CONTRIBUTING.md contributing notes Sep 3, 2015 DEPLOY.md v1.3.0 Feb 29, 2016 EXPLANATIONS.md Confirmed message handling; message property specification Jun 15, 2015 LICENSE adds license May 10, 2015 Makefile mark docs as public-read when syncing Oct 11, 2015 README.md fix notes about shapeless; direct to 1.2.x Feb 29, 2016 build.sbt circe support; update to target akka-streams 2.4.2 Feb 29, 2016 README.md Op-Rabbit An opinionated RabbitMQ library for Scala and Akka. Documentation Browse the latest API Docs online. For announcements, discussions, etc., join the discussion: Op-rabbit discussion forum Issues go here: https://github.com/SpinGo/op-rabbit/issues Intro Op-Rabbit is a high-level, type-safe, opinionated, composable, fault-tolerant library for interacting with RabbitMQ; the following is a high-level feature list: Recovery: Consumers automatically reconnect and subscribe if the connection is lost Messages published will wait for a connection to be available Integration Connection settings pulled from Typesafe config library Asynchronous, concurrent consumption using Scala native Futures or the new Akka Streams project. Common pattern for serialization allows easy integration with serialization libraries such play-json or json4s Common pattern for exception handling to publish errors to Airbrake, Syslog, or all of the above Modular Composition favored over inheritance enabling flexible and high code reuse. Modeled Queue binding, exchange binding modeled with case classes Queue, and Exchange arguments, such as x-ttl, are modeled HeaderValues are modeled; if you try and provide RabbitMQ an invalid type for a header value, the compiler will let you know. Publishing mechanisms also modeled Reliability Builds on the excellent Akka RabbitMQ client library for easy recovery. Built-in consumer error recovery strategy in which messages are re-delivered to the message queue and retried (not implemented for akka-streams integration as retry mechanism affects message order) With a single message, pause all consumers if service health check fails (IE: database unavailable); easily resume the same. Graceful shutdown Consumers and streams can immediately unsubscribe, but stay alive long enough to wait for any messages to finish being processed. Program at multiple levels of abstraction If op-rabbit doesn't do what you need it to, you can either extend op-rabbit or interact directly with akka-rabbitmq Akka RabbitMQ client. Tested Extensive integration tests Installation Add the SpinGo OSS repository and include the dependencies of your choosing: resolvers ++= Seq(   ""SpinGo OSS"" at ""http://spingo-oss.s3.amazonaws.com/repositories/releases"" )  val opRabbitVersion = ""1.3.0""  libraryDependencies ++= Seq(   ""com.spingo"" %% ""op-rabbit-core""        % opRabbitVersion,   ""com.spingo"" %% ""op-rabbit-play-json""   % opRabbitVersion,   ""com.spingo"" %% ""op-rabbit-json4s""      % opRabbitVersion,   ""com.spingo"" %% ""op-rabbit-airbrake""    % opRabbitVersion,   ""com.spingo"" %% ""op-rabbit-akka-stream"" % opRabbitVersion ) Scala Version Compatibility Matrix: Only Scala 2.11.x is supported. If you require Scala 2.10.x, use op-rabbit 1.2.x. module dependsOn version op-rabbit-core akka 2.4.x akka-rabbitmq 2.x shapeless 2.3.x type-safe config >= 1.3.0 op-rabbit-play-json play-json 2.4.x op-rabbit-json4s json4s 3.2.x op-rabbit-circe circe 0.3.x op-rabbit-airbrake airbrake 2.2.x op-rabbit-akka-stream acked-stream 2.1.x akka-stream >= 2.4.2 A high-level overview of the available components: op-rabbit-core API Implements basic patterns for serialization and message processing. op-rabbit-play-json API Easily use Play Json formats to publish or consume messages; automatically sets RabbitMQ message headers to indicate content type. op-rabbit-json4s API Easily use Json4s to serialization messages; automatically sets RabbitMQ message headers to indicate content type. op-rabbit-airbrake API Report consumer exceptions to airbrake, using the Airbrake Java library. op-rabbit-akka-stream API Process or publish messages using akka-stream. Upgrade Guide Refer to Upgrade Guide wiki page for help upgrading. Usage Set up RabbitMQ connection information in application.conf: op-rabbit {   topic-exchange-name = ""amq.topic""   connection {     virtual-host = ""/""     hosts = [""127.0.0.1""]     username = ""guest""     password = ""guest""     port = 5672     ssl = false     timeout = 3s   } }  Note that hosts is an array; Connection attempts will be made to hosts in that order, with a default timeout of 3s. This way you can specify addresses of your rabbitMQ cluster, and if one of the instances goes down, your application will automatically reconnect to another member of the cluster. topic-exchange-name is the default topic exchange to use; this can be overriden by passing exchange = ""my-topic"" to TopicBinding or Message.topic. Boot up the RabbitMQ control actor: import com.spingo.op_rabbit.RabbitControl import akka.actor.{ActorSystem, Props}  implicit val actorSystem = ActorSystem(""such-system"") val rabbitControl = actorSystem.actorOf(Props[RabbitControl]) Set up a consumer: (Topic subscription) (this example uses op-rabbit-play-json) import com.spingo.op_rabbit.PlayJsonSupport._ import com.spingo.op_rabbit._ import play.api.libs.json._  import scala.concurrent.ExecutionContext.Implicits.global case class Person(name: String, age: Int) // setup play-json serializer implicit val personFormat = Json.format[Person]  val subscriptionRef = Subscription.run(rabbitControl) {   import Directives._   // A qos of 3 will cause up to 3 concurrent messages to be processed at any given time.   channel(qos = 3) {     consume(topic(queue(""such-message-queue""), List(""some-topic.#""))) {       (body(as[Person]) & routingKey) { (person, key) =>         /* do work; this body is executed in a separate thread, as            provided by the implicit execution context */         println(s""""""A person named '${person.name}' with age           ${person.age} was received over '${key}'."""""")         ack       }     }   } } Now, test the consumer by sending a message: subscriptionRef.initialized.foreach { _ =>   rabbitControl ! Message.topic(     Person(""Your name here"", 33), ""some-topic.cool"") }  Stop the consumer: subscriptionRef.close()  Note, if your call generates an additional future, you can pass it to ack, and message will be acked based off the Future success, and nacked with Failure (such that the configured RecoveryStrategy if the Future fails:   // ...       (body(as[Person]) & routingKey) { (person, key) =>         /* do work; this body is executed in a separate thread, as            provided by the implicit execution context */         val result: Future[Unit] = myApi.methodCall(person)         ack(result)       }   // ...  Consuming from existing queues If the queue already exists and doesn't match the expected configuration, topic subscription will fail. To bind to an externally configured queue use Queue.passive:   channel(qos = 3) {     consume(Queue.passive(""very-exist-queue"")) { ... It is also possible to optionally create the queue if it doesn't exist, by providing a QueueDefinition instead of a String:   channel(qos = 3) {     consume(Queue.passive(topic(queue(""wow-maybe-queue""), List(""some-topic.#"")))) { ... Accessing additional headers As seen in the example above, you can extract headers in addition to the message body, using op-rabbit's Directives. You can use multiple declaratives via multiple nested functions, as follows: import com.spingo.op_rabbit.properties._  // Nested directives // ...       body(as[Person]) { person =>         optionalProperty(ReplyTo) { replyTo =>           // do work           ack         }       } // ... Or, you can combine directives using & to form a compound directive, as follows: // Compound directive // ...       (body(as[Person]) & optionalProperty(ReplyTo)) { (person, replyTo) =>         // do work         ack       } // ... See the documentation on Directives for more details. Shutting down a consumer The following methods are available on a SubscriptionRef which will allow control over the subscription. /* stop receiving new messages from RabbitMQ immediately; shut down    consumer and channel as soon as pending messages are completed. A    grace period of 30 seconds is given, after which the subscription    forcefully shuts down. (Default of 5 minutes used if duration not    provided) */ subscription.close(30 seconds)  /* Shut down the subscription immediately; don't wait for messages to    finish processing. */ subscription.abort()  /* Future[Unit] which completes once the provided binding has been    applied (IE: queue has been created and topic bindings    configured). Useful if you need to assert you don't send a message    before a message queue is created in which to place it. */ subscription.initialized  // Future[Unit] which completes when the subscription is closed. subscription.closed Recovery strategy: Configured using RecoveryStrategy. By default, uses the default strategy Easiest way to set-up: declare an implicit, e.g.: implicit val recoveryStrategy = RecoveryStrategy.nack()  Publish a message: rabbitControl ! Message.topic(   Person(name = ""Mike How"", age = 33),   routingKey = ""some-topic.very-interest"")  rabbitControl ! Message.queue(   Person(name = ""Ivanah Tinkle"", age = 25),   queue = ""such-message-queue"") By default: Messages will be queued up until a connection is available Messages are monitored via publisherConfirms; if a connection is lost before RabbitMQ confirms receipt of the message, then the message is published again. This means that the message may be delivered twice, the default opinion being that at-least-once is better than at-most-once. You can use UnconfirmedMessage if you'd like at-most-once delivery, instead. If you would like to be notified of confirmation, use the ask pattern: import akka.pattern.ask import akka.util.Timeout import scala.concurrent.duration._ implicit val timeout = Timeout(5 seconds) val received = (   rabbitControl ? Message.queue(     Person(name = ""Ivanah Tinkle"", age = 25),     queue = ""such-message-queue"") ).mapTo[ConfirmResponse] Consuming using Akka streams (this example uses op-rabbit-play-json and op-rabbit-akka-streams) import Directives._ RabbitSource(   rabbitControl,   channel(qos = 3),   consume(queue(     ""such-queue"",     durable = true,     exclusive = false,     autoDelete = false)),   body(as[Person])). // marshalling is automatically hooked up using implicits   runForeach { person =>     greet(person)   } // after each successful iteration the message is acknowledged. Note: RabbitSource yields an AckedSource, which can be combined with an AckedSink (such as MessagePublisherSink). You can convert an acked stream into a normal stream by calling AckedStream.acked; once messages flow passed the acked component, they are considered acknowledged, and acknowledgement tracking is no longer a concern (and thus, you are free to use the akka-stream library in it's entirety). Publishing using Akka streams (this example uses op-rabbit-play-json and op-rabbit-akka-streams) import com.spingo.op_rabbit._ import com.spingo.op_rabbit.stream._ import com.spingo.op_rabbit.PlayJsonSupport._ implicit val workFormat = Format[Work] // setup play-json serializer  /* Each element in source will be acknowledged after publish    confirmation is received */ AckedSource(1 to 15).   map(Message.queue(_, queueName)).   to(MessagePublisherSink(rabbitControl))   .run If you can see the pattern here, combining an akka-stream rabbitmq consumer and publisher allows for guaranteed at-least-once message delivery from head to tail; in other words, don't acknowledge the original message from the message queue until any and all side-effect events have been published to other queues and persisted. Error notification It's important to know when your consumers fail. Out of the box, op-rabbit ships with support for logging to slf4j (and therefore syslog), and also airbrake via op-rabbit-airbrake. Without any additional signal provided by you, slf4j will be used, making error visibility a default. You can report errors to multiple sources by combining error logging strategies; for example, if you'd like to report to both slf4j and to airbrake, import / set the following implicit RabbitErrorLogging in the scope where your consumer is instantiated: import com.spingo.op_rabbit.{Slf4jLogger, AirbrakeLogger}  implicit val rabbitErrorLogging = Slf4jLogger + AirbrakeLogger.fromConfig Implementing your own error reporting strategy is simple; here's the source code for the slf4jLogger: object Slf4jLogger extends RabbitErrorLogging {   def apply(     name: String,     message: String,     exception: Throwable,     consumerTag: String,     envelope: Envelope,     properties: BasicProperties,     body: Array[Byte]): Unit = {      val logger = LoggerFactory.getLogger(name)     logger.error(s""${message}. Body=${bodyAsString(body, properties)}. Envelope=${envelope}"", exception)   } } Notes Shapeless dependency Note, Op-Rabbit depends on shapeless 2.3.0, and there is presently no published version of spray-routing-shapeless2 which works with shapeless 2.3.0. Consider migrating to akka-http, or if you must stay on spray, use op-rabbit 1.2.x, instead. Credits Op-Rabbit was created by Tim Harper This library builds upon the excellent Akka RabbitMQ client by Yaroslav Klymko. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/SpinGo/op-rabbit"	"High-level messaging library for Akka and Op-Rabbit."	"true"
"Graphical User Interfaces"	"ScalaFX"	"http://www.scalafx.org/"	"Scala DSL for creating Graphical User Interfaces that sits on top of JavaFX."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"41"	"7"	"7"	"GitHub - vigoo/scalafxml: Bridging the gap between scalafx and FXML with generated proxies Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 7 Star 41 Fork 7 vigoo/scalafxml Code Issues 5 Pull requests 0 Pulse Graphs Bridging the gap between scalafx and FXML with generated proxies 57 commits 3 branches 0 releases 3 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags SFX-2 dynamic master Nothing to show Nothing to show New pull request Latest commit 22fa5da Feb 19, 2016 vigoo Updated travis.yml Permalink Failed to load latest commit information. core-macros/src/main/scala/scalafxml/core/macros Updated dependencies, fixed warnings, dropped 2.10 Feb 19, 2016 core/src Updated dependencies, fixed warnings, dropped 2.10 Feb 19, 2016 demo/src/main MacWire support + demo Oct 23, 2014 guice/src/main/scala/scalafxml/guice Documentation Dec 22, 2013 macwire/src/main/scala/scalafxml/macwire MacWire support + demo Oct 23, 2014 project Updated dependencies, fixed warnings, dropped 2.10 Feb 19, 2016 subcut/src/main/scala/scalafxml/subcut Updated dependencies, fixed warnings, dropped 2.10 Feb 19, 2016 .gitignore Ignore file update Dec 21, 2013 .travis.yml Updated travis.yml Feb 19, 2016 README.md Update README.md Oct 29, 2014 README.md scalafxml The scalafx library is a great UI DSL that wraps JavaFX classes and provides a nice syntax to work with them from Scala. This library bridges FXML and scalafx by automatically building proxy classes, enabling a more clear controller syntax. Status The main branch contains the initial implementation of the compile time proxy generator, which uses macro annotations. This requires the addition of the macro paradise compiler plugin, but has no runtime dependencies. It depends on ScalaFX 8 and JavaFX 8. The SFX-2 branch is the compile time proxy generator for ScalaFX 2.2 using JavaFX 2. On the dynamic branch there is the first version of the proxy generator which executes runtime. This has a disadvantage of having scala-compiler.jar as a dependency, but has no special compile-time dependencies. The latest published version is 0.2.2. To use it in SBT add: addCompilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.0.1"" cross CrossVersion.full)  libraryDependencies += ""org.scalafx"" %% ""scalafxml-core-sfx8"" % ""0.2.2"" Example The controller's, referenced from the FXML's through the fx:controller attribute, can be implemented as simple Scala classes, getting all the bound controls though the constructor: import scalafx.scene.control.TextField import scalafx.scene.control.Button import scalafx.scene.control.ListView import scalafx.event.ActionEvent import scalafxml.core.macros.sfxml  @sfxml class TestController(     private val input: TextField,     private val create: Button,     private val recentInputs: ListView[String],     private val dep: AnAdditionalDependency) {      // event handlers are simple public methods:     def onCreate(event: ActionEvent) {         // ...     } } Beside the JavaFX controls, additional dependencies can be injected to the controller as well. This injection process is extensible. The following example uses SubCut for injecting the additional dependency: object SubCutDemo extends JFXApp {    implicit val bindingModule = newBindingModule(module => {     import module._      bind [AnAdditionalDependency] toSingle(new AnAdditionalDependency(""subcut dependency""))   })    stage = new JFXApp.PrimaryStage() {     title = ""Test window""     scene = new Scene(         FXMLView(             getClass.getResource(""test.fxml""),             new SubCutDependencyResolver()))   } } but it is also possible to simply give the dependencies by their type or by their name: object SimpleDemo extends JFXApp {    stage = new JFXApp.PrimaryStage() {     title = ""Test window""     scene = new Scene(         FXMLView(getClass.getResource(""test.fxml""),             new DependenciesByType(Map(               typeOf[AnAdditionalDependency] -> new AnAdditionalDependency(""dependency by type""))))    } } Requirements sbt 0.13 is required Related Related blog post explaining how the library works. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/vigoo/scalafxml"	"Scala DSL for creating Graphical User Interfaces that sits on top of JavaFX."	"true"
"Web Frameworks"	"Analogweb"	"http://analogweb.org/"	"Tiny, simple, and pluggable web framework in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"0"	"1"	"0"	"GitHub - analogweb/netty-plugin Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 0 Fork 0 analogweb/netty-plugin Code Issues 1 Pull requests 0 Pulse Graphs No description or website provided. 59 commits 1 branch 11 releases Fetching contributors Java 100.0% Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.9.12 v0.9.11 v0.9.10 v0.9.8 v0.9.7 v0.9.6 v0.9.4 v0.9.3 v0.9.2 v0.9.1 0.9.0 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. src .gitignore .travis.yml README.md pom.xml README.md Analogweb Framework Netty Plugin Analogweb application running on Netty 4! Add this plugin to your application classpath them org.analogweb.core.Servers#run enable to boot Netty server. Quick Start package org.analogweb.hello;  import org.analogweb.annotation.Route; import org.analogweb.core.Servers;  @Route(""/"") public class Hello {    public static void main(String... args) {       Servers.run();   }    @Route   public String hello() {     // Request GET http://localhost:8080/hello and you should see 'Hello World'     return ""Hello World"";   }  } Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/analogweb/netty-plugin"	"Tiny, simple, and pluggable web framework in Scala."	"true"
"Web Frameworks"	"Chaos ★ 182 ⧗ 4"	"https://github.com/mesosphere/chaos"	"A lightweight framework for writing REST services in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"201"	"115"	"32"	"GitHub - mesosphere/chaos: A lightweight framework for writing REST services in Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 115 Star 201 Fork 32 mesosphere/chaos Code Issues 13 Pull requests 6 Pulse Graphs A lightweight framework for writing REST services in Scala. 218 commits 17 branches 40 releases Fetching contributors Scala 78.3% Java 18.4% HTML 2.3% CSS 1.0% Scala Java HTML CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags alternate-s3-wagon aw/#51-get-rid-of-guice-di aw/#55-extract-example aw/#57-macwire aw/#64-cake-pattern-trait-version aw/#64-modules-to-macwire dot-com-downloads gk/upgrade-jackson http-address-option https log4j master mv/gzip_2 releasing-0.7 status-endpoint version-0 wip Nothing to show v0.8.7 v0.8.6 v0.8.5 v0.8.4 v0.8.3 v0.8.2 v0.8.1 v0.8.0 v0.7.2 v0.7.1 v0.7.0 v0.6.7 v0.6.6 v0.6.2 v0.6.1 chaos-0.6.0 chaos-0.5.6 chaos-0.5.5 chaos-0.5.4 chaos-0.5.3 chaos-0.5.2 chaos-0.5.2-SNAPSHOT chaos-0.5.1 chaos-0.5.0 chaos-0.4.9 chaos-0.4.8 chaos-0.4.7 chaos-0.4.6 chaos-0.4.5 chaos-0.4.4 chaos-0.4.3 chaos-0.4.2 chaos-0.4.1 chaos-0.3.6 chaos-0.3.5 chaos-0.3.4 chaos-0.3.3 chaos-0.3.1 chaos-0.3 chaos-0.2.1 Nothing to show New pull request Latest commit 0d83c01 Apr 21, 2016 zunder Merge pull request #66 from mesosphere/mv/gzip_2 … Use Gzip compression. Permalink Failed to load latest commit information. chaos-examples/src/main/scala/mesosphere/chaos/examples Use Gzip compression. Apr 20, 2016 project Use Gzip compression. Apr 20, 2016 src/main Remove welcomeFiles. Apr 21, 2016 .gitignore switch from sbt to maven and add cross version builds Sep 18, 2014 LICENSE Fixing indent bug and adding LICENSE Jun 27, 2013 README.md Fixes #55 - moves chaos examplpe in a separate build. Feb 17, 2016 version.sbt Setting version to 0.8.5-SNAPSHOT Feb 3, 2016 README.md Chaos A lightweight framework for writing REST services in Scala. Chaos (Greek χάος, khaos) refers to the formless or void state preceding the creation of the universe or cosmos in the Greek creation myths. Chaos (the framework) precedes creation of a universe of services. Why yet another framework? At Mesosphere we're building REST services in Scala, and we wanted a solid foundation. We had experience with Dropwizard and Twitter Commons, which are both great Java frameworks, but are a bit hard to use from Scala. We also experimented with Play!, but it does many things besides REST, which adds unnecessary baggage. Design Goals We wanted a framework that is easy to use does one thing really well (REST) feels good in Scala is built on battle-tested and well-supported libraries doesn't try to reinvent the wheel Building Blocks There are great JVM libraries for every part of a REST stack. Chaos just glues these together. Jersey for REST via annotations Guava for lifecycle management and various utilities Jetty as the web server and servlet container Jackson for JSON support Coda Hale's Metrics for JVM and application metrics Getting Started Requirements JDK 1.8+ SBT 0.13.x+ Example App There is an example app in src/main/scala/mesosphere/chaos-examples/. To run the example: sbt run  Make requests to the example endpoints with HTTPie: http localhost:8080/persons http localhost:8080/persons name=Bunny age=42  Built in Endpoints /ping - health check. /metrics - metrics as JSON /logging - configure log levels at runtime Using Chaos in your Project Chaos releases are available from Mesosphere's Maven repository. Maven To add Chaos to a Maven project, add this to your pom.xml: <properties>     <chaos.version>0.5.2</chaos.version> </properties>  ...  <repositories>     <repository>         <id>mesosphere-public-repo</id>         <name>Mesosphere Public Repo</name>         <url>http://downloads.mesosphere.io/maven</url>     </repository> </repositories>  ...  <dependencies>     <dependency>         <groupId>mesosphere</groupId>         <artifactId>chaos</artifactId>         <version>${chaos.version}</version>     </dependency> </dependencies>  SBT To add Chaos to an SBT project, add this to your build.sbt: resolvers += ""Mesosphere Public Repo"" at ""http://downloads.mesosphere.io/maven""  libraryDependencies ++= Seq(   ""mesosphere"" % ""chaos"" % ""0.5.2"",   ""com.sun.jersey"" % ""jersey-bundle"" % ""1.17.1"" )  Getting Help If you have questions, please post on the Chaos Users Group email list. The team at Mesosphere is also happy to answer any questions. Authors Tobias Knaup Florian Leibert Current Users Marathon, an Apache Mesos framework for long-running services. Chronos, a fault tolerant job scheduler that handles dependencies and ISO8601 based schedules. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/mesosphere/chaos"	"A lightweight framework for writing REST services in Scala."	"true"
"Web Frameworks"	"Colossus"	"http://tumblr.github.io/colossus/"	"lightweight framework for building high-performance applications in Scala that require non-blocking network I/O."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"795"	"67"	"68"	"GitHub - tumblr/colossus: I/O and Microservice library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 67 Star 795 Fork 68 tumblr/colossus Code Issues 21 Pull requests 1 Pulse Graphs I/O and Microservice library for Scala 851 commits 13 branches 52 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.7.x collection-trie develop-0.9.0-M1 develop extraTags gh-pages-source-develop gh-pages-source gh-pages http-streaming master metriccontext-tags more-streaming yaka/queue-time Nothing to show v0.8.0 v0.8.0-RC3 v0.8.0-RC2 v0.8.0-RC1 v0.8.0-M1 v0.7.3 v0.7.2 v0.7.2-RC1 v0.7.2-M1 v0.7.1 v0.7.1-RC3 v0.7.1-RC2 v0.7.1-RC1 v0.7.1-M1 v0.7.0 v0.7.0-RC4 v0.7.0-RC3 v0.7.0-RC2 v0.7.0-RC1 v0.7.0-M2 v0.7.0-M1 v0.6.8 v0.6.8-RC1 v0.6.8-M2 v0.6.8-M1 v0.6.7 v0.6.7-RC1 v0.6.6 v0.6.6-RC2 v0.6.5 v0.6.4 v0.6.4-RC3 v0.6.4-RC2 v0.6.3 v0.6.2 v0.6.1 v0.6.1-RC1 v0.6.1-M1 v0.6.0 v0.6.0-RC3 v0.6.0-RC2 v0.6.0-RC1 v0.6.0-M3 v0.6.0-M2 v0.6.0-M1 v0.5.1 v0.5.1-RC2 v0.5.1-M3 v0.5.1-M2 v0.5.1-M1 v0.5.0 0.5.1-RC1 Nothing to show New pull request Latest commit 5ee851d Jul 12, 2016 DanSimon committed on GitHub Merge pull request #427 from tumblr/service-error-logging … break out error logging into overridable method Permalink Failed to load latest commit information. colossus-examples/src/main fixing issues with configuration, sending, frame payload length Jul 1, 2016 colossus-metrics/src more fixes Jun 15, 2016 colossus-testkit/src Refactoring AsyncServiceClient into FutureClient (#415) Jun 24, 2016 colossus-tests/src Merge pull request #420 from tumblr/fix-websocket Jul 8, 2016 colossus/src/main break out error logging into overridable method Jul 12, 2016 project version bump 0.8.1-SNAPSHOT Jun 23, 2016 .gitignore adding header value Jul 6, 2015 .travis.yml travis updates Mar 15, 2016 CONTRIBUTING.md adding contributing Dec 18, 2014 LICENSE initial move to github Nov 12, 2014 README.md covecov.io for colossus Jul 27, 2015 scalastyle-config.xml version change, adding scalastyle config Jan 6, 2015 secrets.tar.enc travis updates Mar 15, 2016 README.md Colossus Colossus is a lightweight I/O framework for building scala services. Full documentation can be found here : http://tumblr.github.io/colossus For general discussion and Q&A, check out the Google Group. Here's an overview of what you can find in this repo colossus : The framework colossus-metrics : high-performance metrics library (does not depend on colossus) colossus-examples : A few simple examples that can be run colossus-testkit : Small library containing a few useful tools for testing colossus-tests : The unit and integration tests for colossus Third-party protocol extensions We're currently asking anyone working on implementing support for new protocols to build your project as a separate repo and publish your own artifacts. This will make it much easier for you to maintain your code as well as help us keep the main codebase slim. Once you have published artifacts that build for the latest Colossus development release, let us know and we'll add it to the list. For any improvements to the core protocols we support, or for general bug fixes and improvements, please see the Contributing Guidelines. Our growing list of known third-party protocol development: MongoDB codec: https://github.com/fehmicansaglam/colossus-extensions License Copyright 2015 Tumblr Inc. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/tumblr/colossus"	"lightweight framework for building high-performance applications in Scala that require non-blocking network I/O."	"true"
"Web Frameworks"	"Finatra ★ 1189 ⧗ 0"	"https://github.com/twitter/finatra"	"A sinatra-inspired web framework for scala, running on top of Finagle."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1296"	"237"	"238"	"GitHub - twitter/finatra: Fast, testable, Scala services built on Twitter-Server and Finagle Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 237 Star 1,296 Fork 238 twitter/finatra Code Issues 17 Pull requests 2 Pulse Graphs Fast, testable, Scala services built on Twitter-Server and Finagle https://twitter.github.io/finatra/ 1,266 commits 4 branches 59 releases 76 contributors Scala 93.7% Python 2.9% Java 2.4% Other 1.0% Scala Python Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop gh-pages-source gh-pages master Nothing to show v2.1.5 v2.1.4 v2.1.3 v2.1.2 v2.1.1 v2.1.0 v2.0.1 v2.0.0 v2.0.0.M2 v2.0.0.M1 finatra-2.2.0 finatra-2.1.6 finatra-1.4.1 finatra-1.4.0 finatra-1.3.9 finatra-1.3.8 finatra-1.3.7 finatra-1.3.4 finatra-1.3.3 finatra-1.3.2 finatra-1.3.1 finatra-1.3.0 finatra-1.2.2 finatra-1.2.0 finatra-1.1.1 finatra-1.1.0 finatra-1.0.3 finatra-1.0.2 finatra-1.0.1 finatra-1.0.0 finatra-0.3.4 finatra-0.3.3 finatra-0.3.2 finatra-0.2.4 finatra-0.2.3 finatra-0.2.1 finatra-0.2.0 finatra-0.1.10 finatra-0.1.9 finatra-0.1.8 finatra-0.1.7 finatra-0.1.6 finatra-0.1.5 finatra-0.1.3 finatra-0.1.2 finatra-0.1.1 finatra-0.1.0 finatra-0.0.1 1.6.0 1.5.4 1.5.3 1.5.2 1.5.1 1.5.0 1.5.0a 1.4.0 1.3.9 1.3.8 1.3.7 Nothing to show New pull request Latest commit 2a45cba Jul 8, 2016 cacoco finatra: Update to next version post-release … Problem/Solution  Update source for development for next release.  RB_ID=850235 Permalink Failed to load latest commit information. .github Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 benchmarks finatra: Narrow visibility on classes/objects in internal packages Jun 27, 2016 bin finagle/finatra/scrooge/twitter-server: bin/travisci updates and fixes Apr 21, 2016 examples finatra: Update to next version post-release Jul 8, 2016 http finatra: Aggregate inject/inject-thrift module in the sbt root Jun 29, 2016 httpclient finatra-httpclient: Pass expected POST body to InMemoryHttpService map Jun 27, 2016 inject-thrift-client-http-mapper finatra: Address lifecycle around com.twitter.inject.app.App#appMain Jun 27, 2016 inject finatra: Add tests for coverage Jul 6, 2016 jackson finatra: Narrow visibility on classes/objects in internal packages Jun 27, 2016 project csl: Release CSL libraries Jul 7, 2016 slf4j finatra: Update BUILD publishing Jun 13, 2016 thrift finatra: Address lifecycle around com.twitter.inject.app.App#appMain Jun 27, 2016 utils finatra: Add tests for coverage Jul 6, 2016 .gitignore Move finatra/examples code to be based on unpublished source for sbt … Jun 22, 2015 .travis.yml Switch to Java 8 and Scala 2.11 May 9, 2016 ADOPTERS.md finatra - Update ADOPTERS.md May 16, 2016 CHANGELOG.md csl: Release CSL libraries Jul 7, 2016 CONTRIBUTING.md finatra: Update CONTRIBUTING.md Jul 7, 2016 LICENSE Add and update files for open-repository Apr 28, 2015 README.md finatra: Update to next version post-release Jul 8, 2016 build.sbt finatra: Update to next version post-release Jul 8, 2016 finatra_logo.png Added transparent logo and updated README.md May 13, 2015 pushdocs.bash finatra - Update for next release version Apr 28, 2016 sbt finatra: Update sbt memory settings May 20, 2016 README.md Finatra Status This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained. Fast, testable, Scala services built on Twitter-Server and Finagle. Getting involved Website: https://twitter.github.io/finatra/ Source: https://github.com/twitter/finatra/ Mailing List: finatra@googlegroups.com Features Production use as Twitter’s HTTP framework ~50 times faster than v1.6 in several benchmarks Powerful feature and integration test support Optional JSR-330 Dependency Injection using Google Guice Jackson based JSON parsing supporting required fields, default values, and custom validations Logback MDC integration with com.twitter.util.Local for contextual logging across futures Presentations Check out our list of recent presentations: Finatra Presentations News Finatra is now built against the latest Finagle v6.33.0 and Twitter Server v1.18.0 releases. Please take a look at our new User Guide! Keep up with the latest news here on our blog. Quick Start To get started we'll focus on building an HTTP API for posting and getting tweets: Domain case class TweetPostRequest(   @Size(min = 1, max = 140) message: String,   location: Option[TweetLocation],   nsfw: Boolean = false)  case class TweetGetRequest(   @RouteParam id: TweetId) Then, let's create a Controller: Controller @Singleton class TweetsController @Inject()(   tweetsService: TweetsService)   extends Controller {    post(""/tweet"") { requestTweet: TweetPostRequest =>     for {       savedTweet <- tweetsService.save(requestTweet)       responseTweet = TweetResponse.fromDomain(savedTweet)     } yield {       response         .created(responseTweet)         .location(responseTweet.id)     }   }    get(""/tweet/:id"") { request: TweetGetRequest =>     tweetsService.getResponseTweet(request.id)   } } Next, let's create a server: Server class TwitterCloneServer extends HttpServer {   override val modules = Seq(FirebaseHttpClientModule)    override def configureHttp(router: HttpRouter): Unit = {     router       .filter[CommonFilters]       .add[TweetsController]   } } And finally, we can write a Feature Test: Feature Test class TwitterCloneFeatureTest extends FeatureTest with Mockito {    override val server = new EmbeddedHttpServer(new TwitterCloneServer)    @Bind val firebaseClient = smartMock[FirebaseClient]    @Bind val idService = smartMock[IdService]    ""tweet creation"" in {     //Setup mocks     idService.getId returns Future(StatusId(""123""))      val tweetResponse = TweetResponse(...)     firebaseClient.put(""/tweets/123.json"", tweetResponse) returns Future.Unit     firebaseClient.get(""/tweets/123.json"")(manifest[TweetResponse]) returns Future(Option(tweetResponse))      //Assert tweet post     val result = server.httpPost(       path = ""/tweet"",       postBody = """"""         {           ""message"": ""Hello #FinagleCon"",           ""location"": {             ""lat"": ""37.7821120598956"",             ""long"": ""-122.400612831116""           },           ""nsfw"": false         }"""""",       andExpect = Created,       withJsonBody = """"""         {           ""id"": ""123"",           ""message"": ""Hello #FinagleCon"",           ""location"": {             ""lat"": ""37.7821120598956"",             ""long"": ""-122.400612831116""           },           ""nsfw"": false         }"""""")      server.httpGetJson[TweetResponse](       path = result.location.get,       andExpect = Ok,       withJsonBody = result.contentString)   }    ""Post bad tweet"" in {     server.httpPost(       path = ""/tweet"",       postBody = """"""         {           ""message"": """",           ""location"": {             ""lat"": ""9999""           },           ""nsfw"": ""abc""         }"""""",       andExpect = BadRequest,       withJsonBody = """"""         {           ""errors"" : [             ""location.lat: [9999.0] is not between -85 and 85"",             ""location.long: field is required"",             ""message: size [0] is not between 1 and 140"",             ""nsfw: 'abc' is not a valid boolean""           ]         }         """""")   } } Detailed Documentation The Finatra project is composed of several libraries. You can find details in a project's README or see the User Guide for detailed information on building applications with Finatra. Example Projects For more detailed information see the README.md within each example project. hello-world A barebones ""Hello World"" service. hello-world-heroku A barebones service that is deployable to Heroku. tiny-url A url shortening example that is deployable to Heroku. twitter-clone An example Twitter-like API for creating and retrieving Tweets. benchmark-server A server used for benchmarking performance compared to a raw finagle-http service. streaming A proof-of-concept streaming JSON service. Authors Steve Cosenza https://github.com/scosenza Christopher Coco https://github.com/cacoco A full list of contributors can be found on GitHub. Follow @finatra on Twitter for updates. License Copyright 2013-2016 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/finatra"	"A sinatra-inspired web framework for scala, running on top of Finagle."	"true"
"Web Frameworks"	"Lift ★ 983 ⧗ 2"	"https://github.com/lift/framework"	"Secure and powerful full stack web framework ()."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1024"	"106"	"241"	"GitHub - lift/framework: Lift Framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 106 Star 1,024 Fork 241 lift/framework Code Issues 140 Pull requests 11 Pulse Graphs Lift Framework http://liftweb.net 3,306 commits 53 branches 54 releases 64 contributors Scala 86.1% JavaScript 10.6% Java 1.3% CSS 1.2% HTML 0.5% Shell 0.3% Scala JavaScript Java CSS HTML Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags ajax-event-extraction ajax-retry-in-order ajk_1801_fixhtmlcmdfunc api-dee asJValue_refactor asc_comet_tagging clean-my-shtml codacy_coverage comet-integration cretaceous-paleogene-extinction dcb-issue-965 doc-stock-func-bridge-attempt dpp_css eventful-evening fail-over future-with-session i-heard-you-like-macros irc_wip_290 joe-ajax-fns-in-session joe-comet-rehydrate joe-trim-pagejs joe-vdom joe_wip_webjar joni_json_scalaz joni_merge_sig joni_893 js-config js-hooks lift-js-spec-localization-fix lkuczera locparams-for-failed-sitemap-lookups marky-mark master migration-script msf_issue_1146 msf_issue_1757 msf_issue_1781 pmb_defaultautosurround pr1001_js_dsl reuben_wip_2010 server-comm-failure-callbacks tcn_issue_1498 tcn_issue_1745 tcn_js_improvements tcn_wip_mocha tcn_wip_webjar time-zone-zone wip-bwm-lazy_mongodb wip-scalaz-crazy-sauce wip_canbind wip_liftjs_30 wip_1017 wip_1018 Nothing to show pre-deprecator-exterminator 3.0-RC3-release 3.0-RC2-release 3.0-RC1-release 3.0-M8-release 3.0-M7-release 3.0-M6-release 3.0-M5-release 3.0-M5-1-release 3.0-M4-release 3.0-M4-1-release 3.0-M3-release 3.0-M2-release 3.0-M1-release 3.0-M0-release 3.0-M0-1-release 2.6.3-release 2.6.2-release 2.6.1-release 2.6-release 2.6-RC2-release 2.6-RC1-release 2.6-M4-release 2.6-M3-release 2.6-M2-release 2.6-M1-release 2.5.4-release 2.5.3-release 2.5.2-release 2.5.1-release 2.5-release 2.5-RC6-release 2.5-RC5-release 2.5-RC4-release 2.5-RC3-release 2.5-RC2-release 2.5-RC1-release 2.5-M4-release 2.5-M3-release 2.5-M2-release 2.5-M1-release 2.4.1-release 2.4-release 2.4-M5-release 2.4-M4-release 2.4-M3-release 2.4-M2-release 2.4-M1-release 2.3-release 2.3-RC5-release 2.3-RC4-release 2.3-RC3-release 2.3-RC2-release 2.3-RC1-release Nothing to show New pull request Latest commit 14f2c11 Jun 29, 2016 Shadowfiend committed on GitHub Merge pull request #1803 from lift/contributions-update … Drop 'small fix' requirement from contributing doc.  Until now, we required that contributions be small fixes or documentation fixes. We've decided to drop that limitation and open Lift to wider contributions, though only committers can merge. Permalink Failed to load latest commit information. artwork Losslessly compressed Lift .png images. Nov 13, 2012 core Correct omniauth-related specs. Mar 15, 2016 docs Add LiftScreen 2.6 to 3.0 migration doc. May 22, 2016 persistence Updated the comment for Record.asJValue Apr 28, 2016 project Bump specs2 to 3.7, scalaz to 7.2.0, fix everything. Feb 29, 2016 scripts Add draft migration script. May 22, 2016 web Parse CSP report as JSON May 17, 2016 .credentials.enc Add encrypted credentials for publishing snapshots. Nov 21, 2015 .gitignore Add 'X-Requested-With' header to liftVanilla Dec 26, 2015 .travis.yml Add soem gitter notification settings. Apr 9, 2016 CONTRIBUTING.md Drop 'small fix' requirement from contributing doc. Jun 28, 2016 LICENSE.txt Add a LICENSE file that contains the full text of the Apache 2.0 lice… Oct 19, 2011 README.md Add 'X-Requested-With' header to liftVanilla Dec 26, 2015 build.sbt Dependency version bumps in advance of 3.0-RC1. Feb 23, 2016 contributors.md contributors.md update Jan 7, 2015 dexy.conf Add dexy config to output stuff into target/docs. Jun 11, 2014 dexy.yaml Dexy runs generated tests for selector examples. Jun 18, 2014 dexyplugin.yaml Add dexy config to output stuff into target/docs. Jun 12, 2014 liftsh Correct liftsh bootstrapping for new sbt. Jun 13, 2015 liftsh.cmd Add a warning comment to the top of liftsh.cmd. Jun 13, 2015 npmsh Added .jshintrc file Dec 30, 2015 travis.sh Added .jshintrc file Dec 30, 2015 unsafePublishLift.sh update build script to build 3.0 from master Jan 17, 2015 README.md The Lift Web Framework Lift is the most powerful, most secure web framework available today. There are Seven Things that distinguish Lift from other web frameworks. Lift applications are: Secure -- Lift apps are resistant to common vulnerabilities including many of the OWASP Top 10 Developer centric -- Lift apps are fast to build, concise and easy to maintain Scalable -- Lift apps are high performance and scale in the real world to handle insane traffic levels Interactive like a desktop app -- Lift's Comet support is unparalled and Lift's ajax support is super-easy and very secure Because Lift applications are written in Scala, an elegant JVM language, you can still use your favorite Java libraries and deploy to your favorite Servlet Container and app server. Use the code you've already written and deploy to the container you've already configured! Pull Requests We will accept pull requests into the Lift codebase if the pull requests meet the following criteria: The request handles an issue that has been discussed on the Lift mailing list and whose solution has been requested (and in general adheres to the spirit of the issue guidelines outlined in CONTRIBUTING.md). The request represents one or more of the following: Documentation including ScalaDoc comments in code Example code Small changes, enhancements, or bug fixes to Lift’s code The request includes a signature at the bottom of the /contributors.md file. For more details, see CONTRIBUTING.md. Getting Started You can create a new Lift project using your favorite build system by adding Lift as a dependency: sbt 0.12.1 Create or update your project/plugins.sbt file with the xsbt-web-plugin: libraryDependencies <+= sbtVersion(v => ""com.github.siasia"" %% ""xsbt-web-plugin"" % (""0.12.0-0.2.11.1""))  Then, add the plugin and Lift to your build.sbt file: seq(webSettings :_*)  libraryDependencies ++= {     val liftVersion = ""2.5-RC1""     Seq(               ""net.liftweb""       %% ""lift-webkit"" % liftVersion % ""compile"",               ""net.liftmodules""   %% ""lift-jquery-module"" % (liftVersion + ""-2.2""),               ""org.eclipse.jetty"" % ""jetty-webapp""        % ""8.1.7.v20120910""  % ""container,test"",               ""org.eclipse.jetty.orbit"" % ""javax.servlet"" % ""3.0.0.v201112011016"" % ""container,test"" artifacts Artifact(""javax.servlet"", ""jar"", ""jar""),               ""ch.qos.logback"" % ""logback-classic"" % ""1.0.6""     ) }  You can learn more on the cookbook. Maven: You can use one of the several archetypes -- lift-archetype-blank, lift-archetype-basic, lift-archetype-jpa-basic -- to generate a new Lift project. You must set archetypeRepository and remoteRepositories to http://scala-tools.org/repo-releases or http://scala-tools.org/repo-snapshots, depending on whether you are using a release or the latest SNAPSHOT. Or, you can add Lift to your pom.xml like so: <dependency>   <groupId>net.liftweb</groupId>   <artifactId>lift-mapper_${scala.version}</artifactId>   <version>2.5.1</version> </dependency>  Where ${scala.version} is 2.9.1 etc. For scala 2.10.x, which is binary compatible, you just use 2.10, and that will work for 2.10.0 ,2.10.1 ,2.10.2 You can learn more on the wiki. Project Organization The Lift Framework is divided into several Git repositories, which in turn are divided into several components that are published independently. This organization enables you to use just the elements of Lift necessary for your project and no more. This Repository This repository, framework, contains the following components: core Core elements used by Lift projects. If you wish to reuse some of Lift's helpers and constructs, such as Box, this component may be all you need. However, a web application will most likely require one or more of Lift's components. web This component includes all of Lift's core HTTP and web handling. Including lift-webkit in your build process should be sufficient for basic applications and will include lift-core as a transitive dependency. persistence This component includes Mapper and Record, Lift's two ORMs. While you needn't use either and can use the ORM of your choice, Mapper and Record integrate nicely with Lift's idioms. Mapper is an ORM for relational databases, while Record is a broader ORM with support for both SQL databases and NoSQL datastores. Other Repostories There are a variety of other repositories available on the Lift GitHub page. While many are concerned with building Lift or are build program archetypes, there are two you will probably encounter fairly frequently as a Lift user: modules The modules repository contains the many add-on modules that are part of the Lift project. If you don't find a module you need here, consider creating a module and sharing it with the community. Please note that the modules project does accept pull requests. examples The examples repository contains the source code for several example Lift applications, including demo.liftweb.com. Building Lift If you simply want to use Lift in your project, add Lift as a dependency to your build system or download the JAR files directly. If you wish to build Lift from source, check out this repository and use the included liftsh script to build some or all of the components you want. git clone https://github.com/lift/framework.git cd framework ./liftsh +update +publish  There is additional documentation on the wiki. Additional Resources Homepage The main Lift website is http://www.liftweb.net. The site contains information on the latest Lift releases, a getting started guide, links to several Lift online books, and additional information. Mailing List The Lift Google Group is the official place for support and is an active, friendly community to boot! It can be found at http://groups.google.com/forum/#!forum/liftweb. Wiki The Lift wiki is hosted on Assembla and can be found at http://www.assembla.com/spaces/liftweb/wiki/. Anyone is welcome to contribute to the wiki; you must create an account and watch the Lift project in order to create or edit wiki pages. ScalaDocs The ScalaDocs for each release of Lift, in additional to the actual JARs, are available on the Liftweb.net site. You can access the source code–based documentation for releases via the site's homepage or by navigating directly to the URL for the specific release. For instance, the Lift 2.5 release can be accessed at http://liftweb.net/api/25/api/. Cookbook You can find up-to-date information on the Lift Cookbook License Lift is open source software released under the Apache 2.0 license. Generally speaking, you must be a committer with signed committer agreement to submit significant changes to Lift. We do, however, accept some small changes and bugfixes into Lift from non-committers as outlined above in the Pull Requests section. Continuous Integration SNAPSHOTs are built by Travis CI Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lift/framework"	"Secure and powerful full stack web framework ()."	"true"
"Web Frameworks"	"peregine ★ 7 ⧗ 35"	"https://github.com/dvarelap/peregrine"	"A simple and async lightweight Scala web framework."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"9"	"4"	"4"	"GitHub - dvarelap/peregrine: Async lightweight Scala web framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 9 Fork 4 dvarelap/peregrine Code Issues 6 Pull requests 1 Wiki Pulse Graphs Async lightweight Scala web framework 90 commits 8 branches 3 releases Fetching contributors Scala 95.5% HTML 4.5% Scala HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.1.0 1.1.1 1.1.2 master v1.2.0 v1.2.1 v1.2.2 v1.2.3 Nothing to show 1.2.0 1.0.1 1.0.0 Nothing to show New pull request Latest commit f170fc0 Dec 17, 2015 dvarelap fixing view problems for config documentation Permalink Failed to load latest commit information. project added publish props/conf Aug 18, 2015 src added error handling for dev env Nov 10, 2015 .gitignore ignoring log/ folder for testing Dec 17, 2015 .travis.yml dropped support for scala 2.10 Nov 9, 2015 LICENSE Initial commit May 18, 2015 README.md bump version to 1.2.2 Nov 11, 2015 build.sbt bump version to 1.2.2 Nov 11, 2015 docs.md fixing view problems for config documentation Dec 17, 2015 README.md peregrine Minimal app: import io.peregrine._  object WebApp extends PeregrineApp {   get(""/hi"") { req =>     ""Hello World!""   } } Install dependency in build.sbt file: scalaVersion := ""2.11.7"" resolvers += ""Twitter"" at ""http://maven.twttr.com"" libraryDependencies += ""com.github.dvarelap"" %% ""peregrine"" % ""1.2.2"" And run with: $ sbt run View at: http://localhost:5000/hi Full Documentation See full documentation here Note Peregrine works only with scala version 2.11+ Licence Copyright 2015 Daniel Varela and other contributors. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/dvarelap/peregrine"	"A simple and async lightweight Scala web framework."	"true"
"Web Frameworks"	"Play ★ 7693 ⧗ 0"	"https://github.com/playframework/playframework"	"Makes it easy to build scalable, fast and real-time web applications with Java & Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"8105"	"756"	"2806"	"GitHub - playframework/playframework: Play Framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 756 Star 8,105 Fork 2,806 playframework/playframework Code Issues 262 Pull requests 25 Pulse Graphs Play Framework http://www.playframework.com/ 7,698 commits 8 branches 105 releases 541 contributors Scala 74.7% Java 23.4% HTML 1.7% Shell 0.1% JavaScript 0.1% Groff 0.0% Scala Java HTML Shell JavaScript Groff Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.0.x 2.1.x 2.2.x 2.3.x 2.4.x 2.5.x fix-5270 master Nothing to show 2.5.4 2.5.3 2.5.2 2.5.1 2.5.0 2.5.0-RC2 2.5.0-RC1 2.5.0-M2 2.5.0-M1 2.4.8 2.4.7 2.4.6 2.4.5 2.4.4 2.4.3 2.4.2 2.4.1 2.4.0 2.4.0-RC5 2.4.0-RC4 2.4.0-RC3 2.4.0-RC2 2.4.0-RC1 2.4.0-M3 2.4.0-M2 2.4.0-M1 2.3.10 2.3.9 2.3.8 2.3.7 2.3.6 2.3.5 2.3.4 2.3.3 2.3.2 2.3.2-RC2 2.3.2-RC1 2.3.1 2.3.0 2.3.0-RC2 2.3.0-RC1 2.3-M1 2.2.6 2.2.5 2.2.4 2.2.3 2.2.3-RC2 2.2.3-RC1 2.2.2 2.2.2-RC4 2.2.2-RC3 2.2.2-RC2 2.2.2-RC1 2.2.1 2.2.1-RC1 2.2.0 2.2.0-RC2 2.2.0-RC1 2.2.0-M3 2.2.0-M2 2.2.0-M1 2.1.6-RC1 2.1.5 2.1.4 2.1.4-RC2 2.1.4-RC1 2.1.3 2.1.3-RC2 2.1.3-RC1 2.1.2 2.1.2-RC2 2.1.2-RC1 2.1.1 2.1.1-RC2 2.1.1-RC1 2.1.1-RC1-2.9.x-backport 2.1.1-2.9.x-backport 2.1.0 2.1-RC4 2.1-RC3 2.1-RC2 2.1-RC1 2.0.8 2.0.7 2.0.6 2.0.5 2.0.5-RC2 2.0.5-RC1 2.0.4 2.0.4-RC2 2.0.4-RC1 2.0.3 2.0.3-RC2 2.0.3-RC1 2.0.2 2.0.2-RC2 2.0.2-RC1 2.0.1 2.0 2.0-beta Nothing to show New pull request Latest commit f8f1883 Jul 15, 2016 richdougherty committed on GitHub Update PR template to say squashing is optional Permalink Failed to load latest commit information. .github Update PR template to say squashing is optional Jul 15, 2016 documentation Don't try to use minified asset in DEV mode (#6291) Jul 10, 2016 framework Fixed #6316 encoding will be applied to asXml (#6317) Jul 12, 2016 templates Remove casting in query (#6290) Jul 1, 2016 .gitignore Replaced iteratees with Akka streams in tests Nov 23, 2015 .travis.yml Add some microbenchmarks (#6297) Jul 6, 2016 CONTRIBUTING.md [doc] Fix PR validation link in contributing guide (#6175) May 21, 2016 README.md Upgrade dependencies Mar 25, 2016 README.md Play Framework - The High Velocity Web Framework The Play Framework combines productivity and performance making it easy to build scalable web applications with Java and Scala. Play is developer friendly with a ""just hit refresh"" workflow and built-in testing support. With Play, applications scale predictably due to a stateless and non-blocking architecture. By being RESTful by default, including assets compilers, JSON & WebSocket support, Play is a perfect fit for modern web & mobile applications. Learn More www.playframework.com Download Install Create a new application Play for Scala developers Play for Java developers Build from source Search or create issues Get help Contribute License This software is licensed under the Apache 2 license, quoted below. Copyright (C) 2009-2016 Lightbend Inc. (https://www.lightbend.com). Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/playframework/playframework"	"Makes it easy to build scalable, fast and real-time web applications with Java & Scala."	"true"
"Web Frameworks"	"Reactive ★ 194 ⧗ 37"	"https://github.com/nafg/reactive"	"FRP and web abstractions, which can be plugged into any web framework (currently only has bindings for Lift)."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"193"	"29"	"40"	"GitHub - nafg/reactive: A simple FRP library and a web UI framework built on it Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 29 Star 193 Fork 40 nafg/reactive Code Issues 25 Pull requests 4 Wiki Pulse Graphs A simple FRP library and a web UI framework built on it http://scalareactive.org 941 commits 27 branches 0 releases Fetching contributors Scala 96.9% HTML 1.7% JavaScript 1.3% CSS 0.1% Scala HTML JavaScript CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: v0.4.0 Switch branches/tags Branches Tags dominiquedevriese-v0.4.0 feature/js2 gh-pages guillaume_demo master scala_2.9.0 scala_2.9.1 squash/routing-refactor tmp v0.4.0 wip/JsStatementStack wip_autocomplete wip/built_in_validation wip_canflatmapsignal wip_canmap2 wip/debugString-srcloc wip_formlets wip_grouped_listeners wip/jscala wip_jseventstream wip/macro-template wip/modularize wip/piglet wip/reactive-transport wip/rich_obs wip/routing-operations wip/routing-refactor Nothing to show Nothing to show New pull request Latest commit e923c96 Jul 3, 2016 nafg fixup! reactive-routing: cross-compile for scala-js Permalink Failed to load latest commit information. project fixup! reactive-routing: cross-compile for scala-js Jul 3, 2016 reactive-core Some dependency version bumps Jul 31, 2015 reactive-jsdsl Update dependencies Jul 1, 2016 reactive-routing fixup! reactive-routing: cross-compile for scala-js Jul 3, 2016 reactive-transport Update dependencies Jul 1, 2016 reactive-web-demo Update dependencies Jul 1, 2016 reactive-web-html Some dependency version bumps Jul 31, 2015 reactive-web-lift Update dependencies Jul 1, 2016 reactive-web-widgets build: set description for all modules Jul 31, 2015 reactive-web Update dependencies Jul 1, 2016 .gitignore Add bin/ to .gitignore May 28, 2014 .travis.yml travis: use jdk8 Jun 29, 2016 LICENSE.txt web-demo: Updated Getting Started; use sonatype rather than git May 18, 2011 README.md Add YourKit credit Sep 4, 2014 build.sbt Update dependencies Jul 1, 2016 version.sbt Bump version to 0.4.0-SNAPSHOT Jun 13, 2013 README.md Project chat: This is the reactive top-level repository. It contains the following subprojects: reactive-core: This is the general-purpose FRP library reactive-web: Powerful abstractions for rich web UIs; web framework agnostic reactive-routing: A fully typesafe bidirectional routing library; web framework agnostic reactive-web-lift: Lift-specific bindings for reactive-web and reactive-routing reactive-web-demo: demo, serves as documentation for the above, and can be viewed live at http://scalareactive.org See the site for more details (http://scalareactive.org) Acknowledgement is providing a free open source license for their full-featured Java Profiler for use with this project. YourKit, LLC makes leading profilers for Java and .NET. Java: http://www.yourkit.com/java/profiler/index.jsp .NET: http://www.yourkit.com/.net/profiler/index.jsp Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nafg/reactive"	"FRP and web abstractions, which can be plugged into any web framework (currently only has bindings for Lift)."	"true"
"Web Frameworks"	"Scalatra ★ 1903 ⧗ 0"	"https://github.com/scalatra/scalatra"	"Tiny Scala high-performance, async web framework, inspired by Sinatra."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1992"	"111"	"282"	"GitHub - scalatra/scalatra: Tiny Scala high-performance, async web framework, inspired by Sinatra Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 111 Star 1,992 Fork 282 scalatra/scalatra Code Issues 62 Pull requests 6 Pulse Graphs Tiny Scala high-performance, async web framework, inspired by Sinatra http://scalatra.org 2,726 commits 23 branches 101 releases 81 contributors Scala 96.1% Shell 2.0% CSS 1.4% Other 0.5% Scala Shell CSS Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 2.4.x Switch branches/tags Branches Tags 2.0.x 2.1.x 2.2.x_2.9 2.2.x_2.10 2.3.x 2.3.x_2.10 2.4.x atmosphere-no-sessions develop feature/no-default-value master metrics prepublish scalatra-3.0 scalatra-host spike/klone-2 spike/macro-2 spike/macroscope spike/plugin topic/async-result-apply topic/async-wrappers topic/http4s topic/nothreadlocal Nothing to show v2.4.1 v2.4.0 v2.4.0.RC3 v2.4.0.RC2 v2.4.0.RC2-2 v2.4.0.RC2-1 v2.4.0.RC1 v2.4.0.M3 v2.4.0.M2_2.11 v2.4.0.M2_2.10 v2.4.0.M1_2.11 v2.4.0.M1_2.10 v2.3.1 v.2.3.0 v.2.3.0.RC3 v2.3.0.RC2 v2.3.0.M1_2.10 v2.2.2_2.10 v2.2.2_2.9 v2.2.1_2.10 v2.2.1 v2.2.0 v2.1.2 v2.0.5 step_2.8.0.RC6-1.2.0 step_2.8.0.RC5-1.2.0 step_2.8.0.RC3-1.2.0 step_2.8.0.RC2-1.2.0 step_2.7.7-1.2.0 step-1.1.5 step-1.1 step-1.0 scalatra_2.9.1.RC4-2.0.0.RC1 scalatra_2.9.1-2.0.2 scalatra_2.9.1-2.0.1 scalatra_2.9.1-2.0.0 scalatra_2.9.0.1-2.0.0.M4 scalatra_2.9.0-2.0.2 scalatra_2.9.0-2.0.1 scalatra_2.9.0-2.0.0 scalatra_2.9.0-2.0.0.RC1 scalatra_2.9.0-2.0.0.M4 scalatra_2.9.0-1-2.0.2 scalatra_2.9.0-1-2.0.1 scalatra_2.9.0-1-2.0.0 scalatra_2.9.0-1-2.0.0.RC1 scalatra_2.9.0-1-2.0.0.M4 scalatra_2.8.2.RC1-2.0.0.RC1 scalatra_2.8.1-2.0.2 scalatra_2.8.1-2.0.1 scalatra_2.8.1-2.0.0 scalatra_2.8.1-2.0.0.RC1 scalatra_2.8.1-2.0.0.M4 scalatra_2.8.1-2.0.0.M3 scalatra_2.8.1-2.0.0.M2 scalatra_2.8.0.RC7-1.2.0 scalatra_2.8.0.RC6-1.2.0 scalatra_2.8.0-2.0.0.M3 scalatra_2.8.0-2.0.0.M2 scalatra_2.8.0-2.0.0.M1 scalatra_2.8.0-1.2.1 scalatra_2.7.7-1.2.1 scalatra_2.7.7-1.2.0 2.10_2.2.0 2.3.0.RC1 2.2.0_2.10-RC3 2.2.0-RC3 2.2.0-RC2 2.2.0-RC1 2.1.1 2.1.0 2.1.0.M2 2.1.0.M1 2.1.0-RC1 2.0.4 2.0.3 2.0.2 2.0.1_2.9 2.0.1_2.8 2.0.0_2.9 2.0.0_2.8 2.0.0.RC1_2.9 2.0.0.RC1_2.8 2.0.0.M4_2.9 2.0.0.M4_2.8 2.0.0.M3 2.0.0.M2 2.0.0.M1 1.2.1_2.8 1.2.1_2.7.7 1.2.0_step_2.8.0.RC6 1.2.0_step_2.8.0.RC5 1.2.0_step_2.8.0.RC3 1.2.0_step_2.8.0.RC2 1.2.0_step_2.7.7 1.2.0_scalatra_2.8.0.RC7 1.2.0_scalatra_2.8.0.RC6 1.2.0_scalatra_2.7.7 1.1.5 1.1 Nothing to show New pull request Latest commit f4385c2 Jul 15, 2016 rossabaker committed on GitHub Merge pull request #578 from megaminx/2.4.x … Escape javascript to avoid Xss Permalink Failed to load latest commit information. atmosphere/src update atmosphere to version 2.4.3 May 17, 2016 auth/src Remove past json files for http://ls.implicit.ly/ May 16, 2015 cache-guava/src Remove past json files for http://ls.implicit.ly/ May 16, 2015 cache/src Fixes double evaluation of result Sep 25, 2015 commands/src scalaz 7.2.1, specs2 3.7.2 Mar 9, 2016 common/src/main/scala/org/scalatra Remove past json files for http://ls.implicit.ly/ May 16, 2015 core/src Escape javascript to avoid Xss Jul 3, 2016 example/src/main Optimize imports by using IntelliJ IDEA Dec 24, 2014 fileupload/src Remove past json files for http://ls.implicit.ly/ May 16, 2015 jetty/src Remove past json files for http://ls.implicit.ly/ May 16, 2015 json/src Fixes #486. May 24, 2015 metrics/src Merge pull request #505 from scalatra/topic/metrics-scala-3.5.x May 17, 2015 notes version 2.4.0 Dec 15, 2015 project Escape javascript to avoid Xss Jul 3, 2016 scalate/src Fix error handling with Future and errorHandler May 24, 2016 scalatest/src/main/scala/org/scalatra/test/scalatest Remove past json files for http://ls.implicit.ly/ May 16, 2015 slf4j/src/main/scala/org/scalatra/slf4j Have valid request/response in scope when an AtmosphereClient is cons… Jun 12, 2015 specs2/src updated specs2 to 3.6.+ May 21, 2015 spring/src Optimize imports by using IntelliJ IDEA Dec 23, 2014 swagger-ext/src Remove past json files for http://ls.implicit.ly/ May 16, 2015 swagger/src updated specs2 to 3.6.+ May 21, 2015 test/src updated specs2 to 3.6.+ May 21, 2015 .gitignore define .jvmopts May 15, 2014 .jvmopts define .jvmopts May 16, 2014 .travis.yml Bump minor versions of libs May 25, 2016 CONTRIBUTING.markdown Found another typo. Oct 20, 2015 CREDITS.md Credits and notes. Jun 11, 2013 LICENSE Rename project to Scalatra. Thanks to Hiram Chirino for the name. Jul 3, 2010 README.markdown Bump minor versions of libs May 25, 2016 crosspaths.sbt fiddle a bit with the build, fix tostring on a binding Mar 28, 2013 publishing.sbt upgrade a bunch of dependencies but not jetty Aug 25, 2013 sbt Fixing typesafe repository. Jul 31, 2015 version.sbt update atmosphere to version 2.4.3 May 17, 2016 README.markdown Scalatra Scalatra is a tiny, Sinatra-like web framework for Scala. Example import org.scalatra._  class ScalatraExample extends ScalatraServlet {   get(""/"") {     <h1>Hello, world!</h1>   } } Documentation If you're just starting out, see the installation and first project sections of our website. Once you've done that, take a look at the Scalatra Guides for documentation on all aspects of the framework, code examples, and more. We also have an extensive selection of Example Applications which accompany the tutorials in the Scalatra Guides. Latest version The latest stable version of Scalatra is 2.4.+, and is published to Maven Central. libraryDependencies += ""org.scalatra"" %% ""scalatra"" % ""2.4.+"" Development version The 2.4.x branch is published as 2.4.{x}-SNAPSHOT to OSSRH. resolvers += ""Sonatype Nexus Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots""  libraryDependencies += ""org.scalatra"" %% ""scalatra"" % ""2.4.{x}-SNAPSHOT"" Community Mailing list: scalatra-user IRC: #scalatra on irc.freenode.org Guidelines for contributing Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalatra/scalatra"	"Tiny Scala high-performance, async web framework, inspired by Sinatra."	"true"
"Web Frameworks"	"Skinny Framework ★ 535 ⧗ 0"	"https://github.com/skinny-framework/skinny-framework"	"A full-stack web app framework upon Scalatra for rapid Development in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"568"	"59"	"60"	"GitHub - skinny-framework/skinny-framework: ""Scala on Rails"" - A full-stack web app framework for rapid development in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 59 Star 568 Fork 60 skinny-framework/skinny-framework Code Issues 12 Pull requests 0 Pulse Graphs 🚝 ""Scala on Rails"" - A full-stack web app framework for rapid development in Scala http://skinny-framework.org/ 1,746 commits 7 branches 145 releases Fetching contributors Scala 69.9% JavaScript 26.6% HTML 1.2% Shell 0.9% Java 0.7% Batchfile 0.7% Scala JavaScript HTML Shell Java Batchfile Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0.x 1.1.x 1.2.x 1.3.x 2.0.x 2.1.x master Nothing to show 2.1.2 2.1.1 2.1.0 2.1.0.RC1 2.0.9 2.0.8 2.0.7 2.0.6 2.0.5 2.0.4 2.0.3 2.0.2 2.0.1 2.0.0 2.0.0.RC4 2.0.0.RC3 2.0.0.RC2 2.0.0.RC1 2.0.0.M4 2.0.0.M3 2.0.0.M2 2.0.0.M2-20150725 2.0.0.M1-20150718 2.0.0.M1-20150716 2.0.0.M1-20150715 1.3.20 1.3.19 1.3.18 1.3.17 1.3.16 1.3.15 1.3.14 1.3.13 1.3.12 1.3.11 1.3.10 1.3.9 1.3.8 1.3.6 1.3.5 1.3.4 1.3.4-1 1.3.3 1.3.2 1.3.1 1.3.0 1.2.10 1.2.9 1.2.8 1.2.7 1.2.6 1.2.5 1.2.4 1.2.3 1.2.2 1.2.1 1.2.0 1.2.0-2 1.2.0-1 1.1.8 1.1.7 1.1.6 1.1.5 1.1.4 1.1.3 1.1.2 1.1.1 1.1.1-1 1.1.0 1.1.0.RC2 1.1.0.RC1 1.1.0-2 1.1.0-1 1.0.18 1.0.17 1.0.16 1.0.15 1.0.14 1.0.13 1.0.12 1.0.11 1.0.10 1.0.9 1.0.8 1.0.7 1.0.6 1.0.6-1 1.0.5 1.0.4 1.0.3 1.0.2 1.0.1 1.0.0 1.0.0-RC13 1.0.0-RC12 1.0.0-RC11 1.0.0-RC10 1.0.0-RC10-5 1.0.0-RC10-4 1.0.0-RC10-2 Nothing to show New pull request Latest commit 1a9487e Jul 16, 2016 seratch Fix a bug on factory girl conf generation Permalink Failed to load latest commit information. assets Bump minor version of deps, fix deprecation warnings May 7, 2016 bin Bump sbt version to 0.13.11, bump minor version of libs Feb 29, 2016 common Various improvements on reverse-scaffold generator Jul 14, 2016 example Revert thymeleaf 3 support Jun 19, 2016 factory-girl Fix #322 FactoryGirl cannot findfactories directory when '%20' appear… May 14, 2016 framework Bump ScalikeJDBC version to 2.4.0.RC2 May 4, 2016 freemarker Bump minor version of deps, fix deprecation warnings May 7, 2016 http-client Bump ScalikeJDBC version to 2.4.0.RC2 May 4, 2016 img Updated logo Jun 16, 2014 json Bump sbt-scalariform version to 1.6.0 Feb 14, 2016 mailer Bump sbt-scalariform version to 1.6.0 Feb 13, 2016 oauth2-controller Remove graphSettings Feb 12, 2016 oauth2 Bump sbt-scalariform version to 1.6.0 Feb 13, 2016 orm Various improvements on reverse-scaffold generator Jul 13, 2016 project Revert thymeleaf 3 support Jun 19, 2016 scaldi Bump sbt-scalariform version to 1.6.0 Feb 13, 2016 skinny-blank-app Various improvements on reverse-scaffold generator Jul 13, 2016 standalone Remove graphSettings Feb 12, 2016 task Fix a bug on factory girl conf generation Jul 16, 2016 test Fixed MockController doesn't mocks multiParams May 19, 2016 thymeleaf Revert thymeleaf 3 support Jun 19, 2016 twitter-controller Remove graphSettings Feb 12, 2016 validator Format code Mar 9, 2016 velocity Bump sbt-scalariform version to 1.6.0 Feb 13, 2016 worker Remove graphSettings Feb 12, 2016 .gitattributes Introduce end-of-line normalization May 10, 2014 .gitignore Add skinny-engine-blank-app Jul 22, 2015 .travis.yml Fix Travis settings May 8, 2016 LICENSE.txt version 2.0.7 Feb 14, 2016 README.md version 2.0.7 Feb 13, 2016 build.sbt Fix build error Jan 4, 2015 create_blank_app.sh Fix blank app creation script Nov 22, 2015 create_homebrew_formula.sh Fix Scala.js support, source maps issue, update new command Nov 21, 2015 create_local_ivy2 Fix blank app creation script Nov 22, 2015 publish.sh version 2.0.8 Mar 26, 2016 publish_local.sh Bump scala 2.11 to 2.11.8 Mar 9, 2016 run_skinny-blank-app_test.sh Addd skinny routes command to display available routes information Jan 9, 2016 setup.sh Fix sbt scripts for global Aug 8, 2014 skinny Bump dependencies, scala version Oct 26, 2015 skinny.bat Updated readme Oct 28, 2013 travis.sh Fix Travis settings May 7, 2016 travis_after.sh Fix coveralls setting May 12, 2015 README.md Skinny Framework Skinny is a full-stack web app framework to build Servlet applications. To put it simply, Skinny framework's concept is Scala on Rails. Skinny is highly inspired by Ruby on Rails and it is optimized for sustainable productivity for ordinary Servlet-based app development. See the website in detail. http://skinny-framework.org/ Global skinny script via Homebrew brew update && brew install skinny Try Skinny Framework now! Download latest skinny-blank-app.zip and unzip it, then just run ./skinny command on your terminal. That’s all! If you’re a Windows user, don’t worry. Use skinny.bat on cmd.exe instead. https://github.com/skinny-framework/skinny-framework/releases Under The MIT License (The MIT License) Copyright (c) skinny-framework.org Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/skinny-framework/skinny-framework"	"A full-stack web app framework upon Scalatra for rapid Development in Scala."	"true"
"Web Frameworks"	"Socko"	"http://sockoweb.org/"	"An embedded Scala web server powered by Netty networking and Akka processing."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Socko Web Server Check Socko out on GitHub SOCKO WEB SERVER Home Features Download Documentation Blog An embedded Scala web server powered by Netty networking and Akka processing. We are currently working on a HTML5 style app called MashupBots. We wanted an open source, lightweight and embeddable Scala web server that can serve static files and support RESTful APIs to our business logic implemented in Akka. We do not need a web application framework: server side templating, caching, session management, etc. We couldn't find a web server that exactly met our requirements, so we decided to write one. We hope you find it as useful as we do. Socko is ... Intended for Scala and Akka developers Put a RESTful API in front of your Akka actors. Serve static files. Use our Unfiltered like routing DSL. Embedded into your App Lightweight v0.2 has > 4000 lines of Scala and Java. Only depedancy is Netty and Akka 2.0 Supportive of HTTP and HTML5 Standards HTTP/S, WebSockets, Compression, Headers, Post Body Decoding, SPDY Fast-ish Open Source Read More ... What Socko is NOT Socko is not a standalone web server. It must be embedded within your application. Socko is not a servlet container. It will not run your servlet or JSP pages. Socko is not a web application or MVC framework like Lift or Play. It does not perform server side HTML templating. We use client side javascript libraries like EmberJS and BackboneJS instead. We couldn't find a web server that exactly met our requirements, so we decided to write one. Home Features Download Documentation Blog © 2012 Vibul, David & Socko Contributors"	"null"	"null"	"An embedded Scala web server powered by Netty networking and Akka processing."	"true"
"Web Frameworks"	"Spray ★ 2284 ⧗ 0"	"https://github.com/spray/spray"	"A suite of scala libraries for building and consuming RESTful web services on top of Akka."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2362"	"198"	"560"	"GitHub - spray/spray: A suite of scala libraries for building and consuming RESTful web services on top of Akka: lightweight, asynchronous, non-blocking, actor-based, testable Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 198 Star 2,362 Fork 560 spray/spray Code Issues 67 Pull requests 18 Wiki Pulse Graphs A suite of scala libraries for building and consuming RESTful web services on top of Akka: lightweight, asynchronous, non-blocking, actor-based, testable http://spray.io 2,652 commits 26 branches 67 releases 76 contributors Scala 63.7% HTML 17.8% CSS 12.3% JavaScript 6.1% Shell 0.1% Scala HTML CSS JavaScript Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/integrate-spray-json feature/json-interpolator feature/openssl-engine-1.0-M7 fix-xxe master release/1.0 release/1.1 release/1.2-M8 release/1.2 release/1.3 ticket/780 w/simplify-deflate w/ssl-note w/852-encode-ChunkedResponseStart-with-empty-entity w/861-support-transfer-encoding-identity w/879-fix-request-level-error-message w/903 w/936-fix-NoEncoding w/959-fix-PathMatcher-NumberMatcher w/984-fix-directives-map wip-954-mathias wip-965-mathias wip-977-mathias wip-992-mathias wip-1006-mathias wip-1019-mathias Nothing to show wip/spdy2/hacky-client-server-push v1.3.3 v1.3.2 v1.3.1_2.11 v1.3.1 v1.3.0 v1.3-RC4 v1.3-RC1 v1.3-M2 v1.3-M1 v1.2.3 v1.2.2 v1.2.1 v1.2.0 v1.2-RC4 v1.2-RC3 v1.2-RC2 v1.2-RC1 v1.2-M8 v1.1.3 v1.1.2 v1.1.1 v1.1.0 v1.1-RC4 v1.1-RC3 v1.1-RC2 v1.1-RC1 v1.1-M8 v1.1-M7 v1.1-M6 v1.1-M5 v1.1-M4.2 v1.1-M4.1 v1.1-M4 v1.0.1 v1.0.0 v1.0-RC4 v1.0-RC3 v1.0-RC2 v1.0-RC1 v1.0-M8.3 v1.0-M8.2 v1.0-M8.1 v1.0-M8 v1.0-M7 v1.0-M6 v1.0-M5 v1.0-M4.2 v1.0-M4.1 v1.0-M4 v1.0-M3.1 v1.0-M3 v1.0-M2 v1.0-M2.x v1.0-M1 v0.9.0 v0.9.0-RC4 v0.9.0-RC3 v0.9.0-RC2 v0.9.0-RC1 v0.8.0 v0.8.0-RC3 v0.8.0-RC2 v0.8.0-RC1 v0.7.0 v0.5.0 old-release-1.0.x Nothing to show New pull request Latest commit b473d9e Jul 13, 2016 sirthias committed on GitHub Merge pull request #1119 from ametrocavich/master … Fix typo Permalink Failed to load latest commit information. docs = site: add scala.world talk reference Oct 22, 2015 examples = examples: remove misleading `CloseAll` in client examples, fixes #921 May 18, 2015 notes Rename domain name from spray.cc to spray.io, rename packages from 'c… Oct 12, 2012 project = site: fix missing APIdoc link Mar 24, 2015 site/src = site: extend production logging timestamps with milli seconds Sep 18, 2015 spray-caching/src Update copyright year to 2015 Mar 8, 2015 spray-can-tests/src/test/scala/spray/can = can: adding test proving effectiveness of the latest fix also for #964 Mar 24, 2015 spray-can/src/main Merge pull request #1033 from spray/wip-965-mathias Mar 24, 2015 spray-client/src Update copyright year to 2015 Mar 8, 2015 spray-http/src Fix typo Jul 13, 2016 spray-httpx/src Merge pull request #1024 from spray/fix-xxe Mar 24, 2015 spray-io-tests/src/test Update copyright year to 2015 Mar 8, 2015 spray-io/src/main Merge pull request #994 from briandignan/bugfix-dnsQueryOfIpWhenSslEn… Mar 23, 2015 spray-routing-tests/src/test = routing: fix tests broken with last commit May 17, 2015 spray-routing/src/main = routing: fix typos in RejectionHandler error messages May 16, 2015 spray-servlet/src Update copyright year to 2015 Mar 8, 2015 spray-testkit/src Update copyright year to 2015 Mar 8, 2015 spray-util/src Update copyright year to 2015 Mar 8, 2015 .gitignore Improve .gitignore, in particular exclude Eclipse project files (closes Mar 30, 2013 .jvmopts = build: use travis docker-based environment Dec 19, 2014 .travis.yml = build: upgrade .travis.yml to Scala 2.10.4 Feb 10, 2015 CHANGELOG = project: update CHANGELOG for 1.x.3 release Mar 24, 2015 CONTRIBUTING.md = build: add CONTRIBUTING.md to show in github pull requests Jul 21, 2013 LICENSE Update copyright year to 2015 Mar 8, 2015 README.markdown = project: add Gitter badge to README Feb 10, 2015 README.markdown Documentation Please see http://spray.io/ for all documentation Code Climate Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/spray/spray"	"A suite of scala libraries for building and consuming RESTful web services on top of Akka."	"true"
"Web Frameworks"	"Unfiltered ★ 626 ⧗ 12"	"https://github.com/unfiltered/unfiltered"	"A modular set of unopinionated primitives for servicing HTTP and WebSocket requests in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"641"	"37"	"115"	"GitHub - unfiltered/unfiltered: A toolkit for servicing HTTP requests in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 37 Star 641 Fork 115 unfiltered/unfiltered Code Issues 31 Pull requests 6 Pulse Graphs A toolkit for servicing HTTP requests in Scala http://unfiltered.databinder.net/ 2,059 commits 74 branches 37 releases 58 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 0.9.0 Switch branches/tags Branches Tags 0.1.7_wsfixes 0.7.x 0.7.1-scala-2.11 0.8.2 0.8.3_trimmed 0.8.3 0.8.4 0.9.0 021 030 050 basicauth_kit bayeux close_problem composed_response_case conneg coooookies default_settings_warning deprecate_qparams directive_filter directives-imports directives drop2_7_7 encode-netty-param-keys event-source gzip_fix header_parsing httponly_cookies jetty9replacement jetty9 jonijetty9 links_267 ls lslocal managed-dispatch-shutdown master netty-chunked-uploads-max-size-255 netty-jar-file-resources netty-server-resources netty-upload-bigfile netty-uploads netty_configurable_exec netty_put_params netty_upgrade_4_0_21 netty4 new_netty_websockets oauth2 passing passingintent pathmap pf013 porthole portmacro resource_handler_order resources-encoder-issues resp_fn_and_then revert-306-0.9.0 sbt_010 scalatest-2.2.0 scalaz server_builder_refactor_netty servlet_writer travis-to-openjdk7 unqualified_names update_netty update_netty_4.0.24 upgrade_scalatest uploader-enhancement websocket_issues websockets-draft-14 ws_order wss_tests xmpp xuwei-k-reflective-calls Nothing to show v0.8.2 v0.8.1 v0.8.0 v0.7.1 0.8.1 0.8.0 0.7.1 0.7.1-beta1 0.7.0 0.7.0-beta1 0.6.8 0.6.7 0.6.6 0.6.5 0.6.4 0.6.3 0.6.2 0.6.1 0.6.0 0.5.3 0.5.2 0.5.1 0.5.0 0.4.1 0.4.0 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 0.2.3 0.2.2 0.2.1 0.2.0 0.1.4 0.1.3 0.1.0 Nothing to show New pull request Latest commit 2e10ca6 Oct 4, 2015 softprops Merge pull request #308 from xuwei-k/json4s-3.3 … update json4s 3.3.0 Permalink Failed to load latest commit information. agents ls versions Jan 11, 2015 directives ls versions Jan 12, 2015 docs Merge pull request #295 from jedesah/patch-2 Apr 13, 2015 filter-async Merge branch 'master' into jonijetty9 Feb 15, 2015 filter-uploads ls versions Jan 12, 2015 filter ls versions Jan 12, 2015 jetty-ajp/src/main/ls Merge branch 'master' into jonijetty9 Feb 15, 2015 jetty Merge branch 'master' into jonijetty9 Feb 15, 2015 json4s update json4s 3.3.0 Oct 4, 2015 library Revert ""updates specs2 version"" Sep 30, 2015 mac ls versions Jan 12, 2015 netty-server ls versions Jan 12, 2015 netty-uploads ls versions Jan 12, 2015 netty-websockets ls versions Jan 12, 2015 netty update netty to latest 4.0.X.Final version (4.0.30.Final) Sep 2, 2015 notes 0.8.4 release notes Jan 11, 2015 oauth ls versions Jan 12, 2015 oauth2 Merge branch 'master' into jonijetty9 Feb 15, 2015 project Revert ""updates specs2 version"" Sep 30, 2015 scalatest Example is slightly changed to fix compilation errors Sep 22, 2015 specs2 Revert ""updates specs2 version"" Sep 30, 2015 src/extern an icon May 30, 2012 uploads ls versions Jan 12, 2015 util ls versions Jan 12, 2015 .gitignore Merge branch 'master' into oauth2 May 31, 2012 .travis.yml bump to openjdk7 Sep 1, 2015 LICENSE Update LICENSE Feb 2, 2014 README.markdown fix servlet version Jan 20, 2015 build.sbt extract settings into build.sbt files Jun 4, 2012 README.markdown Unfiltered See the the Unfiltered documentation for instructions on using the project. Modules library The core application library for Unfiltered. This module provides interfaces and implementations of core request extractors and response combinators. filter Binds the core library to filters in the servlet 3.0 API. filter-async Provides asynchronous support for the filter module jetty Provides an embedded web server abstraction for serving filters. jetty-ajp An embedded server that adheres to the ajp protocol. netty Binds the core library to a Netty channel handler and provides an embedded server. netty-uploads Provides extractors for multipart posts using netty. specs2 Provides helpers for testing Intents with specs2. uploads Provides extractors for multipart posts using the servlet API. json Provides extractors for working with jsonp and transforming json request bodies. scalate Scalate template support. websockets A minimal server websocket interface build on netty Community Join the Unfiltered mailing list on Google Groups or view the previous list on Nabble. Example Apps There's an in-progress example app using Unfiltered made by klaeufer, unfiltered-example-bookmarks. Also, most giter8 templates for Unfiltered contain a bit of example code. unfiltered-netty.g8 g8 template for netty webservers unfiltered-war.g8 g8 template configured with sbt war plugin unfiltered-websockets.g8 g8 template for websocket based chat app unfiltered.g8 g8 template example specs tests QParams validators and org.clapper.avsl.Logger logging configuration unfiltered-gae.g8 g8 template for google app engine deployment coffee-filter.g8 g8 template of unfiltered app using coffeescript and less css unfiltered-oauth-server.g8 g8 template of example unfiltered oauth server Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/unfiltered/unfiltered"	"A modular set of unopinionated primitives for servicing HTTP and WebSocket requests in Scala."	"true"
"Web Frameworks"	"Xitrum"	"http://xitrum-framework.github.io/"	"An async and clustered Scala web framework and HTTP(S) server fusion on top of Netty, Akka, and Hazelcast."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Xitrum Scala web framework · GitHub Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This organization Xitrum Scala web framework http://xitrum-framework.github.io/ xitrum-framework@googlegroups.com Repositories People 2 Filters Sources Forks Scala 369 44 xitrum Async and clustered Scala web framework and HTTP(S) server Updated Jul 6, 2016 HTML 1 2 xitrum-framework.github.io Source code of the home page of Xitrum framework Updated Jun 22, 2016 CSS 7 9 xitrum-doc Xitrum Guide Book and presentations about Xitrum Updated Jun 22, 2016 Scala 14 3 comy Simple URL shortener using Xitrum and MongoDB Updated Jun 22, 2016 Scala 20 5 xitrum-demos Demos for Xitrum Updated Jun 22, 2016 Scala 0 0 xitrum-google-vision Google Vision demo Updated Jun 22, 2016 Scala 2 1 xitrum-multimodule-demo http://groups.google.com/group/xitrum-framework/browse_thread/thread/7588995934854a56 Updated Jun 22, 2016 HTML 16 4 xitrum-new Empty Xitrum project skeleton, like the one created by ""rails new"" Updated Jun 22, 2016 Scala 0 0 xitrum-placeholder xtrium implementation for placeholder.it Updated Jun 22, 2016 Scala 0 0 xitrum-sockjs-test SockJS protocol test for Xitrum: https://github.com/sockjs/sockjs-protocol Updated Jun 22, 2016 Scala 1 1 xitrum-tictactoe Tictactoe web game - Demo for Akka and Xitrum Updated Jun 22, 2016 Scala 27 4 scaposer GNU Gettext .po file loader for Scala Updated Jun 12, 2016 Java 4 0 RhinoCoffeeScript coffee-script.js precompiled to JVM .class file by Rhino Updated May 29, 2016 Scala 16 5 scala-xgettext Scala 2.10 and 2.11 compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext .po file Updated May 23, 2016 Scala 46 2 glokka Library to register and lookup actors by names in an Akka cluster Updated Mar 29, 2016 0 0 xitrum-metrics Updated Jan 7, 2016 Scala 0 0 sclannotation (Not released yet) Updated Aug 13, 2015 Scala 9 0 sclasner Scala classpath scanner Updated Aug 13, 2015 JavaScript 1 0 xitrum-ko Knockout.js plugin for Xitrum Updated Aug 13, 2015 Scala 19 0 xitrum-package SBT plugin for collecting dependency .jar files for standalone Scala programs Updated Aug 13, 2015 Previous 1 2 Next 2 People georgeOsdDev Takeharu.Oshida ngocdaothanh Ngoc Dao Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/"	"An async and clustered Scala web framework and HTTP(S) server fusion on top of Netty, Akka, and Hazelcast."	"true"
"Reactive Web Frameworks"	"Binding.scala ★ 79 ⧗ 0"	"https://github.com/ThoughtWorksInc/Binding.scala"	"A reactive web framework. It enables you use native XML literal syntax to create reactive DOM nodes, which are able to automatically change whenever the data source changes."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"155"	"51"	"11"	"GitHub - ThoughtWorksInc/Binding.scala: Reactive data-binding for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 51 Star 155 Fork 11 ThoughtWorksInc/Binding.scala Code Issues 0 Pull requests 0 Pulse Graphs Reactive data-binding for Scala 343 commits 8 branches 26 releases Fetching contributors Scala 99.4% Shell 0.6% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags BindableRope.map-wip array-publisher gh-pages master rhtml-wip rhtml scalaz-7.1 xmlns-wip Nothing to show v7.0.0 v6.0.1 v6.0.0 v5.4.0 v5.3.1 v5.3.0 v5.2.0 v5.1.0 v4.0.2 v4.0.1 v4.0.0 v3.1.0 v3.0.1 v3.0.0 v2.0.1 v2.0.0 v1.1.2 v1.1.1 v1.1.0 v1.0.6 v1.0.5 v1.0.4 v1.0.3 v1.0.2 v1.0.1 v1.0.0 Nothing to show New pull request Latest commit 1135a93 Jul 9, 2016 Atry Replace deprecated `each` to `bind` Permalink Failed to load latest commit information. core Add link in Scaladoc Jul 9, 2016 dom Backport to Scala 2.10 Jul 9, 2016 project Setup source mapping Apr 15, 2016 .gitignore Merge branch 'ci' of https://github.com/Atry/ci-template-for-sbt-libr… Feb 1, 2016 .sbtopts Setup stack size for sbt Dec 29, 2015 .travis.yml Install Graphviz Jul 9, 2016 LICENSE Add copyright and license statement Jan 6, 2016 README.md Replace deprecated `each` to `bind` Jul 9, 2016 build.sbt Backport to Scala 2.10 Jul 9, 2016 ci.sbt Setup CI for deployment Feb 1, 2016 deploy.sh.disabled Setting version to 7.0.0 Jul 9, 2016 pubring.asc Setup CI for deployment Jan 31, 2016 version.sbt Setting version to 7.0.1-SNAPSHOT Jul 9, 2016 README.md Binding.scala Binding.scala is a data-binding framework for Scala, running on both JVM and Scala.js. Binding.scala can be used as a reactive web framework. It enables you use native XHTML literal syntax to create reactive DOM nodes, which are able to automatically change whenever the data source changes. See Binding.scala • TodoMVC or other live DEMOs as examples for common tasks when working with Binding.scala. Comparison to other reactive web frameworks Binding.scala has more features and less concepts than other reactive web frameworks like Widok or ReactJS. Binding.scala Widok ReactJS Support HTML literal? Yes No Partially supported. Regular HTML does not compile, unless developers manually replaces class and for attributes to className and htmlFor, and manually converts inline styles from CSS syntax to JSON syntax. Algorithm to update DOM Precise data-binding, which is faster than virtual DOM Precise data-binding Virtual DOM differentiation, which requires manually managed key attributes for complicated DOM. Lifecycle management for data-binding expressions Automatically Manually N/A Statically type checking Yes, even for HTML tags and attribues Yes No Learning curve Always easy Unfamiliar DOM creation syntax for newbie. Requires much efforts to understand its corner cases. Easy to start. Requires much more efforts to understand its corner cases. See Design section for more information. Getting started We will build an Binding.scala web page during the following steps. Step 0: Setup a Sbt Scala.js project See http://www.scala-js.org/tutorial/basic/ for information about how to setup such a project. Step 1: Add Binding.scala dependencies into your build.sbt: libraryDependencies += ""com.thoughtworks.binding"" %%% ""dom"" % ""latest.release""  addCompilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.1.0"" cross CrossVersion.full) Step 2: Create a data field, which contains some Var and Vars as data source for your data-binding expressions case class Contact(name: Var[String], email: Var[String])  val data = Vars.empty[Contact] A Var represents a bindable variable, which also implements Binding trait, hence a Var can be seen as a binding expression as well. If another expression depends on a Var, the value of the expression changes whenever value of the Var changes. A Vars represents a sequence of bindable variables, which also implements BindingSeq trait, hence a Vars can be seen as a binding expression of a sequence as well. If another comprehension expression depends on a Vars, the value of the expression changes whenever value of the Vars changes. Step 3: Create a @dom method that contains data-binding expressions @dom def table: Binding[Table] = {   <table border=""1"" cellPadding=""5"">     <thead>       <tr>         <th>Name</th>         <th>E-mail</th>       </tr>     </thead>     <tbody>       {         for (contact <- data) yield {           <tr>             <td>               {contact.name.bind}             </td>             <td>               {contact.email.bind}             </td>           </tr>         }       }     </tbody>   </table> } A @dom method represents a data-binding expression. The return type is always wrapped in a com.thoughtworks.binding.Binding trait. For example @dom def x: Binding[Int] = 1, @dom def message: Binding[String] = ""content"" A @dom method supports HTML literal. Unlike normal XML literal in a normal Scala method, the types of HTML literal are specific subtypes of org.scalajs.dom.raw.Node or com.thoughtworks.binding.BindingSeq[org.scalajs.dom.raw.Node], instead of scala.xml.Node or scala.xml.NodeSeq. So we could have @dom def node: Binding[org.scalajs.dom.raw.HTMLBRElement] = <br/> and @dom def node: Binding[BindingSeq[org.scalajs.dom.raw.HTMLBRElement]] = <br/><br/>. A @dom method consists with other data-binding expressions in two approach: You could use bind method in a @dom method to get value of another Binding. You could use for / yield expression in a @dom method to map a BindingSeq to another. You can nest Node or BindingSeq[Node] in other HTML element literals via { ... } interpolation syntax. Step 4: render the data-binding expressions to DOM in the main method @JsExport def main(): Unit = {   dom.render(document.body, table) } Step 5: Invoke the main method in a HTML page <!DOCTYPE html> <html>   <head>     <script type=""text/javascript"" src=""js-fastopt.js""></script>   </head>   <body>     <script type=""text/javascript"">       SampleMain().main()     </script>   </body> </html> Now you will see a table that just contains a header, because data is empty at the moment. Step 6: Add some <button> to fill data for the table @dom def table: Binding[BindingSeq[Node]] = {   <div>     <button       onclick={ event: Event =>         data.get += Contact(Var(""Yang Bo""), Var(""yang.bo@rea-group.com""))       }     >       Add a contact     </button>   </div>   <table border=""1"" cellPadding=""5"">     <thead>       <tr>         <th>Name</th>         <th>E-mail</th>         <th>Operation</th>       </tr>     </thead>     <tbody>       {         for (contact <- data) yield {           <tr>             <td>               {contact.name.bind}             </td>             <td>               {contact.email.bind}             </td>             <td>               <button                 onclick={ event: Event =>                   contact.name := ""Modified Name""                 }               >                 Modify the name               </button>             </td>           </tr>         }       }     </tbody>   </table> } When you click the ""Add a contact"" button, it appends a new Contact into data, then, Binding.scala knows the relationship between DOM and data, so it decides to append a new <tr> corresponding to the newly appended Contact. And when you click the ""Modify the name"", the name field on contact changes, then, Binding.scala decides to change the content of the corresponding tr to new value of name field. See https://github.com/ThoughtWorksInc/Binding.scala-sample for the complete example. Design Precise data-binding ReactJS requires users provide a render function for each component. The render function should map props and state to a ReactJS's virtual DOM, then ReactJS framework creates a DOM with the same structure as the virtual DOM. When state changes, ReactJS framework invokes render function to get a new virtual DOM. Unfortunately, ReactJS framework does not precisely know what the state changing is. ReactJS framework has to compare the new virtual DOM and the original virtual DOM, and guesses the changeset between the two virtual DOM, then apply the guessed changeset to the real DOM as well. For example, after you prepended a table row, say, <tr>, into an existing <tbody> in a <table>, ReactJS may think you changed the content of the every existing <tr>s of the <tbody>, and appended another <tr> at the tail of the <tbody>. The reason is that the render function for ReactJS does not describe the relationship between state and DOM. Instead, it describes the process to create a virtual DOM. As a result, the render function does not provide any information about the purpose of the state changing, although a data-binding framework should need the information. Unlike ReactJS, a Binding.scala @dom method is NOT a regular function. It is a template that describes the relationship between data source and the DOM. When partial of the data source changed, the Binding.scala framework understands the exact partial DOM corresponding to the partial data. Then, Binding.scala only re-evaluates partial of the @dom method to update the partial DOM. With the help of the ability of precise data-binding provided by Binding.scala, you can get rid of concepts for hinting ReactJS's guessing algorithm, like key attribute, shouldComponentUpdate method, componentDidUpdate method or componentWillUpdate method. Composability The smallest composable unit in ReactJS is a component. It is fair to say that a React component is lighter than an AngularJS controller, while Binding.scala is better than that. The smallest composable unit in Binding.scala is a @dom method. Every @dom method is able to compose other @dom methods via .bind. case class Contact(name: Var[String], email: Var[String])  @dom def bindingButton(contact: Contact): Binding[Button] = {   <button     onclick={ event: Event =>       contact.name := ""Modified Name""     }   >    Modify the name   </button> }  @dom def bindingTr(contact: Contact): Binding[TableRow] = {   <tr>     <td>{ contact.name.bind }</td>     <td>{ contact.email.bind }</td>     <td>{ bindingButton(contact).bind }</td>   </tr> }  @dom def bindingTable(contacts: BindingSeq[Contact]): Binding[Table] = {   <table>     {       for (contact <- contacts) yield {         bindingTr(contact).bind       }     }   </table> }  @JSExport def main(): Unit = {   val data = Vars(Contact(Var(""Yang Bo""), Var(""yang.bo@rea-group.com"")))   dom.render(document.body, bindingTable(data) } You may find out this approach is much simpler than ReactJS, as: Instead of passing props in ReactJS, you just simply provide parameters for Binding.scala. Instead of specifying propTypes in ReactJS, you just simply define the types of parameters in Binding.scala. Instead of raising a run-time error when types of props do not match in ReactJS, you just check the types at compile-time. Lifecycle management for data-binding expressions The ability of precise data-binding in Binding.scala requires listener registrations on the data source. Other reactive frameworks that have the ability ask users manage the lifecycle of data-binding expressions. For example, MetaRx provide a dispose method to unregister the listeners created when building data-binding expressions. The users of MetaRx have the responsibility to call dispose method for every map and flatMap call after the expression changes, otherwise MetaRx leaks memory. Unfortunately, manually disposeing everything is too hard to be right for complicated binding expression. Another reactive web framework Widok did not provide any mechanism to manage lifecycle of of data-binding expressions. As a result, it simply always leaks memory. In Binding.scala, unlike MetaRx or Widok, all data-binding expressions are pure functional, with no side-effects. Binding.scala does not register any listeners when users create individual expressions, thus users do not need to manually unregister listeners for a single expression like MetaRx. Instead, Binding.scala create all internal listeners together, when the user calls dom.render or Binding.watch on the result expression. Note that dom.render or Binding.watch manages listeners on all upstream expressions, not only the direct listeners of the result expression. In brief, Binding.scala separates functionality in two kinds: User-defined @dom methods, which produce pure functional expressions with no side-effects. Calls to dom.render or Binding.watch, which manage all side-effects automatically. HTML literal and statically type checking As you see, you can embed HTML literals in @dom methods in Scala source files. You can also embed Scala expressions in braces in content or attribute values of the HTML literal. @dom def notificationBox(message: String): Binding[Div] = {   <div class=""notification"" title={ s""Tooltip: $message"" }>     {       message     }   </div> } Regardless the similar syntax of HTML literal between Binding.scala and ReactJS, Binding.scala create real DOM instead of ReactJS's virtual DOM. In the above example, <div>...</div> create a DOM element with the type of org.scalajs.dom.html.Div. Then, the magic @dom let the method wrap the result as a Binding. You can even assign the Div to a local variable and invoke native DOM method on the variable: @dom def notificationBox(message: String): Binding[Div] = {   val result: Div = <div class=""notification"" title={ s""Tooltip: $message"" }>     {       message     }   </div>    result.scrollIntoView()    result } scrollIntoView method will be invoked when the Div is created. If you invoked another method that does not defined in Div, the Scala compiler will report an compile-time error instead of bringing the failure to run-time, because Scala is a statically typed language and the Scala compiler understand the type of Div. You may also notice class and title. They are DOM properties or HTML attributes on Div. They are type-checked by Scala compiler as well. For example, given the following typo method: @dom def typo = {   val myDiv = <div typoProperty=""xx"">content</div>   myDiv.typoMethod()   myDiv } The Scala compiler will report errors like this: typo.scala:23: value typoProperty is not a member of org.scalajs.dom.html.Div         val myDiv = <div typoProperty=""xx"">content</div>                      ^ typo.scala:24: value typoMethod is not a member of org.scalajs.dom.html.Div         myDiv.typoMethod()               ^  With the help of static type system, @dom methods can be much robuster than ReactJS components. You can find a complete list of supported properties and methods on scaladoc of scalajs-dom or MDN Custom attributes If you want to suppress the static type checking of attributes, add a data: prefix to the attribute: @dom def myCustomDiv = {   val myDiv = <div data:customAttributeName=""attributeValue""></div>   assert(myDiv.getAttribute(""customAttributeName"") == ""attributeValue"")   myDiv } The Scala compiler will not report errors now. Downloads Binding.scala has an extremely tiny code base. There is only one source file Binding.scala for data-binding expressions, and one source file dom.scala for HTML DOM integration. Core data-binding expressions (Binding.scala) The former module is available for both JVM and Scala.js. You could add it in your build.sbt. // For JVM projects libraryDependencies += ""com.thoughtworks.binding"" %% ""core"" % ""latest.release"" // For Scala.js projects, or JS/JVM cross projects libraryDependencies += ""com.thoughtworks.binding"" %%% ""core"" % ""latest.release"" HTML DOM integration (dom.scala) The latter module is only available for Scala.js. You could add it in your build.sbt. // For Scala.js projects, or JS/JVM cross projects libraryDependencies += ""com.thoughtworks.binding"" %%% ""dom"" % ""latest.release"" Links The API documentation Binding.scala • TodoMVC Other live DEMOs Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ThoughtWorksInc/Binding.scala"	"A reactive web framework. It enables you use native XML literal syntax to create reactive DOM nodes, which are able to automatically change whenever the data source changes."	"true"
"Reactive Web Frameworks"	"Udash"	"http://udash.io/"	"a web framework based on Scala.js with support for property bindings, frontend routing, i18n and much more. It also provides strongly typed client<->server RPC system based on WebSockets."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Udash - Reactive Scala Web Framework - Based on Scala.js"	"null"	"null"	"a web framework based on Scala.js with support for property bindings, frontend routing, i18n and much more. It also provides strongly typed client<->server RPC system based on WebSockets."	"true"
"Reactive Web Frameworks"	"Widok"	"https://widok.github.io/"	"Reactive web framework for the JVM and Scala.js"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"107"	"15"	"12"	"GitHub - widok/widok: Reactive web framework for the JVM and Scala.js Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 107 Fork 12 widok/widok Code Issues 29 Pull requests 2 Pulse Graphs Reactive web framework for the JVM and Scala.js http://widok.github.io/ 536 commits 1 branch 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.2.3 Nothing to show New pull request Latest commit d288fd6 Jul 13, 2016 tindzk Bump dependencies Permalink Failed to load latest commit information. docs Add info about widok-scalajs-react-wrapper Sep 21, 2015 js/src Bump dependencies Jul 13, 2016 project Bump dependencies Jul 13, 2016 shared/src Use MetaRx Jul 30, 2015 .gitignore Ignore .idea/ directory. Oct 4, 2014 .travis.yml Travis: Update Scala version May 23, 2016 README.md README: Add waffle.io badge Apr 29, 2015 README.md Widok See the project page for more information. License Widok is licensed under the terms of the Apache v2.0 license. Authors Tim Nieradzik Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/widok/widok"	"Reactive web framework for the JVM and Scala.js"	"true"
"i18n"	"scala-xgettext ★ 15 ⧗ 94"	"https://github.com/xitrum-framework/scala-xgettext"	"A compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext.po file."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"16"	"6"	"5"	"GitHub - xitrum-framework/scala-xgettext: Scala 2.10 and 2.11 compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext .po file Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 16 Fork 5 xitrum-framework/scala-xgettext Code Issues 3 Pull requests 0 Pulse Graphs Scala 2.10 and 2.11 compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext .po file http://www.slideshare.net/ngocdaothanh/i18nize-scala-program-a-la-gettext 51 commits 1 branch 2 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 1.1 1.0 Nothing to show New pull request Latest commit 4b2ab38 May 23, 2016 ngocdaothanh #9 Remove CrossVersion.full Permalink Failed to load latest commit information. dev Note about publish-signed Oct 1, 2014 project Publish with full Scala version May 7, 2016 src/main Properly support Scala-2.10 along with Scala-2.11 May 7, 2016 .gitignore Ignore ScalaIDE build directory May 7, 2016 CHANGELOG.rst Fix #5 Allow multiple marker methods Oct 1, 2014 MIT-LICENSE First commit Jan 4, 2013 README.rst #9 Remove CrossVersion.full May 23, 2016 build.sbt #9 Remove CrossVersion.full May 23, 2016 poedit.png Fix #2 Friendly to diff tools: Sort content of i18n.pot by msgid Jul 11, 2014 README.rst This is a Scala 2.10 and 2.11 compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext .po file, when you compile the Scala source code files. More info on Scala compiler plugin: http://www.scala-lang.org/node/140 Presentation: I18nize Scala programs à la gettext Discussion group: https://groups.google.com/group/scala-xgettext For Play: https://github.com/georgeOsdDev/play-xgettext Usage This plugin can be used by frameworks like Xitrum to add i18n feature to them. For an example, see this SBT project. Create I18n trait or class In your Scala source code, you need to mark the strings you want to extract by using a trait or class that has these method signatures: t(singular: String): String tn(singular: String, plural: String, n: Long): String  tc(context: String, singular: String): String tcn(context: String, singular: String, plural: String, n: Long): String  The methods can also be: t(singular: String, params: Any*): String tn(singular: String, plural: String, n: Long, params: Any*): String  tc(context: String, singular: String, params: Any*): String tcn(context: String, singular: String, plural: String, n: Long, params: Any*): String  That is, only the first arguments (1 first argument for t, 3 first arguments for tn etc.) are required, all the following arguments are ignored (like params above). You can use Scaposer to implement the methods above. See example. Then in your Scala source code, use them like this: t(""Hello World"")  Extract i18n strings to .pot file To extract i18n strings like ""Hello World"" in the above snippet: Clean your Scala project to force the recompilation of all Scala source code files. Create an empty i18n.pot file in the current working directory. It will be filled with i18n string resources extracted from compiled Scala source code files. Compile your Scala project with -P:xgettext:<i18n trait or class> option. Example: -P:xgettext:xitrum.I18n. If you use SBT, build.sbt should look like this: ... autoCompilerPlugins := true addCompilerPlugin(""tv.cntt"" %% ""xgettext"" % ""1.3"") scalacOptions += ""-P:xgettext:xitrum.I18n"" ...  Copy or rename the .pot file to a .po file, and translate the strings in it to the language if want. ""t"" in "".pot"" means ""template"". You can use plain text editor to edit the .po file, or you can use Poedit. Poedit is very convenient, it can merge new .pot file to existing translated .po file. Content of the .pot file is sorted by msgid, so that it's easier too see diffs between versions of the .pot/.po file. Configure i18n marker method names t, tn, tc, and tcn above are the defaults. If you want to use other names, you can change them to, for example, tr, trn, trc, and trcn, by adding options to Scala compiler: scalacOptions ++= Seq(   ""xitrum.I18n"", ""t:tr"", ""tn:trn"", ""tc:trc"", ""tcn:trcn"" ).map(""-P:xgettext:"" + _)  If you skip an option, its default value will be used. Multiple marker method names Multiple marker methods for t can be configured like this: scalacOptions ++= Seq(   ""xitrum.I18n"", ""t:tr"", ""t:notr"" ).map(""-P:xgettext:"" + _)  Similar for tn, tc, and tcn. With this feature you can, for example, create an i18n library that can display both original strings and translated strings. Load .po file Use Scaposer. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/scala-xgettext"	"A compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext.po file."	"true"
"i18n"	"Scaposer ★ 27 ⧗ 23"	"https://github.com/xitrum-framework/scaposer"	"GNU Gettext.po file loader for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"27"	"6"	"4"	"GitHub - xitrum-framework/scaposer: GNU Gettext .po file loader for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 27 Fork 4 xitrum-framework/scaposer Code Issues 1 Pull requests 0 Pulse Graphs GNU Gettext .po file loader for Scala http://www.slideshare.net/ngocdaothanh/i18nize-scala-program-a-la-gettext 95 commits 2 branches 4 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 1.8 1.7 1.6 1.5 Nothing to show New pull request Latest commit ecacbba Jun 13, 2016 ngocdaothanh Release 1.8 Permalink Failed to load latest commit information. dev project src Fix #15 Parsed result of escaped double quote should not contain the … Jun 13, 2016 .gitignore AUTHORS CHANGELOG MIT-LICENSE README.rst build.sbt Release 1.8 Jun 13, 2016 poedit.png README.rst Scaposer is a GNU gettext po file parser written in Scala. It's strange that there's not many JVM libraries of this kind, see the discussion on Stackoverflow. To extract i18n strings from Scala source code files, use Scala xgettext. Presentation: I18nize Scala programs à la gettext Discussion group: https://groups.google.com/group/scala-xgettext Basic usage See Scaladoc. val po = """""" msgid ""Hello"" msgstr ""Bonjour"" """"""  val result = scaposer.Parser.parse(po) // => An Either, // Left(scaposer.ParseFailure) or // Right(Seq[scaposer.Translation])  Use t methods to get the translations: val translations = result.right.get val i18n         = scaposer.I18n(translations) i18n.t(""Hello"")  // => ""Bonjour""  If there's no translation, or the translation is an empty string (not translated yet), the original input is returned: i18n.t(""Hi"")  // => ""Hi""  Context val po = """""" msgid ""Hello"" msgstr ""Bonjour""  msgctxt ""Casual"" msgid ""Hello"" msgstr ""Salut"" """"""  val translations = scaposer.Parser.parse(po).right.get val i18n         = scaposer.I18n(translations) i18n.tc(""Casual"", ""Hello"")  // => ""Salut""  If there's no translation for the context, the translation without context is tried: i18n.tc(""Missing context"", ""Hello"")  // => ""Bonjour""  Plural-Forms Your po file must define Plural-Forms exactly as at: http://www.gnu.org/software/gettext/manual/html_node/Plural-forms.html#Plural-forms http://www.gnu.org/software/gettext/manual/html_node/Translating-plural-forms.html#Translating-plural-forms Scaposer does not evaluate the plural expression, which is in C language. It just removes spaces in the expression and performs string comparison. See evaluatePluralForms. val po = """""" msgid """" msgstr ""Plural-Forms: nplurals=2; plural=n>1;""  msgid ""I have one apple"" msgid_plural ""I have %d apples"" msgstr[0] ""J'ai une pomme"" msgstr[1] ""J'ai %d pommes"" """"""  val translations = scaposer.Parser.parse(po).right.get val i18n         = scaposer.I18n(translations) i18n.tn(""I have one apple"", ""I have %d apples"", 1)                // => ""J'ai une pomme"" i18n.tn(""I have one apple"", ""I have %d apples"", 2)                // => ""J'ai 2 pommes"" i18n.tcn(""A context"", ""I have one apple"", ""I have %d apples"", 3)  // => ""J'ai 3 pommes""  Merge Po objects You can merge multiple ``I18n``s together. val i18n4 = i18n1 ++ i18n2 ++ i18n3  Just like when you merge maps, translations in i18n3 will overwrite those in i18n2 will overwrite those in i18n1. Use with SBT Supported Scala versions: 2.11.x, 2.10.x build.sbt example: libraryDependencies += ""tv.cntt"" %% ""scaposer"" % ""1.7""  Scaposer is used in Xitrum web framework. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/scaposer"	"GNU Gettext.po file loader for Scala."	"true"
"Authentication"	"akka-http-session ★ 166 ⧗ 3"	"https://github.com/softwaremill/akka-http-session"	"Web&mobile client-side sessions for akka-http based applications, with optional JWT support"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"190"	"29"	"10"	"GitHub - softwaremill/akka-http-session: Web & mobile client-side akka-http sessions, with optional JWT support Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 29 Star 190 Fork 10 softwaremill/akka-http-session Code Issues 1 Pull requests 0 Pulse Graphs Web & mobile client-side akka-http sessions, with optional JWT support http://www.softwaremill.com 104 commits 1 branch 12 releases 3 contributors Scala 91.8% JavaScript 5.3% HTML 2.9% Scala JavaScript HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show release-1.1 release-0.2.6 release-0.2.4 release-0.2.3 release-0.2.2 release-0.2.1 release-0.2.0 release-0.1.4 release-0.1.4-2.0-M1 release-0.1.3 release-0.1.2 release-0.1 Nothing to show New pull request Latest commit 04890c2 May 17, 2016 adamw Fixing docs Permalink Failed to load latest commit information. core/src #9: properly clipping the secret to get a 16-byte encryption key Apr 4, 2016 example/src/main Update to Akka 2.4.2 Feb 17, 2016 jwt/src Updated to use json4s jackson backend. Apr 20, 2016 project #11 removing pgp plugin Apr 22, 2016 .gitignore Example Jun 30, 2015 .travis.yml Fixing travis PR builds Apr 21, 2016 LICENSE Initial commit Jun 30, 2015 README.md Fixing docs May 17, 2016 build.sbt 0.2.6 release Apr 22, 2016 README.md Web & mobile sessions for akka-http akka-http is an Akka module, originating from spray.io, for building reactive REST services with an elegant DSL. akka-http is a great toolkit for building backends for single-page or mobile applications. In almost all apps there is a need to maintain user sessions, make sure session data is secure and cannot be tampered with. akka-http-session provides directives for client-side session management in web and mobile applications, using cookies or custom headers + local storage, with optional Json Web Tokens format support. What is a session? Session data typically contains at least the id or username of the logged in user. This id must be secured so that a session cannot be ""stolen"" or forged easily. Sessions can be stored on the server, either in-memory or in a database, with the session id sent to the client, or entirely on the client in a serialized format. The former approach requires sticky sessions or additional shared storage, while using the latter (which is supported by this library) sessions can be easily deserialized on any server. A session is a string token which is sent to the client and should be sent back to the server on every request. To prevent forging, serialized session data is signed using a server secret. The signature is appended to the session data that is sent to the client, and verified when the session token is received back. akka-http-session features type-safe client-side sessions sessions can be encrypted sessions contain an expiry date cookie or custom header transport support for JWT refresh token support (e.g. to implement ""remember me"") CSRF tokens support Example import akka.http.scaladsl.server.Directives._  import com.softwaremill.session.{SessionConfig, SessionManager} import com.softwaremill.session.SessionDirectives._ import com.softwaremill.session.SessionOptions._  val sessionConfig = SessionConfig.default(""some_very_long_secret_and_random_string_some_very_long_secret_and_random_string"") implicit val sessionManager = new SessionManager[Long](sessionConfig)  path(""login"") {   post {     entity(as[String]) { body =>       setSession(oneOff, usingCookies, 812832L) { ctx =>         ctx.complete(""ok"")       }     }   } } ~ path(""secret"") {   get {     requiredSession(oneOff, usingCookies) { session => // type: Long, or whatever the T parameter is       complete { ""treasure"" }     }   } } ~ path(""logout"") {   get {     invalidateSession(oneOff, usingCookies) {       complete { ""logged out"" }     }   } } You can try out a simple example by running com.softwaremill.example.Example in the example project and opening http://localhost:8080, or you can just take a look at the source. SessionManager & configuration All directives require an implicit instance of a SessionManager[T], which can be created by providing a server secret (via a SessionConfig). The secret should be a long, random string unique to each environment your app is running in. You can generate one with SessionUtil.randomServerSecret(). Note that when you change the secret, all sessions will become invalid. A SessionConfig instance can be created using Typesafe config, The only value that you need to provide is akka.http.session.server-secret, preferably via application.conf (then you can safely call SessionConfig.fromConfig) or by using SessionConfig.default(). You can customize any of the default config options either by modifying them through application.conf or by modifying the SessionConfig case class. If a value has type Option[], you can set it to None by using a none value in the config file. When using cookies, by default the secure attribute of cookies is not set (for development), however it is recommended that all sites use https and all cookies have this attribute set. Client-side sessions All session-related directives take at least two parameters: session continuity: oneOff vs refreshable; specifies what should happen when the session expires. If refreshable and a refresh token is present, the session will be re-created. See below for details. session transport: usingCookies vs usingHeaders Typically, you would create aliases for the session-related directives which use the right parameters basing on the current request and logic specific to your application. Cookies vs header Session data can be sent to the client using cookies or custom headers. The first approach is the simplest to use, as cookies are automatically sent to the server on each request. However, cookies have some security vulnerabilities, and are typically not used in mobile applications. For these scenarios, session data can be transported using custom headers (the names of the headers are configurable in the config). When using headers, you need to store the session (and, if used, refresh-) tokens yourself. These tokens can be stored in-memory, or persistently e.g. using the browser's local storage. You can dynamically decide which transport to use, basing e.g. on the user-agent or other request properties. Basic usage Sessions are typed; the T type parameter in SessionManager[T] determines what data is stored in the session. Basic types like String, Int, Long, Float, Double and Map[String, String] are supported out-of-the box. Support for other types can be added by providing an implicit SessionSerializer[T, String]. For case classes, it's most convenient to use a MultiValueSessionSerializer[T] which should convert the instance into a String -> String map (nested types are not supported on purpose, as session data should be small & simple). Here we are creating a manager where the session content will be a single Long number: import com.softwaremill.session.{SessionConfig, SessionManager}  val sessionConfig = SessionConfig.default(""some_very_long_secret_and_random_string_some_very_long_secret_and_random_string"") implicit val sessionManager = new SessionManager[Long](sessionConfig) The basic directives enable you to set, read and invalidate the session. To create a new client-side session (create and set a new session cookie), you need to use the setSession directive: import akka.http.scaladsl.server.Directives._  import com.softwaremill.session.SessionDirectives._ import com.softwaremill.session.SessionOptions._  path(""login"") {   post {     entity(as[String]) { body =>       setSession(oneOff, usingCookies, 812832L) { ctx =>         ctx.complete(""ok"")       }     }   } } Note that when using cookies, their size is limited to 4KB, so you shouldn't put too much data in there (the signature takes about 50 characters). You can require a session to be present, optionally require a session or get a full description of possible session decode outcomes: path(""secret"") {   get {     requiredSession(oneOff, usingCookies) { session => // type: Long, or whatever the T parameter is       complete { ""treasure"" }     }   } } ~ path(""open"") {   get {     optionalSession(oneOff, usingCookies) { session => // type: Option[Long] (Option[T])       complete { ""small treasure"" }     }   } } ~ path(""detail"") {   get {     // type: SessionResult[Long] (SessionResult[T])     // which can be: Decoded, CreatedFromToken, Expired, Corrupt, NoSession     session(oneOff, usingCookies) { session =>        complete { ""small treasure"" }     }   } } If a required session is not present, by default a 403 is returned. Finally, a session can be invalidated: path(""logout"") {   get {     requiredSession(oneOff, usingCookies) { session =>        invalidateSession(oneOff, usingCookies) {         complete { ""logged out"" }       }     }   } } Encrypting the session It is possible to encrypt the session data by modifying the akka.http.session.encrypt-data config option. When sessions are encrypted, it's not possible to read their content on the client side. The key used for encrypting will be calculated basing on the server secret. Session expiry/timeout By default, sessions expire after a week. This can be disabled or changed with the akka.http.session.max-age config option. Note that when using cookies, even though the cookie sent will be a session cookie, it is possible that the client will have the browser open for a very long time, uses Chrome or FF, or if an attacker steals the cookie, it can be re-used. Hence having an expiry date for sessions is highly recommended. JWT: encoding sessions By default, sessions are encoded into a string using a custom format, where expiry/data/signature parts are separated using -, and data fields are separated using = and url-encoded. You can also encode sessions in the Json Web Tokens format, by adding the additional jwt dependency, which makes use of json4s. To use JWT, you need to create an implicit session encoder before creating a session manager: case class SessionData(...)  implicit val serializer = JValueSessionSerializer.caseClass[SessionData] implicit val encoder = new JwtSessionEncoder[SessionData] implicit val manager = new SessionManager(SessionConfig.fromConfig()) When using JWT, you need to provide a serializer which serializes session data to a JValue instead of a String. A number of implicit serializers for the basic types are present in JValueSessionSerializer, as well as a generic serializer for case classes (used above). There are many tools available to read JWT session data using various platforms, e.g. for Angular. It is also possible to customize the session data content generated by overriding appropriate methods in JwtSessionEncoder (e.g. provide additional claims in the payload). CSRF protection (cookie transport only) CSRF is an attack where an attacker issues a GET or POST request on behalf of a user, if the user e.g. clicks on a specially constructed link. See the OWASP page or the Play! docs for a thorough introduction. Web apps which use cookies for session management should be protected against CSRF attacks. This implementation: assumes that GET requests are non-mutating (have no side effects) uses double-submit cookies to verify requests requires the token to be set in a custom header or (optionally) in a form field generates a new token on the first GET request that doesn't have the token cookie set Note that if the token is passed in a form field, the website isn't protected by HTTPS or you don't control all subdomains, this scheme can be broken. Currently, setting a custom header seems to be a secure solution, and is what a number of projects do (that's why, when using custom headers to send session data, no additional protection is needed). It is recommended to generate a new CSRF token after logging in, see this SO question. A new token can be generated using the setNewCsrfToken directive. By default the name of the CSRF cookie and the custom header matches what AngularJS expects and sets. These can be customized in the config. Example usage: import akka.http.scaladsl.server.Directives._  import com.softwaremill.session.CsrfDirectives._ import com.softwaremill.session.CsrfOptions._  randomTokenCsrfProtection(checkHeader) {   get(""site"") {     // read from disk   } ~   post(""transfer_money"") {     // token already checked   } } Refresh tokens (""remember me"") If you'd like to implement persistent, ""remember-me"" sessions, you should use refreshable instead of oneOff sessions. This is especially useful in mobile applications, where you log in once, and the session is remembered for a long time. Make sure to adjust the akka.http.session.refresh-token.max-age config option appropriately (defaults to 1 month)! You can dynamically decide, basing on the request properties (e.g. a query parameter), if a session should be refreshable or not. Just pass the right parameter to setSession. When using refreshable sessions, in addition to an implicit SessionManager instance, you need to provide an implementation of the RefreshTokenStorage trait. This trait has methods to lookup, store and delete refresh tokens. Typically it would use some persistent storage. The tokens are never stored directly, instead only token hashes are passed to the storage. That way even if the token database is leaked, it won't be possible to forge sessions using the hashes. Moreover, in addition to the token hash, a selector value is stored. That value is used to lookup stored hashes; tokens are compared using using a special constant-time comparison method, to prevent timing attacks. When a session expires or is not present, but the refresh token is (sent from the client using either a cookie, or a custom header), a new session will be created (using the RefreshTokenLookupResult.createSession function), and a new refresh token will be created. Note that you can differentiate between sessions created from refresh tokens and from regular authentication by storing appropriate information in the session data. That way, you can force the user to re-authenticate if the session was created by a refresh token before crucial operations. It is of course possible to read oneOff-session using requiredSession(refreshable, ...). If a session was created as oneOff, using refreshable has no additional effect. Touching sessions The semantics of touch[Required|Optional]Session() are a bit subtle. You can still use expiring client sessions when using refresh tokens. You will then have 2 stages of expiration: expiration of the client session (should be shorter), and expiry of the refresh token. That way you can have strongly-authenticated sessions which expire fast, and weaker-authenticated re-creatable sessions (as described in the paragraph above). When touching an existing session, the refresh token will not be re-generated and extended, only the session cookie. Links Bootzooka, a web application template project using akka-http and akka-http-session Spray session, similar project for spray.io Spray SPA, a single-page-application demo built using spray.io, also containing an implementation of client-side sessions Play framework, a full web framework, from which parts of the session encoding/decoding code was taken Rails security guide, a description of how sessions are stored in Rails Akka issue 16855 for implementing similar functionality straight in Akka Implementing remember me The definitive guide to form-based website authorization The Anatomy of a JSON Web Token Cookies vs tokens Using from SBT For akka version 2.4.4: libraryDependencies += ""com.softwaremill.akka-http-session"" %% ""core"" % ""0.2.5"" libraryDependencies += ""com.softwaremill.akka-http-session"" %% ""jwt""  % ""0.2.5"" // optional Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/softwaremill/akka-http-session"	"Web&mobile client-side sessions for akka-http based applications, with optional JWT support"	"true"
"Authentication"	"play-pac4j ★ 209 ⧗ 1"	"https://github.com/pac4j/play-pac4j"	"Security library managing authentication (CAS, OAuth, OpenID, SAML, LDAP, SQL, JWT...), authorizations and logout for Play 2.x in Java and Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"225"	"31"	"56"	"GitHub - pac4j/play-pac4j: Security library for Play framework 2 in Java and Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 31 Star 225 Fork 56 pac4j/play-pac4j Code Issues 0 Pull requests 1 Wiki Pulse Graphs Security library for Play framework 2 in Java and Scala http://www.pac4j.org 325 commits 10 branches 21 releases 15 contributors Java 78.4% Scala 20.8% Shell 0.8% Java Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.1.x 1.2.x 1.4.x 1.5.x 2.0.x 2.1.x 2.2.x 2.3.x 2.4.x master Nothing to show play-pac4j-2.4.0 play-pac4j-2.3.0 play-pac4j-2.2.0 play-pac4j-2.1.0 play-pac4j-2.0.1 play-pac4j-2.0.0 play-pac4j-1.5.0 play-pac4j-1.4.0 play-pac4j-1.3.0 play-pac4j-1.3.0-RC1 play-pac4j-1.2.3 play-pac4j-1.2.2 play-pac4j-1.2.1 play-pac4j-1.2.0 play-pac4j-1.1.5 play-pac4j-1.1.4 play-pac4j-1.1.3 play-pac4j-1.1.2 play-pac4j-1.1.1 play-pac4j-1.1.0 play-oauth-client-1.0.0 Nothing to show New pull request Latest commit a2ca974 Jul 11, 2016 Jérôme LELEU update to 2.5 Permalink Failed to load latest commit information. src migrate tests to play 2.5 Jun 15, 2016 travis Don't deploy PR to Sonatype Sep 18, 2015 .gitignore ignore eclipse files Jun 15, 2016 .travis.yml Don't deploy PR to Sonatype Sep 18, 2015 LICENSE first commit Nov 10, 2012 NOTICE initial commit Apr 1, 2016 README.md update to 2.5 Jul 11, 2016 findbugs-exclude.xml initial commit Apr 1, 2016 pom.xml update to 2.5 Jul 11, 2016 README.md The play-pac4j project is an easy and powerful security library for Play framework v2 web applications which supports authentication and authorization, but also application logout and advanced features like session fixation and CSRF protection. It's based on Play 2 and on the pac4j security engine. It's available under the Apache 2 license. Several versions of the library are available for the different versions of the Play framework: Play version pac4j version play-pac4j version 2.0 1.7 play-pac4j_java 1.1.x (Java) / play-pac4j_scala2.9 1.1.x (Scala) 2.1 1.7 play-pac4j_java 1.1.x (Java) / play-pac4j_scala2.10 1.1.x (Scala) 2.2 1.7 play-pac4j_java 1.2.x (Java) / play-pac4j_scala 1.2.x (Scala) 2.3 1.7 play-pac4j_java 1.4.x (Java) / play-pac4j_scala2.10 and play-pac4j_scala2.11 1.4.x (Scala) 2.4 1.9 2.3.x (Java & Scala) 2.5 1.9 2.5.x (Java & Scala) Main concepts and components: 1) A client represents an authentication mechanism (CAS, OAuth, SAML, OpenID Connect, LDAP, JWT...) It performs the login process and returns a user profile. An indirect client is for UI authentication while a direct client is for web services authentication 2) An authorizer is meant to check authorizations on the authenticated user profile(s) (role / permission, ...) or on the current web context (IP check, CSRF...) 3) A config defines the security configuration via clients and authorizers 4) The Secure annotation / function or the SecurityFilter protects an url by checking that the user is authenticated and that the authorizations are valid, according to the clients and authorizers configuration. If the user is not authenticated, it performs authentication for direct clients or starts the login process for indirect clients 5) The CallbackController finishes the login process for an indirect client 6) The ApplicationLogoutController logs out the user from the application. Just follow these easy steps to secure your Play 2 web application: 1) Add the required dependencies (play-pac4j + pac4j-* libraries) You need to add a dependency on: the play-pac4j library (groupId: org.pac4j, version: 2.5.0-SNAPSHOT) the appropriate pac4j submodules (groupId: org.pac4j, version: 1.9.0): pac4j-oauth for OAuth support (Facebook, Twitter...), pac4j-cas for CAS support, pac4j-ldap for LDAP authentication, etc. All released artifacts are available in the Maven central repository. 2) Define the configuration (Config + Client + Authorizer) The configuration (org.pac4j.core.config.Config) contains all the clients and authorizers required by the application to handle security. The Config is bound for injection in a SecurityModule (or whatever the name you call it): In Java: public class SecurityModule extends AbstractModule {    @Override   protected void configure() {     FacebookClient facebookClient = new FacebookClient(""fbId"", ""fbSecret"");     TwitterClient twitterClient = new TwitterClient(""twId"", ""twSecret"");      FormClient formClient = new FormClient(baseUrl + ""/loginForm"", new SimpleTestUsernamePasswordAuthenticator());     IndirectBasicAuthClient basicAuthClient = new IndirectBasicAuthClient(new SimpleTestUsernamePasswordAuthenticator());      CasClient casClient = new CasClient(""http://mycasserver/login"");      SAML2ClientConfiguration cfg = new SAML2ClientConfiguration(""resource:samlKeystore.jks"",                     ""pac4j-demo-passwd"", ""pac4j-demo-passwd"", ""resource:openidp-feide.xml"");     cfg.setMaximumAuthenticationLifetime(3600);     cfg.setServiceProviderEntityId(""urn:mace:saml:pac4j.org"");     cfg.setServiceProviderMetadataPath(new File(""target"", ""sp-metadata.xml"").getAbsolutePath());     SAML2Client saml2Client = new SAML2Client(cfg);      OidcClient oidcClient = new OidcClient();     oidcClient.setClientID(""id"");     oidcClient.setSecret(""secret"");     oidcClient.setDiscoveryURI(""https://accounts.google.com/.well-known/openid-configuration"");     oidcClient.addCustomParam(""prompt"", ""consent"");      ParameterClient parameterClient = new ParameterClient(""token"", new JwtAuthenticator(""salt""));      Clients clients = new Clients(""http://localhost:9000/callback"", facebookClient, twitterClient, formClient,       basicAuthClient, casClient, saml2Client, oidcClient, parameterClient);      Config config = new Config(clients);     config.addAuthorizer(""admin"", new RequireAnyRoleAuthorizer<>(""ROLE_ADMIN""));     config.addAuthorizer(""custom"", new CustomAuthorizer());     config.setHttpActionAdapter(new DemoHttpActionAdapter());     bind(Config.class).toInstance(config);   } } In Scala: class SecurityModule(environment: Environment, configuration: Configuration) extends AbstractModule {    override def configure(): Unit = {      val facebookClient = new FacebookClient(""fbId"", ""fbSecret"")     val twitterClient = new TwitterClient(""twId"", ""twSecret"")      val formClient = new FormClient(baseUrl + ""/loginForm"", new SimpleTestUsernamePasswordAuthenticator())     val basicAuthClient = new IndirectBasicAuthClient(new SimpleTestUsernamePasswordAuthenticator())      val casClient = new CasClient(""http://mycasserver/login"")      val cfg = new SAML2ClientConfiguration(""resource:samlKeystore.jks"", ""pac4j-demo-passwd"", ""pac4j-demo-passwd"", ""resource:openidp-feide.xml"")     cfg.setMaximumAuthenticationLifetime(3600)     cfg.setServiceProviderEntityId(""urn:mace:saml:pac4j.org"")     cfg.setServiceProviderMetadataPath(new File(""target"", ""sp-metadata.xml"").getAbsolutePath())     val saml2Client = new SAML2Client(cfg)      val oidcClient = new OidcClient()     oidcClient.setClientID(""id"")     oidcClient.setSecret(""secret"")     oidcClient.setDiscoveryURI(""https://accounts.google.com/.well-known/openid-configuration"")     oidcClient.addCustomParam(""prompt"", ""consent"")      val parameterClient = new ParameterClient(""token"", new JwtAuthenticator(""salt""))      val clients = new Clients(""http://localhost:9000/callback"", facebookClient, twitterClient, formClient,       basicAuthClient, casClient, saml2Client, oidcClient, parameterClient)      val config = new Config(clients)     config.addAuthorizer(""admin"", new RequireAnyRoleAuthorizer[Nothing](""ROLE_ADMIN""))     config.addAuthorizer(""custom"", new CustomAuthorizer())     config.setHttpActionAdapter(new DemoHttpActionAdapter())     bind(classOf[Config]).toInstance(config)   } } http://localhost:8080/callback is the url of the callback endpoint, which is only necessary for indirect clients. Notice that you define: 1) an optional SessionStore using the setSessionStore(sessionStore) method (by default, the PlayCacheStore uses the Play cache to store pac4j data) 2) a specific HttpActionAdapter to handle specific HTTP actions (like redirections, forbidden / unauthorized pages) via the setHttpActionAdapter method. The available implementation is the DefaultHttpActionAdapter, but you can subclass it to define your own HTTP 401 / 403 error pages for example. 3a) Protect urls per Action (Secure) You can protect (authentication + authorizations) the urls of your Play application by using the Secure annotation / function. It has the following behaviour: 1) First, if the user is not authenticated (no profile) and if some clients have been defined in the clients parameter, a login is tried for the direct clients. 2) Then, if the user has a profile, authorizations are checked according to the authorizers configuration. If the authorizations are valid, the user is granted access. Otherwise, a 403 error page is displayed. 3) Finally, if the user is still not authenticated (no profile), he is redirected to the appropriate identity provider if the first defined client is an indirect one in the clients configuration. Otherwise, a 401 error page is displayed. The following parameters are available: 1) clients (optional): the list of client names (separated by commas) used for authentication: in all cases, this filter requires the user to be authenticated. Thus, if the clients is blank or not defined, the user must have been previously authenticated if the client_name request parameter is provided, only this client (if it exists in the clients) is selected. 2) authorizers (optional): the list of authorizer names (separated by commas) used to check authorizations: if the authorizers is blank or not defined, no authorization is checked the following authorizers are available by default (without defining them in the configuration): isFullyAuthenticated to check if the user is authenticated but not remembered, isRemembered for a remembered user, isAnonymous to ensure the user is not authenticated, isAuthenticated to ensure the user is authenticated (not necessary by default unless you use the AnonymousClient) hsts to use the StrictTransportSecurityHeader authorizer, nosniff for XContentTypeOptionsHeader, noframe for XFrameOptionsHeader, xssprotection for XSSProtectionHeader, nocache for CacheControlHeader or securityHeaders for the five previous authorizers csrfToken to use the CsrfTokenGeneratorAuthorizer with the DefaultCsrfTokenGenerator (it generates a CSRF token and saves it as the pac4jCsrfToken request attribute and in the pac4jCsrfToken cookie), csrfCheck to check that this previous token has been sent as the pac4jCsrfToken header or parameter in a POST request and csrf to use both previous authorizers. 3) multiProfile (optional): it indicates whether multiple authentications (and thus multiple profiles) must be kept at the same time (false by default). For example in your controllers: In Java: @Secure(clients = ""FacebookClient"") public Result facebookIndex() {   return protectedIndexView(); } In Scala: def facebookIndex = Secure(""FacebookClient"") { profiles =>  Action { request =>    Ok(views.html.protectedIndex(profiles))  } } 3b) Protect urls via the SecurityFilter In order to protect multiple urls at the same tine, you can configure the SecurityFilter. You need to configure your application to include the SecurityFilter as follows: First define a Filters class in your application (if you have not yet done so). In Java: package filters;  import org.pac4j.play.filters.SecurityFilter; import play.http.HttpFilters; import play.api.mvc.EssentialFilter;  import javax.inject.Inject;  public class Filters implements HttpFilters {    private final SecurityFilter securityFilter;    @Inject   public Filters(SecurityFilter securityFilter) {     this.securityFilter = securityFilter;   }    @Override   public EssentialFilter[] filters() {     return new EssentialFilter[] {securityFilter};   } } In Scala: package filters  import javax.inject.Inject import org.pac4j.play.filters.SecurityFilter import play.api.http.HttpFilters  /**   * Configuration of all the Play filters that are used in the application.   */ class Filters @Inject()(securityFilter: SecurityFilter) extends HttpFilters {    def filters = Seq(securityFilter)  } Then tell your application to use the filters in application.conf: play.http.filters = ""filters.Filters""  See for more information on the use of filters in Play the Play documentation on Filters. Rules for the security filter can be supplied in application.conf. An example is shown below. It consists of a list of filter rules, where the key is a regular expression that will be used to match the url. Make sure that the / is escaped by \ to make a valid regular expression. For each regex key, there are two subkeys: authorizers and clients. Here you can define the correct values, like you would supply to the RequireAuthentication method in controllers. There two exceptions: authorizers can have two special values: _authenticated_ and _anonymous_. _anonymous_ will disable authentication and authorization for urls matching the regex. _authenticated_ will require authentication, but will set clients and authorizers both to null. Rules are applied top to bottom. The first matching rule will define which clients and authorizers are used. When not provided, the value will be null. pac4j.security.rules = [   # Admin pages need a special authorizer and do not support login via Twitter.   {""/admin/.*"" = {     authorizers = ""admin""     clients = ""FormClient""   }}   # Rules for the REST services. These don't specify a client and will return 401   # when not authenticated.   {""/restservices/.*"" = {     authorizers = ""_authenticated_""   }}   # The login page needs to be publicly accessible.   {""/login.html"" = {     authorizers = ""_anonymous_""   }}   # 'Catch all' rule to make sure the whole application stays secure.   {"".*"" = {     authorizers = ""_authenticated_""     clients = ""FormClient,TwitterClient""   }} ]  4) Define the callback endpoint only for indirect clients (CallbackController) For indirect clients (like Facebook), the user is redirected to an external identity provider for login and then back to the application. Thus, a callback endpoint is required in the application. It is managed by the CallbackController which has the following behaviour: 1) the credentials are extracted from the current request to fetch the user profile (from the identity provider) which is then saved in the web session 2) finally, the user is redirected back to the originally requested url (or to the defaultUrl). The following parameters are available: 1) defaultUrl (optional): it's the default url after login if no url was originally requested (/ by default) 2) multiProfile (optional): it indicates whether multiple authentications (and thus multiple profiles) must be kept at the same time (false by default). In the routes file: GET    /callback    @org.pac4j.play.CallbackController.callback() In the SecurityModule: In Java: CallbackController callbackController = new CallbackController(); callbackController.setDefaultUrl(""/""); bind(CallbackController.class).toInstance(callbackController); In Scala: val callbackController = new CallbackController() callbackController.setDefaultUrl(""/"") bind(classOf[CallbackController]).toInstance(callbackController) 5) Get the user profile (ProfileManager) You can get the profile of the authenticated user using profileManager.get(true) (false not to use the session, but only the current HTTP request). You can test if the user is authenticated using profileManager.isAuthenticated(). You can get all the profiles of the authenticated user (if ever multiple ones are kept) using profileManager.getAll(true). Examples: In Java: PlayWebContext context = new PlayWebContext(ctx()); ProfileManager<CommonProfile> profileManager = new ProfileManager(context); Optional<CommonProfile> profile = profileManager.get(true); In Scala: val webContext = new PlayWebContext(request) val profileManager = new ProfileManager[CommonProfile](webContext) val profile = profileManager.get(true) The retrieved profile is at least a CommonProfile, from which you can retrieve the most common attributes that all profiles share. But you can also cast the user profile to the appropriate profile according to the provider used for authentication. For example, after a Facebook authentication: In Java: FacebookProfile facebookProfile = (FacebookProfile) commonProfile; In Scala: val facebookProfile = commonProfile.asInstanceOf[FacebookProfile] 6) Logout (ApplicationLogoutController) You can log out the current authenticated user using the ApplicationLogoutController. It has the following behaviour: 1) after logout, the user is redirected to the url defined by the url request parameter if it matches the logoutUrlPattern 2) or the user is redirected to the defaultUrl if it is defined 3) otherwise, a blank page is displayed. The following parameters are available: 1) defaultUrl (optional): the default logout url if no url request parameter is provided or if the url does not match the logoutUrlPattern (not defined by default) 2) logoutUrlPattern (optional): the logout url pattern that the url parameter must match (only relative urls are allowed by default). In the routes file: GET     /logout     @org.pac4j.play.ApplicationLogoutController.logout() In the SecurityModule: In Java: ApplicationLogoutController logoutController = new ApplicationLogoutController(); logoutController.setDefaultUrl(""/""); bind(ApplicationLogoutController.class).toInstance(logoutController); In Scala: val logoutController = new ApplicationLogoutController() logoutController.setDefaultUrl(""/"") bind(classOf[ApplicationLogoutController]).toInstance(logoutController) Migration guide 2.1.0 (Play 2.4) / 2.2.0 (Play 2.5) -> 2.3.0 (Play 2.4) / 2.4.0 (Play 2.5) The RequiresAuthentication annotation and function have been renamed as Secure with the clients and authorizers parameters (instead of clientName and authorizerName). The UserProfileController class and the getUserProfile method in the Security trait no longer exist and the ProfileManager must be used instead. The ApplicationLogoutController behaviour has slightly changed: even without any url request parameter, the user will be redirected to the defaultUrl if it has been defined 2.0.1 -> 2.1.0 The separate Scala and Java projects have been merged. You need to change the dependency play-pac4j-java or play-pac4j-scala to simply play-pac4j. The getUserProfile method of the Security trait returns a Option[CommonProfile] instead of just a UserProfile. 2.0.0 -> 2.0.1 The DataStore concept is replaced by the pac4j SessionStore concept. The PlayCacheStore does no longer need to be bound in the security module. A new session store could be defined using the config.setSessionStore method. The DefaultHttpActionAdapter does not need to be bound in the security module, but must to be set using the config.setHttpActionAdapter method. 1.5.x -> 2.0.0 play-pac4j v2.0 is a huge refactoring of the previous version 1.5. It takes advantage of the new features of pac4j v1.8 (REST support, authorizations, configuration objects...) and is fully based on dependency injection -> see Play 2.4 migration guide. In Java, the SecurityController and JavaController are deprecated and you need to use the UserProfileController to get the user profile (you can also use the ProfileManager object directly). The ""target url"" concept has disappeared as it was too complicated, it could be simulated though. The SecurityCallbackController is deprecated and you must use the CallbackController. The logout support has been moved to the ApplicationLogoutController. The JavaWebContext and ScalaWebContext have been merged into a new PlayWebContext. The StorageHelper has been removed, replaced by the PlayCacheStore implementation where you can set the timeouts. You can provide your own implementation of the CacheStore if necessary. The PlayLogoutHandler has been moved to the org.pac4j.play.cas.logout package and renamed as PlayCacheLogoutHandler (it relies on the Play Cache). The static specific Config has been replaced by the default org.pac4j.core.config.Config object to define the clients (authentication) and the authorizers (authorizations). Custom 401 / 403 HTTP error pages must now be defined by overriding the DefaultHttpActionAdapter. The isAjax parameter is no longer available as AJAX requests are now automatically detected. The stateless parameter is no longer available as the stateless nature is held by the client itself. The requireAnyRole and requieAllRoles parameters are no longer available and authorizers must be used instead (with the authorizerName parameter). Demo Two demo webapps: play-pac4j-java-demo & play-pac4j-scala-demo are available for tests and implement many authentication mechanisms: Facebook, Twitter, form, basic auth, CAS, SAML, OpenID Connect, JWT... Test them online: http://play-pac4j-java-demo.herokuapp.com and http://play-pac4j-scala-demo.herokuapp.com. Release notes See the release notes. Learn more by browsing the play-pac4j Javadoc and the pac4j Javadoc. Need help? If you have any question, please use the following mailing lists: pac4j users pac4j developers Development The version 2.5.0-SNAPSHOT is under development. Maven artifacts are built via Travis: and available in the Sonatype snapshots repository. This repository must be added in the resolvers of your build.sbt file: resolvers ++= Seq(Resolver.mavenLocal, ""Sonatype snapshots repository"" at ""https://oss.sonatype.org/content/repositories/snapshots/"") Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/pac4j/play-pac4j"	"Security library managing authentication (CAS, OAuth, OpenID, SAML, LDAP, SQL, JWT...), authorizations and logout for Play 2.x in Java and Scala."	"true"
"Authentication"	"play-silhouette ★ 451 ⧗ 1"	"https://github.com/mohiva/play-silhouette"	"Authentication library for Play Framework applications that supports several authentication methods, including OAuth1, OAuth2, OpenID, Credentials or custom authentication schemes."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"481"	"67"	"96"	"GitHub - mohiva/play-silhouette: Silhouette is an authentication library for Play Framework applications that supports several authentication methods, including OAuth1, OAuth2, OpenID, CAS, Credentials, Basic Authentication, Two Factor Authentication or custom authentication schemes. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 67 Star 481 Fork 96 mohiva/play-silhouette Code Issues 6 Pull requests 3 Pulse Graphs Silhouette is an authentication library for Play Framework applications that supports several authentication methods, including OAuth1, OAuth2, OpenID, CAS, Credentials, Basic Authentication, Two Factor Authentication or custom authentication schemes. http://silhouette.mohiva.com 727 commits 4 branches 17 releases 23 contributors Scala 97.2% Shell 2.8% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.0.x 3.0.x gh-pages master Nothing to show 4.0.0 4.0.0-RC1 3.0.5 3.0.4 3.0.3 3.0.2 3.0.1 3.0.0 3.0-RC2 3.0-RC1 2.0.2 2.0.1 2.0 2.0-RC2 2.0-RC1 1.0 0.9 Nothing to show New pull request Latest commit 41c7625 Jul 14, 2016 akkie committed on GitHub Release version 4.0.0 (#487) Permalink Failed to load latest commit information. .github Add issue template and pull request template (#468) Apr 23, 2016 project Release version 4.0.0 (#487) Jul 14, 2016 scripts Code coverage gets not published to Coveralls and other small changes (… Jun 8, 2016 silhouette-cas Create persistence memory module for the CAS provider (#477) May 30, 2016 silhouette-crypto-jca Play Crypto API has been deprecated (#444) May 30, 2016 silhouette-password-bcrypt Define meaningful interface for password re-hashing Apr 23, 2016 silhouette-persistence Remove the persistence in-memory modules (#479) Jun 1, 2016 silhouette-testkit Change parameter order for the CookieAuthenticator (#478) May 31, 2016 silhouette Change parameter order for the CookieAuthenticator (#478) May 31, 2016 site Publish API-Doc to gh-pages branch Mar 14, 2015 .coveralls.yml Change to sbt-coveralls plugin Jan 5, 2014 .editorconfig Add editor configuration file Jan 5, 2014 .gitignore Drop Scala 2.10 support Nov 8, 2015 .travis.yml Code coverage gets not published to Coveralls and other small changes (… Jun 8, 2016 CHANGELOG.md Code coverage gets not published to Coveralls and other small changes (… Jun 8, 2016 CONTRIBUTING.md Update CONTRIBUTING.md Nov 24, 2015 LICENSE Refresh LICENSE file from source Jan 2, 2014 README.md Release version 4.0.0 (#487) Jul 14, 2016 README.md Silhouette Silhouette is an authentication library for Play Framework applications that supports several authentication methods, including OAuth1, OAuth2, OpenID, CAS, Credentials, Basic Authentication, Two Factor Authentication or custom authentication schemes. See the project documentation for more information. Support If you have question regarding Silhouette please use the chat or the mailing list. Please do not use the issue tracker for questions! Contribution Please read the contributing guide before you contribute. It contains very useful tips for a successful contribution. License The code is licensed under Apache License v2.0 and the documentation under CC BY 3.0. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/mohiva/play-silhouette"	"Authentication library for Play Framework applications that supports several authentication methods, including OAuth1, OAuth2, OpenID, Credentials or custom authentication schemes."	"true"
"Authentication"	"play2-auth ★ 560 ⧗ 0"	"https://github.com/t2v/play2-auth"	"Play2.x Authentication and Authorization module."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"583"	"67"	"126"	"GitHub - t2v/play2-auth: Play2.x Authentication and Authorization module Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 67 Star 583 Fork 126 t2v/play2-auth Code Issues 10 Pull requests 3 Wiki Pulse Graphs Play2.x Authentication and Authorization module 259 commits 7 branches 23 releases 18 contributors Scala 76.0% JavaScript 13.1% HTML 10.9% Scala JavaScript HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/authority_factory feature/play2.5 master play2.2 play2.3 play2.4 play21 Nothing to show release0.14.2 release0.14.1 release0.14.0 release0.13.5 release0.13.4 release0.13.3 release0.13.2 release0.13.1 release0.13.0 release0.12.0 release0.11.1 release0.11.0 release0.10.1 release0.10 release0.9 release0.8 release0.7 release0.6 release0.5 release0.4 release0.3 release0.2 release0.1 Nothing to show New pull request Latest commit a15dbaf Apr 15, 2016 xuwei-k committed with gakuzzzz add scaladoc badges (#172) Permalink Failed to load latest commit information. module/src/main/scala/jp/t2v/lab/play2/auth support app-less testing Nov 10, 2015 project release0.14.2 Mar 12, 2016 sample s/db.default.user/db.default.username Jul 6, 2015 social-sample s/db.default.user/db.default.username Jul 5, 2015 social/src/main/scala/jp/t2v/lab/play2/auth/social changed session key name to protected. refs #161 Mar 9, 2016 test/src/main/scala/jp/t2v/lab/play2/auth/test Fixed test removing EhCache Jul 5, 2015 .gitignore Changed that The Play Session might not be used Nov 22, 2012 .travis.yml use container-based infrastructure in travis-ci Jul 5, 2015 LICENSE add LICENSE Apr 24, 2012 README.ja.md add scaladoc badges (#172) Apr 15, 2016 README.md add scaladoc badges (#172) Apr 14, 2016 README.md Play2.x module for Authentication and Authorization This module offers Authentication and Authorization features to Play2.x applications Scaladoc Target This module targets the Scala version of Play2.x. Motivation More secure than Play2.x's Security trait The existing Security trait in Play2.x API does not define an identifier that identifies a user. If you use an Email or a userID as an identifier, users can not invalidate their session if the session cookie is intercepted. This module creates a unique SessionID using a secure random number generator. Even if the sessionId cookie is intercepted, users can invalidate the session by logging in again. Your application can expire sessions after a set time limit. Flexiblity Since the Security trait in Play2.x API returns Action, complicated action methods wind up deeply nested. Play2x-Auth provides a way of composition. Previous Version for Play2.3.x, Please see previous version 0.13.5 README for Play2.2.x, Please see previous version 0.11.1 README Attention Since Play2.3's SimpleResult is renamed to Result, The play2.auth trait signatures are changed at version 0.12.0 Since Play2.2's Result is deprecated, The play2.auth trait signatures are changed at version 0.11.0 Installation Add dependency declarations into your Build.scala or build.sbt file: for Play2.4.x ""jp.t2v"" %% ""play2-auth""        % ""0.14.2"", ""jp.t2v"" %% ""play2-auth-social"" % ""0.14.2"", // for social login ""jp.t2v"" %% ""play2-auth-test""   % ""0.14.2"" % ""test"", play.sbt.Play.autoImport.cache // only when you use default IdContainer  For example your Build.scala might look like this:   val appDependencies = Seq(     ""jp.t2v"" %% ""play2-auth""        % ""0.14.2"",     ""jp.t2v"" %% ""play2-auth-social"" % ""0.14.2"",     ""jp.t2v"" %% ""play2-auth-test""   % ""0.14.2"" % ""test"",     play.sbt.Play.autoImport.cache   ) Usage First create a trait that extends jp.t2v.lab.play2.auth.AuthConfig in app/controllers. // Example import jp.t2v.lab.play2.auth._  trait AuthConfigImpl extends AuthConfig {    /**    * A type that is used to identify a user.    * `String`, `Int`, `Long` and so on.    */   type Id = String    /**    * A type that represents a user in your application.    * `User`, `Account` and so on.    */   type User = Account    /**    * A type that is defined by every action for authorization.    * This sample uses the following trait:    *    * sealed trait Role    * case object Administrator extends Role    * case object NormalUser extends Role    */   type Authority = Role    /**    * A `ClassTag` is used to retrieve an id from the Cache API.    * Use something like this:    */   val idTag: ClassTag[Id] = classTag[Id]    /**    * The session timeout in seconds    */   val sessionTimeoutInSeconds: Int = 3600    /**    * A function that returns a `User` object from an `Id`.    * You can alter the procedure to suit your application.    */   def resolveUser(id: Id)(implicit ctx: ExecutionContext): Future[Option[User]] = Account.findById(id)    /**    * Where to redirect the user after a successful login.    */   def loginSucceeded(request: RequestHeader)(implicit ctx: ExecutionContext): Future[Result] =     Future.successful(Redirect(routes.Message.main))    /**    * Where to redirect the user after logging out    */   def logoutSucceeded(request: RequestHeader)(implicit ctx: ExecutionContext): Future[Result] =     Future.successful(Redirect(routes.Application.login))    /**    * If the user is not logged in and tries to access a protected resource then redirect them as follows:    */   def authenticationFailed(request: RequestHeader)(implicit ctx: ExecutionContext): Future[Result] =     Future.successful(Redirect(routes.Application.login))    /**    * If authorization failed (usually incorrect password) redirect the user as follows:    */   override def authorizationFailed(request: RequestHeader, user: User, authority: Option[Authority])(implicit context: ExecutionContext): Future[Result] = {     Future.successful(Forbidden(""no permission""))   }    /**    * A function that determines what `Authority` a user has.    * You should alter this procedure to suit your application.    */   def authorize(user: User, authority: Authority)(implicit ctx: ExecutionContext): Future[Boolean] = Future.successful {     (user.role, authority) match {       case (Administrator, _)       => true       case (NormalUser, NormalUser) => true       case _                        => false     }   }    /**    * (Optional)    * You can custom SessionID Token handler.    * Default implementation use Cookie.    */   override lazy val tokenAccessor = new CookieTokenAccessor(     /*      * Whether use the secure option or not use it in the cookie.      * Following code is default.      */     cookieSecureOption = play.api.Play.isProd(play.api.Play.current),     cookieMaxAge       = Some(sessionTimeoutInSeconds)   )  } Next create a Controller that defines both login and logout actions. This Controller mixes in the jp.t2v.lab.play2.auth.LoginLogout trait and the trait that you created in first step. object Application extends Controller with LoginLogout with AuthConfigImpl {    /** Your application's login form.  Alter it to fit your application */   val loginForm = Form {     mapping(""email"" -> email, ""password"" -> text)(Account.authenticate)(_.map(u => (u.email, """")))       .verifying(""Invalid email or password"", result => result.isDefined)   }    /** Alter the login page action to suit your application. */   def login = Action { implicit request =>     Ok(html.login(loginForm))   }    /**    * Return the `gotoLogoutSucceeded` method's result in the logout action.    *    * Since the `gotoLogoutSucceeded` returns `Future[Result]`,    * you can add a procedure like the following.    *    *   gotoLogoutSucceeded.map(_.flashing(    *     ""success"" -> ""You've been logged out""    *   ))    */   def logout = Action.async { implicit request =>     // do something...     gotoLogoutSucceeded   }    /**    * Return the `gotoLoginSucceeded` method's result in the login action.    *    * Since the `gotoLoginSucceeded` returns `Future[Result]`,    * you can add a procedure like the `gotoLogoutSucceeded`.    */   def authenticate = Action.async { implicit request =>     loginForm.bindFromRequest.fold(       formWithErrors => Future.successful(BadRequest(html.login(formWithErrors))),       user => gotoLoginSucceeded(user.get.id)     )   }  } Lastly, mix jp.t2v.lab.play2.auth.AuthElement trait and the trait that was created in the first step into your Controllers: object Message extends Controller with AuthElement with AuthConfigImpl {    // The `StackAction` method   //    takes `(AuthorityKey, Authority)` as the first argument and   //    a function signature `RequestWithAttributes[AnyContent] => Result` as the second argument and   //    returns an `Action`    // The `loggedIn` method   //     returns current logged in user    def main = StackAction(AuthorityKey -> NormalUser) { implicit request =>     val user = loggedIn     val title = ""message main""     Ok(html.message.main(title))   }    def list = StackAction(AuthorityKey -> NormalUser) { implicit request =>     val user = loggedIn     val title = ""all messages""     Ok(html.message.list(title))   }    def detail(id: Int) = StackAction(AuthorityKey -> NormalUser) { implicit request =>     val user = loggedIn     val title = ""messages detail ""     Ok(html.message.detail(title + id))   }    // Only Administrator can execute this action.   def write = StackAction(AuthorityKey -> Administrator) { implicit request =>     val user = loggedIn     val title = ""write message""     Ok(html.message.write(title))   }  } Test play2.auth provides test module at version 0.8 You can use FakeRequest with logged-in status. package test  import org.specs2.mutable._  import play.api.test._ import play.api.test.Helpers._ import controllers.{AuthConfigImpl, Messages} import jp.t2v.lab.play2.auth.test.Helpers._  class ApplicationSpec extends Specification {    object config extends AuthConfigImpl    ""Messages"" should {     ""return list when user is authorized"" in new WithApplication {       val result = Messages.list(FakeRequest().withLoggedIn(config)(1))       contentType(result) must equalTo(""text/html"")     }   }  } Import jp.t2v.lab.play2.auth.test.Helpers._ Define instance what is mixed-in AuthConfigImpl object config extends AuthConfigImpl  Call withLoggedIn method on FakeRequest first argument: AuthConfigImpl instance. second argument: user ID of the user who is logged-in at this request It makes enable to test controllers with play2.auth Advanced usage Changing the authorization depending on the request parameters. For example, a Social networking application has a function to edit messages. A user must be able to edit their own messages but not other people's messages. To achieve this you could define Authority as a Function: trait AuthConfigImpl extends AuthConfig {    // Other setup is omitted.     type Authority = User => Future[Boolean]    def authorize(user: User, authority: Authority)(implicit ctx: ExecutionContext): Future[Boolean] = authority(user)  } object Application extends Controller with AuthElement with AuthConfigImpl {    private def sameAuthor(messageId: Int)(account: Account): Future[Boolean] =     Message.getAutherAsync(messageId).map(_ == account)    def edit(messageId: Int) = StackAction(AuthorityKey -> sameAuthor(messageId)) { implicit request =>     val user = loggedIn     val target = Message.findById(messageId)     Ok(html.message.edit(messageForm.fill(target)))   }  } Returning to the originally requested page after login When an unauthenticated user requests access to page requiring authentication, you first redirect the user to the login page, then, after the user successfully logs in, you redirect the user to the page they originally requested. To achieve this change authenticationFailed and loginSucceeded: trait AuthConfigImpl extends AuthConfig {    // Other settings are omitted.    def authenticationFailed(request: RequestHeader)(implicit ctx: ExecutionContext): Future[Result] =     Future.successful(Redirect(routes.Application.login).withSession(""access_uri"" -> request.uri))    def loginSucceeded(request: RequestHeader)(implicit ctx: ExecutionContext): Future[Result] = {     val uri = request.session.get(""access_uri"").getOrElse(routes.Message.main.url.toString)     Future.successful(Redirect(uri).withSession(request.session - ""access_uri""))   }  } Changing the display depending on whether the user is logged in If you want to display the application's index differently to non-logged-in users and logged-in users, you can use OptionalAuthElement instead of AuthElement: object Application extends Controller with OptionalAuthElement with AuthConfigImpl {    // maybeUser is an instance of `Option[User]`.   // `OptionalAuthElement` dont need `AuthorityKey`   def index = StackAction { implicit request =>     val maybeUser: Option[User] = loggedIn     val user: User = maybeUser.getOrElse(GuestUser)     Ok(html.index(user))   }  } For action that doesn't require authorization you can AuthenticationElement instead of AuthElement for authentication without authorization. object Application extends Controller with AuthenticationElement with AuthConfigImpl {    def index = StackAction { implicit request =>     val user: User = loggedIn     Ok(html.index(user))   }  } Return 401 when a request is sent by Ajax Normally, you want to return a login page redirection at a authentication failed. Although, when the request is sent by Ajax you want to instead return 401, Unauthorized. You can do it as follows. def authenticationFailed(request: RequestHeader)(implicit ctx: ExecutionContext) = Future.successful {   request.headers.get(""X-Requested-With"") match {     case Some(""XMLHttpRequest"") => Unauthorized(""Authentication failed"")     case _ => Redirect(routes.Application.login)   } } Action composition play2.auth use stackable-controller Suppose you want to validate a token at every action in order to defeat a Cross Site Request Forgery attack. Since it is impractical to perform the validation in all actions, you would define a trait like this: import jp.t2v.lab.play2.stackc.{RequestWithAttributes, StackableController} import scala.concurrent.Future import play.api.mvc.{Result, Request, Controller} import play.api.data._ import play.api.data.Forms._  trait TokenValidateElement extends StackableController {     self: Controller =>    private val tokenForm = Form(""token"" -> text)    private def validateToken(request: Request[_]): Boolean = (for {     tokenInForm <- tokenForm.bindFromRequest()(request).value     tokenInSession <- request.session.get(""token"")   } yield tokenInForm == tokenInSession).getOrElse(false)    override def proceed[A](request: RequestWithAttributes[A])(f: RequestWithAttributes[A] => Future[Result]): Future[Result] = {     if (validateToken(request)) super.proceed(request)(f)     else Future.successful(BadRequest)   }  } You can use TokenValidateElement trait with AuthElement trait. object Application extends Controller with TokenValidateElement with AuthElement with AuthConfigImpl {    def page1 = StackAction(AuthorityKey -> NormalUser) { implicit request =>     // do something     Ok(html.page1(""result""))   }    def page2 = StackAction(AuthorityKey -> NormalUser) { implicit request =>     // do something     Ok(html.page2(""result""))   }  } Asynchronous Support There are asynchronous libraries ( for example: ReactiveMongo, ScalikeJDBC-Async, and so on ). You should use Future[Result] instead of AsyncResult from Play2.2. You can use AsyncStack instead of StackAction for Future[Result] trait HogeController extends AuthElement with AuthConfigImpl {    def hoge = AsyncStack { implicit req =>     val messages: Future[Seq[Message]] = AsyncDB.withPool { implicit s => Message.findAll }     messages.map(Ok(html.view.messages(_)))   }  } Stateless vs Stateful implementation. Play2x-Auth follows the Play framework's stateless policy. However, Play2x-Auth's default implementation is stateful, because the stateless implementation has the following security risk: If user logs-in to your application in a internet-cafe, then returns home neglecting to logout. If the user logs in again at home they will not invalidate the session. Nevertheless, you want to use a fully stateless implementation then just override the idContainer method of AuthConfig like this: trait AuthConfigImpl extends AuthConfig {    // Other settings omitted.    override lazy val idContainer: AsyncIdContainer[Id] = AsyncIdContainer(new CookieIdContainer[Id])  } You could also store the session data in a Relational Database by overriding the id container. Note: CookieIdContainer doesn't support session timeout. Running The Sample Application git clone https://github.com/t2v/play2-auth.git cd play2-auth sbt ""project sample"" run access to http://localhost:9000/ on your browser. click Apply this script now! login defined accounts Email             | Password | Role alice@example.com | secret   | Administrator bob@example.com   | secret   | NormalUser chris@example.com | secret   | NormalUser  Attention -- Distributed Servers Ehcache, the default cache implementation used by Play2.x, does not work on distributed application servers. If you have distributed servers, use the Memcached Plugin or something similar. License This library is released under the Apache Software License, version 2, which should be included with the source in a file named LICENSE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/t2v/play2-auth"	"Play2.x Authentication and Authorization module."	"true"
"Authentication"	"scala-oauth2-provider ★ 317 ⧗ 0"	"https://github.com/nulab/scala-oauth2-provider"	"OAuth 2.0 server-side implementation written in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"353"	"48"	"69"	"GitHub - nulab/scala-oauth2-provider: OAuth 2.0 server-side implementation written in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 48 Star 353 Fork 69 nulab/scala-oauth2-provider Code Issues 1 Pull requests 1 Wiki Pulse Graphs OAuth 2.0 server-side implementation written in Scala 189 commits 7 branches 34 releases 17 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.15.x fix-expire-stored-token fix-remove-warning master play-2.2.x play-2.3.x play-2.4.x Nothing to show 0.17.2 0.17.1 0.17.0 0.16.1 0.16.0 0.15.1 0.15.0 0.14.0 0.13.3 0.13.2 0.13.1 0.13.0 0.12.1 0.12.0 0.11.0 0.10.0 0.9.1 0.9.0 0.8.0 0.7.4 0.7.3 0.7.2 0.7.1 0.7.0 0.6.1 0.6.0 0.5.1 0.5.0 0.4.0 0.3.0 0.2.2 0.2.1 0.2.0 0.1.0 Nothing to show New pull request Latest commit a8c5556 Jul 15, 2016 tsuyoshizawa committed on GitHub fix remove warning by intelliJ (#96) Permalink Failed to load latest commit information. akka-http-oauth2-provider/src Add akka-http-oauth2-provider sub project. (#94) Jul 5, 2016 play2-oauth2-provider/src added to be able to get authInfo on issue access token Jan 21, 2016 project Add akka-http-oauth2-provider sub project. (#94) Jul 5, 2016 scala-oauth2-core/src fix remove warning by intelliJ (#96) Jul 15, 2016 .gitignore first commit Sep 17, 2013 .travis.yml support Play 2.5 Mar 7, 2016 LICENSE first commit Sep 17, 2013 README.md version 0.17.2 May 26, 2016 README.md oauth2-server for Scala The OAuth 2.0 server-side implementation written in Scala. This provides OAuth 2.0 server-side functionality and supporting function for Play Framework. Play Framework 2.2, 2.3, 2.4 and 2.5 are now supported. The idea of this library originally comes from oauth2-server which is Java implementation of OAuth 2.0. Supported OAuth features This library supports all grant types. Authorization Code Grant Resource Owner Password Credentials Grant Client Credentials Grant Implicit Grant and an access token type called Bearer. Setup If you'd like to use this with Play Framework, add ""play2-oauth2-provider"" to library dependencies of your project. For Play Framework 2.5 libraryDependencies ++= Seq(   ""com.nulab-inc"" %% ""play2-oauth2-provider"" % ""0.17.2"" ) For Play Framework 2.4 libraryDependencies ++= Seq(   ""com.nulab-inc"" %% ""play2-oauth2-provider"" % ""0.16.1"" ) For Play Framework 2.3 libraryDependencies ++= Seq(   ""com.nulab-inc"" %% ""play2-oauth2-provider"" % ""0.14.0"" ) For Play Framework 2.2 libraryDependencies ++= Seq(   ""com.nulab-inc"" %% ""play2-oauth2-provider"" % ""0.7.4"" ) Other frameworks Add ""scala-oauth2-core"" instead. In this case, you need to implement your own OAuth provider working with web framework you use. libraryDependencies ++= Seq(   ""com.nulab-inc"" %% ""scala-oauth2-core"" % ""0.17.2"" ) How to use Implement DataHandler Whether you use Play Framework or not, you have to implement DataHandler trait and make it work with your own User class that may be already defined in your application. case class User(id: Long, name: String, hashedPassword: String)  class MyDataHandler extends DataHandler[User] {    def validateClient(request: AuthorizationRequest): Future[Boolean] = ???    def findUser(request: AuthorizationRequest): Future[Option[User]] = ???    def createAccessToken(authInfo: AuthInfo[User]): Future[AccessToken] = ???    def getStoredAccessToken(authInfo: AuthInfo[User]): Future[Option[AccessToken]] = ???    def refreshAccessToken(authInfo: AuthInfo[User], refreshToken: String): Future[AccessToken] = ???    def findAuthInfoByCode(code: String): Future[Option[AuthInfo[User]]] = ???    def findAuthInfoByRefreshToken(refreshToken: String): Future[Option[AuthInfo[User]]] = ???    def deleteAuthCode(code: String): Future[Unit] = ???    def findAccessToken(token: String): Future[Option[AccessToken]] = ???    def findAuthInfoByAccessToken(accessToken: AccessToken): Future[Option[AuthInfo[User]]] = ???  } If your data access is blocking for the data storage, then you just wrap your implementation in the DataHandler trait with Future.successful(...). For more details, refer to Scaladoc of DataHandler. AuthInfo DataHandler returns AuthInfo as authorized information. AuthInfo is made up of the following fields. case class AuthInfo[User](   user: User,   clientId: Option[String],   scope: Option[String],   redirectUri: Option[String] ) user user is authorized by DataHandler clientId clientId which is sent from a client has been verified by DataHandler If your application requires client_id for client authentication, you can get clientId as below val clientId = authInfo.clientId.getOrElse(throw new InvalidClient()) scope inform the client of the scope of the access token issued redirectUri This value must be enabled on authorization code grant Work with Play Framework You should follow four steps below to work with Play Framework. Customizing Grant Handlers Define a controller to issue access token Assign a route to the controller Access to an authorized resource You want to use which grant types are supported or to use a customized handler for a grant type, you should override the handlers map in a customized TokenEndpoint trait. class MyTokenEndpoint extends TokenEndpoint {   override val handlers = Map(     OAuthGrantType.AUTHORIZATION_CODE -> new AuthorizationCode(),     OAuthGrantType.REFRESH_TOKEN -> new RefreshToken(),     OAuthGrantType.CLIENT_CREDENTIALS -> new ClientCredentials(),     OAuthGrantType.PASSWORD -> new Password(),     OAuthGrantType.IMPLICIT -> new Implicit()   ) } Here's an example of a customized TokenEndpoint that 1) only supports the password grant type, and 2) customizes the password grant type handler to not require client credentials: class MyTokenEndpoint extends TokenEndpoint {   val passwordNoCred = new Password() {     override def clientCredentialRequired = false   }    override val handlers = Map(     OAuthGrantType.PASSWORD -> passwordNoCred   ) } Define your own controller with mixining OAuth2Provider trait provided by this library to issue access token with customized TokenEndpoint. import scalaoauth2.provider._ object OAuth2Controller extends Controller with OAuth2Provider {   override val tokenEndpoint = new MyTokenEndpoint()    def accessToken = Action.async { implicit request =>     issueAccessToken(new MyDataHandler())   } } Then, assign a route to the controller that OAuth clients will access to. POST    /oauth2/access_token                    controllers.OAuth2Controller.accessToken  Finally, you can access to an authorized resource like this: import scalaoauth2.provider._ object MyController extends Controller with OAuth2Provider {   def list = Action.async { implicit request =>     authorize(new MyDataHandler()) { authInfo =>       val user = authInfo.user // User is defined on your system       // access resource for the user     }   } } If you'd like to change the OAuth workflow, modify handleRequest methods of TokenEndPoint and ProtectedResource traits. Using Action composition You can write more easily authorize action by using Action composition. Play Framework's documentation is here. object MyController extends Controller {    import scalaoauth2.provider.OAuth2ProviderActionBuilders._    def list = AuthorizedAction(new MyDataHandler()) { request =>     val user = request.authInfo.user // User is defined on your system     // access resource for the user   } } Examples Play Framework 2.5 https://github.com/tsuyoshizawa/scala-oauth2-provider-example-skinny-orm Play Framework 2.3 https://github.com/davidseth/scala-oauth2-provider-slick Play Framework 2.2 https://github.com/oyediyildiz/scala-oauth2-provider-example https://github.com/tuxdna/play-oauth2-server Application using this library Typetalk Backlog Flic by Shortcut Labs Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nulab/scala-oauth2-provider"	"OAuth 2.0 server-side implementation written in Scala."	"true"
"Authentication"	"SecureSocial ★ 1166 ⧗ 0"	"https://github.com/jaliss/securesocial"	"A module that provides OAuth, OAuth2 and OpenID authentication for Play Framework applications."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1173"	"111"	"560"	"GitHub - jaliss/securesocial: A module that provides OAuth, OAuth2 and OpenID authentication for Play Framework applications Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 111 Star 1,173 Fork 560 jaliss/securesocial Code Issues 47 Pull requests 28 Wiki Pulse Graphs A module that provides OAuth, OAuth2 and OpenID authentication for Play Framework applications http://www.securesocial.ws 546 commits 16 branches 11 releases 86 contributors Scala 78.4% Java 11.0% HTML 6.0% Ruby 1.4% Erlang 1.1% NewLisp 1.1% Perl6 1.0% Scala Java HTML Ruby Erlang NewLisp Perl6 Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.x 2.0.x 2.0.3 2.0.4 2.0.5 2.0.6 2.0.7 2.0.8 2.0.9 2.0.10 2.0.11 2.0.12 2.1.x gh-pages master-play-2.2.x master Nothing to show 3.0-M4 3.0-M3 3.0-M2 3.0-M1 3.0-M1-play-2.2.x 2.1.4 2.1.3 2.1.2 2.1.1 2.1.0 2.0.13 Nothing to show New pull request Latest commit 72d1d18 Dec 15, 2015 jaliss updated changelog Permalink Failed to load latest commit information. conf One-letter typo correction Nov 2, 2013 docs/src/manual Merge pull request #568 from aladagemre/patch-1 Nov 29, 2015 module-code added missing @inject annotation Dec 15, 2015 project - upgraded to Play 2.4.4 Nov 29, 2015 samples fixed conflicts Nov 29, 2015 test-kit - Updated changelog Mar 4, 2014 .gitignore bump Play version to 2.3.1 Jun 26, 2014 ChangeLog updated changelog Dec 15, 2015 LICENSE.txt Initial commit Nov 11, 2011 README.textile fix typo in README Dec 29, 2013 Readme-Icons.txt Initial commit Nov 11, 2011 securesocial.sbt - Master is now compatible with Play 2.3. Use branch master-play.2.2.… Jun 24, 2014 README.textile SecureSocial for Play 2 SecureSocial allows you to add an authentication UI to your app that works with services based on OAuth1, OAuth2 and OpenID protocols. SecureSocial provides Scala and Java APIs so you can integrate it using your preferred language. Check the project web site for more information: http://www.securesocial.ws SecureSocial for Play 1.x The old version of SecureSocial is under the 1.x branch now. The ‘master’ branch is for the Play 2 version only. Written by Jorge Aliss (@jaliss) License SecureSocial is distributed under the Apache License, Version 2.0. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/jaliss/securesocial"	"A module that provides OAuth, OAuth2 and OpenID authentication for Play Framework applications."	"true"
"Authorization"	"deadbolt-2 ★ 398 ⧗ 3"	"https://github.com/schaloner/deadbolt-2"	"A Play 2.x module supporting role-based and proprietary authorization; idiomatic APIs for Scala and Java APIs are provided."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"415"	"44"	"49"	"GitHub - schaloner/deadbolt-2: An authorization module for the Play framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 44 Star 415 Fork 49 schaloner/deadbolt-2 Code Issues 4 Pull requests 0 Pulse Graphs An authorization module for the Play framework 130 commits 4 branches 4 releases 7 contributors Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0 D2-1.x deadbolt-2.1 master Nothing to show v1.1.3 v1.1.2 v1.1.1 old-arch Nothing to show New pull request Latest commit 2d06e20 May 27, 2016 schaloner latest master commits Permalink Failed to load latest commit information. deadbolt-2-core @ dfc5cba latest master commits Mar 17, 2016 deadbolt-2-guide @ d8a2839 latest master commits Mar 17, 2016 deadbolt-2-java @ 02db617 latest master commits May 27, 2016 deadbolt-2-java-examples @ c124384 synched with latest submodules May 6, 2016 deadbolt-2-scala @ faf4ccd synched with latest submodules May 6, 2016 deadbolt-2-scala-examples @ 7a36348 synched with latest submodules May 6, 2016 images added link to book Oct 3, 2015 .gitignore initial shift to new architecture Nov 29, 2012 .gitmodules added external repos as submodules Dec 11, 2012 LICENSE.txt Added Apache 2 license Dec 16, 2015 README.markdown Update README.markdown Mar 2, 2016 README.markdown Deadbolt 2 - An authorisation system for Play 2 Deadbolt is a powerful authorisation mechanism for defining access rights to certain controller methods or parts of a view. For a complete guide, please refer to the Deadbolt website. The deadbolt-2 repository in GitHub is a collection of submodules. I highly recommend that you fork/follow/clone specific submodules in place of this aggregate module. The submodules are: deadbolt-2-core https://github.com/schaloner/deadbolt-2-core deadbolt-2-java https://github.com/schaloner/deadbolt-2-java Documentation: https://deadbolt-java.readme.io deadbolt-2-scala https://github.com/schaloner/deadbolt-2-scala Documentation: https://deadbolt-scala.readme.io deadbolt-2-guide https://github.com/schaloner/deadbolt-2-guide Demonstration applications can be found at deadbolt-2-java-examples https://github.com/schaloner/deadbolt-2-java-examples see it in action at http://deadbolt-2-java.herokuapp.com deadbolt-2-scala-examples https://github.com/schaloner/deadbolt-2-scala-examples see it in action at http://deadbolt-2-scala.herokuapp.com Integrating Deadbolt for Scala with Auth0 GitHub Activator template The Deadbolt book If you want to most complete documentation on Deadbolt, take a look at the book! You can find it at the Leanpub website. Which version should I use? See http://schaloner.github.io/ for version and usage information. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/schaloner/deadbolt-2"	"A Play 2.x module supporting role-based and proprietary authorization; idiomatic APIs for Scala and Java APIs are provided."	"true"
"Cryptography"	"Scrypto ★ 17 ⧗ 21"	"https://github.com/ScorexProject/scrypto"	"All-purpose cryptographic framework."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"19"	"6"	"5"	"GitHub - input-output-hk/scrypto: Cryptographic primitives for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 19 Fork 5 input-output-hk/scrypto Code Issues 0 Pull requests 0 Pulse Graphs Cryptographic primitives for Scala 230 commits 4 branches 6 releases 4 contributors Java 86.6% Scala 13.4% Java Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.2.0 devel master temp Nothing to show v1.0.4 v1.0.3 v1.0.2 v1.0.1 1.2.0-RC1 1.1.0 Nothing to show New pull request Latest commit d85abfa Jul 15, 2016 catena2w Add sized outputs to hashes Permalink Failed to load latest commit information. project update sb version Jul 15, 2016 src Add sized outputs to hashes Jul 15, 2016 .gitignore Crypto functions from scorex project Dec 22, 2015 COPYING License Dec 23, 2015 README.md Fix readme May 26, 2016 build.sbt Add shapeless to dependencies Jul 15, 2016 lock.sbt Add shapeless to dependencies Jul 15, 2016 scalastyle-config.xml Scalastyle plugin Jan 5, 2016 README.md Scrypto Scrypto is an open source cryptographic toolkit designed to make it easier and safer for developers to use cryptography in their applications. It was extracted from Scorex, open-source modular blockchain & cryptocurrency framework. Public Domain. Get Scrypto Scrypto is available on Sonatype for Scala 2.11! resolvers += ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" You can use Scrypto in your sbt project by simply adding the following dependency to your build file: libraryDependencies += ""org.consensusresearch"" %% ""scrypto"" % ""1.1.0"" Hash functions Supported hash algorithms are: Blake Blake2b BMW CubeHash Echo Fugue Groestl Hamsi JH Keccak Luffa Sha SHAvite SIMD Skein Whirlpool Take a look at CryptographicHash interface and use supported hash algorithms like Keccak512(""some string or bytes"") All provided hash functions are secure, and their implementations are thread safe. Hash chain It's possible to apply hash functions sequentially to create more secure hash function. The most well-known X11 hash chain is available from this library. You can easily create your own hash chain function: import scorex.crypto.applyHashes object MyCustomHash extends CryptographicHash {   override val DigestSize: Int = 64   override def hash(input: Message): Digest = applyHashes(input, Blake512, Sha512, Groestl512, Skein512) } or just val myHashChain = hashChain(Blake512, BMW512, Groestl512, Skein512, JH512, Keccak512, Luffa512, Wirlpool) Note, that hash chain will be as good as the strongest of the algorithms included in the chain. Commutative hash You can create commutative hash from any hash function with CommutativeHash case class like CommutativeHash(Sha256). A hash function h is commutative if h(x,y)==h(y,x) , for all x and y. Binary-to-text Encoding Schemes Scrypto has implementations of few binary-to-text encoding schemes: Base16 Base58 Base64 Example:   val encoded = Base64.encode(data)   val restored = Base64.decode(encoded)   restored shouldBe data Signing functions Scrypto supports following elliptic curves: Curve25519(& Ed25519) Example:   val curveImpl = new Curve25519   val keyPair = curveImpl.createKeyPair()   val sig = curveImpl.sign(keyPair._1, message)   assert(curveImpl.verify(sig, message, keyPair._2)) Note on security: Scrypto provides Scala wrapper for Curve25519-Java by Whisper Systems, so has the same security properties. JDK's SecureRandom used to obtain seed bytes. Authenticated data structure Scrypto supports following authenticated data structure: Merkle tree Skip list Example: TODO Tests Run sbt test from a folder contains the framework to launch tests. License The code is under Public Domain CC0 license means you can do anything with it. Full license text is in COPYING file Contributing Your contributions are always welcome! Please submit a pull request or create an issue to add a new cryptographic primitives or better implementations. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ScorexProject/scrypto"	"All-purpose cryptographic framework."	"true"
"Testing"	"Gatling"	"http://gatling.io"	"Async Scala-Akka-Netty based Stress Tool."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Gatling Project, Stress Tool"	"null"	"null"	"Async Scala-Akka-Netty based Stress Tool."	"true"
"Testing"	"ScalaCheck ★ 953 ⧗ 0"	"https://github.com/rickynils/scalacheck"	"Property-based testing for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1018"	"60"	"171"	"GitHub - rickynils/scalacheck: Property-based testing for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 60 Star 1,018 Fork 171 rickynils/scalacheck Code Issues 37 Pull requests 0 Wiki Pulse Graphs Property-based testing for Scala http://www.scalacheck.org 855 commits 20 branches 34 releases 48 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.8_sonatype 1.9_sonatype 1.10.0_sonatype 1.10.1_sonatype 1.11.0_sonatype 1.11.1_sonatype 1.11.3_sonatype 1.11.5_sonatype 1.11.6_sonatype 1.12.x 1.12.2_sonatype 1.12.3_sonatype 1.12.4_sonatype 1.12.5_sonatype 1.13.0_sonatype 1.13.1_sonatype 1.13.2_sonatype issue192 master scalacheck2 Nothing to show 1.13.2 1.13.1 1.13.0 1.12.5 1.12.4 1.12.3 1.12.2 1.12.1 1.12.0 1.11.6 1.11.5 1.11.4 1.11.3 1.11.2 1.11.1 1.11.0 1.10.1 1.10.0 1.10.0-b1 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1.1 1.1 1.0 1.0-RC1 0.7 0.2 0.1 Nothing to show New pull request Latest commit af01b00 Jul 11, 2016 rickynils Bump version to 1.13.2 Permalink Failed to load latest commit information. doc Fix typo Jun 6, 2016 examples update example projects Feb 10, 2015 js/src/main/scala/org/scalacheck 2015 -> 2016 Feb 3, 2016 jvm/src Bump ScalaJS 0.6.8 Apr 7, 2016 project Add mima filters Jun 30, 2016 src Fix doc of mapOfN Jul 8, 2016 tools Scala 2.12.0-M4 Apr 14, 2016 .gitignore Split ScalaCheck into JVM and JS projects Nov 26, 2014 .travis.yml Bump Scala 2.12 to 2.12.0-M5 Jun 30, 2016 LICENSE 1.13.1 release notes Apr 14, 2016 README.markdown fix broken link in README Jul 16, 2015 RELEASE ScalaCheck 1.13.2 (resolves #250) Jul 11, 2016 TODO Implement a test runner with scalajs.testbridge. Nov 26, 2014 build.sbt Bump version to 1.13.2 Jul 11, 2016 README.markdown ScalaCheck ScalaCheck is a library written in Scala and used for automated property-based testing of Scala or Java programs. ScalaCheck was originally inspired by the Haskell library QuickCheck, but has also ventured into its own. ScalaCheck has no external dependencies other than the Scala runtime, and works great with sbt, the Scala build tool. It is also fully integrated in the test frameworks ScalaTest and specs2. You can of course also use ScalaCheck completely standalone, with its built-in test runner. ScalaCheck is used by several prominent Scala projects, for example the Scala compiler and the Akka concurrency framework. For more information and downloads, please visit http://www.scalacheck.org Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/rickynils/scalacheck"	"Property-based testing for Scala."	"true"
"Testing"	"ScalaMeter"	"https://scalameter.github.io/"	"Performance & memory footprint measuring, regression testing."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"277"	"30"	"39"	"GitHub - scalameter/scalameter: Microbenchmarking and performance regression testing framework for the JVM platform. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 30 Star 277 Fork 39 scalameter/scalameter Code Issues 29 Pull requests 1 Pulse Graphs Microbenchmarking and performance regression testing framework for the JVM platform. http://scalameter.github.io 603 commits 16 branches 3 releases Fetching contributors Scala 64.4% JavaScript 27.6% Java 4.6% CSS 2.7% HTML 0.7% Scala JavaScript Java CSS HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.11 gh-pages issue/3 issue/6 issue/15 jenkins master topic/fast-collection-tests topic/refined-history topic/visualization-format topic/visualization version/0.5-M2 version/0.5 version/0.6 version/0.7 zabo-master Nothing to show v0.4 0.4-M1 0.2 Nothing to show New pull request Latest commit a1ab155 May 4, 2016 axel22 Wrap lines. Permalink Failed to load latest commit information. project scalameter-core/src src tools .drone.sec .drone.yml .gitignore .gitlab-ci.yml .travis.yml LICENSE README.md appveyor.yml version.sbt README.md CI service Status Description Travis Linux container tests Drone Linux container tests AppVeyor Windows tests scalameter-examples ScalaMeter benchmark example projects Maven ScalaMeter artifact on Maven ScalaMeter Microbenchmarking and performance regression testing framework for the JVM platform. Website: scalameter.github.io Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalameter/scalameter"	"Performance & memory footprint measuring, regression testing."	"true"
"Testing"	"ScalaMock"	"http://scalamock.org"	"Scala native mocking framework"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"218"	"25"	"42"	"GitHub - paulbutcher/ScalaMock: Native Scala mocking framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 25 Star 218 Fork 42 paulbutcher/ScalaMock Code Issues 51 Pull requests 5 Pulse Graphs Native Scala mocking framework http://scalamock.org/ 407 commits 11 branches 6 releases Fetching contributors Scala 98.5% Java 1.4% JavaScript 0.1% Scala Java JavaScript Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags ScalaMock2 gh-pages issue_85_overloads_with_targs master-2.10 master objectmocks quasiquotes scalamock4 typemacros v3.2_2.10 v3.2_2.11 Nothing to show v3.2_2.11 v3.2_2.10 v3.2.1_2.11 v3.2.1_2.10 v3.2-RC1_2.11 v3.2-RC1_2.10 Nothing to show New pull request Latest commit cf8aa67 Apr 20, 2015 pawel-wiejacha ScalaMock-3.2.2 Permalink Failed to load latest commit information. core/src/main/scala/org/scalamock Allow mocking methods with up to 22 parameters Apr 14, 2015 core_tests/src Added copyright notice Apr 15, 2015 examples/src Consistent license and copyright across all files Feb 6, 2015 frameworks PathMockFactory (PR #106) - additional tests and clean up Apr 19, 2015 project ScalaMock-3.2.2 Apr 20, 2015 .gitignore Add Sublime project Jan 11, 2013 .travis.yml Scala 2.11.1 May 24, 2014 README.md ScalaMock-3.2.2 Apr 20, 2015 RELEASE_NOTES.md ScalaMock 3.2.1 Dec 4, 2014 ScalaMock.sublime-project Oops - wrong path Jan 11, 2013 rootdoc.txt Add root package documentation Jun 26, 2012 README.md ScalaMock Native Scala mocking. Official website: http://scalamock.org/ Examples Expectations-First Style def testTurtle {   val m = mock[Turtle]                              // Create mock Turtle object    (m.setPosition _).expects(10.0, 10.0)             //   (m.forward _).expects(5.0)                        // Set expectations   (m.getPosition _).expects().returning(15.0, 10.0) //     drawLine(m, (10.0, 10.0), (15.0, 10.0))           // Exercise System Under Test } Record-then-Verify (Mockito) Style def testTurtle {   val m = stub[Turtle]                              // Create stub Turtle    (m.getPosition _).when().returns(15.0, 10.0)      // Setup return values    drawLine(m, (10.0, 10.0), (15.0, 10.0))           // Exercise System Under Test    (m.setPosition _).verify(10.0, 10.0)              // Verify expectations met   (m.forward _).verify(5.0)                         // } Full worked example Features Fully typesafe Full support for Scala features such as: Polymorphic (type parameterised) methods Operators (methods with symbolic names) Overloaded methods Type constraints ScalaTest and Specs2 integration Downloading Download from Sonatype. To use ScalaMock in sbt with ScalaTest add the following to your project file: libraryDependencies +=   ""org.scalamock"" %% ""scalamock-scalatest-support"" % ""3.2.2"" % ""test"" and with Specs2: libraryDependencies +=   ""org.scalamock"" %% ""scalamock-specs2-support"" % ""3.2.2"" % ""test"" Documentation Quick Start User Guide Scaladoc Future Plans Check our roadmap. Acknowledgements YourKit is kindly supporting open source projects with its full-featured Java Profiler. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications. Take a look at YourKit's leading software products: YourKit Java Profiler and YourKit .NET Profiler. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/paulbutcher/scalamock"	"Scala native mocking framework"	"true"
"Testing"	"scalaprops ★ 115 ⧗ 6"	"https://github.com/scalaprops/scalaprops"	"Another property based testing library for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"126"	"14"	"8"	"GitHub - scalaprops/scalaprops: property based testing library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 126 Fork 8 scalaprops/scalaprops Code Issues 4 Pull requests 0 Wiki Pulse Graphs property based testing library for Scala 477 commits 39 branches 23 releases 6 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.1.x-scalaz-7.1.8 0.1.x Cogen-List Cogen-laws CorecursiveList Gen-is-Plus GenCogenTest NaturalTrans-Gen-rebase NaturalTrans-Gen SI-2712-2.11.8-tl-201604151108 Scala-2.12.0-M4-2 Scala-2.12.0-M4 assert-macros cats-issue-455 chi-square first-last-min-max-option-maybe first-last-min-max func hoist js-Array latest-Scala-2.12.0-M4 master matryoshka monad-state-laws-rebase monadStateLaws neko-996 neko optimize-Cogen-Bytes pull-req-24 scalaz-1137-strict-tree scalaz-1159 scalaz-1163 scalaz-snapshot scalaz1022 scalaz1023 scodec size-frequency z-1153 z-1180 Nothing to show v0.3.2 v0.3.1 v0.3.0 v0.2.1 v0.2.0 v0.1.17 v0.1.16 v0.1.15 v0.1.14 v0.1.13 v0.1.12 v0.1.11 v0.1.10 v0.1.9 v0.1.8 v0.1.7 v0.1.6 v0.1.5 v0.1.4 v0.1.3 v0.1.2 v0.1.1 v0.1.0 Nothing to show New pull request Latest commit d27ebdd Jul 11, 2016 xuwei-k fix sonatypeReleaseAll Permalink Failed to load latest commit information. core/src/main/scala/scalaprops support scala-js Mar 19, 2016 gen fix cogenFuture Jul 2, 2016 project fix sonatypeReleaseAll Jul 11, 2016 scalaprops fix cogenFuture Jul 2, 2016 scalazlaws/src/main/scala/scalaprops support scala-js Mar 19, 2016 .gitignore Ignore .idea Jul 30, 2015 .travis.yml Scala 2.12.0-M5 Jul 1, 2016 README.md update README.md Jun 16, 2016 screencast.gif add screencast May 6, 2015 version.sbt Setting version to 0.3.3-SNAPSHOT Jun 16, 2016 README.md scalaprops property based testing library for Scala features real scala.FunctionN generators using Cogen (aka CoArbitrary in QuickCheck). scalaprops can generate not only constant Functions flexible parameter settings for each test( ScalaCheck doesn't have this feature ) timeout flexible law checking like discipline discipline uses only String for test id. but scalaprops can use other than String scalaz integration laws for scalaz typeclasses Gen and Cogen instances of scalaz datatypes immutable random number generator scalaprops does not use scala.util.Random because scala.util.Random is mutable default implementation is Mersenne Twister (JVM) or Tiny Mersenne Twister (Scala.js) Scala.js support latest stable version testFrameworks += new TestFramework(""scalaprops.ScalapropsFramework"")  parallelExecution in Test := false // currently, does not support parallel execution  libraryDependencies += ""com.github.scalaprops"" %% ""scalaprops"" % ""0.3.2"" % ""test"" libraryDependencies += ""com.github.scalaprops"" %% ""scalaprops-scalazlaws"" % ""0.3.2"" % ""test"" or you can use sbt plugin API Documentation sxr snapshot version resolvers += Opts.resolver.sonatypeSnapshots  testFrameworks += new TestFramework(""scalaprops.ScalapropsFramework"")  parallelExecution in Test := false  libraryDependencies += ""com.github.scalaprops"" %% ""scalaprops"" % ""0.3.3-SNAPSHOT"" % ""test"" libraryDependencies += ""com.github.scalaprops"" %% ""scalaprops-scalazlaws"" % ""0.3.3-SNAPSHOT"" % ""test"" API Documentation sxr for scalaz 7.1.x https://github.com/scalaprops/scalaprops/tree/0.1.x Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalaprops/scalaprops"	"Another property based testing library for Scala"	"true"
"Testing"	"ScalaTest ★ 380 ⧗ 3"	"https://github.com/scalatest/scalatest"	"A testing tool for Scala and Java developers."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"424"	"51"	"136"	"GitHub - scalatest/scalatest: A testing tool for Scala and Java developers Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 51 Star 424 Fork 136 scalatest/scalatest Code Issues 103 Pull requests 66 Pulse Graphs A testing tool for Scala and Java developers 3,618 commits 71 branches 53 releases Fetching contributors Scala 99.5% Other 0.5% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 3.0.x Switch branches/tags Branches Tags 2.1.x-for-scala-2.11.0-RC3 2.1.x-for-scala-2.11.0-RC4 2.1.x 2.2.x-for-scala-2.12.x 2.2.x-for-scala-2.12.0-M1 2.2.x-for-scala-2.12.0-M2 2.2.x-for-scala-2.12.0-M3 2.2.x 2.2.3-and-greater 2.3.x 3.0.x-for-scala-2.12.x 3.0.x-for-scala-2.12.0-M3 3.0.x assertion-experiment-from-3.0.x build2.2.0-M1ForScala2.11 buildFor2.1.4ForScala2.11 buildFor2.1.5ForScala2.11 buildFor2.1.6ForScala2.11 buildFor2.1.7ForScala2.11 buildFor2.11.0-M7 contain-refactor diverging-puzzle experiment-any-styles experiment-async-info-note-alert-markup experiment-back-to-unit experiment-deadlock experiment-either-stack-depth-exception experiment-junit-xml-ordering experiment-modularization experiment-possible-outcome experiment-postpone-modularization experiment-pretty-refactor experiment-remove-getStackDepthFun-calls experiment-serial-async experiment-serial-exec-ctx experiment-sync-suite experiment-two-funsuites experiment-two-scalatests experiment-unit-to-any experiment-use-any-in-conductor experiment-use-future-outcome feature-assertion-result-type feature-async-before-and-after feature-checked-equality feature-clean-up-exceptions feature-equasets-experiement1 feature-equasets feature-gen feature-insertion-order-set feature-merging-cs-pretty-position-877 feature-merging-into-3.0.x-for-rc1 feature-merging-pretty-position-pr-into-3.0.x feature-scalactic-algebra funkiness good-bad-lone-type-param inspectable-approach master-before-2.3.x-merge master max-discarded-factor-for-23x problem-in-2.11.0-RC3 rainy-day registration-experiment scala-2.11-M8 scala-2.11.0-RC1 scala-2.11.0-RC2 slicing-enabler splice-is-the-spice-of-macros temp-fixture-traits-assertion temp-local-branch-for-should-verb-refactor to-release-as-2.0.M6-SNAP-for-scala2.11.0-M3 torch-for-more-general-inspectors Nothing to show release-3.0.0-SNAP2-for-scala-2.11-and-2.10 release-3.0.0-SNAP1-for-scala-2.11-and-2.10 release-3.0.0-RC4 release-3.0.0-RC3 release-3.0.0-RC3-for-scala-2.10-2.11-2.12 release-3.0.0-RC2-for-scala-2.10-2.11-2.12 release-3.0.0-RC1-with-scaladoc release-3.0.0-RC1-for-scala-2.11-and-2.10 release-3.0.0-M15-for-scala-2.11-and-2.10 release-3.0.0-M14-for-scala-2.11-and-2.10 release-3.0.0-M11-for-scala-2.11-and-2.10 release-3.0.0-M10-for-scala-2.11-and-2.10 release-3.0.0-M9-for-scala-2.11-and-2.10 release-3.0.0-M8 release-2.2.6-for-scala-2.11-and-2.10 release-2.2.6-M1-for-scala-2.11-and-2.10 release-2.2.5-for-scala-2.11-and-2.10 release-2.2.4-for-scala-2.11-and-2.10 release-2.2.3-SNAP1-for-scala-2.11-and-2.10 release-2.2.2-for-scala-2.11-and-2.10 release-2.2.1-for-scala-2.11-and-2.10 release-2.2.0-for-scala-2.11-and-2.10 release-2.2.0-RC2-for-scala-2.11-and-2.10 release-2.2.0-RC1-for-scala-2.11-and-2.10 release-2.2.0-M1-for-scala-2.10 release-2.1.7-for-scala-2.11 release-2.1.7-for-scala-2.10 release-2.1.6-for-scala-2.11 release-2.1.6-for-scala-2.10 release-2.1.5-for-scala-2.11 release-2.1.5-for-scala-2.10 release-2.1.4-for-scala-2.11 release-2.1.4-for-scala-2.10 release-2.1.3-for-scala-2.11.0-RC4 release-2.1.3-for-scala-2.11 release-2.1.3-for-scala-2.10 release-2.1.2-for-scala-2.11.0-RC3 release-2.1.2-for-scala-2.10 release-2.1.1-for-scala-2.11.0-RC2 release-2.1.1-for-scala-2.10 release-2.1.0-for-scala-2.11.0-RC1 release-2.1.0-for-scala-2.10 release-2.1.0-RC3-for-scala-2.11.0-M8 release-2.1.0-RC3-for-scala-2.10 release-2.1.0-RC2-for-scala-2.11.0-M8 release-2.1.0-RC2-for-scala-2.10 release-2.0.RC3-for-scala-2.10 release-2.0.RC2-for-scala-2.10 release-2.0.RC1-for-scala-2.10 release-2.0.M8-for-scala-2.10 release-2.0.M7-for-scala-2.10 release-2.0.M6-for-scala-2.10 release-2.0-for-scala-2.10 Nothing to show New pull request Latest commit e34fe66 Jul 12, 2016 bvenners Fixed the github URL. Permalink Failed to load latest commit information. ant-lib initial version for github, created from current version of ScalaTest… Apr 8, 2013 common-test.js/src/main/scala/org/scalatest Rewritten LineNumberHelper using macro. May 21, 2015 common-test/src/main/scala/org/scalatest Reorganized imports. Jun 21, 2016 examples/src/test Updated TimeLimitedTests about Signaler, and fixed the broken examples. May 19, 2016 lib initial version for github, created from current version of ScalaTest… Apr 9, 2013 nbproject initial version for github, created from current version of ScalaTest… Apr 9, 2013 notes Changes to notes for 2.2.0 release. Jun 7, 2014 project Fixed the github URL. Jul 12, 2016 scalactic-macro/src/main Reorganized imports. Jun 21, 2016 scalactic-test/src/test/scala/org/scalactic Reorganized imports. Jun 21, 2016 scalactic/src/main Merge branch 'cs-imports-cleanup' of https://github.com/cheeseng/scal… Jun 21, 2016 scalatest-test/src Bumping up scala-js version to 2.12.0-M5. Jul 2, 2016 scalatest.js/src/main/scala/org/scalatest Fixed broken compile in scala-js. Jun 8, 2016 scalatest/src/main Merge branch 'cs-feature-insertion-order-set' of https://github.com/c… Jul 2, 2016 support Fix docjavatag conversion of TagAnnotation.java May 28, 2014 webapp Added remaining html5 input tags Jul 14, 2013 .gitattributes Added .gitattributes file that contains settings to use unix-style EOL. Apr 9, 2013 .gitignore Work in progress scalatest.js. Mar 6, 2015 .travis.yml Bump up to use Scala 2.11.8 and 2.10.6 to build in travis. Jun 8, 2016 2.9_excludes Renamed scalautils to scalactic. Apr 24, 2014 LICENSE initial version for github, created from current version of ScalaTest… Apr 9, 2013 NOTICE initial version for github, created from current version of ScalaTest… Apr 9, 2013 README.md Make scala-js tests to run, and bump up to scala 2.12.0-M5 in README.md. Jul 4, 2016 addmem.sh initial version for github, created from current version of ScalaTest… Apr 9, 2013 build.xml Removed the use of no-specialization in both ant and sbt build. May 14, 2014 migration.txt initial version for github, created from current version of ScalaTest… Apr 9, 2013 osgi.bnd Renamed scalautils to scalactic. Apr 24, 2014 pom_template.xml Fix OSGi tests Apr 9, 2014 scalactic-osgi.bnd Renamed scalautils to scalactic. Apr 24, 2014 scalactic-pom_template.xml Renamed scalautils to scalactic. Apr 24, 2014 todo.txt Renamed Spec and SpecLike to RefSpec and RefSpecLike, moved them into… Nov 14, 2015 travis_build.sh Added another testQuick try for RegularTests4 in travis. Jun 8, 2016 updateCopyright.bash added bash script to update Copyright to current year and added all j… Aug 29, 2013 use-ldquo-and-rdquo.bash script to replace &#8220; with &ldquo; and &#8221; with &rdquo; and r… Sep 7, 2013 README.md ScalaTest ScalaTest is a free, open-source testing toolkit for Scala and Java programmers. Official Website: http://www.scalatest.org/ Using ScalaTest Setup Please visit Download and Setup for download and setup instructions. Quick Start Please visit Quick Start for steps to get started quickly. Building ScalaTest Pre-Requisites The followings are needed for building ScalaTest: JDK 6, 7 or 8 SBT 0.13.2 for JDK 6 or 7, use the following options in your SBT launch file: SBT_OPTS=""-server -Xms512M -Xmx3000M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:NewRatio=8 -XX:MaxPermSize=512M""  for JDK 8, use the following SBT options instead: SBT_OPTS=""-server -Xms512M -Xmx3000M -Xss1M  -XX:+UseConcMarkSweepGC -XX:NewRatio=8""  Building and Running Tests This command will build and run the regular tests: $ sbt test To run generated all tests, you'll need to increase maximum heap size to -Xmx5000M, and use this command instead: $ sbt gentests/test You can also run different groups generated tests separately: $ sbt genMustMatchersTests1/test $ sbt genMustMatchersTests2/test $ sbt genMustMatchersTests3/test $ sbt genMustMatchersTests4/test $ sbt genGenTests/test $ sbt genTablesTests/test $ sbt genInspectorsTests/test $ sbt genInspectorsShorthandsTests1/test $ sbt genInspectorsShorthandsTests2/test $ sbt genTheyTests/test $ sbt genContainTests1/test $ sbt genContainTests2/test $ sbt genSortedTests/test $ sbt genLoneElementTests/test $ sbt genEmptyTests/test $ sbt genSafeStyleTests/test  What it does is simply switch to gentests project and run test. Building Exmaples You can build examples project using this command: $ sbt examples/compile Packaging You can package the ScalaTest JAR file using this command: $ sbt package The resulting JAR file will be produced in target/scala-2.11/. You can also publish it to your local Ivy repository using this command: $ sbt publishLocal Or publish it to local maven repository using this command: $ sbt publishM2 Publishing To publish to Sonatype, you first need to make sure you have the following: A GPG client is installed on your command line path. For more information, please refer to GNU Privacy Guard Website. You have created your GPG keys and distributed your public key to hkp://pool.sks-keyservers.net/. For more information, please refer to How To Generate PGP Signatures With Maven. You have been granted the right to publish using org.scalatest and org.scalactic domain. By default, ScalaTest build will read your Sonatype credentials from ~/.ivy2/.credentials, which is a properties file that looks like this: realm=Sonatype Nexus Repository Manager host=oss.sonatype.org user=xxxxxxxx password=xxxxxxxx  You can use SCALATEST_NEXUS_LOGIN and SCALATEST_NEXUS_PASSWORD environment variables to override Sonatype credentials. For signing, ScalaTest build will use ~/.gnupg/secring.gpg by default and prompt for GPG passphase if required. Alternatively you can use SCALATEST_GPG_FILE to use a different GPG file, and use SCALATEST_GPG_PASSPHASE to provide GPG passphase to avoid input prompt. If you would like to export a particular private key into a separate GPG file, you can use the following command: $ gpg --export-secret-keys example@example.com > example-secret-key.gpg With Sonatype credentials and GPG file in place, you can now publish to Sonatype. To publish scalactic, scalatest and scalatest-app (for Scala and Scala-js, version 2.11 and 2.10, and make sure you're on Java 6) to Sonatype, use the following command: $ sbt clean publishSigned ""project scalatestAppJS"" clean publishSigned ++2.10.6 ""project scalatestApp"" clean publishSigned ""project scalatestAppJS"" clean publishSigned To publish scalactic, scalatest and scalatest-app (for Scala and Scala-js, version 2.12, and make sure you're on Java 8) to Sonatype, use the following command: $ sbt ++2.12.0-M5 clean publishSigned ""project scalatestAppJS"" clean publishSigned Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalatest/scalatest"	"A testing tool for Scala and Java developers."	"true"
"Testing"	"Scalive ★ 131 ⧗ 1"	"https://github.com/xitrum-framework/scalive"	"Connect a Scala REPL to running JVM processes without any prior setup; this library is used for inspecting systems in production mode."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"131"	"18"	"9"	"GitHub - xitrum-framework/scalive: Connect a Scala REPL to running JVM processes without any prior setup Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 18 Star 131 Fork 9 xitrum-framework/scalive Code Issues 5 Pull requests 1 Pulse Graphs Connect a Scala REPL to running JVM processes without any prior setup http://youtu.be/h45QQ45D9P8 109 commits 3 branches 6 releases 3 contributors Java 89.2% Scala 7.0% Shell 2.6% Batchfile 1.2% Java Scala Shell Batchfile Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags dev master multi_class_loader Nothing to show 1.5 1.4 1.3 1.2 1.1 1.0 Nothing to show New pull request Latest commit 8da470f May 23, 2015 ngocdaothanh Update deps Permalink Failed to load latest commit information. dev Fix #12 Update default Scala from 2.11.2 to 2.11.4 Dec 31, 2014 project Update deps May 23, 2015 src/main/java/scalive Fix #12 Update default Scala from 2.11.2 to 2.11.4 Dec 30, 2014 .gitignore Add .gitignore Jan 29, 2014 MIT-LICENSE Add MIT-LICENSE Jan 30, 2014 README.md Fix #12 Update default Scala from 2.11.2 to 2.11.4 Dec 30, 2014 build.sbt Update deps May 23, 2015 README.md This tool allows you to connect a Scala REPL console to running Oracle (Sun) JVM processes without any prior setup at the target process. Download Download and extract scalive-1.5.zip, you will see: scalive-1.5/   scalive   scalive.cmd   scalive-1.5.jar    scala-library-2.10.4.jar   scala-compiler-2.10.4.jar   scala-reflect-2.10.4.jar    scala-library-2.11.4.jar   scala-compiler-2.11.4.jar   scala-reflect-2.11.4.jar  scala-library, scala-compiler, and scala-reflect of the appropriate version will be loaded to your running JVM process, if they have not been loaded. The REPL console needs these libraries. For example, your process has already loaded scala-library 2.10.4 by itself, but scala-compiler and scala-reflect haven't been loaded, Scalive will automatically load their version 2.10.4. If none of them has been loaded, Scalive will load version 2.11.4. For convenience, Scala 2.10.4 and 2.11.4 JARs are preincluded. If your process is using a different Scala version, you need to manually download the corresponding JARs from the Internet and save them in the same directory as above. Usage Run the shell script scalive (*nix) or scalive.cmd (Windows). To see a list of running JVM processes and their process IDs: scalive  To connect a Scala REPL console to a process: scalive <pid>  How to add your own JARs Scalive only automatically adds scala-library.jar, scala-compiler.jar, scala-reflect.jar, and scalive.jar to the system classpath. If you want to load additional classes in other JARs, first run these in the REPL console to add the JAR to the system class loader: val cl         = ClassLoader.getSystemClassLoader.asInstanceOf[java.net.URLClassLoader] val searchDirs = Array(""/dir/containing/the/jar"") val jarbase    = ""mylib""  // Will match ""mylibxxx.jar"", convenient when there's version number in the file name scalive.Classpath.findAndAddJar(cl, searchDirs, jarbase)  Now the trick is just quit the REPL console and connect it to the target process again. You will be able to use your classes in the JAR as normal: import mylib.foo.Bar ...  Note that :cp doesn't work. How Scalive works Scalive uses the Attach API to tell the target process to load an agent. The agent then creates a TCP server to let the Scalive process interact with the target process. The Scalive process acts as a client. See also liverepl, a similar REPL console for Clojure. Known issues 1. For simplicity and to avoid memory leak when you attach/detach many times, Scalive only supports processes with only the default system class loader, without additional class loaders (Ex: normal standalone JVM processes, like Play or Xitrum in production mode). Processes with multiple class loaders like Tomcat or SBT are not supported (with SBT, you already has the SBT console, so it's not a big deal). 2. These features will be added in the future: Use up/down arrows keys to navigate the console history, pasting multiline block of code etc. Use tab key for autocompletion Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/scalive"	"Connect a Scala REPL to running JVM processes without any prior setup; this library is used for inspecting systems in production mode."	"true"
"Testing"	"Specs2 ★ 500 ⧗ 6"	"https://github.com/etorreborre/specs2"	"Software Specifications for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"514"	"32"	"126"	"GitHub - etorreborre/specs2: Software Specifications for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 32 Star 514 Fork 126 etorreborre/specs2 Code Issues 4 Pull requests 0 Pulse Graphs Software Specifications for Scala http://specs2.org 3,658 commits 17 branches 121 releases 42 contributors Scala 82.6% JavaScript 14.6% CSS 1.1% Other 1.7% Scala JavaScript CSS Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags fix/isolated gh-pages master specs2-2.x specs2-three-topic/home-page specs2-three topic/datatables-show-take2 topic/datatables-show topic/eff topic/exit-codes topic/scala-2.12.0-M4 topic/scala-2.12.0-M5 topic/scalaz-stream-remove topic/specification-structure-refactor topic/typechecked-warnings topic/versions-on-website topic/2.12 Nothing to show SPECS2-DOC-1.6 SPECS2-3.8.4 SPECS2-3.8.3 SPECS2-3.8.2 SPECS2-3.8.1 SPECS2-3.8 SPECS2-3.8-20160502205721-23790d0 SPECS2-3.8-20160502201250-14ffaa6 SPECS2-3.7.3 SPECS2-3.7.2 SPECS2-3.7.2-20160303132317-61de873 SPECS2-3.7.1 SPECS2-3.7.1-20160303130343-b1e5385 SPECS2-3.7.1-20160303101046-b1e5385 SPECS2-3.7.1-20160303100421-b1e5385 SPECS2-3.7.1-20160212125328-83c8273 SPECS2-3.7 SPECS2-3.6.6 SPECS2-3.6.6-20160103041630-461039e SPECS2-3.6.5 SPECS2-3.6.4 SPECS2-3.6.4-20151016053644-0ca99ef SPECS2-3.6.3 SPECS2-3.6.2 SPECS2-3.6.1 SPECS2-3.6.1-20150618235732-9b3267d SPECS2-3.6 SPECS2-3.6-20150530005327-6476871 SPECS2-3.6-20150519234533-20e311a SPECS2-3.5 SPECS2-3.5-20150429115212-d817c01 SPECS2-3.5-20150429005731-d441bf4 SPECS2-3.5-20150428120937-48c299f SPECS2-3.5-20150426232103-442b450 SPECS2-3.5-20150421030134-582d033 SPECS2-3.4 SPECS2-3.4-20150416001459-d7706b6 SPECS2-3.3.1 SPECS2-3.3.1-20150405231649-79c1053 SPECS2-3.3 SPECS2-3.2 SPECS2-3.2-20150329233502-5bbb976 SPECS2-3.2-20150328050811-f6f2463 SPECS2-3.2-20150328050811-d146044 SPECS2-3.2-20150326231645-27f838e SPECS2-3.1.1 SPECS2-3.1 SPECS2-3.1-20150321223007-81e756a SPECS2-3.1-20150321213348-1b7c0a4 SPECS2-3.1-20150319225601-f3a8cbb SPECS2-3.0.1 SPECS2-3.0.1-20150319102835-176e393 SPECS2-3.0.1-20150318021309-9577d90 SPECS2-3.0.1-20150312050040-d541934 SPECS2-3.0.1-20150312050040-3893a53 SPECS2-3.0 SPECS2-3.0-M4-20150227014550-82f068c SPECS2-3.0-20150306133021-ac0f819 SPECS2-3.0-20150306092744-4d03363 SPECS2-2.4.17 SPECS2-2.4.16 SPECS2-2.4.15 SPECS2-2.4.13 SPECS2-2.4.12 SPECS2-2.4.11 SPECS2-2.4.10 SPECS2-2.4.9 SPECS2-2.4.8 SPECS2-2.4.7 SPECS2-2.4.6 SPECS2-2.4.5 SPECS2-2.4.4 SPECS2-2.4.3 SPECS2-2.4.2 SPECS2-2.4.1 SPECS2-2.4 SPECS2-2.3.13 SPECS2-2.3.12 SPECS2-2.3.11 SPECS2-2.3.10 SPECS2-2.3.9 SPECS2-2.3.8 SPECS2-2.3.7 SPECS2-2.3.6 SPECS2-2.3.5 SPECS2-2.3.4 SPECS2-2.3.3 SPECS2-2.3.2 SPECS2-2.3.1 SPECS2-2.3 SPECS2-2.2.3 SPECS2-2.2.2 SPECS2-2.2.1 SPECS2-2.2 SPECS2-2.1.1 SPECS2-2.1 SPECS2-2.0 SPECS2-2.0-RC2 SPECS2-2.0-RC1 SPECS2-1.14 Nothing to show New pull request Latest commit 55a2256 Jul 13, 2016 nuttycom committed with etorreborre Add missing imports to ""Create Your Own"" matchers section. (#493) … Fixes #492 Permalink Failed to load latest commit information. analysis/src use the eff monad instead of monad transformers Dec 29, 2015 cats/src added a cats module for Xor matchers Apr 21, 2016 codata/src/main/scala/codata updated Scala 2.12 to 2.12.0-M5 Jul 4, 2016 common/src updated Scala 2.12 to 2.12.0-M5 Jul 4, 2016 core/src updated Scala 2.12 to 2.12.0-M5 Jul 4, 2016 examples/src/test/scala/examples compiling with 2.12.0-M3 Oct 26, 2015 form/src better compatibility with scalaz 7.1 and 7.2 Dec 29, 2015 guide/src/test Add missing imports to ""Create Your Own"" matchers section. (#493) Jul 13, 2016 gwt/src fixed the comment warnings during the scaladoc phase Jul 30, 2015 html/src fixed the user guide generation May 16, 2016 junit/src use the latest eff May 8, 2016 markdown/src re-introduced fatal warnings for 2.11. fixes #404 Jul 30, 2015 matcher-extra/src renamed a source directory for matcher-extras Jul 4, 2016 matcher/src/main/scala/org/specs2/matcher re-throw Timeout failures in mutable specifications when using Future… Jun 24, 2016 mock/src don't try to register matchers when a spy is called for real. fixes #428 Nov 8, 2015 notes minor modifications to notes and published 3.8.4 Jun 20, 2016 project updated shapeless for Scala 2.12.0-M5 Jul 4, 2016 scalacheck/src Keep the stacktrace of a Failure created in a ScalaCheck property. fi… Jul 11, 2016 tests/src Keep the stacktrace of a Failure created in a ScalaCheck property. fi… Jul 11, 2016 .gitignore set the version file to 3.1 Mar 20, 2015 .travis.yml fixed the scala version in the travis file Apr 25, 2016 LICENSE.txt removed the dependency on scalaz-stream May 2, 2016 README.markdown added a link to the main site in the readme Jun 16, 2016 pom.xml prepared the 3.7.1 release Mar 3, 2016 release-procedure.txt updated the version number for the 1.12.3 release Nov 14, 2012 sbt upgraded sbt to 0.13.11 Mar 26, 2016 scalastyle-config.xml added more warnings and more style checking Mar 25, 2015 try-specs2.sh added a try-specs2 script May 27, 2016 version.sbt minor modifications to notes and published 3.8.4 Jun 20, 2016 README.markdown Go to specs2.org to learn more about specs2! Installation instructions You need to download and install sbt. Then execute the following command: sbt update publishLocal  Then you can generate the User Guide with: sbt testOnly org.specs2.guide.UserGuide -- html  This should create html files in the target/specs2-reports directory. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/etorreborre/specs2"	"Software Specifications for Scala."	"true"
"Testing"	"µTest ★ 156 ⧗ 2"	"https://github.com/lihaoyi/utest"	"A tiny, portable testing library for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"167"	"14"	"27"	"GitHub - lihaoyi/utest: A tiny, portable testing library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 167 Fork 27 lihaoyi/utest Code Issues 21 Pull requests 4 Pulse Graphs A tiny, portable testing library for Scala 180 commits 4 branches 3 releases 16 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.3 master minimize test Nothing to show 0.4.3 0.4.1 0.4.0 Nothing to show New pull request Latest commit 927cebc Apr 13, 2016 lihaoyi Merge pull request #91 from alexeyr/patch-1 … Make suggested dependencies test-only Permalink Failed to load latest commit information. project arrow-asserts Mar 11, 2016 utest add testpath to docs Mar 13, 2016 .gitignore experimental async response support Sep 30, 2014 .travis.yml tweak-travis Mar 12, 2016 Sizes.png add size comparison May 6, 2014 build.sbt 0.4.3 Mar 12, 2016 readme.md Make suggested dependencies test-only Apr 12, 2016 readme.md µTest 0.4.3 uTest (pronounced micro-test) is a lightweight testing library for Scala. Its key features are: Less than 1000 lines of code A fancy set of macro-powered asserts A unique execution model Integration with SBT Cross compiles to ScalaJS Contents Getting Started Defining and Running a Test Suite Nesting Tests Sharing Setup Fixtures Other Ways of Naming tests Asynchronous Tests Macro Asserts Arrow Asserts Intercept Eventually and Continually Assert Match Compile Error Test Utilities TestPath Local Retries Configuring uTest Output Formatting Suite Retries Test Wrapping Execution Model SBT Command-line Interface ScalaJS Why uTest Development Tips Changelog Getting Started Most people coming to uTest will be running tests through SBT. Add the following to your build.sbt and you can immediately begin defining and running tests programmatically. libraryDependencies += ""com.lihaoyi"" %% ""utest"" % ""0.4.3"" % ""test""  testFrameworks += new TestFramework(""utest.runner.Framework"") To use it with Scala.js, swap out the libraryDependencies with libraryDependencies += ""com.lihaoyi"" %%% ""utest"" % ""0.4.3"" % ""test""  testFrameworks += new TestFramework(""utest.runner.Framework"") Defining and Running a Test Suite Put this in your src/test/scala/ folder: package test.utest.examples.examples  import utest._  object HelloTests extends TestSuite{   val tests = this{     'test1{       throw new Exception(""test1"")     }     'test2{       1     }     'test3{       val a = List[Byte](1, 2)       a(10)     }   } } You can then run this via sbt myproject/test  Which should produce this output: [info] -----------------------Starting Suite test.utest.examples.HelloTests----------------------- [info] test.utest.examples.HelloTests.test1 [info]              java.lang.Exception: test1 [info] test.utest.examples.HelloTests.test2     Success [info] 1 [info] test.utest.examples.HelloTests.test3 [info]              java.lang.IndexOutOfBoundsException: 10 [info] test.utest.examples.HelloTests       Success [info] -----------------------------------Results----------------------------------- [info] test.utest.examples.HelloTests       Success [info]     test1        Failure('java.lang.Exception: test1') [info]     test2        Success [info]          1 [info]     test3        Failure('java.lang.IndexOutOfBoundsException: 10') [info] Failures: [info] test.utest.examples.HelloTests.test1 [info]              java.lang.Exception: test1 [info] test.utest.examples.HelloTests$$anonfun$4$$anonfun$apply$1.apply(HelloTests.scala:8) [info] test.utest.examples.HelloTests$$anonfun$4$$anonfun$apply$1.apply(HelloTests.scala:6) [info] test.utest.examples.HelloTests.test3 [info]              java.lang.IndexOutOfBoundsException: 10 [info] scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:51) [info] scala.collection.immutable.List.apply(List.scala:83) [info] test.utest.examples.HelloTests$$anonfun$4$$anonfun$apply$3.apply(HelloTests.scala:15) [info] test.utest.examples.HelloTests$$anonfun$4$$anonfun$apply$3.apply(HelloTests.scala:6) [info] Tests: 4 [info] Passed: 2 [info] Failed: 2 [error] Failed tests: [error]     test.utest.examples.HelloTests [error] (utestJVM/test:testOnly) sbt.TestsFailedException: Tests unsuccessful [error] Total time: 1 s, completed Mar 12, 2016 5:24:54 PM At first the tests are run one at a time; after they've all completed, a summary is printed, followed by the stack traces for any tests which failed. You can configure the test suite to change how these things are printed. Nesting Tests Note that tests within the suite can nested within each other, but only directly. E.g. you cannot define tests within if-statements or for-loops. uTest relies on the test structure to be statically known at compile time. They can be nested arbitrarily deep: package test.utest.examples  import utest._  object NestedTests extends TestSuite{   val tests =  this{     val x = 1     'outer{       val y = x + 1       'inner{         val z = y + 1         'innerest{           assert(             x == 1,             y == 2,             z == 3           )           (x, y, z)         }       }     }   } } Which when run prints: [info] -----------------Starting Suite test.utest.examples.NestedTests----------------- [info] test.utest.examples.NestedTests.outer.inner.innerest     Success [info] (1,2,3) [info] test.utest.examples.NestedTests.outer.inner      Success [info] test.utest.examples.NestedTests.outer        Success [info] test.utest.examples.NestedTests      Success [info] -----------------------------------Results----------------------------------- [info] test.utest.examples.NestedTests      Success [info]     outer        Success [info]         inner        Success [info]             innerest     Success [info]                     (1,2,3) [info] [info] Tests: 4 [info] Passed: 4 [info] Failed: 0 As you can see, the various three are nested within each other. Sharing Setup Fixtures You can use the nested of tests to group related tests together, and have them share common initialization code or fixtures by defining them in the enclosing test: package test.utest.examples  import utest._  object SharedSetupTests extends TestSuite{   val tests = this{     var x = 0     'A{       x += 1       'X{         x += 2         assert(x == 3)         x       }       'Y{         x += 3         assert(x == 4)         x       }     }     'B{       x += 4       'Z{         x += 5         assert(x == 9)         x       }     }   } } Here, we are sharing the initialization of the variable x between all the various sub-tests in the same folder. Despite being shared lexically, these helpers are re-created for each test that is run, so if they contain mutable state (e.g. mutable collections, or vars) you do not need to worry about the mutations from multiple tests interfering with each other. This gives lets you concisely share common setup code between related tests in the same grouping, while avoiding interference between tests due to mutation of shared fixtures. If you want the fixtures to really-truly be shared between individual tests, define it outside the this{} block: package test.utest.examples  import utest._  object SharedFixturesTests extends TestSuite{   var x = 0   val tests = this{     'A{       x += 1       'X{         x += 2         assert(x == 4)         x       }       'Y{         x += 3         assert(x == 8)         x       }     }     'B{       x += 4       'Z{         x += 5         assert(x == 21)         x       }     }   } } And you'll see that the changes to x are being shared between the invocations of all the tests: A increments it by 1, X by 1 then 2, to 4, Y by 1 and 3 to 8, and so on. This allows you to avoid doing work repeatedly initializing things, but you need to be careful the tests aren't mutating shared state that could interfere! For more detail on this and other things related to test execution, see below. Other Ways of Naming tests uTest also allows you to use strings to define test keys, if you wish to make your test names longer and more descriptive: ""test with spaces""-{   throw new Exception(""test1"") } 'test2-run(1)  Note that you can also use the 'symbol-... syntax, if your tests are concise and you want to make them really concise. The ""string""-{...}, 'symbol{...} and 'symbol-... syntaxes are all entirely equivalent. The last way of defining tests is with the utest.* symbol, e.g. import utest._  val test = TestSuite{   'test1{     throw new Exception(""test1"")   }   'test2{     * - {1 == 1}     * - {2 == 2}     * - {3 == 3}   }   'test3{     val a = List[Byte](1, 2)     a(10)   } } Tests defined using the * symbol are give the numerical names ""0"", ""1"", ""2"", etc.. This is handy if you have a very large number of very simple test cases (perhaps you've delegated the heavy lifting to a helper function), but still want to be able to run them separately. Asynchronous Tests val tests = this {   ""testSuccess"" - {     Future {       assert(true)     }   }   ""testFail"" - {     Future {       assert(false)     }   }   ""normalSuccess"" - {     assert(true)   }   ""normalFail"" - {     assert(false)   } }  tests.runAsync().map {    results =>   assert(results.toSeq(0).value.isSuccess) // root   assert(results.toSeq(1).value.isSuccess) // testSuccess   assert(results.toSeq(2).value.isFailure) // testFail   assert(results.toSeq(3).value.isSuccess) // normalSuccess } You can have tests which return (have a last expression being) a Future[T] instead of a normal value. You can run the suite using .runAsync to return a Future of the results, or you can continue using .run which will wait for all the futures to complete before returning. In Scala.js, calling .run on a test suite with futures in it throws an error instead of waiting, since you cannot wait in Scala.js. When running the test suites from SBT, you do not need worry about any of this run vs runAsync stuff: the test runner will handle it for you and provide the correct results. Macro Asserts val x = 1 val y = ""2"" assert(   x > 0,   x == y )  // utest.AssertionError: x == y // x: Int = 1 // y: String = 2 uTest comes with a macro-powered asserts that provide useful debugging information in the error message. These take one or more boolean expressions, and when they fail, will print out the names, types and values of any local variables used in the expression that failed. This makes it much easier to see what's going on than Scala's default assert, which gives you the stack trace and nothing else. uTest also wraps any exceptions thrown within the assert, to help trace what went wrong: val x = 1L val y = 0L assert(x / y == 10)  // utest.AssertionError: assert(x / y == 10) // caused by: java.lang.ArithmeticException: / by zero // x: Long = 1 // y: Long = 0 The origin exception is stored as the cause of the utest.AssertionError, so the original stack trace is still available for you to inspect. Arrow Asserts 1 ==> 1 // passes Array(1, 2, 3) ==> Array(1, 2, 3) // passes try{   1 ==> 2 // throws }catch{case e: java.lang.AssertionError =>   e } You can use a ==> b as a shorthand for assert(a == b). This results in pretty code you can easily copy-paste into documentation. Intercept val e = intercept[MatchError]{   (0: Any) match { case _: String => } } println(e)  // scala.MatchError: 0 (of class java.lang.Integer) intercept allows you to verify that a block raises an exception. This exception is caught and returned so you can perform further validation on it, e.g. checking that the message is what you expect. If the block does not raise one, an AssertionError is raised. As with assert, intercept adds debugging information to the error messages if the intercept fails or throws an unexpected Exception. Eventually and Continually val x = Seq(12) eventually(x == Nil)  // utest.AssertionError: eventually(x == Nil) // x: Seq[Int] = List(12) In addition to a macro-powered assert, uTest also provides macro-powered versions of eventually and continually. These are used to test asynchronous concurrent operations: eventually(tests: Boolean*): ensure that the boolean values of tests all become true at least once within a certain period of time. continually(tests: Boolean*): ensure that the boolean values of tests all remain true and never become false within a certain period of time. These are implemented via a retry-loop, with a default retry interval of 0.1 second and retries up to a total of 1 second. If you want to change this behavior, you can shadow the implicit values retryInterval and retryMax, for example this: implicit val retryMax = RetryMax(300.millis) implicit val retryInterval = RetryInterval(50.millis) Would set the retry-loop to happen every 50ms up to a max of 300ms. Together, these two operations allow you to easily test asynchronous operations. You can use them to help verify Liveness properties (that condition must eventually be met) and Safety properties (that a condition is never met) As with assert, eventually and continually add debugging information to the error messages if they fail. Assert Match assertMatch(Seq(1, 2, 3)){case Seq(1, 2) =>} // AssertionError: Matching failed Seq(1, 2, 3) assertMatch is a convenient way of checking that a value matches a particular shape, using Scala's pattern matching syntax. This includes support for use of | or _ or if-guards within the pattern match. This gives you additional flexibility over a traditional assert(a == Seq(1, 2)), as you can use _ as a wildcard e.g. using assertMatch(a){case Seq(1, _)=>} to match any 2-item Seq whose first item is 1. As with assert, assertMatch adds debugging information to the error messages if the value fails to match or throws an unexpected Exception while evaluating. Compile Error compileError(""true * false"") // CompileError.Type(""value * is not a member of Boolean"")  compileError(""(}"") // CompileError.Parse(""')' expected but '}' found."") compileError is a macro that can be used to assert that a fragment of code (given as a literal String) fails to compile. If the code compiles successfully, compileError will fail the compilation run with a message. If the code fails to compile, compileError will return an instance of CompileError, one of CompileError.Type(pos: String, msgs: String*) or CompileError.Parse(pos: String, msgs: String*) to represent typechecker errors or parser errors In general, compileError works similarly to intercept, except it does its checks (that a snippet of code fails) and errors (if it doesn't fail) at compile-time rather than run-time. If the code fails as expected, the failure message is propagated to runtime in the form of a CompileError object. You can then do whatever additional checks you want on the failure message, such as verifying that the failure message contains some string you expect to be there. The compileError macro compiles the given string in the local scope and context. This means that you can refer to variables in the enclosing scope, i.e. the following example will fail to compile because the variable x exists. val x = 0  compileError(""x + x""), // [error] compileError check failed to have a compilation error The returned CompileError object also has a handy .check method, which takes a position-string indicating where the error is expected to occur, as well as zero-or-more messages which are expected to be part of the final error message. This is used as follows:     compileError(""true * false"").check(       """"""     compileError(""true * false"").check(                        ^       """""",       ""value * is not a member of Boolean""     ) Note that the position-string needs to exactly match the line of code the compile-error occured on. This includes any whitespace on the left, as well as any unrelated code or comments sharing the same line as the compileError expression. Test Utilities uTest provides a range of test utilities that aren't strictly necessary, but aim to make your writing of tests much more convenient and DRY. TestPath package test.utest.examples  import utest._  object TestPathTests extends TestSuite{   val tests = this{     'testPath{       'foo {         assert(implicitly[utest.framework.TestPath].value == Seq(""testPath"", ""foo""))       }     }   } } uTest exposes the path to the current test to the body of the test via the utest.framework.TestPath implicit. This can be used to One example is the Fastparse test suite, which uses the name of the test to provide the repository that it needs to clone and parse: def checkRepo(filter: String => Boolean = _ => true)              (implicit testPath: utest.framework.TestPath) = {   val url = ""https://github.com/"" + testPath.value.last   import sys.process._   val name = url.split(""/"").last   if (!Files.exists(Paths.get(""target"", ""repos"", name))){     println(""CLONING"")     Seq(""git"", ""clone"", url, ""target/repos/""+name, ""--depth"", ""1"").!   }   checkDir(""target/repos/""+name, filter) }  ""lihaoyi/fastparse"" - checkRepo() ""scala-js/scala-js"" - checkRepo() ""scalaz/scalaz"" - checkRepo() ""milessabin/shapeless"" - checkRepo() ""akka/akka""- checkRepo() ""lift/framework"" - checkRepo() ""playframework/playframework"" - checkRepo() ""PredictionIO/PredictionIO"" - checkRepo() ""apache/spark"" - checkRepo() This allows us to keep the tests DRY - avoiding having to repeat the name of the repo in the name of the test for every test we define - as well as ensuring that they always stay in sync. If you need additional metadata such as line-numbers or file-paths or class or package names, you can use the SourceCode library's implicits to pull them in for you. Local Retries object LocalRetryTests extends utest.TestSuite{   val flaky = new FlakyThing   def tests = this{     'hello - retry(3){       flaky.run     }   } } You can wrap individual tests, or even individual expressions, in a retry block to make them retry a number of times before failing. That is very useful for dealing with small points of flakiness within your test suite. A retry block simply retries its body up to the specified number of times; the first run that doesn't throw an exception returns the value returned by that run. You can also use Suite Retries if you want to configure retries more globally across your test suite. Configuring uTest uTest allows for some basic configuration: Output Formatting You can control how the output of tests gets printed via overriding methods on the TestSuite object: /**  * Override this to control how individual test results get pretty-printed   * while a test run is in progress; return `None` to print nothing   */ def formatSingle(path: Seq[String], r: Result): Option[String] /**  * Override this to control how the entire suite gets pretty-printed; return  * `None` to print nothing.  */ def format(results: Tree[Result]): Option[String]  /**  * Controls how many characters are printed for any test results before or   * the contents of variables in failed assertions before it gets truncated  */ def utestTruncateLength: Int /**  * Whether output gets printed in color  */* def formatColor: Boolean Note that you can override these on every TestSuite individually, to configure them differently for each set of tests if you want. If you want to share your configuration across every test, pull these into a custom MyTestSuite class extending TestSuite and have your test suites extend from MyTestSuite instead. Suite Retries You can mix in the TestSuite.Retries trait and define the utestRetryCount int to enable test-level retries for all tests within a suite: object SuiteRetryTests extends TestSuite with TestSuite.Retries{   override val utestRetryCount = 3   val flaky = new FlakyThing   def tests = this{     'hello{       flaky.run     }   } }  You can also use Local Retries if you want to only retry within specific tests or expressions instead of throughout the entire suite. Test Wrapping uTest exposes the utestWrap function that you can override on any test suite: def utestWrap(runBody: => concurrent.Future[Any])                (implicit ec: ExecutionContext): concurrent.Future[Any] This is a flexible function that wraps every test call; you can use it to perform initialization before evaluating runBody, or to perform cleanup after the completion of runBody via runBody.onComplete, or even to perform retries by executing runBody multiple times. If you want to perform messy before/after logic around every individual test, override utestWrap. runBody is a future to support asynchronous testing, which is the only way to test things like Ajax calls in Scala.js Per-Run Setup/Teardown If you need to perform some action (initialize a database, cleanup the filesystem, etc.) not just per-test but per-run, you can do that by defining a custom utest.runner.Framework and overriding the setup and teardown methods: class CustomFramework extends utest.runner.Framework{   override def setup() = {     println(""Setting up CustomFramework"")   }   override def teardown() = {     println(""Tearing down CustomFramework"")   } } And then telling SBT to run tests using the custom framework: testFrameworks += new TestFramework(""test.utest.CustomFramework""), This is handy for setup/teardown that is necessary but too expensive to do before/after every single test, which would be the case if you used Test Wrapping to do it. Execution Model val test = TestSuite{   var x = 0   'A{     x += 1     'X{       x += 2       assert(x == 3)     }     'Y{       x += 3       assert(x == 4)     }   }   'B{     x += 4     'Z{       x += 5       assert(x == 9)     }   } }  val results = test.run() println(results.leaves.count(_.value.isSuccess)) // 3 The example above demonstrates a subtlety of how uTest suites are run: despite all the tests being able to refer to the same lexically-scoped value x, each tests modifications to x happen entirely independently of the others, allowing all three of the leaf-tests to pass. This allows you to easily place re-usable fixtures anywhere convenient within the test tree. If you want to create a shared resource that is lazily initialized when needed in one of the tests and used throughout them, simply make it a lazy val outside the TestSuite{ ... } block. Test-discovery is done entirely at compile-time by the TestSuite{ ... } macro, and is independent of execution of the tests: val tests = TestSuite{   timesRun += 1   'A{     assert(false)     'B{       'C{         1       }     }   } }  // listing tests B and C works despite failure of A println(tests.toSeq.map(_.name)) // Seq(_, A, B, C) As you can see, even though the assert(false) comes before the declaration of tests B and C, these tests are still listable and inspectable because they were registered at compile time using the TestSuite{ ... } macro. Having a clean separation between test-discovery and test-execution is generally considered to be a good thing, and uTest's execution model strictly enforces this by doing test-discovery at compile-time. Thus, you can always inspect the test hierarchy without having to execute arbitrary test code. At the same time, uTest preserves all the convenience of sharing common setup code via lexical scoping, while avoiding the pitfall of shared-state between tests, giving you the best of both worlds in terms of convenience and isolation. SBT Command-line Interface sbt> test  Runs all tests in your project. Tests are defined in any class inheriting from TestSuite, e.g.: package HelloTests   object HelloTestsuite extends TestSuite{   val tests = this{     'hello{       'world{         val x = 1         val y = 2         assert(x != y)         (x, y)       }     }     'test2{       val a = 1       val b = 2       assert(a == b)     }   } } If you want to only run a particular test file:  sbt> test-only -- HelloTests.HelloTestsuite   Or even a particular test within a particular file sbt> test-only -- HelloTests.HelloTestsuite.hello sbt> test-only -- HelloTests.HelloTestsuite.hello.world sbt> test-only -- HelloTests.HelloTestsuite.test2  The total number of tests includes the non-leaf tests like hello and HelloTestsuite. Also, the tests which return a value (like world) have that value printed out inside the Success() tag: this is handy for doing a visual sanity-check at the end of the run to make sure the tests are doing what you think they are ScalaJS uTest is completely compatible with ScalaJS: the above sections on defining a test suite, asserts and the test-running API all work unchanged under ScalaJS, with minor differences: ScalaJS does not support parallelism, and as such only single-threaded ExecutionContexts like utest.ExecutionContext.runNow or scala.scalajs.concurrent.JSExecutionContext.runNow work. When run via SBT, --parallel has no effect. eventually and continually are not supported, as they rely on a blocking retry-loop whereas you can't block in ScalaJS Apart from these differences, there should be no problem compiling uTest TestSuites via ScalaJS and running them using Rhino or in the browser. Why uTest uTest began as an attempt to port ScalaTest and Specs2 to ScalaJS. After struggling with that, I realized that both ScalaTest and Specs2 were going to be difficult to port to ScalaJS for a few reasons: They have a large number of dependencies on JVM-specific things, such as Symbols or Reflection or ClassLoaders, which are not supported by ScalaJS They have huge codebases: 400,000 lines of code for ScalaTest and 50,000 lines for Specs2. This huge mass of code makes it difficult to pinpoint the parts which are incompatible with ScalaJS. If you don't believe that uTest is much smaller than the alternatives, let the jars speak for themselves: uTest tries to provide most of what you want as a developer, while leaving out all the unnecessary functionality that ScalaTest and Specs2 provide: Fluent English-like code: matchers like shouldBe or should not be or mustbe_== don't really add anything, and it doesn't really matter whether you name each test block using should, when, can, must, feature(""..."") or it should ""..."" These add nothing and clutter up the API and code base. You certainly don't need 8 different sets of them. Legacy code, like ScalaTests time package, now obsolete with the introduction of scala.concurrent.duration. Such a a rich command-line interface: with a simple API, any user who wants to do heavy customization of the test running can simply do it in code, and writing a small amount of Scala with a trivial command-line runner will likely be easier than wrestling with mountains of command-line configuration flags to try to make the runner do what you want. While improving on the basic things that matters Better macro-asserts which are both more-useful and more-simply-implemented than those provided by ScalaTest Compile-time test registration, which allows completely separating test-discovery and execution. A simpler, straightforward API that makes using uTest as a library much easier. Raw size: at less than 1000 lines of code, uTest is 1/400th the size of ScalaTest and 1/50th the size of Specs2, and with almost no dependencies. Its small size means that you can trivially use uTest as a library within a larger application without worrying about it significantly increasing the size of your packaged artifacts, or pulling in weird dependencies. Development Tips To run all the test on the entire matrix of Scala versions (2.10.4 and 2.11.0) and backends (JVM and JS), simply run: sbt +test  You can also use more targeted commands e.g. utestJS/test which would only re-test the Javascript backend under scala 2.10.4. To publish use sbt +publishSigned  Changelog 0.4.3 Generalize ==> asserts to work on Arrays Shiny new CI build and Gitter Chat 0.4.2 Move errors into utest.* from utest.framework.* Removed acyclic hack variable 0.4.1 Fix usage of by-name function calls within tests #55 0.4.0 foo: @Show annotation lets you tell uTest to print out arbitrary expressions within the test suite when things fail, in addition to the default of local variables You can use a ==> b to assert equality within a test suite, in a form that's pretty enough to use as documentation compileError now properly passes when an expression would fail to compile due to @compileTimeOnly annotations Configuring uTest has been overhauled. Scala 2.12.0-M3 support Fix some warnings appearing when the -Ydead-code flag is used Added TestPath implicit to make the path to the current test available for usage inside the test body or helpers. 0.3.1 Published for Scala.js 0.6.1 0.3.0 Published for Scala.js 0.6.0 Removed JsCrossBuild now that Scala.js supports cross-building via crossProject compileTimeOnly has been re-introduced, so invalid use of test DSL should fail with nice errors Removed --throw flag in favor of ""native"" SBT error reporting 0.2.4 Added support for asynchronous tests which return a Future. 0.2.3 Updated to work against ScalaJS 0.5.4 0.2.2 Introduced CompileError.check(pos: String, msgs: String*) to simplify the common pattern of checking that the error occurs in the right place and with the message you expect. Changed the file layout expected by JsCrossBuild, to expect the shared files to be in js/shared/ and jvm/shared/, rather than in shared/. This is typically achieved via symlinks, and should make the cross-build play much more nicely with IDEs. 0.2.1 Fix bug in utestJsSettings pulling in the wrong version of uTest 0.2.0 Introduced the compileError macro to allow testing of compilation errors. Stack traces are now only shown for the user code, with the uTest/SBT internal stack trace ignored, making them much less spammy and noisy. Introduced the * symbol, which can be used in place of a test name to get sequentially numbered test names. 0.1.9 ScalaJS version is now built against ScalaJS 0.5.3 Fixed linking errors in ScalaJS version, to allow proper operation of the new optimization 0.1.8 Fixed bug causing local-defs in assert macro to fail 0.1.7 Extracted out utestJvmSettings and utestJsSettings for use outside the JsCrossBuild plugin, for people who don't want to use the plugin. 0.1.6 Print paths of failing tests after completion to make C&P-ing re-runs more convenient Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/utest"	"A tiny, portable testing library for Scala."	"true"
"JSON"	"argonaut"	"http://argonaut.io/"	"Purely Functional JSON in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"375"	"33"	"78"	"GitHub - argonaut-io/argonaut: Purely functional JSON parser and library in scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 33 Star 375 Fork 78 argonaut-io/argonaut Code Issues 21 Pull requests 1 Wiki Pulse Graphs Purely functional JSON parser and library in scala. http://argonaut.io 873 commits 6 branches 29 releases Fetching contributors Scala 93.9% Shell 6.1% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master series/6.0.x series/6.1a.x series/6.1.x topic/auto-codec topic/jawn Nothing to show v6.2-M3 v6.2-M2 v6.2-M1 v6.1 v6.1a v6.1-M6 v6.1-M5 v6.1-M4 v6.1-M3 v6.1-M2 v6.1-M1 v6.0.4 v6.0.3 v6.0.2 v6.0.1 v6.0 v6.0-RC3 v6.0-RC2 v6.0-RC1 v6.0-M7 v6.0-M6 v6.0-M5 v6.0-M4 v6.0-M3 v6.0-M2 v6.0-M2-2.9.2 v6.0-M1 v6.0-M1-2.10.0 v6.0-M1-2.9.2 Nothing to show New pull request Latest commit 62d0bdb Jun 25, 2016 Sean Parsons Setting version to 6.2-SNAPSHOT Permalink Failed to load latest commit information. argonaut-benchmark/src/main Trim down the Scalaz tests massively and rig up dependencies between … Oct 6, 2015 argonaut-cats/src * update cats to version 0.4.0 and create a cats extension package to… Feb 3, 2016 argonaut-jawn/src Fix compilation failure in JawnParserSpecification. Mar 13, 2016 argonaut-monocle/src add JsonPath, jNullPrism and jDescendants Jun 9, 2016 argonaut-scalaz/src Add Traverse[DecodeResult] May 24, 2016 argonaut/src Merge pull request #240 from alexarchambault/master Jun 19, 2016 notes Fix typo Apr 1, 2016 project Bumped the release plugin and set it up correctly. Jun 25, 2016 .gitignore Merge commit '1dfd69cc7120d79e429d3d508d51dcbb28c9a84c' into topic/jn… Dec 5, 2014 .travis.yml update dependencies Apr 26, 2016 CONTRIBUTORS Clean-up licensing. Dec 17, 2012 COPYING fix url Feb 23, 2014 LICENCE licence in original form. May 2, 2013 LICENCE.ephox licence in original form. May 2, 2013 README.md Update README.md Mar 18, 2016 sbt Move all the things around. Sep 17, 2015 version.sbt Setting version to 6.2-SNAPSHOT Jun 25, 2016 README.md Argonaut What is Argonaut? Argonaut is a JSON library for Scala, providing a rich library for parsing, printing and manipulation as well as convenient codecs for translation to and from scala data types. Argonaut is licenced under BSD3 (see LICENCE). See more at http://argonaut.io. Documentation User Docs Scala Docs Examples SBT Settings Just add argonaut as a dependency. Stable: ""io.argonaut"" %% ""argonaut"" % ""6.1""  Latest: ""io.argonaut"" %% ""argonaut"" % ""6.2-SNAPSHOT"" changing() ""io.argonaut"" %% ""argonaut-scalaz"" % ""6.2-SNAPSHOT"" changing() ""io.argonaut"" %% ""argonaut-monocle"" % ""6.2-SNAPSHOT"" changing()  Note that the 6.1.x release supports scala 2.10.* and 2.11.* with scalaz 7.1.*. Note that the 6.2 development stream supports scala 2.10.* and 2.11.* with scalaz 7.2.*. Release Add to ~/.sbt/0.13/sonatype.sbt credentials += Credentials(""Sonatype Nexus Repository Manager"",                            ""oss.sonatype.org"",                            ""<username>"",                            ""<password>"")  For a snapshot build run: ./sbt +publish For a release build run: ./sbt ""release cross""  Note for a release build you will want to enter the details for the release build number and then the subsequent build number. At this step it is fine to enter the original build number as the next number (for example when doing Milestone or RC builds). As an example: Release version [6.0] : 6.0-M3 Next version [6.1-SNAPSHOT] : 6.0-SNAPSHOT  Provenance Argonaut was initially developed to support products at Ephox. The library was open-sourced under a BSD License, drawing users, support and improvements from a number of contributors. The initial developers have since left the employment of Ephox and now maintain this fork argonaut-io/argonaut. It is expected that major releases will now come from this repository. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/argonaut-io/argonaut"	"Purely Functional JSON in Scala."	"true"
"JSON"	"circe ★ 402 ⧗ 0"	"https://github.com/travisbrown/circe"	"JSON library based on Argonaut, depends on Cats"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"494"	"40"	"76"	"GitHub - travisbrown/circe: Yet another JSON library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 40 Star 494 Fork 76 travisbrown/circe Code Issues 28 Pull requests 7 Pulse Graphs Yet another JSON library for Scala http://circe.io 630 commits 30 branches 12 releases 27 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags backport/0.1.2/66 backport/0.2.1/110 backport/0.4.1/273 fix/65 fix/217 gh-pages master optimization/seq-decoder scalajs starter/320 topic/abstract-decoder topic/accumulating-decoders topic/async topic/cats-0.4.0 topic/configuring-generic-derivation topic/contributing topic/cursor-optimization topic/derivation-overhaul topic/export-hook-1.0.2 topic/fix-readme topic/fix-travis-ci topic/gd2-tests topic/hook-less topic/js-printing topic/more-memory topic/printer-fix topic/simplify-printing topic/sketchy-optimization topic/spray-results update/cats-0.6.1 Nothing to show v0.5.0-M2 v0.5.0-M1 v0.4.1 v0.4.0 v0.4.0-RC2 v0.4.0-RC1 v0.3.0 v0.2.1 v0.2.0 v0.1.2 v0.1.1 v0.1.0 Nothing to show New pull request Latest commit c934acf Jul 14, 2016 travisbrown committed on GitHub Merge pull request #327 from travisbrown/update/cats-0.6.1 … Update Cats to 0.6.1 Permalink Failed to load latest commit information. benchmark/src Update to Cats 0.6.0 May 19, 2016 core/shared/src/main/scala/io/circe Avoid boxing on Double operations Jun 20, 2016 examples Update example versions and use circe-java8 Apr 23, 2016 generic/shared/src/main/scala/io/circe/generic Fix style error May 30, 2016 hygiene/src/main/scala/io/circe/hygiene Add hygiene compilation test sub-project Jun 19, 2016 jackson/src/main/scala/io/circe/jackson Remove unused imports Mar 11, 2016 java8/src Update to Cats 0.6.0 May 19, 2016 jawn/src/main/scala/io/circe/jawn Replace and deprecate Json methods Feb 25, 2016 literal/src/main/scala/io/circe/literal Remove unused imports Mar 11, 2016 numbers/shared/src Improve numbers test coverage a bit Apr 30, 2016 optics/src update JsonPath methods as per discussion Jun 8, 2016 parser Rearrangements and other changes Feb 13, 2016 project Update Scala.js and other versions Jun 17, 2016 refined/shared/src/main/scala/io/circe Move to FlatSpec + GeneratorDrivenPropertyChecks for tests Apr 28, 2016 scalajs/src/main/scala/io/circe/scalajs Remove unused imports Mar 11, 2016 spray/src Move to FlatSpec + GeneratorDrivenPropertyChecks for tests Apr 28, 2016 streaming/src/main/scala/io/circe/streaming Minor improvements to streaming Mar 19, 2016 tests Fix emapTry test Jun 19, 2016 .gitignore New approach to generic codec derivation Aug 9, 2015 .jvmopts Add Scala.js support Aug 13, 2015 .travis.yml Fix hygiene in Travis CI Jun 19, 2016 CONTRIBUTING.md Fix typo Apr 7, 2016 DESIGN.md Minor formatting change Dec 22, 2015 LICENSE Initial sketch Aug 1, 2015 LICENSE.argonaut Initial sketch Aug 1, 2015 LICENSE.ephox Initial sketch Aug 1, 2015 NOTICE Rename jfc to circe Aug 9, 2015 README.md add Reonomy to adopters list Jul 8, 2016 build.sbt Update Cats to 0.6.1 Jul 14, 2016 scalastyle-config.xml Extend allowed line length from 100 to 120 characters Feb 25, 2016 version.sbt Setting version to 0.5.0-SNAPSHOT Jun 19, 2016 README.md circe circe (pronounced SUR-see, or KEER-kee in classical Greek, or CHEER-chay in Ecclesiastical Latin) is a JSON library for Scala (and Scala.js). The rest of this page tries to give some justification for its existence. There are also API docs. circe's working title was jfc, which stood for ""JSON for cats"". The name was changed for a number of reasons. Table of contents Quick start Community Related projects Examples Adopters Why? Dependencies and modularity Parsing Lenses Codec derivation Aliases Documentation Testing Performance Usage Encoding and decoding Transforming JSON Contributors and participation Warnings and known issues License Quick start circe is published to Maven Central and cross-built for Scala 2.10 and 2.11, so you can just add the following to your build: val circeVersion = ""0.4.1""  libraryDependencies ++= Seq(   ""io.circe"" %% ""circe-core"",   ""io.circe"" %% ""circe-generic"",   ""io.circe"" %% ""circe-parser"" ).map(_ % circeVersion) If you are using circe's generic derivation (with Scala 2.10), or the macro annotation @JsonCodec (with Scala 2.10 or Scala 2.11), you'll also need to include the MacroParadise compiler plugin in your build: addCompilerPlugin(   ""org.scalamacros"" % ""paradise"" % ""2.1.0"" cross CrossVersion.full ) Then type sbt console to start a REPL and then paste the following (this will also work from the root directory of this repository): scala> import io.circe._, io.circe.generic.auto._, io.circe.parser._, io.circe.syntax._ import io.circe._ import io.circe.generic.auto._ import io.circe.parser._ import io.circe.syntax._  scala> sealed trait Foo defined trait Foo  scala> case class Bar(xs: List[String]) extends Foo defined class Bar  scala> case class Qux(i: Int, d: Option[Double]) extends Foo defined class Qux  scala> val foo: Foo = Qux(13, Some(14.0)) foo: Foo = Qux(13,Some(14.0))  scala> foo.asJson.noSpaces res0: String = {""Qux"":{""d"":14.0,""i"":13}}  scala> decode[Foo](foo.asJson.spaces4) res1: cats.data.Xor[io.circe.Error,Foo] = Right(Qux(13,Some(14.0))) No boilerplate, no runtime reflection. Community Related projects The following open source projects are either built on circe or provide circe support: Actor Messenger: A platform for instant messaging. akka-http-json: A library that supports using circe for JSON marshalling and unmarshalling in Akka HTTP. circe-yaml: A library that uses SnakeYAML to support parsing YAML 1.1 into circe's Json. cornichon: A DSL for JSON API testing. Cosmos: An API for DCOS services that uses circe. Enumeratum: Enumerations for Scala with circe integration. Featherbed: A REST client library with circe support. Finch: A library for building web services with circe support. fintrospect: HTTP contracts for Finagle with circe support. fluflu: A Fluentd logger. Github4s: A GitHub API wrapper written in Scala. content-api-models: The Guardian's Content API Thrift models. http4s: A purely functional HTTP library for client and server applications. Iglu Schema Repository: A JSON Schema repository with circe support. jsactor: An actor library for Scala.js with circe support. jwt-circe: A JSON Web Token implementation with circe support. kadai-log: A logging library with circe support. msgpack4z-circe: A MessagePack implementation with circe support. play-circe: circe support for Play!. Rapture: Support for using circe's parsing and AST in Rapture JSON. roc: A PostgreSQL client built on Finagle. sangria-circe: circe marshalling for Sangria, a GraphQL implementation. scalist: A Todoist API client. Slick-pg: Slick extensions for PostgreSQL. telepooz: A Scala wrapper for the Telegram Bot API built on circe. Zenith: Functional HTTP library built on Unfiltered and circe. Examples The following projects provide examples, templates, or benchmarks that include circe: https://github.com/alanphillips78/akka-http-microservice-blueprint https://github.com/bneil/fcs_boilerplate https://github.com/gvolpe/simple-http4s-api https://github.com/notvitor/akka-http-circe-json-template https://github.com/stephennancekivell/some-jmh-json-benchmarks-circe-jackson Adopters Are you using circe? Please consider opening a pull request to list your organization here: Reonomy SoundCloud (transforming 200,000,000 JSON events every hour in MapReduce ETLs) TabMo (parsing more than 100k events per second with Akka Stream and Spark) Why? Argonaut is a great library. It's by far the best JSON library for Scala, and the best JSON library on the JVM. If you're doing anything with JSON in Scala, you should be using Argonaut. circe is a fork of Argonaut with a few important differences. Dependencies and modularity circe depends on cats instead of Scalaz, and the core project has only one dependency (cats-core). Other subprojects bring in dependencies on Jawn (for parsing in the jawn subproject), Shapeless (for automatic codec derivation in generic), and Twitter Util (for tools for asynchronous parsing in async), but it would be possible to replace the functionality provided by these subprojects with alternative implementations that use other libraries. Parsing circe doesn't include a JSON parser in the core project, which is focused on the JSON AST, zippers, and codecs. The jawn subproject provides support for parsing JSON via a Jawn facade. Jawn is fast, it offers asynchronous parsing, and best of all it lets us drop a lot of the fussiest code in Argonaut. The jackson subproject supports using Jackson for both parsing and printing. circe also provides a parser subproject that provides parsing support for Scala.js, with JVM parsing provided by io.circe.jawn and JavaScript parsing from scalajs.js.JSON. Lenses circe doesn't use or provide lenses in the core project. This is related to the first point above, since Monocle has a Scalaz dependency, but we also feel that it simplifies the API. The 0.3.0 release added an experimental optics subproject that provides Monocle lenses (note that this will require your project to depend on both Scalaz and cats). Codec derivation circe does not use macros or provide any kind of automatic derivation in the core project. Instead of Argonaut's limited macro-based derivation (which does not support sealed trait hierarchies, for example), circe includes a subproject (generic) that provides generic codec derivation using Shapeless. This subproject is currently a simplified port of argonaut-shapeless that provides fully automatic derivation of instances for case classes and sealed trait hierarchies. It also includes derivation of ""incomplete"" case class instances (see my recent blog post for details). Aliases circe aims to simplify Argonaut's API by removing all operator aliases. This is largely a matter of personal taste, and may change in the future. Documentation The Argonaut documentation is good, but it could be better: to take just one example, it can be hard to tell at a glance why there are three different Cursor, HCursor, and ACursor types. In this particular case, circe introduces an abstraction over cursors that makes the relationship clearer and allows these three types to share API documentation. Testing I'd like to provide more complete test coverage (in part via Discipline), but it's early days for this. Performance circe aims to be more focused on performance. I'm still experimenting with the right balance, but I'm open to using mutability, inheritance, and all kinds of other horrible things under the hood if they make circe faster (the public API does not and will never expose any of this, though). My initial benchmarks suggest this is at least kind of working (higher numbers are better): Benchmark                          Mode  Cnt        Score       Error  Units DecodingBenchmark.decodeFoosC     thrpt   40     3711.680 ±    22.766  ops/s DecodingBenchmark.decodeFoosA     thrpt   40     1519.045 ±    11.373  ops/s DecodingBenchmark.decodeFoosP     thrpt   40     2032.834 ±    27.033  ops/s DecodingBenchmark.decodeFoosPico  thrpt   40     2003.106 ±    10.463  ops/s DecodingBenchmark.decodeFoosS     thrpt   40     7053.699 ±    35.127  ops/s  DecodingBenchmark.decodeIntsC     thrpt   40    19101.875 ±   324.123  ops/s DecodingBenchmark.decodeIntsA     thrpt   40     8000.093 ±   215.702  ops/s DecodingBenchmark.decodeIntsP     thrpt   40    18160.031 ±    68.777  ops/s DecodingBenchmark.decodeIntsPico  thrpt   40    11979.085 ±    89.793  ops/s DecodingBenchmark.decodeIntsS     thrpt   40    81279.228 ±  1203.751  ops/s  EncodingBenchmark.encodeFoosC     thrpt   40     7353.158 ±   133.633  ops/s EncodingBenchmark.encodeFoosA     thrpt   40     5638.358 ±    30.315  ops/s EncodingBenchmark.encodeFoosP     thrpt   40     2324.075 ±    17.868  ops/s EncodingBenchmark.encodeFoosPico  thrpt   40     5056.317 ±    45.876  ops/s EncodingBenchmark.encodeFoosS     thrpt   40     5307.422 ±    29.666  ops/s  EncodingBenchmark.encodeIntsC     thrpt   40   117885.093 ±  2151.059  ops/s EncodingBenchmark.encodeIntsA     thrpt   40    72986.276 ±  1561.295  ops/s EncodingBenchmark.encodeIntsP     thrpt   40    55117.582 ±   650.154  ops/s EncodingBenchmark.encodeIntsPico  thrpt   40    31602.757 ±   351.578  ops/s EncodingBenchmark.encodeIntsS     thrpt   40    40509.667 ±   560.439  ops/s  ParsingBenchmark.parseFoosC       thrpt   40     2869.779 ±    61.898  ops/s ParsingBenchmark.parseFoosA       thrpt   40     2615.299 ±    25.881  ops/s ParsingBenchmark.parseFoosP       thrpt   40     1970.493 ±    90.383  ops/s ParsingBenchmark.parseFoosPico    thrpt   40     3113.232 ±    29.081  ops/s ParsingBenchmark.parseFoosS       thrpt   40     3725.056 ±    68.794  ops/s  ParsingBenchmark.parseIntsC       thrpt   40    13062.151 ±   209.713  ops/s ParsingBenchmark.parseIntsA       thrpt   40    11066.850 ±   159.308  ops/s ParsingBenchmark.parseIntsP       thrpt   40    18980.265 ±    91.351  ops/s ParsingBenchmark.parseIntsPico    thrpt   40    15184.314 ±    37.808  ops/s ParsingBenchmark.parseIntsS       thrpt   40    15495.935 ±   388.922  ops/s  PrintingBenchmark.printFoosC      thrpt   40     4090.218 ±    38.804  ops/s PrintingBenchmark.printFoosA      thrpt   40     2863.570 ±    19.091  ops/s PrintingBenchmark.printFoosP      thrpt   40     9042.816 ±    49.199  ops/s PrintingBenchmark.printFoosPico   thrpt   40     4759.601 ±    20.467  ops/s PrintingBenchmark.printFoosS      thrpt   40     7297.047 ±    28.168  ops/s  PrintingBenchmark.printIntsC      thrpt   40    24596.715 ±    66.366  ops/s PrintingBenchmark.printIntsA      thrpt   40    15611.121 ±   140.017  ops/s PrintingBenchmark.printIntsP      thrpt   40    66283.874 ±   731.534  ops/s PrintingBenchmark.printIntsPico   thrpt   40    23703.796 ±   188.186  ops/s PrintingBenchmark.printIntsS      thrpt   40    53015.753 ±   462.472  ops/s  And allocation rates (lower is better): Benchmark                                              Mode  Cnt        Score        Error   Units  DecodingBenchmark.decodeFoosC:gc.alloc.rate.norm      thrpt   20  1308424.455 ±      0.881    B/op DecodingBenchmark.decodeFoosA:gc.alloc.rate.norm      thrpt   20  3779097.640 ±      2.456    B/op DecodingBenchmark.decodeFoosP:gc.alloc.rate.norm      thrpt   20  2201336.820 ±      1.588    B/op DecodingBenchmark.decodeFoosPico:gc.alloc.rate.norm   thrpt   20   506696.832 ±      1.608    B/op DecodingBenchmark.decodeFoosS:gc.alloc.rate.norm      thrpt   20   273184.238 ±      0.458    B/op  DecodingBenchmark.decodeIntsC:gc.alloc.rate.norm      thrpt   20   291360.090 ±      0.174    B/op DecodingBenchmark.decodeIntsA:gc.alloc.rate.norm      thrpt   20   655448.200 ±      0.387    B/op DecodingBenchmark.decodeIntsP:gc.alloc.rate.norm      thrpt   20   369144.097 ±      0.189    B/op DecodingBenchmark.decodeIntsPico:gc.alloc.rate.norm   thrpt   20   235400.144 ±      0.280    B/op DecodingBenchmark.decodeIntsS:gc.alloc.rate.norm      thrpt   20    38136.021 ±      0.041    B/op  EncodingBenchmark.encodeFoosC:gc.alloc.rate.norm      thrpt   20   395272.225 ±      0.433    B/op EncodingBenchmark.encodeFoosA:gc.alloc.rate.norm      thrpt   20   521136.306 ±      0.595    B/op EncodingBenchmark.encodeFoosP:gc.alloc.rate.norm      thrpt   20  1367800.719 ±      7.263    B/op EncodingBenchmark.encodeFoosPico:gc.alloc.rate.norm   thrpt   20   281992.346 ±      0.674    B/op EncodingBenchmark.encodeFoosS:gc.alloc.rate.norm      thrpt   20   377856.318 ±      0.615    B/op  EncodingBenchmark.encodeIntsC:gc.alloc.rate.norm      thrpt   20    64160.016 ±      7.129    B/op EncodingBenchmark.encodeIntsA:gc.alloc.rate.norm      thrpt   20    80152.023 ±      0.044    B/op EncodingBenchmark.encodeIntsP:gc.alloc.rate.norm      thrpt   20    71352.030 ±      0.058    B/op EncodingBenchmark.encodeIntsPico:gc.alloc.rate.norm   thrpt   20    58992.057 ±      0.115    B/op EncodingBenchmark.encodeIntsS:gc.alloc.rate.norm      thrpt   20    76176.042 ±      0.081    B/op  ParsingBenchmark.parseFoosC:gc.alloc.rate.norm        thrpt   20   765800.586 ±      1.133    B/op ParsingBenchmark.parseFoosA:gc.alloc.rate.norm        thrpt   20  1488760.635 ±      1.228    B/op ParsingBenchmark.parseFoosP:gc.alloc.rate.norm        thrpt   20   987720.805 ±      1.551    B/op ParsingBenchmark.parseFoosPico:gc.alloc.rate.norm     thrpt   20   639464.525 ±      1.014    B/op ParsingBenchmark.parseFoosS:gc.alloc.rate.norm        thrpt   20   252256.440 ±      0.838    B/op  ParsingBenchmark.parseIntsC:gc.alloc.rate.norm        thrpt   20   121272.129 ±      0.250    B/op ParsingBenchmark.parseIntsA:gc.alloc.rate.norm        thrpt   20   310280.151 ±      0.289    B/op ParsingBenchmark.parseIntsP:gc.alloc.rate.norm        thrpt   20   216448.089 ±      0.171    B/op ParsingBenchmark.parseIntsPico:gc.alloc.rate.norm     thrpt   20   141808.118 ±      0.239    B/op ParsingBenchmark.parseIntsS:gc.alloc.rate.norm        thrpt   20   109000.117 ±      0.229    B/op  PrintingBenchmark.printFoosC:gc.alloc.rate.norm       thrpt   20   425240.419 ±      0.810    B/op PrintingBenchmark.printFoosA:gc.alloc.rate.norm       thrpt   20   621288.585 ±   1069.068    B/op PrintingBenchmark.printFoosP:gc.alloc.rate.norm       thrpt   20   351360.184 ±      0.356    B/op PrintingBenchmark.printFoosPico:gc.alloc.rate.norm    thrpt   20   431268.348 ±   1058.404    B/op PrintingBenchmark.printFoosS:gc.alloc.rate.norm       thrpt   20   372992.228 ±      0.442    B/op  PrintingBenchmark.printIntsC:gc.alloc.rate.norm       thrpt   20    74464.067 ±      7.127    B/op PrintingBenchmark.printIntsA:gc.alloc.rate.norm       thrpt   20   239712.107 ±      0.206    B/op PrintingBenchmark.printIntsP:gc.alloc.rate.norm       thrpt   20    24144.025 ±      0.048    B/op PrintingBenchmark.printIntsPico:gc.alloc.rate.norm    thrpt   20    95472.072 ±      0.140    B/op PrintingBenchmark.printIntsS:gc.alloc.rate.norm       thrpt   20    24048.032 ±      0.062    B/op  The Foos benchmarks work with a map containing case class values, and the Ints ones are an array of integers. C suffixes indicate circe's throughput, A is for Argonaut, P is for play-json, Pico is for picopickle, and S is for spray-json. Note that spray-json's approach to failure handling is different from the approaches of the other libraries listed here (it simply throws exceptions), and this difference should be taken into account when comparing its results with the others. Usage This section needs a lot of expanding. Encoding and decoding circe uses Encoder and Decoder type classes for encoding and decoding. An Encoder[A] instance provides a function that will convert any A to a JSON, and a Decoder[A] takes a Json value to either an exception or an A. circe provides implicit instances of these type classes for many types from the Scala standard library, including Int, String, and others. It also provides instances for List[A], Option[A], and other generic types, but only if A has an Encoder instance. Transforming JSON Suppose we have the following JSON document: import io.circe._, io.circe.generic.auto._, io.circe.jawn._, io.circe.syntax._ import cats.data.Xor  val json: String = """"""   {     ""id"": ""c730433b-082c-4984-9d66-855c243266f0"",     ""name"": ""Foo"",     ""counts"": [1, 2, 3],     ""values"": {       ""bar"": true,       ""baz"": 100.001,       ""qux"": [""a"", ""b""]     }   } """"""  val doc: Json = parse(json).getOrElse(Json.Null) In order to transform this document we need to create an HCursor with the focus at the document's root: val cursor: HCursor = doc.hcursor We can then use various operations to move the focus of the cursor around the document and to ""modify"" the current focus: val reversedNameCursor: ACursor =   cursor.downField(""name"").withFocus(_.mapString(_.reverse)) We can then return to the root of the document and return its value with top: val reversedName: Option[Json] = reversedNameCursor.top The result will contain the original document with the ""name"" field reversed. Contributors and participation circe is a fork of Argonaut, and if you find it at all useful, you should thank Mark Hibberd, Tony Morris, Kenji Yoshida, and the rest of the Argonaut contributors. circe is currently maintained by Travis Brown, Alexandre Archambault, and Vladimir Kostyukov. After the 0.4.0 release, all pull requests will require two sign-offs by a maintainer to be merged. The circe project supports the Typelevel code of conduct and wants all of its channels (Gitter, GitHub, etc.) to be welcoming environments for everyone. Please see the contributors' guide for details on how to submit a pull request. Warnings and known issues Please note that generic derivation will not work on Scala 2.10 unless you've added the Macro Paradise plugin to your build. See the quick start section above for details. Generic derivation may not work as expected when the type definitions that you're trying to derive instances for are at the same level as the attempted derivation. For example: scala> import io.circe.Decoder, io.circe.generic.auto._ import io.circe.Decoder import io.circe.generic.auto._  scala> sealed trait A; case object B extends A; object X { val d = Decoder[A] } defined trait A defined object B defined object X  scala> object X { sealed trait A; case object B extends A; val d = Decoder[A] } <console>:19: error: could not find implicit value for parameter d: io.circe.Decoder[X.A]    object X { sealed trait A; case object B extends A; val d = Decoder[A] }                                                                       ^ This is unfortunately a limitation of the macro API that Shapeless uses to derive the generic representation of the sealed trait. You can manually define these instances, or you can arrange the sealed trait definition so that it is not in the same immediate scope as the attempted derivation (which is typically what you want, anyway). For large or deeply-nested case classes and sealed trait hierarchies, the generic derivation provided by the generic subproject may stack overflow during compilation, which will result in the derived encoders or decoders simply not being found. Increasing the stack size available to the compiler (e.g. with sbt -J-Xss64m if you're using SBT) will help in many cases, but we have at least one report of a case where it doesn't. More generally, the generic derivation provided by the generic subproject works for a wide range of test cases, and is likely to just work for you, but it relies on macros (provided by Shapeless) that rely on compiler functionality that is not always perfectly robust (""SI-7046 is like playing roulette""), and if you're running into problems, it's likely that they're not your fault. Please file an issue here or ask a question on the Gitter channel, and we'll do our best to figure out whether the problem is something we can fix. When using the io.circe.generic.JsonCodec annotation, the following will not compile: import io.circe.generic.JsonCodec  @JsonCodec sealed trait A case class B(b: String) extends A case class C(c: Int) extends A In cases like this it's necessary to define a companion object for the root type after all of the leaf types: import io.circe.generic.JsonCodec  @JsonCodec sealed trait A case class B(b: String) extends A case class C(c: Int) extends A  object A See this issue for additional discussion (this workaround may not be necessary in future versions). circe's representation of numbers is designed not to lose precision during decoding into integral or arbitrary-precision types, but precision may still be lost during parsing. This shouldn't happen when using Jawn for parsing, but scalajs.js.JSON parses JSON numbers into a floating point representation that may lose precision (even when decoding into a type like BigDecimal; see this issue for an example). License circe is licensed under the Apache License, Version 2.0 (the ""License""); you may not use this software except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/travisbrown/circe"	"JSON library based on Argonaut, depends on Cats"	"true"
"JSON"	"jackson-module-scala ★ 255 ⧗ 2"	"https://github.com/FasterXML/jackson-module-scala"	"Add-on module for Jackson to support Scala-specific datatypes."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"273"	"23"	"75"	"GitHub - FasterXML/jackson-module-scala: Add-on module for Jackson (http://wiki.fasterxml.com/JacksonHome) to support Scala-specific datatypes Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 23 Star 273 Fork 75 FasterXML/jackson-module-scala Code Issues 49 Pull requests 5 Wiki Pulse Graphs Add-on module for Jackson (http://wiki.fasterxml.com/JacksonHome) to support Scala-specific datatypes http://wiki.fasterxml.com/JacksonModuleScala 611 commits 14 branches 59 releases 26 contributors Scala 98.6% Java 1.3% Ruby 0.1% Scala Java Ruby Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 gh-pages master multi-project proguard rc4-release Nothing to show jackson-module-scala-2.8.0.rc2 jackson-module-scala-2.8.0.rc1 jackson-module-scala-2.7.5 jackson-module-scala-2.7.4 jackson-module-scala-2.7.3 jackson-module-scala-2.7.2 jackson-module-scala-2.6.7 jackson-module-scala-2.6.6 jackson-module-scala-2.6.5 jackson-module-scala-2.6.4 jackson-module-scala-2.6.3 jackson-module-scala-2.6.2 jackson-module-scala-2.6.1 jackson-module-scala-2.6.0-rc4 jackson-module-scala-2.6.0-rc2 jackson-module-scala-2.6.0-1 jackson-module-scala-2.5.3 jackson-module-scala-2.5.2 jackson-module-scala-2.5.1 jackson-module-scala-2.5.0 jackson-module-scala-2.4.5 jackson-module-scala-2.4.5-1 jackson-module-scala-2.4.4 jackson-module-scala-2.4.3 jackson-module-scala-2.4.2 jackson-module-scala-2.4.1 jackson-module-scala-2.4.0 jackson-module-scala-2.4.0-rc2 jackson-module-scala-2.4.0-rc1 jackson-module-scala-2.3.5 jackson-module-scala-2.3.4 jackson-module-scala-2.3.3 jackson-module-scala-2.3.2 jackson-module-scala-2.3.1 jackson-module-scala-2.3.0 jackson-module-scala-2.3.0-rc1 jackson-module-scala-2.2.3 jackson-module-scala-2.2.2 jackson-module-scala-2.2.1 jackson-module-scala-2.2.0 jackson-module-scala-2.2.0-rc1 jackson-module-scala-2.1.5 jackson-module-scala-2.1.4 jackson-module-scala-2.1.3 jackson-module-scala-2.1.2 jackson-module-scala-2.1.1 jackson-module-scala-2.1.0 jackson-module-scala-2.0.4 jackson-module-scala-2.0.3 jackson-module-scala-2.0.2 jackson-module-scala-2.0.0 jackson-module-scala-2.0.0-RC2 jackson-module-scala-1.9.3 jackson-module-scala-1.9.2 jackson-module-scala-1.9.1 jackson-module-scala-1.9.0 jackson-module-scala-0.5.0 fork_scala-2.10.0-M7 fork_before_scala-2.10.0_changes Nothing to show New pull request Latest commit 1fa6df8 Jul 15, 2016 christophercurrie Update to 2.8.0 Permalink Failed to load latest commit information. DEV Added CLA for dave, to give him committor rights & get things moving … Feb 8, 2011 notes Prepare for release Dec 1, 2014 project Upgrade to sbt-release 1.0.3 Jul 15, 2016 release-notes Update release notes Apr 30, 2013 src Fix rc-enabled version regex Jul 15, 2016 .gitignore Restore openjdk6 testing Apr 4, 2015 .travis.yml Java 7 bootclasspath Jul 15, 2016 Berksfile Simplify AnnotationIntrospector Jul 19, 2015 Berksfile.lock Simplify AnnotationIntrospector Jul 19, 2015 README.md Update readme to point to latest released version. Mar 1, 2016 Vagrantfile Simplify AnnotationIntrospector Jul 19, 2015 build.sbt Update to 2.8.0 Jul 15, 2016 release.sbt Upgrade to sbt-release 1.0.3 Jul 15, 2016 version.sbt Setting version to 2.8.0-SNAPSHOT Jul 15, 2016 README.md IMPORTANT Please use versions 2.6.5, or 2.7.2; earlier released versions are known to contain significant bugs. Overview Jackson is a fast JSON processor for Java that supports three models: streaming, node, and object mapping (akin to the three independent models SAX, DOM, and JAXB in XML). The object mapping model is a high-level processing model that allows the user to project JSON data onto a domain-specific data model appropriate for their application, without having to deal with the low-level mechanics of JSON parsing. It is the standard object mapping parser implementaton in Jersey, the reference implementation for JSR-311 (Java API for Restful Web Services). Scala is a functional programming language for the JVM that supports Java interoperability. Its standard library is quite distinct from Java, and does not fulfill the expectations of Jacksons default mappings. Notably, Scala collections do not derive from java.util.Collection or its subclasses, and Scala properties do not (by default) look like Java Bean properties. The Scala Module supports serialization and limited deserialization of Scala Case Classes, Sequences, Maps, Tuples, Options, and Enumerations. Caveats Support for class constructor arguments currently depends upon Paranamer, specifically an implementation that depends upon constructor parameter names being present in the class debug information. Since this is the default in Scala, it is usually not an issue, but since it's possible to turn this off, be aware that the current version will throw an exception if it cannot find the constructor parameter names. Future versions may permit configuration to suppress this exception. Usage To use the Scala Module in Jackson, simply register it with the ObjectMapper instance: val mapper = new ObjectMapper() mapper.registerModule(DefaultScalaModule) DefaultScalaModule is a Scala object that includes support for all currently supported Scala data types. If only partial support is desired, the component traits can be included individually: val module = new OptionModule with TupleModule {} val mapper = new ObjectMapper() mapper.registerModule(module) You can also mixin ScalaObjectMapper (experimental) to get rich wrappers that automatically convert scala manifests directly into TypeReferences for Jackson to use: val mapper = new ObjectMapper() with ScalaObjectMapper mapper.registerModule(DefaultScalaModule) val myMap = mapper.readValue[Map[String,Tuple2[Int,Int]]](src) Consult the Scaladoc for further details. Building The master branch often depends on SNAPSHOT versions of the core Jackson projects, which are published to the Sonatype OSS Repository. To make these dependencies available, create a file called sonatype.sbt in the same directory as build.sbt with the following content. The project .gitignore file intentionally prevents this file from being checked in. resolvers += ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots"" Download, docs Check out Wiki. API Scaladocs can be found on the project site but they are not really well suited to end users, as most classes are implementation details of the module. Acknowledgements Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/FasterXML/jackson-module-scala"	"Add-on module for Jackson to support Scala-specific datatypes."	"true"
"JSON"	"jawn ★ 191 ⧗ 2"	"https://github.com/non/jawn"	"Fast json parser (According to them, competetive with java gson/jackson speed)."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"205"	"17"	"23"	"GitHub - non/jawn: Jawn is for parsing jay-sawn (JSON) Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 17 Star 205 Fork 23 non/jawn Code Issues 10 Pull requests 3 Pulse Graphs Jawn is for parsing jay-sawn (JSON) 186 commits 7 branches 15 releases 14 contributors Scala 96.5% Python 3.5% Scala Python Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.9 bug/equality-issue bug/fix-number-parsing bug/incomplete-async-fix master topic/ast-spring-cleaning topic/facade Nothing to show v0.8.4 v0.8.3 v0.8.2 v0.8.1 v0.8.0 v0.7.4 v0.7.2 v0.7.1 v0.7.0 v0.6.0 v0.5.5 v0.5.4 v0.5.3 v0.5.1 v0.5.0 Nothing to show New pull request Latest commit 5f03ff5 Mar 6, 2016 non Merge pull request #51 from ceedubs/patch-1 … Update readme version numbers to 0.8.4 Permalink Failed to load latest commit information. ast Actually fix the 2.10 test error issue. May 26, 2015 benchmark Major updates to Jawn. May 26, 2015 parser Tighten up number parsing to forbid invalid input. Feb 19, 2016 project Release to Sonatype, standardize artifacts. Jul 18, 2015 support Fix bug where an Integer to long to be represented by a Long would th… Oct 29, 2015 .gitignore initial commit Sep 29, 2012 .travis.yml Fix typo in .travis.yml. Feb 17, 2016 README.md Update readme version numbers to 0.8.4 Feb 29, 2016 build.sbt Require JMH only in benchmark. Jul 28, 2015 randjson.py basic benchmarking is live Sep 30, 2012 randjson2.py add 2nd random json generator Feb 7, 2014 version.sbt Setting version to 0.8.5-SNAPSHOT Feb 19, 2016 README.md Jawn ""Jawn is for parsing jay-sawn."" Origin The term ""jawn"" comes from the Philadelphia area. It conveys about as much information as ""thing"" does. I chose the name because I had moved to Montreal so I was remembering Philly fondly. Also, there isn't a better way to describe objects encoded in JSON than ""things"". Finally, we get a catchy slogan. Jawn was designed to parse JSON into an AST as quickly as possible. Overview Jawn consists of three parts: A fast, generic JSON parser A small, somewhat anemic AST Support packages which parse to third-party ASTs Currently Jawn is competitive with the fastest Java JSON libraries (GSON and Jackson) and in the author's benchmarks it often wins. It seems to be faster than any other Scala parser that exists (as of July 2014). Given the plethora of really nice JSON libraries for Scala, the expectation is that you are here for (1) and (3) not (2). Quick Start Jawn supports Scala 2.10 and 2.11. Here's a build.sbt snippet that shows you how to depend on Jawn for your project: resolvers += Resolver.sonatypeRepo(""releases"")  // use this if you just want jawn's parser, and will implement your own facade libraryDependencies += ""org.spire-math"" %% ""jawn-parser"" % ""0.8.4""  // use this if you want jawn's parser and also jawn's ast libraryDependencies += ""org.spire-math"" %% ""jawn-ast"" % ""0.8.4"" If you want to use Jawn's parser with another project's AST, see the ""Supporting external ASTs with Jawn"" section. For example, with Spray you would say: libraryDependencies += ""org.spire-math"" %% ""jawn-spray"" % ""0.8.4"" There are a few reasons you might want to do this: The library's built-in parser is significantly slower than Jawn Jawn supports more input types (ByteBuffer, File, etc.) You need asynchronous JSON parsing (NOTE: previous to version 0.8.3 the support libraries would have been named ""spray-support"" instead of ""jawn-spray"".) Dependencies jawn-parser has no dependencies other than Scala. jawn-ast depends on jawn-parser but nothing else. The various support projects (e.g. jawn-argonaut) depend on the library they are supporting. Parsing Jawn's parser is both fast and relatively featureful. Assuming you want to get back an AST of type J and you have a Facade[J] defined, you can use the following parse signatures: Parser.parseUnsafe[J](String) → J Parser.parseFromString[J](String) → Try[J] Parser.parsefromPath[J](String) → Try[J] Parser.parseFromFile[J](File) → Try[J] Parser.parseFromChannel[J](ReadableByteChannel) → Try[J] Parser.parseFromByteBuffer[J](ByteBuffer) → Try[J] Jawn also supports asynchronous parsing, which allows users to feed the parser with data as it is available. There are three modes: SingleValue waits to return a single J value once parsing is done. UnwrapArray if the top-level element is an array, return values as they become available. ValueStream parser one-or-more json values separated by whitespace Here's an example: import jawn.ast import jawn.AsyncParser import jawn.ParseException  val p = ast.JParser.async(mode = AsyncParser.UnwrapArray)  def chunks: Stream[String] = ??? def sink(j: ast.JValue): Unit = ???  def loop(st: Stream[String]): Either[ParseException, Unit] =   st match {     case s #:: tail =>       p.absorb(s) match {         case Right(js) =>           js.foreach(sink)           loop(tail)         case Left(e) =>           Left(e)       }     case _ =>       p.finish().right.map(_.foreach(sink))   }  loop(chunks) You can also call jawn.Parser.async[J] to use async parsing with an arbitrary data type (provided you also have an implicit Facade[J]). Supporting external ASTs with Jawn Jawn currently supports six external ASTs directly: Argonaut (6.0.4) Json4s (3.2.11) Play (2.3.6) Rojoma (2.4.3) Rojoma-v3 (3.2.1) Spray (1.3.1) Each of these subprojects provides a Parser object (an instance of SupportParser[J]) that is parameterized on the given project's AST (J). The following methods are available: Parser.parseUnsafe(String) → J Parser.parseFromString(String) → Try[J] Parser.parsefromPath(String) → Try[J] Parser.parseFromFile(File) → Try[J] Parser.parseFromChannel(ReadableByteChannel) → Try[J] Parser.parseFromByteBuffer(ByteBuffer) → Try[J] These methods parallel those provided by jawn.Parser. For the following snippets, XYZ is one of (argonaut, json4s, play, rojoma, rojoma-v3 or spray): This is how you would include the subproject in build.sbt: resolvers += Resolver.sonatypeRepo(""releases"")  libraryDependencies += ""org.spire-math"" %% jawn-""XYZ"" % ""0.8.4"" This is an example of how you might use the parser into your code: import jawn.support.XYZ.Parser  val myResult = Parser.parseFromString(myString) Do-It-Yourself Parsing Jawn supports building any JSON AST you need via type classes. You benefit from Jawn's fast parser while still using your favorite Scala JSON library. This mechanism is also what allows Jawn to provide ""support"" for other libraries' ASTs. To include Jawn's parser in your project, add the following snippet to your build.sbt file: resolvers += Resolver.sonatypeRepo(""releases"")  libraryDependencies += ""org.spire-math"" %% ""jawn-parser"" % ""0.8.4"" To support your AST of choice, you'll want to define a Facade[J] instance, where the J type parameter represents the base of your JSON AST. For example, here's a facade that supports Spray: import spray.json._ object Spray extends SimpleFacade[JsValue] {   def jnull() = JsNull   def jfalse() = JsFalse   def jtrue() = JsTrue   def jnum(s: String) = JsNumber(s)   def jint(s: String) = JsNumber(s)   def jstring(s: String) = JsString(s)   def jarray(vs: List[JsValue]) = JsArray(vs)   def jobject(vs: Map[String, JsValue]) = JsObject(vs) } Most ASTs will be easy to define using the SimpleFacade or MutableFacade traits. However, if an ASTs object or array instances do more than just wrap a Scala collection, it may be necessary to extend Facade directly. You can also look at the facades used by the support projects to help you create your own. This could also be useful if you wanted to use an older version of a supported library. Using the AST Access For accessing atomic values, JValue supports two sets of methods: get-style methods and as-style methods. The get-style methods return Some(_) when called on a compatible JSON value (e.g. strings can return Some[String], numbers can return Some[Double], etc.), and None otherwise: getBoolean → Option[Boolean] getString → Option[String] getLong → Option[Long] getDouble → Option[Double] getBigInt → Option[BigInt] getBigDecimal → Option[BigDecimal] In constrast, the as-style methods will either return an unwrapped value (instead of returning Some(_)) or throw an exception (instead of returning None): asBoolean → Boolean // or exception asString → String // or exception asLong → Long // or exception asDouble → Double // or exception asBigInt → BigInt // or exception asBigDecimal → BigDecimal // or exception To access elements of an array, call get with an Int position: get(i: Int) → JValue // returns JNull if index is illegal To access elements of an object, call get with a String key: get(k: String) → JValue // returns JNull if key is not found Both of these methods also return JNull if the value is not the appropraite container. This allows the caller to chain lookups without having to check that each level is correct: val v: JValue = ???  // returns JNull if a problem is encountered in structure of 'v'. val t: JValue = v.get(""novels"").get(0).get(""title"")  // if 'v' had the right structure and 't' is JString(s), then Some(s). // otherwise, None. val titleOrNone: Option[String] = t.getString  // equivalent to titleOrNone.getOrElse(throw ...) val titleOrDie: String = t.asString Updating The atomic values (JNum, JBoolean, JNum, and JString) are immutable. Objects are fully-mutable and can have items added, removed, or changed: set(k: String, v: JValue) → Unit remove(k: String) → Option[JValue] If set is called on a non-object, an exception will be thrown. If remove is called on a non-object, None will be returned. Arrays are semi-mutable. Their values can be changed, but their size is fixed: set(i: Int, v: JValue) → Unit If set is called on a non-array, or called with an illegal index, an exception will be thrown. (A future version of Jawn may provide an array whose length can be changed.) Profiling Jawn uses JMH along with the sbt-jmh plugin. Running Benchmarks The benchmarks are located in the benchmark project. You can run the benchmarks by typing benchmark/run from SBT. There are many supported arguments, so here are a few examples: Run all benchmarks, with 10 warmups, 10 iterations, using 3 threads: benchmark/run -wi 10 -i 10 -f1 -t3 Run just the CountriesBench test (5 warmups, 5 iterations, 1 thread): benchmark/run -wi 5 -i 5 -f1 -t1 .*CountriesBench Benchmark Issues Currently, the benchmarks are a bit fiddily. The most obvious symptom is that if you compile the benchmarks, make changes, and compile again, you may see errors like: [error] (benchmark/jmh:generateJavaSources) java.lang.NoClassDefFoundError: jawn/benchmark/Bla25Bench  The fix here is to run benchmark/clean and try again. You will also see intermittent problems like: [error] (benchmark/jmh:compile) java.lang.reflect.MalformedParameterizedTypeException  The solution here is easier (though frustrating): just try it again. If you continue to have problems, consider cleaning the project and trying again. (In the future I hope to make the benchmarking here a bit more resilient. Suggestions and pull requests gladly welcome!) Files The benchmarks use files located in benchmark/src/main/resources. If you want to test your own files (e.g. mydata.json), you would: Copy the file to benchmark/src/main/resources/mydata.json. Add the following code to JmhBenchmarks.scala: class MyDataBench extends JmhBenchmarks(""mydata.json"") Jawn has been tested with much larger files, e.g. 100M - 1G, but these are obviously too large to ship with the project. With large files, it's usually easier to comment out most of the benchmarking methods and only test one (or a few) methods. Some of the slower JSON parsers get much slower for large files. Interpreting the results Remember that the benchmarking results you see will vary based on: Hardware Java version JSON file size JSON file structure JSON data values I have tried to use each library in the most idiomatic and fastest way possible (to parse the JSON into a simple AST). Pull requests to update library versions and improve usage are very welcome. Future Work More support libraries could be added. It's likely that some of Jawn's I/O could be optimized a bit more, and also made more configurable. The heuristics around all-at-once loading versus input chunking could definitely be improved. In cases where the user doesn't need fast lookups into JSON objects, an even lighter AST could be used to improve parsing and rendering speeds. Strategies to cache/intern field names of objects could pay big dividends in some cases (this might require AST changes). If you have ideas for any of these (or other ideas) please feel free to open an issue or pull request so we can talk about it. Disclaimers Jawn only supports UTF-8 when parsing bytes. This might change in the future, but for now that's the target case. You can always decode your data to a string, and handle the character set decoding using Java's standard tools. Jawn's AST is intended to be very lightweight and simple. It supports simple access, and limited mutable updates. It intentionally lacks the power and sophistication of many other JSON libraries. Copyright and License All code is available to you under the MIT license, available at http://opensource.org/licenses/mit-license.php. Copyright Erik Osheim, 2012-2015. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/non/jawn"	"Fast json parser (According to them, competetive with java gson/jackson speed)."	"true"
"JSON"	"json4s ★ 654 ⧗ 0"	"https://github.com/json4s/json4s"	"Project aims to provide a single AST to be used by other scala json libraries."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"703"	"60"	"179"	"GitHub - json4s/json4s: A single AST to be used by other scala json libraries Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 60 Star 703 Fork 179 json4s/json4s Code Issues 123 Pull requests 5 Wiki Pulse Graphs A single AST to be used by other scala json libraries http://json4s.org 635 commits 5 branches 36 releases 60 contributors Scala 96.8% Shell 3.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 3.5 Switch branches/tags Branches Tags 3.2 3.3 3.4 3.5 gh-pages Nothing to show v3.4.0 v3.3.0.RC6 v3.3.0.RC5 v3.3.0.RC4 v3.3.0.RC3 v3.3.0.RC2 v3.3.0.RC1 v3.2.11_2.10 v3.2.11 v3.2.10_2.11 v3.2.10_2.10 v3.2.10 v.3.2.9_2.11 v.3.2.9_2.10 v.3.2.9 v.3.2.8_2.10 v3.2.8 v3.2.7_2.10 v.3.2.7 v3.2.5_2.10 v3.2.5 v3.2.4_2.10 v3.2.3_2.10 v3.2.3 v3.2.2_2.10 v3.2.2 v.3.2.0 v.3.2.0_scala2.10 3.3.0 3.2.6_2.10 3.2.6 3.2.4 3.2.1_2.10 3.2.1 3.1.0 2.10-3.1.0 Nothing to show New pull request Latest commit 6a771d6 Jul 4, 2016 xuwei-k committed on GitHub fix scalacheck scope Permalink Failed to load latest commit information. ast/src/main/scala/org/json4s Remove comment out lines Feb 13, 2016 benchmark/src/main/scala/org/json4s/benchmark Convert procedures to methods May 26, 2016 core Formats: add allowNull to the list of variables copied to the new ins… Jun 16, 2016 examples/src/main/scala/org/json4s/examples Merge pull request #370 from insano10/3.4 Jun 15, 2016 ext/src/main/scala/org/json4s/ext set the timezone from the date formatter on any dates it deserialises… May 13, 2016 jackson/src/main/scala/org/json4s/jackson Convert procedures to methods May 26, 2016 mongo/src/main/scala/org/json4s/mongo remove unused imports (#368) Apr 28, 2016 native-lift/src/main/scala/org/json4s/native/lift All the regular sources compile Aug 24, 2012 native/src/main/scala/org/json4s/native Convert procedures to methods May 26, 2016 project fix scalacheck scope Jul 4, 2016 scalap/src/main/scala/org/json4s/scalap Convert procedures to methods May 26, 2016 scalaz Parse case class with single field May 25, 2016 tests/src/test Formats: add allowNull to the list of variables copied to the new ins… Jun 16, 2016 .gitignore update the versions of the dependencies, build for 2.10 only Dec 29, 2012 .travis.yml Scala 2.12.0-M5 Jul 4, 2016 CONTRIBUTING.md update CONTRIBUTING.md Jun 21, 2016 LICENSE All the tests pass again after the reorganization Aug 24, 2012 README.md Update readme Jun 19, 2016 benchresults-beans.md run benchmark with scala 2.10 too Mar 20, 2013 benchresults-lift.md pass the writer on to jsonast.quote Mar 20, 2013 build.sbt version 3.3.0.RC2 May 16, 2015 publish.sh Use sbt-javaversioncheck to detect packging on JDK 8 Sep 14, 2015 sbt Bump sbt launcher version May 16, 2015 README.md JSON4S At this moment there are at least 6 json libraries for scala, not counting the java json libraries. All these libraries have a very similar AST. This project aims to provide a single AST to be used by other scala json libraries. At this moment the approach taken to working with the AST has been taken from lift-json and the native package is in fact lift-json but outside of the lift project. Lift JSON This project also attempts to set lift-json free from the release schedule imposed by the lift framework. The Lift framework carries many dependencies and as such it's typically a blocker for many other scala projects when a new version of scala is released. So the native package in this library is in fact verbatim lift-json in a different package name, this means that your import statements will change if you use this library. import org.json4s._ import org.json4s.native.JsonMethods._ After that everything works exactly the same as it would with lift-json Jackson In addition to the native parser there is also an implementation that uses jackson for parsing to the AST. The jackson module includes most of the jackson-module-scala functionality and the ability to use it with the lift-json AST. To use jackson instead of the native parser: import org.json4s._ import org.json4s.jackson.JsonMethods._ Be aware that the default behavior of the jackson integration is to close the stream when it's done. If you want to change that: import com.fasterxml.jackson.databind.SerializationFeature org.json4s.jackson.JsonMethods.mapper.configure(SerializationFeature.CLOSE_CLOSEABLE, false) Guide Parsing and formatting utilities for JSON. A central concept in lift-json library is Json AST which models the structure of a JSON document as a syntax tree. sealed abstract class JValue case object JNothing extends JValue // 'zero' for JValue case object JNull extends JValue case class JString(s: String) extends JValue case class JDouble(num: Double) extends JValue case class JDecimal(num: BigDecimal) extends JValue case class JInt(num: BigInt) extends JValue case class JLong(num: Long) extends JValue case class JBool(value: Boolean) extends JValue case class JObject(obj: List[JField]) extends JValue case class JArray(arr: List[JValue]) extends JValue  type JField = (String, JValue) All features are implemented in terms of above AST. Functions are used to transform the AST itself, or to transform the AST between different formats. Common transformations are summarized in a following picture. Summary of the features: Fast JSON parser LINQ style queries Case classes can be used to extract values from parsed JSON Diff & merge DSL to produce valid JSON XPath like expressions and HOFs to manipulate JSON Pretty and compact printing XML conversions Serialization Low level pull parser API Installation You can add the json4s as a dependency in following ways. Note, replace {latestVersion} with correct Json4s version. You can find available versions here: http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.json4s%22 SBT users For the native support add the following dependency to your project description: val json4sNative = ""org.json4s"" %% ""json4s-native"" % ""{latestVersion}"" For the Jackson support add the following dependency to your project description: val json4sJackson = ""org.json4s"" %% ""json4s-jackson"" % ""{latestVersion}"" Maven users For the native support add the following dependency to your pom: <dependency>   <groupId>org.json4s</groupId>   <artifactId>json4s-native_${scala.version}</artifactId>   <version>{latestVersion}</version> </dependency> For the jackson support add the following dependency to your pom: <dependency>   <groupId>org.json4s</groupId>   <artifactId>json4s-jackson_${scala.version}</artifactId>   <version>{latestVersion}</version> </dependency> Extras ext Support for Enum, Joda-Time, ... scalaz Applicative style parsing with Scalaz native-lift Support for Box Migration from older versions 3.3.0 -> json4s 3.3 basically should be source code compatible with 3.2.x. Since json4s 3.3.0, We've started using MiMa for binary compatibility verification not to repeat the bin compatibility issue described here. The behavior of .toOption on JValue has changed. Now both JNothing and JNull return None. For the old behavior you can use toSome which will only turn a JNothing into a None. All the merged pull requests: https://github.com/json4s/json4s/pulls?q=is%3Apr+is%3Aclosed+milestone%3A3.3 3.0.0 -> JField is no longer a JValue. This means more type safety since it is no longer possible to create invalid JSON where JFields are added directly into JArrays for instance. Most noticeable consequence of this change is that map, transform, find and filter come in two versions: def map(f: JValue => JValue): JValue def mapField(f: JField => JField): JValue def transform(f: PartialFunction[JValue, JValue]): JValue def transformField(f: PartialFunction[JField, JField]): JValue def find(p: JValue => Boolean): Option[JValue] def findField(p: JField => Boolean): Option[JField] //... Use *Field functions to traverse fields in the JSON, and use the functions without 'Field' in the name to traverse values in the JSON. 2.2 -> Path expressions were changed after 2.2 version. Previous versions returned JField which unnecessarily complicated the use of the expressions. If you have used path expressions with pattern matching like: val JField(""bar"", JInt(x)) = json \ ""foo"" \ ""bar"" It is now required to change that to: val JInt(x) = json \ ""foo"" \ ""bar"" Parsing JSON Any valid json can be parsed into internal AST format. For native support: scala> import org.json4s._ scala> import org.json4s.native.JsonMethods._  scala> parse("""""" { ""numbers"" : [1, 2, 3, 4] } """""") res0: org.json4s.JsonAST.JValue =       JObject(List((numbers,JArray(List(JInt(1), JInt(2), JInt(3), JInt(4))))))  scala> parse(""""""{""name"":""Toy"",""price"":35.35}"""""", useBigDecimalForDouble = true) res1: org.json4s.package.JValue =        JObject(List((name,JString(Toy)), (price,JDecimal(35.35)))) For jackson support: scala> import org.json4s._ scala> import org.json4s.jackson.JsonMethods._  scala> parse("""""" { ""numbers"" : [1, 2, 3, 4] } """""") res0: org.json4s.JsonAST.JValue =       JObject(List((numbers,JArray(List(JInt(1), JInt(2), JInt(3), JInt(4))))))  scala> parse(""""""{""name"":""Toy"",""price"":35.35}"""""", useBigDecimalForDouble = true) res1: org.json4s.package.JValue =        JObject(List((name,JString(Toy)), (price,JDecimal(35.35)))) Producing JSON You can generate json in 2 modes either in DoubleMode or in BigDecimalMode; the former will map all decimal values into a JDouble the latter into a JDecimal. For the double mode dsl use: import org.json4s.JsonDSL._ // or import org.json4s.JsonDSL.WithDouble._ For the big decimal mode dsl use: import org.json4s.JsonDSL.WithBigDecimal._ DSL rules Primitive types map to JSON primitives. Any seq produces JSON array. scala> val json = List(1, 2, 3)  scala> compact(render(json)) res0: String = [1,2,3] Tuple2[String, A] produces field. scala> val json = (""name"" -> ""joe"")  scala> compact(render(json)) res1: String = {""name"":""joe""} ~ operator produces object by combining fields. scala> val json = (""name"" -> ""joe"") ~ (""age"" -> 35)  scala> compact(render(json)) res2: String = {""name"":""joe"",""age"":35} Any value can be optional. Field and value is completely removed when it doesn't have a value. scala> val json = (""name"" -> ""joe"") ~ (""age"" -> Some(35))  scala> compact(render(json)) res3: String = {""name"":""joe"",""age"":35}  scala> val json = (""name"" -> ""joe"") ~ (""age"" -> (None: Option[Int]))  scala> compact(render(json)) res4: String = {""name"":""joe""} Extending the dsl To extend the dsl with your own classes you must have an implicit conversion in scope of signature: type DslConversion = T => JValue Example object JsonExample extends App {   import org.json4s._   import org.json4s.JsonDSL._   import org.json4s.jackson.JsonMethods._    case class Winner(id: Long, numbers: List[Int])   case class Lotto(id: Long, winningNumbers: List[Int], winners: List[Winner], drawDate: Option[java.util.Date])    val winners = List(Winner(23, List(2, 45, 34, 23, 3, 5)), Winner(54, List(52, 3, 12, 11, 18, 22)))   val lotto = Lotto(5, List(2, 45, 34, 23, 7, 5, 3), winners, None)    val json =     (""lotto"" ->       (""lotto-id"" -> lotto.id) ~       (""winning-numbers"" -> lotto.winningNumbers) ~       (""draw-date"" -> lotto.drawDate.map(_.toString)) ~       (""winners"" ->         lotto.winners.map { w =>           ((""winner-id"" -> w.id) ~            (""numbers"" -> w.numbers))}))    println(compact(render(json))) } scala> JsonExample {""lotto"":{""lotto-id"":5,""winning-numbers"":[2,45,34,23,7,5,3],""winners"": [{""winner-id"":23,""numbers"":[2,45,34,23,3,5]},{""winner-id"":54,""numbers"":[52,3,12,11,18,22]}]}} Example produces following pretty printed JSON. Notice that draw-date field is not rendered since its value is None: scala> pretty(render(JsonExample.json))  {   ""lotto"":{     ""lotto-id"":5,     ""winning-numbers"":[2,45,34,23,7,5,3],     ""winners"":[{       ""winner-id"":23,       ""numbers"":[2,45,34,23,3,5]     },{       ""winner-id"":54,       ""numbers"":[52,3,12,11,18,22]     }]   } } Merging & Diffing Two JSONs can be merged and diffed with each other. Please see more examples in MergeExamples.scala and DiffExamples.scala. scala> import org.json4s._ scala> import org.json4s.jackson.JsonMethods._  scala> val lotto1 = parse(""""""{          ""lotto"":{            ""lotto-id"":5,            ""winning-numbers"":[2,45,34,23,7,5,3],            ""winners"":[{              ""winner-id"":23,              ""numbers"":[2,45,34,23,3,5]            }]          }        }"""""")  scala> val lotto2 = parse(""""""{          ""lotto"":{            ""winners"":[{              ""winner-id"":54,              ""numbers"":[52,3,12,11,18,22]            }]          }        }"""""")  scala> val mergedLotto = lotto1 merge lotto2  scala> pretty(render(mergedLotto)) res0: String = {   ""lotto"":{     ""lotto-id"":5,     ""winning-numbers"":[2,45,34,23,7,5,3],     ""winners"":[{       ""winner-id"":23,       ""numbers"":[2,45,34,23,3,5]     },{       ""winner-id"":54,       ""numbers"":[52,3,12,11,18,22]     }]   } }  scala> val Diff(changed, added, deleted) = mergedLotto diff lotto1 changed: org.json4s.JsonAST.JValue = JNothing added: org.json4s.JsonAST.JValue = JNothing deleted: org.json4s.JsonAST.JValue = JObject(List((lotto,JObject(List(JField(winners, JArray(List(JObject(List((winner-id,JInt(54)), (numbers,JArray( List(JInt(52), JInt(3), JInt(12), JInt(11), JInt(18), JInt(22)))))))))))))) Querying JSON ""LINQ"" style JSON values can be extracted using for-comprehensions. Please see more examples in JsonQueryExamples.scala. scala> import org.json4s._ scala> import org.json4s.native.JsonMethods._  scala> val json = parse(""""""          { ""name"": ""joe"",            ""children"": [              {                ""name"": ""Mary"",                ""age"": 5              },              {                ""name"": ""Mazy"",                ""age"": 3              }            ]          }        """""")  scala> for {          JObject(child) <- json          JField(""age"", JInt(age))  <- child        } yield age res0: List[BigInt] = List(5, 3)  scala> for {          JObject(child) <- json          JField(""name"", JString(name)) <- child          JField(""age"", JInt(age)) <- child          if age > 4        } yield (name, age) res1: List[(String, BigInt)] = List((Mary,5)) XPath + HOFs Json AST can be queried using XPath like functions. Following REPL session shows the usage of '\', '\\', 'find', 'filter', 'transform', 'remove' and 'values' functions. The example json is: {   ""person"": {     ""name"": ""Joe"",     ""age"": 35,     ""spouse"": {       ""person"": {         ""name"": ""Marilyn"",         ""age"": 33       }     }   } } Translated to DSL syntax: scala> import org.json4s._ scala> import org.json4s.native.JsonMethods._ or scala> import org.json4s.jackson.JsonMethods._ scala> import org.json4s.JsonDSL._  scala> val json: JObject =   (""person"" ->     (""name"" -> ""Joe"") ~     (""age"" -> 35) ~     (""spouse"" ->       (""person"" ->         (""name"" -> ""Marilyn"") ~         (""age"" -> 33)       )     )   )  scala> json \\ ""spouse"" res0: org.json4s.JsonAST.JValue = JObject(List(       (person,JObject(List((name,JString(Marilyn)), (age,JInt(33)))))))  scala> compact(render(res0)) res1: String = {""person"":{""name"":""Marilyn"",""age"":33}}  scala> compact(render(json \\ ""name"")) res2: String = {""name"":""Joe"",""name"":""Marilyn""}  scala> compact(render((json removeField { _ == JField(""name"", JString(""Marilyn"")) }) \\ ""name"")) res3: String = ""Joe""  scala> compact(render(json \ ""person"" \ ""name"")) res4: String = ""Joe""  scala> compact(render(json \ ""person"" \ ""spouse"" \ ""person"" \ ""name"")) res5: String = ""Marilyn""  scala> json findField {          case JField(""name"", _) => true          case _ => false        } res6: Option[org.json4s.JsonAST.JValue] = Some((name,JString(Joe)))  scala> json filterField {          case JField(""name"", _) => true          case _ => false        } res7: List[org.json4s.JsonAST.JField] = List(JField(name,JString(Joe)), JField(name,JString(Marilyn)))  scala> json transformField {          case JField(""name"", JString(s)) => (""NAME"", JString(s.toUpperCase))        } res8: org.json4s.JsonAST.JValue = JObject(List((person,JObject(List( (NAME,JString(JOE)), (age,JInt(35)), (spouse,JObject(List( (person,JObject(List((NAME,JString(MARILYN)), (age,JInt(33)))))))))))))  scala> json.values res8: scala.collection.immutable.Map[String,Any] = Map(person -> Map(name -> Joe, age -> 35, spouse -> Map(person -> Map(name -> Marilyn, age -> 33)))) Indexed path expressions work too and values can be unboxed using type expressions. scala> val json = parse(""""""          { ""name"": ""joe"",            ""children"": [              {                ""name"": ""Mary"",                ""age"": 5              },              {                ""name"": ""Mazy"",                ""age"": 3              }            ]          }        """""")  scala> (json \ ""children"")(0) res0: org.json4s.JsonAST.JValue = JObject(List((name,JString(Mary)), (age,JInt(5))))  scala> (json \ ""children"")(1) \ ""name"" res1: org.json4s.JsonAST.JValue = JString(Mazy)  scala> json \\ classOf[JInt] res2: List[org.json4s.JsonAST.JInt#Values] = List(5, 3)  scala> json \ ""children"" \\ classOf[JString] res3: List[org.json4s.JsonAST.JString#Values] = List(Mary, Mazy) Extracting values Case classes can be used to extract values from parsed JSON. Non-existing values can be extracted into scala.Option and strings can be automatically converted into java.util.Dates. Please see more examples in ExtractionExampleSpec.scala. scala> import org.json4s._ scala> import org.json4s.jackson.JsonMethods._  scala> implicit val formats = DefaultFormats // Brings in default date formats etc.  scala> case class Child(name: String, age: Int, birthdate: Option[java.util.Date]) scala> case class Address(street: String, city: String) scala> case class Person(name: String, address: Address, children: List[Child])  scala> val json = parse(""""""          { ""name"": ""joe"",            ""address"": {              ""street"": ""Bulevard"",              ""city"": ""Helsinki""            },            ""children"": [              {                ""name"": ""Mary"",                ""age"": 5,                ""birthdate"": ""2004-09-04T18:06:22Z""              },              {                ""name"": ""Mazy"",                ""age"": 3              }            ]          }        """""")  scala> json.extract[Person] res0: Person = Person(joe,Address(Bulevard,Helsinki),List(Child(Mary,5,Some(Sat Sep 04 18:06:22 EEST 2004)), Child(Mazy,3,None)))  scala> val addressJson = json  \ ""address""  // Extract address object scala> addressJson.extract[Address] res1: Address = Address(Bulevard,Helsinki)  scala> (json \ ""children"").extract[List[Child]]  // Extract list of objects res2: List[Child] = List(Child(Mary,5,Some(Sat Sep 04 23:36:22 IST 2004)), Child(Mazy,3,None)) By default the constructor parameter names must match json field names. However, sometimes json field names contain characters which are not allowed characters in Scala identifiers. There's two solutions for this (see LottoExample.scala for bigger example). Use back ticks. scala> case class Person(`first-name`: String) Use transform function to postprocess AST. scala> case class Person(firstname: String) scala> json transformField {          case (""first-name"", x) => (""firstname"", x)        } If the json field names are snake case (i.e.: separated_by_underscores), but the case class uses camel case (i.e.: firstLetterLowercaseAndNextWordsCapitalized), you can convert the keys during the extraction using camelizeKeys: scala> import org.json4s._ scala> import org.json4s.native.JsonMethods._ scala> implicit val formats = DefaultFormats scala> val json = parse(""""""{""first_name"":""Mary""}"""""") scala> case class Person(firstName: String)  scala> json.camelizeKeys.extract[Person] res0: Person = Person(Mazy) See the ""Serialization"" section below for details on converting a class with camel case fields into json with snake case keys. Extraction function tries to find the best matching constructor when case class has auxiliary constructors. For instance extracting from JSON {""price"":350} into the following case class will use the auxiliary constructor instead of the primary constructor. scala> case class Bike(make: String, price: Int) {          def this(price: Int) = this(""Trek"", price)        } scala> parse("""""" {""price"":350} """""").extract[Bike] res0: Bike = Bike(Trek,350) Primitive values can be extracted from JSON primitives or fields. scala> (json \ ""name"").extract[String] res0: String = ""joe""  scala> ((json \ ""children"")(0) \ ""birthdate"").extract[Date] res1: java.util.Date = Sat Sep 04 21:06:22 EEST 2004 DateFormat can be changed by overriding 'DefaultFormats' (or by implmenting trait 'Formats'). scala> implicit val formats = new DefaultFormats {          override def dateFormatter = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"")        } JSON object can be extracted to Map[String, _] too. Each field becomes a key value pair in result Map. scala> val json = parse(""""""          {            ""name"": ""joe"",            ""addresses"": {              ""address1"": {                ""street"": ""Bulevard"",                ""city"": ""Helsinki""              },              ""address2"": {                ""street"": ""Soho"",                ""city"": ""London""              }            }          }"""""")  scala> case class PersonWithAddresses(name: String, addresses: Map[String, Address]) scala> json.extract[PersonWithAddresses] res0: PersonWithAddresses(""joe"", Map(""address1"" -> Address(""Bulevard"", ""Helsinki""),                                      ""address2"" -> Address(""Soho"", ""London""))) Note that when the extraction of an Option[_] fails, the default behavior of extract is to return None. You can make it fail with a [MappingException] instead, you can override this behavior by using a custom Formats object: val formats: Formats = new DefaultFormats {   override val strictOptionParsing: Boolean = true } Serialization Case classes can be serialized and deserialized. Please see other examples in SerializationExamples.scala. scala> import org.json4s._ scala> import org.json4s.native.Serialization scala> import org.json4s.native.Serialization.{read, write}  scala> implicit val formats = Serialization.formats(NoTypeHints)  scala> val ser = write(Child(""Mary"", 5, None))  scala> read[Child](ser) res1: Child = Child(Mary,5,None) If you're using jackson instead of the native one: scala> import org.json4s._ scala> import org.json4s.jackson.Serialization scala> import org.json4s.jackson.Serialization.{read, write}  scala> implicit val formats = Serialization.formats(NoTypeHints)  scala> val ser = write(Child(""Mary"", 5, None))  scala> read[Child](ser) res1: Child = Child(Mary,5,None) Serialization supports: Arbitrarily deep case class graphs All primitive types, including BigInt and Symbol List, Seq, Array, Set and Map (note, keys of the Map must be strings: Map[String, _]) scala.Option java.util.Date Polymorphic Lists (see below) Recursive types Serialization of fields of a class (see below) Custom serializer functions for types which are not supported (see below) If the class contains camel case fields (i.e: firstLetterLowercaseAndNextWordsCapitalized) but you want to produce a json string with snake casing (i.e.: separated_by_underscores), you can use the snakizeKeys method: scala> val ser = write(Person(""Mary"")) ser: String = {""firstName"":""Mary""}  scala> compact(render(parse(ser).snakizeKeys)) res0: String = {""first_name"":""Mary""} Serializing polymorphic Lists Type hints are required when serializing polymorphic (or heterogeneous) Lists. Serialized JSON objects will get an extra field named 'jsonClass' (the name can be changed by overriding 'typeHintFieldName' from Formats). scala> trait Animal scala> case class Dog(name: String) extends Animal scala> case class Fish(weight: Double) extends Animal scala> case class Animals(animals: List[Animal])  scala> implicit val formats = Serialization.formats(ShortTypeHints(List(classOf[Dog], classOf[Fish])))  scala> val ser = write(Animals(Dog(""pluto"") :: Fish(1.2) :: Nil)) ser: String = {""animals"":[{""jsonClass"":""Dog"",""name"":""pluto""},{""jsonClass"":""Fish"",""weight"":1.2}]}  scala> read[Animals](ser) res0: Animals = Animals(List(Dog(pluto), Fish(1.2))) ShortTypeHints outputs short classname for all instances of configured objects. FullTypeHints outputs full classname. Other strategies can be implemented by extending TypeHints trait. Serializing fields of a class To enable serialization of fields, a FieldSerializer can be added for some type: implicit val formats = DefaultFormats + FieldSerializer[WildDog]() Now the type WildDog (and all subtypes) gets serialized with all its fields (+ constructor parameters). FieldSerializer takes two optional parameters which can be used to intercept the field serialization: case class FieldSerializer[A: Manifest](   serializer:   PartialFunction[(String, Any), Option[(String, Any)]] = Map(),   deserializer: PartialFunction[JField, JField] = Map() ) Those PartialFunctions are called just before a field is serialized or deserialized. Some useful PFs to rename and ignore fields are provided: val dogSerializer = FieldSerializer[WildDog](   renameTo(""name"", ""animalname"") orElse ignore(""owner""),   renameFrom(""animalname"", ""name""))  implicit val formats = DefaultFormats + dogSerializer Serializing classes defined in traits or classes We've added support for case classes defined in a trait. But they do need custom formats. I'll explain why and then how. Why? For classes defined in a trait it's a bit difficult to get to their companion object, which is needed to provide default values. We could punt on those but that brings us to the next problem, the compiler generates an extra field in the constructor of such case classes. The first field in the constructor of those case classes is called $outer and is of type of the defining trait. So somehow we need to get an instance of that object, naively we could scan all classes and collect the ones that are implementing the trait, but when there are more than one: which one to take? How? I've chosen to extend the formats to include a list of companion mappings for those case classes. So you can have formats that belong to your modules and keep the mappings in there. That will then make default values work and provide the much needed $outer field. trait SharedModule {   case class SharedObj(name: String, visible: Boolean = false) }  object PingPongGame extends SharedModule implicit val formats: Formats =   DefaultFormats.withCompanions(classOf[PingPongGame.SharedObj] -> PingPongGame)  val inst = PingPongGame.SharedObj(""jeff"", visible = true) val extr = Extraction.decompose(inst) extr must_== JObject(""name"" -> JString(""jeff""), ""visible"" -> JBool(true)) extr.extract[PingPongGame.SharedObj] must_== inst Serializing non-supported types It is possible to plug in custom serializer + deserializer functions for any type. Now, if we have a non case class Interval (thus, not supported by default), we can still serialize it by providing following serializer. scala> class Interval(start: Long, end: Long) {          val startTime = start          val endTime = end        }  scala> class IntervalSerializer extends CustomSerializer[Interval](format => (          {            case JObject(JField(""start"", JInt(s)) :: JField(""end"", JInt(e)) :: Nil) =>              new Interval(s.longValue, e.longValue)          },          {            case x: Interval =>              JObject(JField(""start"", JInt(BigInt(x.startTime))) ::                      JField(""end"",   JInt(BigInt(x.endTime))) :: Nil)          }        ))  scala> implicit val formats = Serialization.formats(NoTypeHints) + new IntervalSerializer Custom serializer is created by providing two partial functions. The first evaluates to a value if it can unpack the data from JSON. The second creates the desired JSON if the type matches. Extensions Module json4s-ext contains extensions to extraction and serialization. Following types are supported. // Lift's box implicit val formats = org.json4s.DefaultFormats + new org.json4s.native.ext.JsonBoxSerializer  // Scala enums implicit val formats = org.json4s.DefaultFormats + new org.json4s.ext.EnumSerializer(MyEnum) // or implicit val formats = org.json4s.DefaultFormats + new org.json4s.ext.EnumNameSerializer(MyEnum)  // Joda Time implicit val formats = org.json4s.DefaultFormats ++ org.json4s.ext.JodaTimeSerializers.all XML support JSON structure can be converted to XML node and vice versa. Please see more examples in XmlExamples.scala. scala> import org.json4s.Xml.{toJson, toXml} scala> val xml =          <users>            <user>              <id>1</id>              <name>Harry</name>            </user>            <user>              <id>2</id>              <name>David</name>            </user>          </users>  scala> val json = toJson(xml) scala> pretty(render(json)) res3: String = {   ""users"":{     ""user"":[{       ""id"":""1"",       ""name"":""Harry""     },{       ""id"":""2"",       ""name"":""David""     }]   } } Now, the above example has two problems. First, the id is converted to String while we might want it as an Int. This is easy to fix by mapping JString(s) to JInt(s.toInt). The second problem is more subtle. The conversion function decides to use JSON array because there's more than one user-element in XML. Therefore a structurally equivalent XML document which happens to have just one user-element will generate a JSON document without JSON array. This is rarely a desired outcome. These both problems can be fixed by following transformation function. scala> json transformField {          case (""id"", JString(s)) => (""id"", JInt(s.toInt))          case (""user"", x: JObject) => (""user"", JArray(x :: Nil))        } Other direction is supported too. Converting JSON to XML: scala> toXml(json) res5: scala.xml.NodeSeq = NodeSeq(<users><user><id>1</id><name>Harry</name></user><user><id>2</id><name>David</name></user></users>) Low level pull parser API Pull parser API is provided for cases requiring extreme performance. It improves parsing performance by two ways. First, no intermediate AST is generated. Second, you can stop parsing at any time, skipping rest of the stream. Note, this parsing style is recommended only as an optimization. Above mentioned functional APIs are easier to use. Consider following example which shows how to parse one field value from a big JSON. scala> val json = """"""   {     ...     ""firstName"": ""John"",     ""lastName"": ""Smith"",     ""address"": {       ""streetAddress"": ""21 2nd Street"",       ""city"": ""New York"",       ""state"": ""NY"",       ""postalCode"": 10021     },     ""phoneNumbers"": [       { ""type"": ""home"", ""number"": ""212 555-1234"" },       { ""type"": ""fax"", ""number"": ""646 555-4567"" }     ],     ...   }""""""  scala> val parser = (p: Parser) => {          def parse: BigInt = p.nextToken match {            case FieldStart(""postalCode"") => p.nextToken match {              case IntVal(code) => code              case _ => p.fail(""expected int"")            }            case End => p.fail(""no field named 'postalCode'"")            case _ => parse          }           parse        }  scala> val postalCode = parse(json, parser) postalCode: BigInt = 10021 Pull parser is a function Parser => A, in this example it is concretely Parser => BigInt. Constructed parser recursively reads tokens until it finds FieldStart(""postalCode"") token. After that the next token must be IntVal, otherwise parsing fails. It returns parsed integer and stops parsing immediately. FAQ Q1: I have a JSON object and I want to extract it to a case class: scala> case class Person(name: String, age: Int) scala> val json = """"""{""name"":""joe"",""age"":15}"""""" But extraction fails: scala> parse(json).extract[Person] org.json4s.MappingException: Parsed JSON values do not match with class constructor A1: Extraction does not work for classes defined in REPL. Compile the case class definitions with scalac and import those to REPL. Kudos The original idea for DSL syntax was taken from Lift mailing list (by Marius). The idea for AST and rendering was taken from Real World Haskell book. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/json4s/json4s"	"Project aims to provide a single AST to be used by other scala json libraries."	"true"
"JSON"	"persist-json ★ 7 ⧗ 60"	"https://github.com/nestorpersist/json"	"Fast json parser."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"7"	"5"	"5"	"GitHub - nestorpersist/json: Persist-Json, a Fast Json Parser Written in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 5 Star 7 Fork 5 nestorpersist/json Code Issues 0 Pull requests 0 Pulse Graphs Persist-Json, a Fast Json Parser Written in Scala 106 commits 2 branches 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. license project src .gitignore README.md build.sbt changes.txt README.md Persist-Json is a new fast Json parser written in Scala. It includes: A standard Scala tree form of Json based on Scala immutable collections. A very fast Json parser that takes a Json string and produces the Scala form. A very fast Compact unparser that takes the Scala form and produces a compact single-line Json string. A Pretty unparser that takes the Scala form and produces a nicely formatted Json string. A set of methods for working with the Scala form that augment the existing standard collection methods. A mapper that converts between the Scala form and user-defined classes. Documentation ScalaDoc Reference Persist-Json can be referenced in sbt from Maven Central as ""com.persist"" % ""persist-json_2.11"" % ""1.1.3""  History This parser started as part of the OStore NoSQL database project. OStore OStore makes extensive use of Json and needed a pure-Scala Json parser whose output would be immutable Scala data. For this purpose, the Twitter parser based on the parsing combinator example in the Odersky book was choosen. Twitter Json Although it met the functional need, it was much too slow. The OStore Json parser mostly maintains the API of the Twitter Json parser but is very much faster. Since the OStore Json parser has utility beyond its use in OStore, it was broken off as this separate Persist-Json GitHub project. Thanks YourKit is kindly supporting the Persist-Json open source project with its full-featured Java Profiler. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications. Take a look at YourKit's leading software products: YourKit Java Profiler and YourKit .NET Profiler. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nestorpersist/json"	"Fast json parser."	"true"
"JSON"	"play-json"	"https://github.com/playframework/playframework/tree/master/framework/src/play-json"	"Flexible and powerful JSON manipulation, validation and serialization, with no reflection at runtime."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"8105"	"756"	"2806"	"playframework/framework/src/play-json at master · playframework/playframework · GitHub Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 756 Star 8,105 Fork 2,806 playframework/playframework Code Issues 262 Pull requests 25 Pulse Graphs Branch: master Switch branches/tags Branches Tags 2.0.x 2.1.x 2.2.x 2.3.x 2.4.x 2.5.x fix-5270 master Nothing to show 2.5.4 2.5.3 2.5.2 2.5.1 2.5.0 2.5.0-RC2 2.5.0-RC1 2.5.0-M2 2.5.0-M1 2.4.8 2.4.7 2.4.6 2.4.5 2.4.4 2.4.3 2.4.2 2.4.1 2.4.0 2.4.0-RC5 2.4.0-RC4 2.4.0-RC3 2.4.0-RC2 2.4.0-RC1 2.4.0-M3 2.4.0-M2 2.4.0-M1 2.3.10 2.3.9 2.3.8 2.3.7 2.3.6 2.3.5 2.3.4 2.3.3 2.3.2 2.3.2-RC2 2.3.2-RC1 2.3.1 2.3.0 2.3.0-RC2 2.3.0-RC1 2.3-M1 2.2.6 2.2.5 2.2.4 2.2.3 2.2.3-RC2 2.2.3-RC1 2.2.2 2.2.2-RC4 2.2.2-RC3 2.2.2-RC2 2.2.2-RC1 2.2.1 2.2.1-RC1 2.2.0 2.2.0-RC2 2.2.0-RC1 2.2.0-M3 2.2.0-M2 2.2.0-M1 2.1.6-RC1 2.1.5 2.1.4 2.1.4-RC2 2.1.4-RC1 2.1.3 2.1.3-RC2 2.1.3-RC1 2.1.2 2.1.2-RC2 2.1.2-RC1 2.1.1 2.1.1-RC2 2.1.1-RC1 2.1.1-RC1-2.9.x-backport 2.1.1-2.9.x-backport 2.1.0 2.1-RC4 2.1-RC3 2.1-RC2 2.1-RC1 2.0.8 2.0.7 2.0.6 2.0.5 2.0.5-RC2 2.0.5-RC1 2.0.4 2.0.4-RC2 2.0.4-RC1 2.0.3 2.0.3-RC2 2.0.3-RC1 2.0.2 2.0.2-RC2 2.0.2-RC1 2.0.1 2.0 2.0-beta Nothing to show Create new file Find file History playframework/framework/src/play-json/ Latest commit 92b652b Jul 7, 2016 naferx committed with gmethvin Remove deprecated code in JsError (#6304) Permalink .. Failed to load latest commit information. src Remove deprecated code in JsError (#6304) Jul 7, 2016 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/playframework/playframework/tree/master/framework/src/play-json"	"Flexible and powerful JSON manipulation, validation and serialization, with no reflection at runtime."	"true"
"JSON"	"qbproject ★ 10 ⧗ 7"	"https://github.com/qb-project/qbproject"	"Scala Libs around JSON and API developement for Play Framework."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"10"	"7"	"3"	"GitHub - qb-project/qbproject: Scala Libs around JSON and API developement for Play Framework Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 7 Star 10 Fork 3 qb-project/qbproject Code Issues 12 Pull requests 0 Wiki Pulse Graphs Scala Libs around JSON and API developement for Play Framework http://qb-project.org 152 commits 4 branches 6 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags CSV_Refactoring dev-0.3.1 dev master Nothing to show v0.4.2 v0.4.1.2 v0.4.0-rc7 v0.4.0-rc2 v0.4.0-rc1 0.4.0-b1 Nothing to show New pull request Latest commit d78320e Nov 2, 2015 edgarmueller Update README.md Permalink Failed to load latest commit information. project qbschema/src .gitignore .travis.yml LICENSE README.md Update README.md Nov 2, 2015 version.sbt README.md qbProject NOTE: This project is currently unmaintained for the time being. You are nevertheless encouraged to leave feedback or open issues. If you are interested in maintaining this project, please contact us. qbProject is a set of scala libraries around JSON and API development using the Play Framework. Developed by M Cube, EclipseSource and Contributors 2014. Licensed under Apache License 2. qbSchema The core component is qbSchema, a library allowing to declare JSON schemas which can be used for validation, transformation and meta description for your JSON entities. Especially the meta descriptions open ups some very useful usecases: It can be exported and consumed by UI generation libraries. It helps with data imports from different sources and it can be used to inject schemas/validation to your application at runtime. Defining a schema is as simple as: val todoSchema = qbClass(   ""title"" -> qbString(maxLength(140)),   ""description"" -> qbString,   ""dueDate"" -> qbDateTime ) Then it can be used for validation of JSON. It uses JsResult from Play for monadique results and proper error aggregation. val input = Json.obj(   ""title"" -> ""Hello World"",   ""description"" -> ""Say hello to all the peeps out there"",   ""dueDate"" -> new DateTime().toString() )  // Validate JSON directly val result: JsResult[JsObject] = QBValidator.validate(todoSchema)(input)  // Or use the Action provided by qbplay def echoAction = ValidatingAction(todoSchema) {    request => Ok(request.validatedJson) } qbSchema offers a variety of schema manipulation operations, so-called SchemaOps. They allow to add and remove fields, merge schemas, make fields optional and so on. // make the description field optional val optionalDescriptionSchema = todoSchema ? ""description""  // add an author to the todoSchema val todoWithAuthor = todoSchema ++ qbClass(    ""author"" -> qbClass(     ""name"" -> qbString,     ""email"" -> qbEmail   ) ) qbSchemas can be serailized to JSON schema, which can be used for JSON schema compliant clients such as form generators. qbSchema can also be generated by incoming JSON schema. This allows to inject new schemas into your system at runtime. Json.prettyPrint(Json.toJson(todoSchema)) results in a JSON schema based representation: {   ""type"" : ""object"",   ""properties"" : {     ""title"" : {       ""type"" : ""string"",       ""maxLength"" : 140     },     ""description"" : {       ""type"" : ""string""     },     ""dueDate"" : {       ""type"" : ""string"",       ""format"" : ""date-time""     }   },   ""additionalProperties"" : false,   ""required"" : [ ""title"", ""description"", ""dueDate"" ] } qbSchema was designed with extensibility in mind and as such allows introducing new types or annotating existing types with rules or additional meta data. // Define your new QBType class QBImage(rules: Set[ValidationRule[JsString]]) extends QBStringImpl(rules) // Create a DSL helper def image = new QBImage(Set.empty)  // Create a schema with your newly created type. val schema = qbClass(""img"" -> image) val instance = Json.obj(""img"" -> ""otto.png"")  val isQBImage = (qbType: QBType) => qbType.isInstanceOf[QBImage]  // Use the helpers provided by QB to transform your JSON. E.g. prepending an URL to every QBImage type. schema.transform(instance)(   isQBImage -> { case JsString(path) => JsString(""public/images/"" + path) } ) Getting started For a tutorial on how to get started please see the qb homepage. // Add a resolver ""QB repository"" at ""http://dl.bintray.com/qbproject/maven""  // Add these dependencies val qbSchema        = ""org.qbproject""    %% ""qbschema""    % ""0.4.2"" Contribute QB is under constant developement and we appreciate any feedback, suggestions, feature requests, bug reports or contributions. So please don't hesitate to open an issue on the bugtracker or drop us an email. Build We use Travis CI to build qb: Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/qb-project/qbproject"	"Scala Libs around JSON and API developement for Play Framework."	"true"
"JSON"	"scalajack ★ 73 ⧗ 5"	"https://github.com/gzoller/ScalaJack"	"Fast 'n easy JSON serialization with optional MongoDB support. Uses Jackson under the hood."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"74"	"11"	"4"	"GitHub - gzoller/ScalaJack: Fast JSON parser/generator for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 11 Star 74 Fork 4 gzoller/ScalaJack Code Issues 0 Pull requests 0 Pulse Graphs Fast JSON parser/generator for Scala 266 commits 4 branches 21 releases Fetching contributors Scala 96.3% Java 3.7% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.6.1 develop feature/paramExp master Nothing to show 4.7.2 4.7.1 4.7.0 4.6.2 4.6.1 4.6.0 4.5.1 4.5.0 4.4.6 4.4.5 4.4.4 4.4.3 4.4.2 4.4.1 4.4.0 4.3.4 4.3.3 4.3.2 4.3.1 4.3 4.2 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. core/src Minor fix in parameterized type handling Jul 10, 2016 mongo/src whole mess of performance improvements Mar 22, 2016 mysql/src Removed scala compiler dependency Nov 20, 2014 notes Performance improvements Mar 22, 2016 project build updated Jan 8, 2016 .gitignore .travis.yml Getting ready for TravisCI Sep 19, 2015 LICENSE Add License Jul 15, 2016 README.md version bump Jul 10, 2016 TODO Removed scala compiler dependency Nov 20, 2014 README.md CI service Status Description Travis Linux container tests ScalaJack Very fast JSON parser/generator for Scala case classes using Jackson that requires virtually no annotations or ""helper"" code. Advanced Features: Handles tuples Limited 'any' support Handles default values for case class fields Rich configuration of trait type hint/value Supports value classes Pluggable reader/render (for non-JSON encodings in the future) Use ScalaJack is extremely simple to use. Include it in your projects by adding the following to your build.sbt: libraryDependencies ++= Seq(""co.blocke"" %% ""scalajack"" % ""4.7.2"")  If you want to use the optional MongoDB serialization support include this as well: libraryDependencies ++= Seq(""co.blocke"" %% ""scalajack_mongo"" % ""4.7.2"")  ScalaJack is hosted on Bintray/JCenter now so if you're using sbt v0.13.9+ you should find it with no issues. If you're on sbt v0.13.11 you may need to enable the bintray resolver in your build.sbt with useJCenter := true Now you're good to go! Let's use ScalaJack in your project to serialize/de-serialize a case class object into JSON: import co.blocke.scalajack._  val sj = ScalaJack() val js = sj.render( myCaseObj )  // serialization val myObj = sj.read[MyCaseClass](js) // deserialization  Couldn't be simpler. Trait support ScalaJack can handle traits too. To do this you'll need the help of a type hint in the JSON. This tells ScalaJack what actual class to create in support of the trait. The default type hint is ""_hint"" but you can set whatever you want (very powerful for 3rd party JSON!) You set your own type hint with a VisitorContext object as shown below. Don't forget to use the same type hint name for render and read. package com.myproj  trait Pet { val name:String } case class Dog( name:String ) extends Pet case class Cat( name:String ) extends Pet  val p : Pet = Dog(""Fido"") val sj = ScalaJack() val js = sj.render(p)  // {""_hint"":""com.myproj.Dog"",""name"":""Fido""} val vc = VisitorContext(hintMap=Map(""default""->""kind"")) val js2 = sj.render(p,vc) // {""kind"":""com.myproj.Dog"",""name"":""Fido""}  // Be sure to match type hints when reading! val d1 = sj.read[Pet](js) val d2 = sj.read[Pet](js2,vc) There's one more cool trick we can do with traits. Imagine you're parsing JSON from a 3rd party (i.e. you don't control its structure) and you're modeling nested traits but want the type hint field to be different. ScalaJack's VisitorContext object allows you specify per-trait hints like so: package com.myproj  trait Animal {     val name:String } case class Dog(name:String) extends Animal case class Cat(name:String) extends Animal trait Pet {     val kind:Animal     val food:String } case class NicePet(kind:Animal, food:String) extends Pet case class GrumpyPet(kind:Animal, food:String) extends Pet  val pets = List(NicePet(Dog(""Fido""),""kibbles""),GrumpyPet(Cat(""Meow""),""fish"")) val sj = ScalaJack() val vc = VisitorContext(hintMap = Map(         ""default""->""_hint"",         ""com.myproj.Pet""->""_happy"",         ""com.myproj.Animal""->""_kind"")) val js = sj.render(pets,vc) // produces: [{""_happy"":""com.myproj.NicePet"",""kind"":{""_kind"":""com.myproj.Dog"",""name"":""Fido""},""food"":""kibbles""},{""_happy"":""com.myproj.GrumpyPet"",""kind"":{""_kind"":""com.myproj.Cat"",""name"":""Meow""},""food"":""fish""}] Note how you get different type hints for specific traits. This can be invaluable for advanced JSON parsing of 3rd party data. Custom Value Class JSON (NOTE: Custom value class JSON handling has changed since ScalaJack 3.x. In some ways its more limited but also much simpler.) Many JSON parsers have the ability to perform custom generations/parsings for fields or data types. Adding this feature in the conventional way would slow down the parser quite a lot, so ScalaJack does something a little different using an optional ""mode"" for value classes that allows you to read/render custom JSON for them. Let's demonstrate via an example. Let's imagine we have a value class PosixDate that has a long as its single value type, holding a Unix timestamp. Let's further assume we use this class in a case class for server stats like this: class PosixDate( val ts:Long = (new Date()).getTime/1000 ) extends AnyVal {     def toDate : Date = new Date(ts*1000)     def isBefore( pd:PosixDate ) = this.ts < pd.ts     def isAfter( pd:PosixDate )  = this.ts > pd.ts     override def toString() = ts.toString }  case class ServerStats( instanceName:String, upSince:PosixDate ) Good enough. Now let's render some JSON: val ss = ServerStats( ""admin"", new PosixDate() ) val js = sj.render( ss )  // Output: {""instanceName"":""admin"",""upSince"":1383317215} For my use, though, I might want a specific format for my timestamp in the JSON. I can accomplish this by handling custom JSON for my value classes using another field of VisitorContext, like this: val handler = ValClassHandler(     (s) => DateTimeFormat.forPattern(""MMMM, yyyy"").parseDateTime(s),     (v) => '""'+DateTimeFormat.forPattern(""MMMM, yyyy"").print(v.asInstanceOf[DateTime])+'""' ) val vc = VisitorContext(valClassMap = Map(""com.myproj.PosixDate""->handler)) val js = sj.render(ss,vc) // Output: {""instanceName"":""admin"",""upSince"":""November, 2013""} The handler consists of 2 functions: one that accepts a string (for reads) and produces the appropriate object, presumably with whatever necessary manipulations you wish on the string, and another function for renders that accepts an object from your value class and produces whatever string value you wish for the JSON. MongoDB Persistence NOTE: As of version 4.5.1 ScalaJack supports the new asynchronous MongoDB drivers for Scala, deprecating Casbah support. If you need Casbah, version 4.4.6 is the last version supporting Casbah. ScalaJack doesn't wrap the MongoDB persistence libraries--that's not its mission. It does provide a way to convert case classes (and traits)to/from Documents. You'll need to include the mongo support package: import co.blocke.scalajack._ import mongo._  val sjMongo = ScalaJack(MongoFlavor()) // produce a Mongo-flavored ScalaJack val mydbo  = sjMongo.render( myCaseClass ) val myCC   = sjMongo.read[MyClass]( mydbo ) The VisitorContext modifications work here too, as before with JSON. You can (and should) specify the key field(s) for your classes with the @DBKey annotation as shown here: case class Sample( @DBKey lastName:String, birthDate:Long, hobbies:List[String] ) Compound keys are also supported: case class Sample( @DBKey lastName:String, @DBKey birthDate:Long, hobbies:List[String] ) Support has been added for Mongo's ObjectId type if you wish to use this directly. case class Sample( @DBKey _id:ObjectId, stuff:Int ) Once you have your Document, use MongoDB's native Scala API as you normally would. CSV Support CSV support is provided but by necessity is very limited. CSV itself is not sufficiently expressive to handle complex data structures such as fields n a class that are themselves classes, collections, etc. Therefore the following rules apply: Case class only support Classes must be ""flat"" no List/Array, Map, or collection fields no fields that are classes Field ordering in the CSV must match the declared field order of the class Optional fields of simple type are OK, for example OptionString Classes must be concrete. Traits are not allowed. (How would you supply the type hint?) Parameterized classes are OK as long as the parameters conform to the above rules During a read of a CSV record, if a field is empty ""foo,,bar"" it may mean the empty value is optional, which will be read in as None. It may also mean that a field in the class has a default value on the constructor, in which case the default value is applied. If the class field corresponding to the empty spot in the CSV is neither Option nor a default-value field then it is viewed as an empty value. Usage CSV rendering is an inexact science, so here's how this works in ScalaJack. First you'll need to instantiate a ScalaJack instance with CSVFlavor as shown in the code below. Null values are represented as null in the CSV. null (unquoted) read in is a null value in the object. ""null"" (quoted) is a String with value ""null"" (not a real null value). For example ""foo,null,bar"" contains a null. ""foo,""null"",bar"" shows a string having value ""null"". Confused yet? Optional None is rendered as blank in the CSV and when read in, like ""foo,,bar"". As per the (unofficial) CSV spec: Fields may be enclosed in double-quotes String values containing double quotes must be enclosed by double quotes and the double-quote char in the string must be escaped by another double quote like this: ""I read """"Lord of the Rings"""" last year"" String values that contain a field separator (comma) must be enclosed in double quotes. Use CSV support like this: case class Foo(name:String, age:Int) val sjCSV  = ScalaJack(CSVFlavor()) sjCSV.render( Foo(""Fred"", 32) ) // Fred,32 sjCSV.read[Foo]( ""Fred,32"" ) // Foo(""Fred"",32) MySQL Support The MySQL support provided in the ScalaJack 3.x series has been removed for the time being. I wasn't entirely happy with it. It may be back in a future release if there is a swell of people interested in it. VisitorContext ScalaJack uses an optional VisitorContext object you can pass into read/render to control certain aspects of how data is processed. Looking at the definition of VisitorContext is a good starting point: case class VisitorContext(     isCanonical    : Boolean = true,    // allow non-string keys in Maps--not part of JSON spec     isValidating   : Boolean = false,     estFieldsInObj : Int     = 128,     valClassMap    : Map[String,ValClassHandler] = Map.empty[String,ValClassHandler],     hintMap        : Map[String,String] = Map(""default"" -> ""_hint""),  // per-class type hints (for nested classes)     hintValueRead   : Map[String,(String)=>String] = Map.empty[String,(String)=>String], // per-class type hint value -> class name     hintValueRender : Map[String,(String)=>String] = Map.empty[String,(String)=>String]  // per-class type class name -> hint value     ) Let's look at these fields one-by-one. isCanonical=true is standard JSON. In some strange situations you may wish JSON-like notation that does not use strings as keys. You would set this field to false to allow that. Note that this is not really JSON and won't many libraries (like Mongo) assume and require string-based keys for JSON objects. isValidating controls which of ScalaJack's 2 parsers is used. The non-validating parser (isValidating=false) is a bit faster but doesn't make much effort in telling you why JSON parsing failed. The validating parser is a little slower but has better error reporting. estFieldsInObj is also something you'll likely want to set for non-validating parsing. Part of its speed is pre-allocated buffers, so you'll need to guess a reasonable maximum field count for the largest expected object in your data. If you use the validating parser you can ignore this field as another reason the validating parser is slower is that it can auto-scale its buffers without your help. valClassMap is a map of value class name (fully-qualified) to ValClassHandler. An example of its use was shown above in the section on custom JSON for value classes. If you don't need custom JSON you can ignore this. hintMap is a map of class name (fully-qualified) to trait type hint string. Note there must always be a ""default"" entry in your map or you risk breaking. hintValueRead/hintValueRender maps allow you to use strings other than fully-qualified class names as a hint value. This can be very valuable if the ""discriminator"" field for your JSON is provided by a 3rd party, or you'd like to hide the internal details of hint-handling from an external system. They key to both maps is the fully-qualified trait name. The value of hintValueRead is a function that accepts a String (the ""friendly"" hint value) and emits a fully-qualified class name. In a simple implementation this may just prepend a package hierarchy, a la ""com.foo.something"", but it can be whatever you want. hintValueRender goes the other way. It's values are functions accepting a String (a fully-qualified class name) and emits a friendly hint value. View/SpliceInto Feature If you've ever had the need to support view ""projections"" to/from a large master object, these functions will help. Below is a contrived example of a User object that has some protected fields we don't want to easily project out (to make it save for a UI, for example): case class User(    name:String,    age:Int,    password:String,  // don't want password or ssn going up to the UI    ssn:String )  case class SafeUser(    name:String,    age:Int )  val safe = ScalaJack.view[SafeUser]( fullUser ) // fullUser is of type User, safe will be SafeUser Note that the field names and types of the view class (SafeUser above) must be exactly the same as the corresponding fields of the master class, and you shouldn't have extra fields that aren't present somewhere in the master class. Field order, however, is not important. You can also go the other way...incorporating data from a view object back into a master object. val updatedUser = ScalaJack.spliceInto( user, newSafeUser ) // updatedUser will be of type User Assumptions Case classes (or traits for case classes) only Options of value None are removed from generated JSON (e.g. from List or Map) Default parameters are not supported at this time Primitive/Simple Data types supported: Int Boolean Long Char Double Float String Byte Short java.util.UUID org.joda.time.DateTime Enumeration.Value Value Class Collections supported: scala.Option scala.collection.immutable.List scala.collection.immutable.Map scala.collection.immutable.Set scala.collection.immutable.HashMap scala.collection.immutable.HashSet scala.collection.immutable.ListMap scala.collection.immutable.ListSet scala.collection.immutable.Queue scala.collection.immutable.Seq scala.collection.immutable.Vector scala.collection.mutable.ArrayBuffer scala.collection.mutable.ArraySeq scala.collection.mutable.HashMap scala.collection.mutable.HashSet scala.collection.mutable.IndexedSeq scala.collection.mutable.LinearSeq scala.collection.mutable.LinkedHashMap scala.collection.mutable.LinkedHashSeq scala.collection.mutable.ListBuffer scala.collection.mutable.ListMap scala.collection.mutable.Map scala.collection.mutable.MutableList scala.collection.mutable.OpenHashMap scala.collection.mutable.Queue scala.collection.mutable.ResizableArray scala.collection.mutable.Seq scala.collection.mutable.Set scala.collection.mutable.Stack scala.collection.mutable.WeakHashMap New for v4.3! Limited support of Any type is supported. You can use primitives, Lists[Any], Maps[String,Any], and nested List/Map with Any. Don't get too cute, though. Type inference is pretty primal and limited to a few basic types: String, Int, Double, Boolean, and null. See v4/AnyTests.scala for examples. Why? The natual and expected question when developing a library for a function that already exists in the marketplace is ""Why?"". Jackson has its own Scala module, and there is also a wonderful library called Salat that I've been using for years that does JSON parsing for Scala. What does ScalaJack offer these two don't? Salat is very full-featured. It gives you a high level of control over the parsing process including custom serializers for non-standard types. Unlike a lot of JSON parsers that require ""helper"" code, and/or lots of annotations, Salat introspects Scala case classes and does it all almost completely automatically. After using Salat for a long time I began to be curious how its performance stacked up against other JSON parsers. (In complete fairness, Salat's JSON handling features evolved some time after its primary mission of MongoDB DAO access.) I discovered Jackson's relatively new Scala module and found it blazing fast, but... I didn't like the way Enumeration and Option types were handled. It also didn't handle traits that I could see (serializing Dog and Cat, where both are a case classes extending trait Animal, and the parser can sort them out). It was configurable enough--but required a lot of manual fidgeting with annotations and such. ScalaJack aimed for Jackson's speed and at least the key parts of Salat's seamless case class handling. ScalaJack is indeed faster than Salat (about twice as fast!) but losing nearly all of Salat's configurability. It does handle traits w/type hints seamlessly. Unlike Salat (at the time of this writing) ScalaJack also supports arbitrary nesting of data structures to allow you to construct sophisticated data structures with ease. If you're OK with gatining lots of speed at the price of my assumptions, ScalaJack is a great thing! Blöcke Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/gzoller/ScalaJack"	"Fast 'n easy JSON serialization with optional MongoDB support. Uses Jackson under the hood."	"true"
"JSON"	"sonofjson ★ 19 ⧗ 110"	"https://github.com/wspringer/sonofjson"	"A Scala library for dealing with JSON in a way that makes it almost feel native."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"21"	"2"	"5"	"GitHub - wspringer/sonofjson: Better JSON support for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 21 Fork 5 wspringer/sonofjson Code Issues 4 Pull requests 2 Pulse Graphs Better JSON support for Scala 58 commits 2 branches 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/map master Nothing to show Nothing to show New pull request Latest commit d23d400 Nov 4, 2014 wspringer Update travis. Permalink Failed to load latest commit information. project Clean up sbt Sep 21, 2014 src Hmmm, had to drop some functionality in order to support Scala 2.11. … Nov 4, 2014 .gitignore Ignoring all target directories Sep 21, 2014 .travis.yml Update travis. Nov 4, 2014 README.md Update README.md Nov 4, 2014 build.sbt Hmmm, had to drop some functionality in order to support Scala 2.11. … Nov 4, 2014 README.md SON of JSON A Scala library for dealing with JSON in a way that makes it almost feel native. If you want to understand how it compares to json4s, you might be interested to read about it here. ① It requires just one import import nl.typeset.sonofjson._ ② Creating an object is easy // You can parse it from a String val person = parse(""""""{ ""name"" : { ""first"" : ""John"", ""last"" : ""Doe"" } }"""""")  // Or build it yourself val person = obj(   name = obj(      first = ""John"",      last = ""Doe""   ) )  ③ Accessing it is easy too // Access the object and ask for a String representation person.name.first.as[String]  // Or leave it to SON of JSON to get you a String representation var first: String = person.name.first  ④ Modifying it is even easier person.name.first = ""Jack""  ⑤ And rendering it to JSON is just render(person)  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/wspringer/sonofjson"	"A Scala library for dealing with JSON in a way that makes it almost feel native."	"true"
"JSON"	"spray-json ★ 494 ⧗ 0"	"https://github.com/spray/spray-json"	"Lightweight, clean and efficient JSON implementation in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"527"	"29"	"111"	"GitHub - spray/spray-json: A lightweight, clean and simple JSON implementation in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 29 Star 527 Fork 111 spray/spray-json Code Issues 61 Pull requests 8 Pulse Graphs A lightweight, clean and simple JSON implementation in Scala 225 commits 6 branches 15 releases 26 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/customizable-jsonFormats feature/2.0.0 i/43-better-printing master release/1.x wip-mathias Nothing to show v1.3.2 v1.3.1 v1.3.0 v1.2.6 v1.2.5 v1.2.5-scala-2.11.0-RC3 v1.2.4 v1.2.3 v1.2.2 v1.2.1 v1.2 v1.1.1 v1.1.0 v1.0.1 v1.0.0 Nothing to show New pull request Latest commit 7277ddc Jul 7, 2016 sirthias Upgrade to Scala 2.11.8, 2.12.0-M5 and latest specs2 and scalacheck v… … …ersions Permalink Failed to load latest commit information. images Add source for Conversions.png and instructions how to edit, refs #95 Mar 13, 2014 notes Prepare for release 1.2.3 Nov 28, 2012 project = Upgrade dependencies, enable Scala 2.12 crossbuild Feb 17, 2016 src +#182 allows BigInt/BigDecimal to be obtained from JsStrings Apr 22, 2016 .gitignore Upgrade to SBT 0.11.0 Nov 9, 2011 .travis.yml In Travis run tests for all cross-built scala versions Mar 12, 2014 CHANGELOG Prepare for release 1.3.2 May 6, 2015 LICENSE Add README content, LICENSE and CHANGELOG stub May 9, 2011 README.markdown Prepare for release 1.3.2 May 6, 2015 build.sbt Upgrade to Scala 2.11.8, 2.12.0-M5 and latest specs2 and scalacheck v… Jul 7, 2016 README.markdown spray-json is a lightweight, clean and efficient JSON implementation in Scala. It sports the following features: A simple immutable model of the JSON language elements An efficient JSON parser Choice of either compact or pretty JSON-to-string printing Type-class based (de)serialization of custom objects (no reflection, no intrusion) No external dependencies spray-json allows you to convert between String JSON documents JSON Abstract Syntax Trees (ASTs) with base type JsValue instances of arbitrary Scala types as depicted in this diagram: Installation spray-json is available from the repo.spray.io repository. The latest release is 1.3.2 and is built against Scala 2.10.5 and Scala 2.11.6. If you use SBT you can include spray-json in your project with libraryDependencies += ""io.spray"" %%  ""spray-json"" % ""1.3.2"" Usage spray-json is really easy to use. Just bring all relevant elements in scope with import spray.json._ import DefaultJsonProtocol._ // if you don't supply your own Protocol (see below) and do one or more of the following: Parse a JSON string into its Abstract Syntax Tree (AST) representation val source = """"""{ ""some"": ""JSON source"" }"""""" val jsonAst = source.parseJson // or JsonParser(source) Print a JSON AST back to a String using either the CompactPrinter or the PrettyPrinter val json = jsonAst.prettyPrint // or .compactPrint Convert any Scala object to a JSON AST using the pimped toJson method val jsonAst = List(1, 2, 3).toJson Convert a JSON AST to a Scala object with the convertTo method val myObject = jsonAst.convertTo[MyObjectType] In order to make steps 3 and 4 work for an object of type T you need to bring implicit values in scope that provide JsonFormat[T] instances for T and all types used by T (directly or indirectly). The way you normally do this is via a ""JsonProtocol"". JsonProtocol spray-json uses SJSONs Scala-idiomatic type-class-based approach to connect an existing type T with the logic how to (de)serialize its instances to and from JSON. (In fact spray-json even reuses some of SJSONs code, see the 'Credits' section below). This approach has the advantage of not requiring any change (or even access) to Ts source code. All (de)serialization logic is attached 'from the outside'. There is no reflection involved, so the resulting conversions are fast. Scalas excellent type inference reduces verbosity and boilerplate to a minimum, while the Scala compiler will make sure at compile time that you provided all required (de)serialization logic. In spray-jsons terminology a 'JsonProtocol' is nothing but a bunch of implicit values of type JsonFormat[T], whereby each JsonFormat[T] contains the logic of how to convert instance of T to and from JSON. All JsonFormat[T]s of a protocol need to be ""mece"" (mutually exclusive, collectively exhaustive), i.e. they are not allowed to overlap and together need to span all types required by the application. This may sound more complicated than it is. spray-json comes with a DefaultJsonProtocol, which already covers all of Scala's value types as well as the most important reference and collection types. As long as your code uses nothing more than these you only need the DefaultJsonProtocol. Here are the types already taken care of by the DefaultJsonProtocol: Byte, Short, Int, Long, Float, Double, Char, Unit, Boolean String, Symbol BigInt, BigDecimal Option, Either, Tuple1 - Tuple7 List, Array immutable.{Map, Iterable, Seq, IndexedSeq, LinearSeq, Set, Vector} collection.{Iterable, Seq, IndexedSeq, LinearSeq, Set} JsValue In most cases however you'll also want to convert types not covered by the DefaultJsonProtocol. In these cases you need to provide JsonFormat[T]s for your custom types. This is not hard at all. Providing JsonFormats for Case Classes If your custom type T is a case class then augmenting the DefaultJsonProtocol with a JsonFormat[T] is really easy: case class Color(name: String, red: Int, green: Int, blue: Int)  object MyJsonProtocol extends DefaultJsonProtocol {   implicit val colorFormat = jsonFormat4(Color) }  import MyJsonProtocol._ import spray.json._  val json = Color(""CadetBlue"", 95, 158, 160).toJson val color = json.convertTo[Color] The jsonFormatX methods reduce the boilerplate to a minimum, just pass the right one the companion object of your case class and it will return a ready-to-use JsonFormat for your type (the right one is the one matching the number of arguments to your case class constructor, e.g. if your case class has 13 fields you need to use the jsonFormat13 method). The jsonFormatX methods try to extract the field names of your case class before calling the more general jsonFormat overloads, which let you specify the field name manually. So, if spray-json has trouble determining the field names or if your JSON objects use member names that differ from the case class fields you can also use jsonFormat directly. There is one additional quirk: If you explicitly declare the companion object for your case class the notation above will stop working. You'll have to explicitly refer to the companion objects apply method to fix this: case class Color(name: String, red: Int, green: Int, blue: Int) object Color  object MyJsonProtocol extends DefaultJsonProtocol {   implicit val colorFormat = jsonFormat4(Color.apply) } If your case class is generic in that it takes type parameters itself the jsonFormat methods can also help you. However, there is a little more boilerplate required as you need to add context bounds for all type parameters and explicitly refer to the case classes apply method as in this example: case class NamedList[A](name: String, items: List[A])  object MyJsonProtocol extends DefaultJsonProtocol {   implicit def namedListFormat[A :JsonFormat] = jsonFormat2(NamedList.apply[A]) } NullOptions The NullOptions trait supplies an alternative rendering mode for optional case class members. Normally optional members that are undefined (None) are not rendered at all. By mixing in this trait into your custom JsonProtocol you can enforce the rendering of undefined members as null. (Note that this only affect JSON writing, spray-json will always read missing optional members as well as null optional members as None.) Providing JsonFormats for other Types Of course you can also supply (de)serialization logic for types that aren't case classes. Here is one way to do it: class Color(val name: String, val red: Int, val green: Int, val blue: Int)  object MyJsonProtocol extends DefaultJsonProtocol {   implicit object ColorJsonFormat extends RootJsonFormat[Color] {     def write(c: Color) =       JsArray(JsString(c.name), JsNumber(c.red), JsNumber(c.green), JsNumber(c.blue))      def read(value: JsValue) = value match {       case JsArray(Vector(JsString(name), JsNumber(red), JsNumber(green), JsNumber(blue))) =>         new Color(name, red.toInt, green.toInt, blue.toInt)       case _ => deserializationError(""Color expected"")     }   } }  import MyJsonProtocol._  val json = Color(""CadetBlue"", 95, 158, 160).toJson val color = json.convertTo[Color] This serializes Color instances as a JSON array, which is compact but does not make the elements semantics explicit. You need to know that the color components are ordered ""red, green, blue"". Another way would be to serialize Colors as JSON objects: object MyJsonProtocol extends DefaultJsonProtocol {   implicit object ColorJsonFormat extends RootJsonFormat[Color] {     def write(c: Color) = JsObject(       ""name"" -> JsString(c.name),       ""red"" -> JsNumber(c.red),       ""green"" -> JsNumber(c.green),       ""blue"" -> JsNumber(c.blue)     )     def read(value: JsValue) = {       value.asJsObject.getFields(""name"", ""red"", ""green"", ""blue"") match {         case Seq(JsString(name), JsNumber(red), JsNumber(green), JsNumber(blue)) =>           new Color(name, red.toInt, green.toInt, blue.toInt)         case _ => throw new DeserializationException(""Color expected"")       }     }   } } This is a bit more verbose in its definition and the resulting JSON but transports the field semantics over to the JSON side. Note that this is the approach spray-json uses for case classes. JsonFormat vs. RootJsonFormat According to the JSON specification not all of the defined JSON value types are allowed at the root level of a JSON document. A JSON string for example (like ""foo"") does not constitute a legal JSON document by itself. Only JSON objects or JSON arrays are allowed as JSON document roots. In order to distinguish, on the type-level, ""regular"" JsonFormats from the ones producing root-level JSON objects or arrays spray-json defines the RootJsonFormat type, which is nothing but a marker specialization of JsonFormat. Libraries supporting spray-json as a means of document serialization might choose to depend on a RootJsonFormat[T] for a custom type T (rather than a ""plain"" JsonFormat[T]), so as to not allow the rendering of illegal document roots. E.g., the SprayJsonSupport trait of spray-routing is one notable example of such a case. All default converters in the DefaultJsonProtocol producing JSON objects or arrays are actually implemented as RootJsonFormat. When ""manually"" implementing a JsonFormat for a custom type T (rather than relying on case class support) you should think about whether you'd like to use instances of T as JSON document roots and choose between a ""plain"" JsonFormat and a RootJsonFormat accordingly. JsonFormats for recursive Types If your type is recursive such as case class Foo(i: Int, foo: Foo) you need to wrap your format constructor with lazyFormat and supply an explicit type annotation: implicit val fooFormat: JsonFormat[Foo] = lazyFormat(jsonFormat(Foo, ""i"", ""foo"")) Otherwise your code will either not compile (no explicit type annotation) or throw an NPE at runtime (no lazyFormat wrapper). Note, that lazyFormat returns a JsonFormat even if it was given a RootJsonFormat which means it isn't picked up by SprayJsonSupport. To get back a RootJsonFormat just wrap the complete lazyFormat call with another call to rootFormat. Credits Most of type-class (de)serialization code is nothing but a polished copy of what Debasish Ghosh made available with his SJSON library. These code parts therefore bear his copyright. Additionally the JSON AST model is heavily inspired by the one contributed by Jorge Ortiz to Databinder-Dispatch. License spray-json is licensed under APL 2.0. Mailing list Please use the spray-user mailing list if you have any questions. Patch Policy Feedback and contributions to the project, no matter what kind, are always very welcome. However, patches can only be accepted from their original author. Along with any patches, please state that the patch is your original work and that you license the work to the spray-json project under the project’s open source license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/spray/spray-json"	"Lightweight, clean and efficient JSON implementation in Scala."	"true"
"YAML"	"MoultingYAML ★ 15 ⧗ 13"	"https://github.com/jcazevedo/moultingyaml"	"Type-class based YAML serialization and deserialization on top of SnakeYAML."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"20"	"2"	"5"	"GitHub - jcazevedo/moultingyaml: A Scala wrapper for SnakeYAML Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 20 Fork 5 jcazevedo/moultingyaml Code Issues 0 Pull requests 0 Pulse Graphs A Scala wrapper for SnakeYAML 107 commits 1 branch 2 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.2 v0.1 Nothing to show New pull request Latest commit a5d0024 Jul 15, 2016 jcazevedo Remove unused imports Permalink Failed to load latest commit information. project Update dependencies Jul 7, 2016 src Remove unused imports Jul 15, 2016 .gitignore Add ensime files to .gitignore Jul 2, 2015 .travis.yml Update dependencies Jul 7, 2016 LICENSE.md Fix license date Nov 18, 2015 README.md Fix coveralls badge Jul 7, 2016 build.sbt Remove unused imports Jul 15, 2016 README.md MoultingYAML MoultingYAML is a Scala wrapper for SnakeYAML based on spray-json. Its basic idea is to provide a simple immutable model of the YAML language, built on top of SnakeYAML models, as well as a type-class based serialization and deserialization of custom objects. Installation MoultingYAML's latest release is 0.2 and is built against Scala 2.11.7. To use it in an existing SBT project, add the following dependency to your build.sbt: libraryDependencies += ""net.jcazevedo"" %% ""moultingyaml"" % ""0.2"" Usage In order to use MoultingYAML, bring all relevant elements into scope with: import net.jcazevedo.moultingyaml._ import net.jcazevedo.moultingyaml.DefaultYamlProtocol._ // if you don't supply your own protocol You can then parse a YAML string into its Abstract Syntax Tree (AST) representation with: val source = """"""- Mark McGwire                |- Sammy Sosa                |- Ken Griffey"""""".stripMargin val yamlAst = source.parseYaml It is also possible to print a YAML AST back to a String using the prettyPrint method: val yaml = yamlAst.prettyPrint If more fine-grained control over the printed yaml is needed, it is possible to use the configurable print method. For example, to enclose everything in double quotes: val yaml = yamlAst.print(scalarStyle = DoubleQuoted) YAML provide different scalar styles to choose from, controlled by the argument scalarStyle of the print method. The possible values for scalarStyle are Plain, SingleQuoted, DoubleQuoted, Literal and Folded. Refer to the YAML specification for details on each representation. In addition, YAML also has flow styles, in order to be able to use explicit indicators instead of indentation to denote scope. The flow style is controlled by the flowStyle argument of the print method. The possible values for flowStyle are Flow, Block and Auto. Block style uses indentation, whereas Flow style relies on explicit indicators to denote scope. The Auto flow style attempts to combine both the Block and Flow style within the same document. Scala objects can be converted to a YAML AST using the pimped toYaml method: val yamlAst = List(1, 2, 3).toYaml Convert a YAML AST to a Scala object with the convertTo method: val myList = yamlAst.convertTo[List[Int]] In order to support calling the toYaml and convertTo methods for an object of type T, you need to have implicit values in scope that provide YamlFormat[T] instances for T and all types used by T (directly or indirectly). You normally do that through a YamlProtocol. YamlProtocol YamlProtocols follow the same design as spray-json's JsonProtocols, which in turn are based on SJSON's. It's a type-class based approach that connects an existing type T with the logic of how to (de)serialize its instances to and from YAML. A YamlProtocol is a bunch of implicit values of type YamlFormat[T], where each YamlFormat[T] contains the logic of how to convert instances of T to and from YAML. MoultingYAML comes with a DefaultYamlProtocol, which already covers all of Scala's value types as well as the most important reference and collection types. The following are types already taken care of by the DefaultYamlProtocol: Byte, Short, Int, Long, Float, Double, Char, Unit, Boolean String, Symbol BigInt, BigDecimal Option, Either, Tuple1 - Tuple7 List, Array immutable.{Map, Iterable, Seq, IndexedSeq, LinearSeq, Set, Vector} collection.{Iterable, Seq, IndexedSeq, LinearSeq, Set} When you want to convert types not covered by the DefaultYamlProtocol, you need to provide a YamlFormat[T] for your custom types. Prodiving YamlFormats for Case Classes If your custom type T is a case class then augmenting the DefaultYamlProtocol with a YamlFormat[T] can be done using the yamlFormatX helpers, where X stands for the number of fields in the case class: case class Color(name: String, red: Int, green: Int, blue: Int)  object MyYamlProtocol extends DefaultYamlProtocol {   implicit val colorFormat = yamlFormat4(Color) }  import MyYamlProtocol._ import net.jcazevedo.moultingyaml._  val yaml = Color(""CadetBlue"", 95, 158, 160).toYaml val color = yaml.convertTo[Color] If you explicitly declare the companion object for your case class the notation above will stop working. You'll have to explicitly refer to the companion object's apply method to fix this: case class Color(name: String, red: Int, green: Int, blue: Int) object Color  object MyYamlProtocol extends DefaultYamlProtocol {   implicit val colorFormat = yamlFormat4(Color.apply) } If your case class has a type parameter the yamlFormat methods can also help you. However, there is a little more boilerplate required as you need to add context bounds for all type parameters and explicitly refer to the case classes apply method as in this example: case class NamedList[A](name: String, items: List[A])  object MyYamlProtocol extends DefaultYamlProtocol {   implicit def namedListFormat[A: YamlFormat] = yamlFormat2(NamedList.apply[A]) } NullOptions As in spray-json, the NullOptions trait supplies an alternative rendering mode for optional case class members. Normally optional members that are undefined (None) are not rendered at all. By mixing in this trait into your custom YamlProtocol you can enforce the rendering of undefined members as null. (Note that this only affects YAML writing, MoultingYAML will always read missing optional members as well as null optional members as None) Providing YamlFormats for other Types To provide (de)serialization logic for types that aren't case classes, one has to define the write and read methods of YamlFormat. Here is one example: class Color(val name: String, val red: Int, val green: Int, val blue: Int)  object MyYamlProtocol extends DefaultYamlProtocol {   implicit object ColorYamlFormat extends YamlFormat[Color] {     def write(c: Color) =       YamlArray(         YamlString(c.name),         YamlNumber(c.red),         YamlNumber(c.green),         YamlNumber(c.blue))      def read(value: YamlValue) = value match {       case YamlArray(         Vector(           YamlString(name),           YamlNumber(red: Int),           YamlNumber(green: Int),           YamlNumber(blue: Int))) =>         new Color(name, red, green, blue)       case _ => deserializationError(""Color expected"")     }   } }  import MyYamlProtocol._  val yaml = new Color(""CadetBlue"", 95, 158, 160).toYaml val color = yaml.convertTo[Color] This serializes Color instances as a YAML array. Another way would be to serialize Colors as YAML mappings, which are called YAML objects in MoultingYAML: object MyYamlProtocol extends DefaultYamlProtocol {   implicit object ColorYamlFormat extends YamlFormat[Color] {     def write(c: Color) = YamlObject(       YamlString(""name"") -> YamlString(c.name),       YamlString(""red"") -> YamlNumber(c.red),       YamlString(""green"") -> YamlNumber(c.green),       YamlString(""blue"") -> YamlNumber(c.blue)     )     def read(value: YamlValue) = {       value.asYamlObject.getFields(         YamlString(""name""),         YamlString(""red""),         YamlString(""green""),         YamlString(""blue"")) match {         case Seq(           YamlString(name),           YamlNumber(red: Int),           YamlNumber(green: Int),           YamlNumber(blue: Int)) =>           new Color(name, red, green, blue)         case _ => deserializationError(""Color expected"")       }     }   } } Credits Most of MoultingYAML's type-class (de)serialization code was inspired by spray-json, by Mathias Doenitz. spray-json was, in turn, inspired by the SJSON library by Debasish Ghosh. Both deserve credits here. License MoultingYAML is licensed under the MIT license. See LICENSE.md for details. Contributions Feedback and contributions to the project are very welcome. Use GitHub's issue tracker to report any issues you might have when using the project. Submit code contributions via GitHub's pull requests. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/jcazevedo/moultingyaml"	"Type-class based YAML serialization and deserialization on top of SnakeYAML."	"true"
"Serialization"	"Chill ★ 327 ⧗ 0"	"https://github.com/twitter/chill"	"Extensions for the Kryo serialization library to ease configuration in systems like Hadoop and Storm."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"345"	"169"	"79"	"GitHub - twitter/chill: Scala extensions for the Kryo serialization library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 169 Star 345 Fork 79 twitter/chill Code Issues 27 Pull requests 4 Wiki Pulse Graphs Scala extensions for the Kryo serialization library https://twitter.com/scalding 534 commits 16 branches 34 releases Fetching contributors Scala 49.0% Java 46.2% Shell 4.7% Protocol Buffer 0.1% Scala Java Shell Protocol Buffer Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags 0.7.x add-coveralls add-stream coverage develop-0.4 develop-0.4.1 develop-0.4.2 develop feature/add_akka feature/chill_all_update feature/scala210 ianoc/kryoUpgrade master oscar/0.7.3 release/chill-0.7.2-RC1 release/0.7.0 Nothing to show v0.8.0 v0.7.4 v0.7.3 help 0.8.0 0.7.4 0.7.2 0.7.1 0.7.0 0.6.0 0.5.2 0.5.1 0.5.0 0.4.1 0.4.0 0.3.6 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 0.2.3 0.2.2 0.2.1 0.2.0 0.1.4 0.1.3 0.1.2 0.1.1 0.1.0 0.0.4 0.0.3 0.0.2 Nothing to show New pull request Latest commit f5b28ca Feb 11, 2016 ianoc Setting version to 0.8.1-SNAPSHOT Permalink Failed to load latest commit information. chill-akka/src Make 2.10.4 the default, move to scalatest Aug 28, 2014 chill-algebird/src Make 2.10.4 the default, move to scalatest Aug 28, 2014 chill-avro/src fix Nov 7, 2014 chill-bijection/src Review comments Aug 17, 2015 chill-hadoop Merge branch 'develop' of github.com:twitter/chill into Kryo3Upgrade Feb 11, 2016 chill-java Merge branch 'develop' of github.com:twitter/chill into Kryo3Upgrade Feb 11, 2016 chill-protobuf/src Make 2.10.4 the default, move to scalatest Aug 28, 2014 chill-scala/src Use Try rather than Either Feb 10, 2016 chill-scrooge/src/main/scala/com/twitter/chill/scrooge Use matching classloader in scrooge hack Dec 26, 2015 chill-storm/src/main/java/com/twitter/chill/storm Fixes a few issues found with testing scalding Jul 23, 2013 chill-thrift/src/main/java/com/twitter/chill/thrift Adds chill-thrift Nov 4, 2013 project Update the build Feb 1, 2016 scripts Add IKryoRegistrar and use it Jul 8, 2013 .gitignore Add to CHANGES.md Feb 2, 2016 .travis.yml Build docs in CI Feb 11, 2016 CHANGES.md Add changes Feb 11, 2016 LICENSE initial serializers. Oct 15, 2012 NOTICE add notice and readme. Oct 15, 2012 README.md Add changes Feb 11, 2016 build.sbt Merge branch 'develop' into Kryo3Upgrade Feb 9, 2016 sbt Update CHANGES, readme, version and sbt versions in prep for rls Oct 22, 2015 version.sbt Setting version to 0.8.1-SNAPSHOT Feb 11, 2016 README.md Chill Extensions for the Kryo serialization library including serializers and a set of classes to ease configuration of Kryo in systems like Hadoop, Storm, Akka, etc. Building Chill ./sbt > compile # to build chill > publishM2 # to publish chill to your local .m2 repo > publish-local # publish to local ivy repo. Chill has a set of subprojects: chill-java, chill-hadoop, chill-storm and chill-scala. Other than chill-scala, all these projects are written in Java so they are easy to use on any JVM platform. Chill-Java The chill-java package includes the KryoInstantiator class (factory for Kryo instances) and the IKryoRegistrar interface (adds Serializers to a given Kryo). These two are composable to build instantiators that create instances of Kryo that have the options and serializers you need. The benefit of this over a direct Kryo instance is that a Kryo instance is mutable and not serializable, which limits the safety and reusability of code that works directly with them. To deserialize or serialize easily, look at KryoPool: int POOL_SIZE = 10; KryoPool kryo = KryoPool.withByteArrayOutputStream(POOL_SIZE, new KryoInstantiator()); byte[] ser = kryo.toBytesWithClass(myObj); Object deserObj = kryo.fromBytes(myObj); The KryoPool is a thread-safe way to share Kryo instances and temporary output buffers. Chill Config Hadoop, Storm, and Akka all use a configuration that is basically equivalent to a Map[String, String]. The com.twitter.chill.config package makes it easy to build up KryoInstantiator instances given a Config instance, which is an abstract class acting as a thin wrapper over whatever configuration data the system, such as Hadoop, Storm or Akka, might give. To configure a KryoInstantiator use ConfiguredInstantiator with either reflection, which takes a class name and instantiates that KryoInstantiator, or an instance of KryoInstantiator and serializes that instance to use later: class TestInst extends KryoInstantiator { override def newKryo = sys.error(""blow up"") }  // A new Config: val conf = new JavaMapConfig // Set-up class-based reflection of our instantiator: ConfiguredInstantiator.setReflect(conf, classOf[TestInst]) val cci = new ConfiguredInstantiator(conf) cci.newKryo // uses TestInst //Or serialize a particular instance into the config to use later (or another node):  ConfiguredInstantiator.setSerialized(conf, new TestInst) val cci2 = new ConfiguredInstantiator(conf) cci2.newKryo // uses the particular instance we passed above Chill in Scala Scala classes often have a number of properties that distinguish them from usual Java classes. Often scala classes are immutable, and thus have no zero argument constructor. Secondly, object in scala is a singleton that needs to be carefully serialized. Additionally, scala classes often have synthetic (compiler generated) fields that need to be serialized, and by default Kryo does not serialize those. In addition to a ScalaKryoInstantiator which generates Kryo instances with options suitable for scala, chill provides a number of Kryo serializers for standard scala classes (see below). The MeatLocker Many existing systems use Java serialization. MeatLocker is an object that wraps a given instance using Kryo serialization internally, but the MeatLocker itself is Java serializable. The MeatLocker allows you to box Kryo-serializable objects and deserialize them lazily on the first call to get: import com.twitter.chill.MeatLocker  val boxedItem = MeatLocker(someItem)  // boxedItem is java.io.Serializable no matter what it contains. val box = roundTripThroughJava(boxedItem) box.get == boxedItem.get // true! To retrieve the boxed item without caching the deserialized value, use meatlockerInstance.copy. Serializers for Scala classes These are found in the chill-scala directory in the chill jar (originally this project was only scala serializers). Chill provides support for singletons, scala Objects and the following types: Scala primitives scala.Enumeration values scala.Symbol scala.reflect.Manifest scala.reflect.ClassManifest scala.Function[0-22] closure cleaning (removing unused $outer references). Collections and sequences scala.collection.immutable.Map scala.collection.immutable.List scala.collection.immutable.Vector scala.collection.immutable.Set scala.collection.mutable.{Map, Set, Buffer, WrappedArray} all 22 scala tuples Chill-bijection Bijections and Injections are useful when considering serialization. If you have an Injection from T to Array[Byte] you have a serialization. Additionally, if you have a Bijection between A and B, and a serialization for B, then you have a serialization for A. See BijectionEnrichedKryo for easy interop between bijection and chill. KryoInjection: easy serialization to byte Arrays KryoInjection is an injection from Any to Array[Byte]. To serialize using it: import com.twitter.chill.KryoInjection  val bytes:  Array[Byte]    = KryoInjection(someItem) val tryDecode: scala.util.Try[Any] = KryoInjection.invert(bytes) KryoInjection can be composed with Bijections and Injections from com.twitter.bijection. Chill-Akka To use, add a key to your config like:     akka.actor.serializers {       kryo = ""com.twitter.chill.akka.AkkaSerializer""     }  Then for the super-classes of all your message types, for instance, scala.Product, write:    akka.actor.serialization-bindings {      ""scala.Product"" = kryo    } If you want to use the chill.config.ConfiguredInstantiator see ConfiguredAkkaSerializer otherwise, subclass AkkaSerializer and override kryoInstantiator to control how the Kryo object is created. Community and Documentation This, and all github.com/twitter projects, are under the Twitter Open Source Code of Conduct. Additionally, see the Typelevel Code of Conduct for specific examples of harassing behavior that are not tolerated. To learn more and find links to tutorials and information around the web, check out the Chill Wiki. The latest ScalaDocs are hosted on Chill's Github Project Page. Discussion occurs primarily on the Chill mailing list. Issues should be reported on the GitHub issue tracker. Maven Chill modules are available on Maven Central. The current groupid and version for all modules is, respectively, ""com.twitter"" and 0.8.0 and each scala project is published for 2.10 and 2.11. Search search.maven.org when in doubt. Authors Oscar Boykin https://twitter.com/posco Mike Gagnon https://twitter.com/MichaelNGagnon Sam Ritchie https://twitter.com/sritchie License Copyright 2012 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/chill"	"Extensions for the Kryo serialization library to ease configuration in systems like Hadoop and Storm."	"true"
"Serialization"	"Pickling ★ 729 ⧗ 3"	"https://github.com/scala/pickling"	"Fast, customizable, boilerplate-free pickling support."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"769"	"63"	"72"	"GitHub - scala/pickling: Fast, customizable, boilerplate-free pickling support for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 63 Star 769 Fork 72 scala/pickling Code Issues 106 Pull requests 7 Wiki Pulse Graphs Fast, customizable, boilerplate-free pickling support for Scala http://lampwww.epfl.ch/~hmiller/pickling 999 commits 25 branches 15 releases 15 contributors Scala 99.8% Java 0.2% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 0.11.x Switch branches/tags Branches Tags 0.8.x 0.9.x 0.10.x 0.10.2-snapshot 0.11.x 2.10.x debugging-existentials docs issue-128-original issue-158 issue/runtime-singleton issue/128 master oopsla2013-master oopsla2013 pickle-versioning revert-293-topic/uuid topic/perf-problem topic/wip-issue-15 update-readme wip-NIO-ByteBuffer wip/hash-map-picklers wip/java-picklers wip/macro-runtime-pickler-generator wip/validating-pickle-format-and-tests Nothing to show v0.11.0-M2 v0.11.0-M1 v0.10.1_2.11 v0.10.1_2.10 v0.10.0_2.11 v0.10.0_2.10 v0.10.0-M4 v0.10.0-M3 v0.10.0-M1 v0.9.1_2.11 v0.9.1_2.10 v0.9.0_2.11 v0.9.0_2.10 v0.8.0_2.11 v0.8.0_2.10 Nothing to show New pull request Latest commit e2fb4f3 Jul 5, 2016 jsuereth committed on GitHub Merge pull request #435 from jvican/issue-385 … Add test case for #385 Permalink Failed to load latest commit information. benchmark Fix compat error in 2.10 and wikigraph issues May 24, 2016 core/src Merge pull request #435 from jvican/issue-385 Jul 5, 2016 macro-test/src Minor cleanup of FastTypeTag changes to ensure all tag logic goes thr… Apr 12, 2016 notes Fixes from review / start of release notes. Mar 30, 2016 project Update scalatest and fix scalatest issue 873 May 7, 2016 sandbox-test/src/test/scala/scala/pickling Add custom runtime generators for Either May 5, 2016 test-util/src/test/java/pickling Add facility to check Java pickling in negative test Feb 12, 2015 .gitattributes new starr with the Iterable[_] fix Mar 12, 2013 .gitignore WIP, a new go at reworking the IR Aug 15, 2014 .mailmap new starr with the Iterable[_] fix Mar 12, 2013 .travis.yml Add jorge to travis notifications Apr 17, 2016 LICENSE LICENSE cleanup Nov 18, 2014 README.md Warn people not to use 0.11.0-M1 Jun 21, 2016 build.sbt Set version to second milestone release `0.11.0-M2` Jun 1, 2016 notes.md Implemented the beginnigns of unpickler logic. Jul 30, 2015 README.md scala/pickling Scala Pickling is an automatic serialization framework made for Scala. It's fast, boilerplate-free, and allows users to easily swap in/out different serialization formats (such as binary, or JSON), or even to provide their own custom serialization format. Defaults mode scala> import scala.pickling.Defaults._, scala.pickling.json._ scala> case class Person(name: String, age: Int)  scala> val pkl = Person(""foo"", 20).pickle pkl: pickling.json.pickleFormat.PickleType = JSONPickle({   ""$type"": ""Person"",   ""name"": ""foo"",   ""age"": 20 })  scala> val person = pkl.unpickle[Person] person: Person = Person(foo,20) For more, flip through, or watch the ScalaDays 2013 presentation! For deeper technical details, we've also written an OOPSLA 2013 paper on scala/pickling, Instant Pickles: Generating Object-Oriented Pickler Combinators for Fast and Extensible Serialization. Get Scala Pickling Scala Pickling is available on Sonatype for Scala 2.10 and Scala 2.11! You can use Scala Pickling in your sbt project by simply adding the following dependency to your build file: libraryDependencies += ""org.scala-lang.modules"" %% ""scala-pickling"" % ""0.10.1"" Please, don't use the version 0.11.0-M1 since it's not production ready and it's still under ongoing development. What makes it different? Scala Pickling... can be Language-Neutral if you want it to be. Changing the format of your serialized data is as easy as importing the correct implicit pickle format into scope. Out of the box, we currently support a fast Scala binary format, as well as JSON. Support is currently planned for other formats. Or, you can even roll your own custom pickle format! is Automatic. That is, without any boilerplate at all, one can instruct the framework to figure out how to serialize an arbitrary class instance. No need to register classes, no need to implement any methods. Allows For Unanticipated Evolution. That means that you don’t have to extend some marker trait in order to serialize a given Scala class. Just import the scala.pickling package and call pickle on the instance that you would like to serialize. gives you more Typesafety. No more errors from serialization/deserialization propagating to arbitrary points in your program. Unlike Java Serialization, errors either manifest themselves as compile-time errors, or runtime errors only at the point of unpickling. has Robust Support For Object-Orientation. While Scala Pickling is based on the elegant notion of pickler combinators from functional programming, it goes on to extend the traditional form of pickler combinators to be able to handle open class hierarchies. That means that if you pickle an instance of a subclass, and then try to unpickle as a superclass, you will still get back an instance of the original subclass. Happens At Compile-Time. That means that it’s super-performant because serialization-related code is typically generated at compile-time and inlined where it is needed in your code. Scala Pickling is essentially fully-static, reflection is only used as a fallback when static (compile-time) generation fails. Optimizing performance Pickling enables optimizing performance through configuration, in case the pickled objects are known to be simpler than in the general case. Disabling cyclic object graphs By default, Pickling can serialize cyclic object graphs (for example, for serializing doubly-linked lists). However, this requires bookkeeping at run time. If pickled objects are known to be not cyclic (for example, simple lists or trees), then this additional bookkeeping can be disabled using the following import: import scala.pickling.shareNothing._ If objects are pickled in a tight loop, this import can lead to a significant performance improvement. Static serialization without reflection To pickle objects of types like Any Pickling uses run-time reflection, since not enough information is available at compile time. However, Pickling supports a static-only mode that ensures no run-time reflection is used. In this mode, pickling objects that would otherwise require run-time reflection causes compile-time errors. The following import enables static-only serialization: import scala.pickling.static._  // Avoid run-time reflection A la carte import If you want, Pickling lets you import specific parts (functions, ops, picklers, and format) so you can customize each part. import scala.pickling._         // This imports names only import scala.pickling.json._    // Imports PickleFormat import scala.pickling.static._  // Avoid runtime pickler  // Import pickle ops import scala.pickling.Defaults.{ pickleOps, unpickleOps }  // Alternatively import pickle function // import scala.pickling.functions._  // Import picklers for specific types import scala.pickling.Defaults.{ stringPickler, intPickler, refPicklerUnpickler, nullPickler }  case class Pumpkin(kind: String) // Manually generate a pickler using macro implicit val pumpkinPickler = Pickler.generate[Pumpkin] implicit val pumpkinUnpickler = Unpickler.generate[Pumpkin]  val pckl = Pumpkin(""Kabocha"").pickle val pump = pckl.unpickle[Pumpkin] DIY protocol stack There are also traits available for picklers to mix and match your own convenience object to import from. If you're a library author, you can provide the convenience object as your protocol stack that some or all of the pickling parts: ops functions picklers format scala> case class Apple(kind: String) defined class Apple  scala> val appleProtocol = {      |              import scala.pickling._      |              new pickler.PrimitivePicklers with pickler.RefPicklers      |                  with json.JsonFormats {      |                // Manually generate pickler for Apple      |                implicit val applePickler = PicklerUnpickler.generate[Apple]      |                // Don't fall back to runtime picklers      |                implicit val so = static.StaticOnly      |                // Provide custom functions      |                def toJsonString[A: Pickler](a: A): String =      |                  functions.pickle(a).value      |                def fromJsonString[A: Unpickler](s: String): A =      |                  functions.unpickle[A](json.JSONPickle(s))      |              }      |            } appleProtocol: scala.pickling.pickler.PrimitivePicklers with scala.pickling.pickler.RefPicklers with scala.pickling.json.JsonFormats{implicit val applePickler: scala.pickling.Pickler[Apple] with scala.pickling.Unpickler[Apple] with scala.pickling.Generated; implicit val so: scala.pickling.static.StaticOnly.type; def toJsonString[A](a: A)(implicit evidence$1: scala.pickling.Pickler[A]): String; def fromJsonString[A](s: String)(implicit evidence$2: scala.pickling.Unpickler[A]): A} = $anon$1@2b033c35 Now your library user can import appleProtocol as follows: scala> import appleProtocol._ import appleProtocol._  scala>  toJsonString(Apple(""honeycrisp"")) res0: String = {   ""$type"": ""Apple"",   ""kind"": ""honeycrisp"" }  scala> fromJsonString(res0) res1: Apple = Apple(honeycrisp)  Other ways of getting Pickling If you would like to run the latest development version of scala/pickling (0.10.2-SNAPSHOT), you also need to add the Sonatype ""snapshots"" repository resolver to your build file: libraryDependencies += ""org.scala-lang.modules"" %% ""scala-pickling"" % ""0.10.2-SNAPSHOT""  resolvers += Resolver.sonatypeRepo(""snapshots"") For a more illustrative example, see a sample sbt project which uses Scala Pickling. Or you can just directly download the 0.10.1 jar (Scala 2.10, Scala 2.11). Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala/pickling"	"Fast, customizable, boilerplate-free pickling support."	"true"
"Serialization"	"ScalaBuff ★ 196 ⧗ 11"	"https://github.com/SandroGrzicic/ScalaBuff"	"a Scala Protocol Buffers (protobuf) compiler"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"206"	"20"	"67"	"GitHub - SandroGrzicic/ScalaBuff: the scala protocol buffers (protobuf) compiler Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 20 Star 206 Fork 67 SandroGrzicic/ScalaBuff Code Issues 36 Pull requests 4 Wiki Pulse Graphs the scala protocol buffers (protobuf) compiler 351 commits 2 branches 15 releases 18 contributors Scala 96.8% Protocol Buffer 3.2% Scala Protocol Buffer Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 1.4.0 1.3.9 1.3.8 1.3.7 1.3.6 1.3.5 1.3.4 1.3.2 1.3.1 1.3.0 1.2.2 1.2.1 1.2.0 1.1.2 1.1.1 Nothing to show New pull request Latest commit cd0b8be Mar 8, 2016 SandroGrzicic Merge pull request #122 from jasonf20/master … SpeedOptimization: Generate `while` instead of `for` when optimizeForSpeed is on Permalink Failed to load latest commit information. project Add sbt assembly plugin for standalone jar Mar 4, 2016 scalabuff-compiler/src Merge pull request #122 from jasonf20/master Mar 8, 2016 scalabuff-runtime/src/main/net/sandrogrzicic/scalabuff Removed support for Scala 2.9. Upgraded version of ScalaTest. Fixed v… Feb 10, 2015 .gitignore Merge pull request #93 from Castaglia/use-defaults-if-unknown-enum-ex… Feb 9, 2015 LICENSE Updated license year. Mar 4, 2013 README.md Update documentation Mar 4, 2016 README.md ScalaBuff is a Scala Protocol Buffers (protobuf) compiler. It takes .proto files and outputs valid Scala classes that can be used by your code to receive or send protobuf messages. Both the ScalaBuff generator and the generated Scala classes depend on Google's Java runtime for Protocol Buffers, which is provided with ScalaBuff. If you want to utilize ScalaBuff to generate your Scala classes from .proto sources, you'll need to either download the source or download the packaged JAR for your Scala version from the Sonatype OSS repository. If you download the sources, you can easily run it from SBT or by generating a fat Jar with sbt assembly: $ sbt assembly $ java -jar target/scala-2.11/scalabuff-compiler-assembly-1.4.0.jar --proto_path=INPUT_DIR --scala_out=OUTPUT_DIR If you just want to use ScalaBuff-generated classes in your SBT-managed project, here's the dependency to add (located on the Sonatype OSS repository): ""net.sandrogrzicic"" %% ""scalabuff-runtime"" % ""[desired_version]"" The latest release is 1.4.0 with support for Scala 2.10 and 2.11. If you'd like to use SBT with ScalaBuff to auto-generate Scala protobuf classes from .proto sources, try the sbt-scalabuff project. The ScalaBuff Wiki contains more information. For API documentation, see the project Scaladoc. For any questions or general discussion, you can use the ScalaBuff Google Group but please feel free to create new issues for bug reports or feature requests. Thanks! Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/SandroGrzicic/ScalaBuff"	"a Scala Protocol Buffers (protobuf) compiler"	"true"
"Serialization"	"ScalaPB"	"http://trueaccord.github.io/ScalaPB/"	"A Protocol Buffer generator for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"223"	"34"	"35"	"GitHub - trueaccord/ScalaPB: Protocol buffer compiler for Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 34 Star 223 Fork 35 trueaccord/ScalaPB Code Issues 7 Pull requests 0 Pulse Graphs Protocol buffer compiler for Scala. http://trueaccord.github.io/ScalaPB/ 446 commits 11 branches 56 releases 14 contributors Scala 95.6% Protocol Buffer 4.3% Shell 0.1% Scala Protocol Buffer Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.4.x gh-pages grpc lens master packed_enums parser-wip parsing-fp parsing-wip protoc-jar win Nothing to show v0.5.33 v0.5.32 v0.5.31 v0.5.30 v0.5.29 v0.5.28 v0.5.27 v0.5.26 v0.5.25 v0.5.24 v0.5.23 v0.5.22 v0.5.21 v0.5.20 v0.5.19 v0.5.18 v0.5.17 v0.5.16 v0.5.15 v0.5.14 v0.5.13 v0.5.12 v0.5.11 v0.5.10 v0.5.9 v0.5.8 v0.5.7 v0.5.6 v0.5.5 v0.5.1 v0.5.0 v0.4.21 v0.4.20 v0.4.19 v0.4.18 v0.4.17 v0.4.16 v0.4.15 v0.4.14 v0.4.13 v0.4.12 v0.4.11 v0.4.10 v0.4.9 v0.4.8 v0.4.7 v0.4.6 v0.4.5 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0 v0.3.4 v0.3.3 v0.3.2 Nothing to show New pull request Latest commit 7c941fe Jul 14, 2016 thesamet Setting version to 0.5.34-SNAPSHOT Permalink Failed to load latest commit information. .github Add ISSUE_TEMPLATE.md May 29, 2016 compiler-plugin Add basic support for Any Jul 14, 2016 e2e Add basic support for Any Jul 14, 2016 examples Update scalapb.sbt Jun 28, 2016 project Upgrade protobuf-runtime-scala to 0.1.10 Jun 25, 2016 proptest/src/test/scala Fix support for map<> for proto2. Jun 25, 2016 protobuf/scalapb Add option to Convert wrapper types to optional primitives. Jun 25, 2016 scalapb-runtime-grpc/src/main/scala/com/trueaccord/scalapb/grpc Expose service descriptors in companion object. May 7, 2016 scalapb-runtime Add basic support for Any Jul 14, 2016 scalapbc Scalapbc: accept protoc version as first command line argument. May 30, 2016 third_party/google/protobuf Add Google's well-known protocol buffers. Jul 13, 2016 .gitignore Initial work on custom options. Jun 5, 2016 .travis.yml Upgrade to Scala 2.11.8 Jun 25, 2016 CHANGELOG.md Add auto-generated changelog Jun 12, 2016 LICENSE Initial commit Oct 1, 2014 README.md Fix typo Jul 9, 2016 build.sbt Add basic support for Any Jul 14, 2016 e2e.sh Upgrade to Scala 2.11.8 Jun 25, 2016 make_plugin_proto.sh Make directory names not have backticks when reserved words are used Jul 13, 2016 sonatype.sbt Upgrade release plugins. Nov 15, 2015 version.sbt Setting version to 0.5.34-SNAPSHOT Jul 14, 2016 README.md ScalaPB ScalaPB is a protocol buffer compiler (protoc) plugin for Scala. It will generate Scala case classes, parsers and serializers for your protocol buffers. ScalaPB generates case classes that can co-exist in the same project alongside the Java-generated code for ProtocolBuffer. This makes it easy to gradually migrate an existing project from the Java version of protocol buffers to Scala. This is acheived by having the ScalaPB generated code use the proto file as part of the package name (in contrast to Java which uses the file name in CamelCase as an outer class) Each top-level message and enum is written to a separate Scala file. This results in a significant improvement in incremental compilations. Another cool feature of ScalaPB is that it can optionally generate methods that convert a Java protocol buffer to a Scala protocol buffer and vice versa. This is useful if you are gradually migrating a large code base from Java protocol buffers to Scala. The optional Java conversion is required if you want to use fromAscii (parsing ASCII representation of a protocol buffer). The current implementation delegates to the Java version. Highlights Supports proto2 and proto3 Easily update nested structure in functional way using lenses Scala.js integration GRPC integration Compatible with SparkSQL (through a helper library) Conversion to and from JSON Support user-defined options (in 0.5.29) Versions Version Description 0.4.x Stable, works with Protobuf 2.6.x 0.5.x Unstable development version. 0.6.x To be released. Supports Protobuf 2.6.x and Protobuf 3.0.x Installing To automatically generate Scala case classes for your messages add ScalaPB's sbt plugin to your project. Create a file named project/scalapb.sbt containing the following line: addSbtPlugin(""com.trueaccord.scalapb"" % ""sbt-scalapb"" % ""0.4.20"")  Add the following line to your build.sbt: import com.trueaccord.scalapb.{ScalaPbPlugin => PB}  PB.protobufSettings  For additional configuration options, see ScalaPB SBT Settings documentation Using ScalaPB Documentation is available at ScalaPB website. Testing ScalaPB uses ScalaCheck to aggressively test the generated code. The test generates many different sets of proto files. The sets are growing in complexity: number of files, references to messages from other protos, message nesting and so on. Then, test data is generated to populate this protocol schema, then we check that the ScalaPB generated code behaves exactly like the reference implementation in Java. Running the tests: $ sbt test  The tests take a few minutes to run. There is a smaller test suite called e2e that uses the sbt plugin to compile the protos and runs a series of ScalaChecks on the outputs. To run it: $ ./e2e.sh  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/trueaccord/ScalaPB"	"A Protocol Buffer generator for Scala."	"true"
"Serialization"	"scodec ★ 382 ⧗ 2"	"https://github.com/scodec/scodec"	"A combinator library for working with binary data."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"405"	"36"	"52"	"GitHub - scodec/scodec: Scala combinator library for working with binary data Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 36 Star 405 Fork 52 scodec/scodec Code Issues 3 Pull requests 0 Wiki Pulse Graphs Scala combinator library for working with binary data http://scodec.org 644 commits 16 branches 39 releases 23 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: series/1.10.x Switch branches/tags Branches Tags gh-pages series/1.0.x series/1.1.x series/1.2.x series/1.3.x series/1.4.x series/1.5.x series/1.6.x series/1.7.x series/1.8.x-2.12 series/1.8.x series/1.9.x series/1.10.x topic/int-perf topic/sjs v1.0.x Nothing to show v1.10.2 v1.10.1 v1.10.0 v1.9.0 v1.8.3 v1.8.2 v1.8.1 v1.8.0 v1.8.0-for-2.12.0-M1 v1.7.2 v1.7.1 v1.7.0 v1.7.0-RC2 v1.7.0-RC1 v1.6.0 v1.5.0 v1.4.0 v1.3.2 v1.3.1 v1.3.0 v1.2.2 v1.2.1 v1.2.0 v1.1.0 v1.1.0-M1 v1.0.0 v1.0.0-for-211 v1.0.0-SNAP7 v1.0.0-SNAP6 v1.0.0-SNAP5 v1.0.0-RC2 v1.0.0-RC1 v1.0.0-M3 v1.0.0-M2 v1.0.0-M1 rm 1.0.0.SNAP4 1.0.0.SNAP2 1.0.0.SNAP1 Nothing to show New pull request Latest commit 1b51857 Jul 4, 2016 mpilquist Setting version to 1.10.3-SNAPSHOT Permalink Failed to load latest commit information. benchmark/src/main/scala Minor optimizations to UUID codec Jul 29, 2015 js/src/main/scala/scodec split off Charset support for JVM and JS Sep 5, 2015 jvm/src Upgraded to latest scodec-build release and addressed warnings Feb 8, 2016 licenses Remove dependency on scalaz-core Jan 5, 2015 project Prep for 1.10.2 build Jul 4, 2016 shared/src Decode correctly for odd-nibble-sized codecs Jun 27, 2016 .gitignore Initial commit Apr 22, 2013 .sbtopts Add SBT options Sep 5, 2015 .travis.yml Travis config updates Sep 5, 2015 CHANGELOG.md Prep for 1.10.2 build Jul 4, 2016 LICENSE Build updates for repo move Feb 5, 2014 NOTICE Remove dependency on scalaz-core Jan 6, 2015 README.md Removed stale reference to Scalaz in README Feb 8, 2016 TODO.md Doc updates Feb 17, 2014 build.sbt Prep for 1.10.2 build Jul 4, 2016 version.sbt Setting version to 1.10.3-SNAPSHOT Jul 4, 2016 README.md scodec Scala combinator library for working with binary data. Design Constraints This library focuses on contract-first and pure functional encoding and decoding of binary data. The following design constraints are considered: Binary structure should mirror protocol definitions and be self-evident under casual reading Mapping binary structures to types should be statically verified Encoding and decoding should be purely functional Failures in encoding and decoding should provide descriptive errors Compiler plugin should not be used As a result, the library is implemented as a combinator based DSL. Performance is considered but yields to the above design constraints. Acknowledgements The library uses Shapeless and is heavily influenced by scala.util.parsing.combinator. Administrative This project is licensed under a 3-clause BSD license. The scodec mailing list contains release announcements and is generally a good place to go for help. Also consider using the scodec tag on StackOverflow. People are expected to follow the Typelevel Code of Conduct when discussing scodec on the Github page, Gitter channel, mailing list, or other venues. Concerns or issues can be sent to Michael Pilquist (mpilquist@gmail.com) or to Typelevel. Introduction The primary abstraction is a Codec[A], which supports encoding a value of type A to a BitVector and decoding a BitVector to a value of type A. The codecs package provides a number of predefined codecs and combinators.     import scodec._     import scodec.bits._     import codecs._      // Create a codec for an 8-bit unsigned int followed by an 8-bit unsigned int followed by a 16-bit unsigned int     val firstCodec = (uint8 ~ uint8 ~ uint16)      // Decode a bit vector using that codec     val result: DecodeResult[(Int ~ Int ~ Int)] = Codec.decode(firstCodec, BitVector(0x10, 0x2a, 0x03, 0xff))      // Sum the result     val add3 = (_: Int) + (_: Int) + (_: Int)     val sum: DecodeResult[Int] = result map add3 Automatic case class binding is supported via Shapeless HLists:     case class Point(x: Int, y: Int, z: Int)      val pointCodec = (int8 :: int8 :: int8).as[Point]      val encoded: Attempt[BitVector] = pointCodec.encode(Point(-5, 10, 1))     // Successful(BitVector(24 bits, 0xfb0a01))      val decoded: Attempt[DecodeResult[Point]] = pointCodec.decode(hex""0xfb0a01"".bits)     // Successful(DecodeResult(Point(-5,10,1),BitVector(empty))) Codecs can also be implicitly resolved, resulting in usage like:     // Assuming Codec[Point] is in implicit scope      val encoded: Attempt[BitVector] = Codec.encode(Point(-5, 10, 1))     // Successful(BitVector(24 bits, 0xfb0a01))      val decoded: Attempt[DecodeResult[Point]] = Codec.decode[Point](hex""0xfb0a01"".bits)     // Successful(DecodeResult(Point(-5,10,1),BitVector(empty))) New codecs can be created by either implementing the Codec trait or by passing an encoder function and decoder function to the Codec apply method. Typically, new codecs are created by applying one or more combinators to existing codecs. See the guide for detailed documentation. Also, see ScalaDoc. Especially: Codec codecs package Examples There are various examples in the test directory, including codecs for: UDP Datagrams MPEG Packets libpcap Files The scodec-protocols has production quality codecs for the above examples. The bitcoin-scodec library has a codec for the Bitcoin network protocol. The scodec-msgpack library provides codecs for MessagePack. Getting Binaries See the releases page on the website. Building This project uses sbt and requires node.js to be installed in order to run Scala.js tests. To build, run sbt publish-local. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scodec/scodec"	"A combinator library for working with binary data."	"true"
"Serialization"	"Scrooge"	"http://twitter.github.io/scrooge/"	"An Apache Thrift code generator for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"489"	"198"	"162"	"GitHub - twitter/scrooge: A Thrift parser/generator Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 198 Star 489 Fork 162 twitter/scrooge Code Issues 29 Pull requests 3 Pulse Graphs A Thrift parser/generator http://twitter.github.io/scrooge/ 1,137 commits 7 branches 21 releases Fetching contributors Scala 61.2% Java 14.8% HTML 8.6% Thrift 6.8% Objective-C 5.0% Shell 2.2% Other 1.4% Scala Java HTML Thrift Objective-C Shell Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop gh-pages master mustache nshkrob/test nshkrob/test2 sbt11 Nothing to show scrooge-4.8.0 scrooge-4.7.0 scrooge-4.6.0 scrooge-4.5.0 scrooge-4.4.0 scrooge-4.3.0 scrooge-4.2.0 scrooge-4.1.0 scrooge-4.0.0 scrooge-3.20.0 scrooge-3.19.0 scrooge-3.18.1 scrooge-3.18.0 scrooge-3.17.0 org=scrooge,name=scrooge,version=3.0.1 org=scrooge,name=scrooge,version=3.0.0 org=com.twitter,name=scrooge,version=1.1.7 org=com.twitter,name=scrooge,version=1.1.6 org=com.twitter,name=scrooge,version=1.1.5 org=com.twitter,name=scrooge,version=1.1.4 org=com.twitter,name=scrooge,version=1.1.3 Nothing to show New pull request Latest commit a96517c Jul 7, 2016 vkostyukov committed with jenkins csl: Release CSL libraries … Problem / Solution  Finagle 6.36 Util 6.35 Ostrich 9.19 TwitterServer 1.21 Finatra 2.2.0 Scrooge 4.8.0  RB_ID=849873 Permalink Failed to load latest commit information. .github Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 bin finagle/finatra/scrooge/twitter-server: bin/travisci updates and fixes Apr 21, 2016 demos/scrooge-maven-demo csl: Release CSL libraries Jul 7, 2016 doc/src/sphinx scrooge-docs: how to get scrooge-sbt-plugin Jun 6, 2016 project csl: Release CSL libraries Jul 7, 2016 scrooge-benchmark source: fix more unused imports for scala 2.11.8 Mar 29, 2016 scrooge-core scrooge-generator: Revert `HasThriftStructCodec3` changes May 30, 2016 scrooge-generator-tests scrooge-generator-tests: Disable Lua test Jun 9, 2016 scrooge-generator Cross build for Scala 2.12 Jun 27, 2016 scrooge-linter scrooge: cleanup various build warnings Dec 21, 2015 scrooge-maven-plugin csl: Release CSL libraries Jul 7, 2016 scrooge-sbt-plugin scrooge-sbt-plugin, finagle: Handle compiling for multiple languages … Jun 29, 2016 scrooge-serializer scrooge: cleanup various build warnings Dec 21, 2015 .gitignore [split] scrooge: Cache mustache resources to improve generation perfo… Oct 7, 2014 .travis.yml scrooge: fix crossScalaVersion for scrooge-generator-tests Jun 27, 2016 CHANGES csl: Release CSL libraries Jul 7, 2016 CONFIG.ini [split] improve SocialGraphUtil exception logging Sep 11, 2014 CONTRIBUTING.md Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 GROUPS [split] improve SocialGraphUtil exception logging Sep 11, 2014 LICENSE [split] improve SocialGraphUtil exception logging Sep 11, 2014 OWNERS util, finagle, scrooge: Add ryano to More Owners Files Feb 4, 2016 README.md scrooge-sbt-plugin, finagle: Handle compiling for multiple languages … Jun 29, 2016 pushsite.bash [split] improve SocialGraphUtil exception logging Sep 11, 2014 sbt Switch to Java 8 and Scala 2.11 May 9, 2016 README.md Scrooge Scrooge is a thrift code generator written in Scala, which currently generates code for Scala, Java, Cocoa, Android and Lua. It's meant to be a replacement for the apache thrift code generator, and generates conforming, compatible binary codecs by building on top of libthrift. It integrates with the finagle project, exporting stats and finagle APIs, and makes it easy to build high throughput, low latency, robust thrift servers and clients. Part of the motivation behind scrooge's scala implementation is that since Scala is API-compatible with Java, you can use the apache thrift code generator to generate Java files and use them from within Scala, but the generated code uses Java collections and mutable ""bean"" classes, causing some annoying boilerplate conversions to be hand-written. Scrooge bypasses the problem by generating Scala code directly. It also uses Scala syntax so the generated code is much more compact. There is a comprehensive set of unit tests, which generate code, compile it, and execute it to verify expectations, as well as gold files to make it easy to review the effects of changes to the generator. Status This project is used in production at Twitter (and many other organizations), and is actively developed and maintained. Building the develop branch locally You will need the develop branch of util. Finagle depends on scrooge-core, so the order in which you build dependencies should be: in util: ./sbt publishLocal in scrooge: ./sbt publishLocal in finagle: ./sbt publishLocal You will need the develop branch of finagle to run tests in scrooge-generator-tests, but you do not need it to build scrooge otherwise. Full Documentation https://twitter.github.io/scrooge/ Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/scrooge"	"An Apache Thrift code generator for Scala."	"true"
"Serialization"	"validation ★ 128 ⧗ 15"	"https://github.com/jto/validation"	"Advanced validation & serialization for JSON, HTML form data, etc, with no reflection at runtime."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"148"	"16"	"19"	"GitHub - jto/validation: validation api extracted from play Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 16 Star 148 Fork 19 jto/validation Code Issues 3 Pull requests 3 Pulse Graphs validation api extracted from play 110 commits 11 branches 2 releases Fetching contributors Scala 99.6% Other 0.4% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0.x develop feature/drone feature/freeish feature/interpredted feature/json4s feature/tut gh-pages master shapeless v2.0 Nothing to show 1.0.2 1.0.1 Nothing to show New pull request Latest commit 6ebf764 Jul 4, 2016 jto disable compilation warns as error in console Permalink Failed to load latest commit information. date-tests/src/test/scala v2.0 Jun 27, 2016 docs/src/main/tut Remove occurences of play.api.data.mapping in docs by jto.validation Jun 27, 2016 play-scalajs-example v2.0 Jun 27, 2016 project v2.0 Jun 27, 2016 scripts v2.0 Jun 27, 2016 validation-core/src v2.0 Jun 27, 2016 validation-delimited/src v2.0 Jun 27, 2016 validation-form/src v2.0 Jun 27, 2016 validation-jsjson/src v2.0 Jun 27, 2016 validation-jsonast v2.0 Jun 27, 2016 validation-playjson/src v2.0 Jun 27, 2016 validation-xml/src v2.0 Jun 27, 2016 .gitignore v2.0 Jun 27, 2016 .jvmopts v2.0 Jun 27, 2016 .travis.yml v2.0 Jun 27, 2016 PUBLISH.md v2.0 Jun 27, 2016 README.md Remove occurences of play.api.data.mapping in docs by jto.validation Jun 27, 2016 build.sbt disable compilation warns as error in console Jul 4, 2016 version.sbt v2.0 Jun 27, 2016 README.md The unified data validation library Overview The unified validation API aims to provide a comprehensive toolkit to validate data from any format against user defined rules, and transform them to other types. Basically, assuming you have this: import play.api.libs.json._ import jto.validation._  case class Person(name: String, age: Int, lovesChocolate: Boolean)  val json = Json.parse(""""""{   ""name"": ""Julien"",   ""age"": 28,   ""lovesChocolate"": true }"""""")  implicit val personRule = {   import jto.validation.playjson.Rules._   Rule.gen[JsValue, Person] } It can do this: scala> personRule.validate(json) res0: jto.validation.VA[Person] = Valid(Person(Julien,28,true)) BUT IT'S NOT LIMITED TO JSON It's also a unification of play's Form Validation API, and its Json validation API. Being based on the same concepts as play's Json validation API, it should feel very similar to any developer already working with it. The unified validation API is, rather than a totally new design, a simple generalization of those concepts. Design The unified validation API is designed around a core defined in package jto.validation, and ""extensions"". Each extension provides primitives to validate and serialize data from / to a particular format (Json, form encoded request body, etc.). See the extensions documentation for more information. To learn more about data validation, please consult Validation and transformation with Rule, for data serialization read Serialization with Write. If you just want to figure all this out by yourself, please see the Cookbook. Using the validation api in your project Add the following dependencies your build.sbt as needed: resolvers += Resolver.sonatypeRepo(""releases"")  val validationVersion = ""2.0""  libraryDependencies ++= Seq(   ""io.github.jto"" %% ""validation-core""      % validationVersion,   ""io.github.jto"" %% ""validation-playjson""  % validationVersion,   ""io.github.jto"" %% ""validation-jsonast""   % validationVersion,   ""io.github.jto"" %% ""validation-form""      % validationVersion,   ""io.github.jto"" %% ""validation-delimited"" % validationVersion,   ""io.github.jto"" %% ""validation-xml""       % validationVersion   // ""io.github.jto"" %%% ""validation-jsjson""    % validationVersion ) Play dependencies Validation Play 2.0 2.5.3 1.1.x 2.4.x 1.0.2 2.3.x Documentation Documentation is here Validating and transforming data Combining Rules Serializing data with Write Combining Writes Validation Inception Play's Form API migration Play's Json API migration Extensions: Supporting new types Exporting Validations to Javascript using Scala.js Cookbook Release notes v2.0 Migration guide Contributors Julien Tournay - http://jto.github.io Olivier Blanvillain - https://github.com/OlivierBlanvillain Nick - https://github.com/stanch Ian Hummel - https://github.com/themodernlife Arthur Gautier - https://github.com/baloo Jacques B - https://github.com/Timshel Alexandre Tamborrino - https://github.com/atamborrino Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/jto/validation"	"Advanced validation & serialization for JSON, HTML form data, etc, with no reflection at runtime."	"true"
"Serialization"	"µPickle"	"http://lihaoyi.github.io/upickle-pprint/upickle/"	"A lightweight serialization library for Scala that works in ScalaJS, allowing transfer of structured data between the JVM and JavaScript."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"µPickle 0.4.1 µPickle 0.4.1 uPickle (pronounced micro-pickle) is a lightweight serialization library for Scala. It's key features are: Less than 1000 lines of code Zero-reflection 100% static serialization and deserialization Human-readable JSON encoding, with a fast JSON API A large, well-defined set of supported types, with well-defined semantics Handling of default values and custom keys, for maintaining backwards compatiblity while schemas change Minimal dependencies: Only depends on Jawn on the JVM, and on the Javascript standard library in Scala.js Works in ScalaJS, allowing transfer of structured data between the JVM and Javascript Getting Started Add the following to your SBT config: libraryDependencies += ""com.lihaoyi"" %% ""upickle"" % ""0.4.1""   And then you can immediately start writing and reading common Scala objects to strings: import upickle.default._  write(1)                          ==> ""1""  write(Seq(1, 2, 3))               ==> ""[1,2,3]""  read[Seq[Int]](""[1, 2, 3]"")       ==> List(1, 2, 3)  write((1, ""omg"", true))           ==> """"""[1,""omg"",true]""""""  type Tup = (Int, String, Boolean)  read[Tup](""""""[1, ""omg"", true]"""""") ==> (1, ""omg"", true) ScalaJS For ScalaJS applications, use this dependencies instead: libraryDependencies += ""com.lihaoyi"" %%% ""upickle"" % ""0.4.1"" Other than that, everything is used the same way. upickle-0.2.8 is only compatible with ScalaJS 0.6.x. Scala 2.10 If you are using Scala 2.10 (instead of 2.11) be sure to add this dependency as well: libraryDependencies += ""org.scalamacros"" %% s""quasiquotes"" % ""2.0.0"" % ""provided""   This applies both for ScalaJVM and ScalaJS. Supported Types Out of the box, uPickle supports writing and reading the following types: Boolean, Byte, Char, Short, Int, Long, Float, Double Tuples from 1 to 22 Immutable Seq, List, Vector, Set, SortedSet, Option, Array, Maps, and all other collections with a reasonable CanBuildFrom implementation Duration, Either Stand-alone case classes and case objects, and their generic equivalents, Non-generic case classes and case objects that are part of a sealed trait or sealed class hierarchy sealed trait and sealed classes themselves, assuming that all subclasses are picklable UUIDs null Readability/writability is recursive: a container such as a Tuple or case class is only readable if all its contents are readable, and only writable if all its contents are writable. That means that you cannot serialize a List[Any], since uPickle doesn't provide a generic way of serializing Any. Case classes are only serializable up to 22 fields. Case classes are serialized using the apply and unapply methods on their companion objects. This means that you can make your own classes serializable by giving them companions apply and unapply. sealed hierarchies are serialized as tagged unions: whatever the serialization of the actual object, together with the fully-qualified name of its class, so the correct class in the sealed hierarchy can be reconstituted later. That concludes the list of supported types. Anything else is not supported. Default Picklers This is a non-comprehensive list of what the most commonly-used types pickle to using uPickle. To begin, let's import upickle import upickle.default._  Booleans are serialized as JSON booleans write(true: Boolean)              ==> ""true"" write(false: Boolean)             ==> ""false"" Numbers are serialized as JSON numbers write(12: Int)                    ==> ""12"" write(12: Short)                  ==> ""12"" write(12: Byte)                   ==> ""12"" write(12.5f: Float)               ==> ""12.5"" write(12.5: Double)               ==> ""12.5"" Except for Longs, which too large for Javascript. These are serialized as JSON Strings, keeping the interchange format compatible with the browser's own JSON parser, which provides the best performance in Scala.js write(12: Long)                   ==> ""\""12\"""" write(4000000000000L: Long)       ==> ""\""4000000000000\"""" Special values of Doubles and Floats are also serialized as Strings write(1.0/0: Double)              ==> ""\""Infinity\"""" write(Float.PositiveInfinity)     ==> ""\""Infinity\"""" write(Float.NegativeInfinity)     ==> ""\""-Infinity\"""" Both Chars and Strings are serialized as Strings write('o')                        ==> ""\""o\"""" write(""omg"")                      ==> ""\""omg\"""" Arrays and most immutable collections are serialized as JSON lists write(Array(1, 2, 3))             ==> ""[1,2,3]""  // You can pass in an `indent` parameter to format it nicely write(Array(1, 2, 3), indent = 4)  ==>   """"""[     |    1,     |    2,     |    3     |]"""""".stripMargin  write(Seq(1, 2, 3))               ==> ""[1,2,3]"" write(Vector(1, 2, 3))            ==> ""[1,2,3]"" write(List(1, 2, 3))              ==> ""[1,2,3]"" import collection.immutable.SortedSet write(SortedSet(1, 2, 3))         ==> ""[1,2,3]"" Options are serialized as JSON lists with 0 or 1 element write(Some(1))                    ==> ""[1]"" write(None)                       ==> ""[]"" Tuples of all sizes (1-22) are serialized as heterogenous JSON lists write((1, ""omg""))                 ==> """"""[1,""omg""]"""""" write((1, ""omg"", true))           ==> """"""[1,""omg"",true]"""""" Case classes of sizes 1-22 are serialized as JSON dictionaries with the keys being the names of each field case class Thing(myFieldA: Int, myFieldB: String) case class Big(i: Int, b: Boolean, str: String, c: Char, t: Thing) import upickle._ write(Thing(1, ""gg""))             ==> """"""{""myFieldA"":1,""myFieldB"":""gg""}"""""" write(Big(1, true, ""lol"", 'Z', Thing(7, """"))) ==>   """"""{""i"":1,""b"":true,""str"":""lol"",""c"":""Z"",""t"":{""myFieldA"":7,""myFieldB"":""""}}""""""  write(Big(1, true, ""lol"", 'Z', Thing(7, """")), indent = 4) ==>   """"""{     |    ""i"": 1,     |    ""b"": true,     |    ""str"": ""lol"",     |    ""c"": ""Z"",     |    ""t"": {     |        ""myFieldA"": 7,     |        ""myFieldB"": """" Sealed hierarchies are serialized as tagged values, the serialized object tagged with the full name of the instance's class: sealed trait IntOrTuple case class IntThing(i: Int) extends IntOrTuple case class TupleThing(name: String, t: (Int, Int)) extends IntOrTuple write(IntThing(1)) ==> """"""{""$type"":""example.Sealed.IntThing"",""i"":1}""""""  write(TupleThing(""naeem"", (1, 2))) ==>   """"""{""$type"":""example.Sealed.TupleThing"",""name"":""naeem"",""t"":[1,2]}""""""  // You can read tagged value without knowing its // type in advance, just use type of the sealed trait read[IntOrTuple](""""""{""$type"":""example.Sealed.IntThing"",""i"": 1}"""""") ==> IntThing(1) Serializability is recursive; you can serialize a type only if all its members are serializable. That means that collections, tuples and case-classes made only of serializable members are themselves serializable case class Foo(i: Int) case class Bar(name: String, foos: Seq[Foo]) write((((1, 2), (3, 4)), ((5, 6), (7, 8)))) ==>   """"""[[[1,2],[3,4]],[[5,6],[7,8]]]""""""  write(Seq(Thing(1, ""g""), Thing(2, ""k""))) ==>   """"""[{""myFieldA"":1,""myFieldB"":""g""},{""myFieldA"":2,""myFieldB"":""k""}]""""""  write(Bar(""bearrr"", Seq(Foo(1), Foo(2), Foo(3)))) ==>   """"""{""name"":""bearrr"",""foos"":[{""i"":1},{""i"":2},{""i"":3}]}"""""" Nulls serialize into JSON nulls, as you would expect write(Bar(null, Seq(Foo(1), null, Foo(3)))) ==>   """"""{""name"":null,""foos"":[{""i"":1},null,{""i"":3}]}"""""" uPickle only throws exceptions on unpickling; if a pickler is properly defined, serializing a data structure to a String should never throw an exception. On unpickling, uPickle throws one of two subclasses of upickle.Invalid: upickle.Invalid.Json: thrown when unpickling fails at the first step which attempst to convert the incoming String into semi-structured JSON data. The exception contains data about where parsing failed (input, line, col) as well as a human-readable error message. upickle.Invalid.Data: thrown when unpickling fails at the second step which attempts to convert the parsed JSON tree into structured data of the desired type. Contains the offending JSON subtree in data, along with a human-readable error message. Defaults If a field is missing upon deserialization, uPickle uses the default value if one exists 'reading{    read[FooDefault](""{}"")                ==> FooDefault(10, ""lol"")   read[FooDefault](""""""{""i"": 123}"""""")    ==> FooDefault(123,""lol"") If a field at serialization time has the same value as the default, uPickle leaves it out of the serialized blob 'writing{   write(FooDefault(i = 11, s = ""lol""))  ==> """"""{""i"":11}""""""   write(FooDefault(i = 10, s = ""lol""))  ==> """"""{}""""""   write(FooDefault())                   ==> """"""{}"""""" This allows you to make schema changes gradually, assuming you have already pickled some data and want to add new fields to the case classes you pickled. Simply give the new fields a default value (e.g. """" for Strings, or wrap it in an Option[T] and make the default None) and uPickle will happily read the old data, filling in the missing field using the default value. Custom Keys uPickle allows you to specify the key that a field is serialized with via a @key annotation import derive.key case class KeyBar(@key(""hehehe"") kekeke: Int) write(KeyBar(10))                     ==> """"""{""hehehe"":10}"""""" read[KeyBar](""""""{""hehehe"": 10}"""""")    ==> KeyBar(10) Practically, this is useful if you want to rename the field within your Scala code while still maintaining backwards compatibility with previously-pickled objects. Simple rename the field and add a @key(""..."") with the old name so uPickle can continue to work with the old objects correctly. You can also use @key to change the name used when pickling the case class itself. Normally case classes are pickled without their name, but an exception is made for members of sealed hierarchies which are tagged with their fully-qualified name. uPickle allows you to use @key to override what the class is tagged with: import derive.key sealed trait A @key(""Bee"") case class B(i: Int) extends A case object C extends A write(B(10))                          ==> """"""{""$type"":""Bee"",""i"":10}"""""" read[B](""""""{""$type"":""Bee"",""i"":10}"""""") ==> B(10) This is useful in cases where: you wish to rename the class within your Scala code, or move it to a different package, but want to preserve backwards compatibility with previously pickled instances of that class you try to tackle the resource issue (bandwidth, storage, CPU) because FQNs might get quite long Custom Picklers Apart from customizing the keys used to store the fields of a class, uPickle also allows you to completely replace the default Reader/Writer used to write that class. For classes you control. You need to provide an implicit Reader/Writer pair in the companion object: import upickle.Js class CustomThing2(val i: Int, val s: String) object CustomThing2{   implicit val thing2Writer = upickle.default.Writer[CustomThing2]{     case t => Js.Str(t.i + "" "" + t.s)   }   implicit val thing2Reader = upickle.default.Reader[CustomThing2]{     case Js.Str(str) =>       val Array(i, s) = str.split("" "")       new CustomThing2(i.toInt, s)   } } In this example, instead of pickling to a normal Js.Obj, we pickle to a Js.Str, storing both i and s as part of that single string. Note that when writing custom picklers, it is entirely up to you to get it right, e.g. making sure that an object that gets round-trip pickled/unpickled comes out the same as when it started. Custom Configuration Often, there will be times that you want to customize something on a project-wide level. uPickle provides hooks in letting you subclass the upickle.Api trait to create your own bundles apart from the in-built upickle.default and upickle.legacy. You have multiple levels of possible customization: upickle.Api, which lets you customize the annotate methods, which are used to tag sealed hierarchies with their class names. This is used to e.g. distinguish upickle.default's $type attribute vs upickle.legacy's array-wrapper upickle.AttributeTagged, which assumes you want the standard annotate method using a type attribute, but letting you customize the attribute. In both these cases, you are also free to override the CaseR and CaseW implicits, which controls how case classes are serialized. For example, you could make it serialize to a JSON array rather than a dictionary. Or, for example, convert the camelCase Scala identifiers to snake_case or some other convention: object SnakePickle extends upickle.AttributeTagged{   def camelToSnake(s: String) = {     val res = s.split(""(?=[A-Z])"", -1).map(_.toLowerCase).mkString(""_"")     res   }   override def CaseR[T: this.Reader, V]                     (f: T => V,                      names: Array[String],                      defaults: Array[Js.Value]) = {     super.CaseR[T, V](f, names.map(camelToSnake), defaults)   }   override def CaseW[T: this.Writer, V]                     (f: V => Option[T],                      names: Array[String],                      defaults: Array[Js.Value]) = {     super.CaseW[T, V](f, names.map(camelToSnake), defaults)   } } // Default read-writing upickle.default.write(Thing(1, ""gg"")) ==> """"""{""myFieldA"":1,""myFieldB"":""gg""}"""""" upickle.default.read[Thing](""""""{""myFieldA"":1,""myFieldB"":""gg""}"""""") ==> Thing(1, ""gg"")  // snake_case_keys read-writing SnakePickle.write(Thing(1, ""gg"")) ==> """"""{""my_field_a"":1,""my_field_b"":""gg""}"""""" SnakePickle.read[Thing](""""""{""my_field_a"":1,""my_field_b"":""gg""}"""""") ==> Thing(1, ""gg"") If you are using uPickle to convert JSON from another source into Scala data structures, you may find the following encoding of Option[T] more convenient than the defaults: object OptionPickler extends upickle.AttributeTagged {   override implicit def OptionW[T: Writer]: Writer[Option[T]] = Writer {     case None    => Js.Null     case Some(s) => implicitly[Writer[T]].write(s)   }    override implicit def OptionR[T: Reader]: Reader[Option[T]] = Reader {     case Js.Null     => None     case v: Js.Value => Some(implicitly[Reader[T]].read.apply(v))   } } This custom configuration allows you to treat nulls as Nones and anything else as Some(...)s. Simply import OptionPickler._ instead of the normal uPickle import throughout your project and you'll have the customized reading/writing available to you. Limitations uPickle is a work in progress, and doesn't currently support: Circular object graphs Reflective reading and writing Read/writing of untyped values e.g. Any Read/writing arbitrarily shaped objects Read/writing case classes with multiple parameter lists. Most of these limitations are inherent in the fact that ScalaJS does not support reflection, and are unlikely to ever go away. In general, uPickle is designed to serialize statically-typed, tree-shaped, immutable data structures. Anything more complex is out of scope. JSON API Although uPickle's object read/writing API makes does not expose you to it, under the hood it uses a nice JSON serialization format. Despite being less-compact than binary formats, this allows for very-fast serializing and deserializing from Strings on both Scala-JVM (which has other alternatives) and ScalaJS, where JSON is really your only choice. The JSON API is minimal but nonetheless very convenient, and can be used directly. uPickle bundles two very-fast JSON parsers, which it uses for parsing strings into structured-trees, before then marshalling them into typed objects. Jawn on the JVM JSON.parse on Scala.js That makes uPickle's JSON library competitive with the highest performance JSON libraries both on the JVM (GSON, Jackson, etc.) as well as in Javascript. uPickle's JSON API is exposed in two places: in our upickle.Js.* AST: object Js {   object Js {      sealed trait Value extends Any {       def value: Any        /**         * Returns the `String` value of this [[Js.Value]], fails if it is not         * a [[Js.Str]]         */       def str = this match{         case Str(value) => value         case _ => throw Invalid.Data(this, ""Expected Js.Str"")       }       /**         * Returns the key/value map of this [[Js.Value]], fails if it is not         * a [[Js.Obj]]         */       def obj = this match{         case Obj(value @ _*) => value.toMap         case _ => throw Invalid.Data(this, ""Expected Js.Obj"")       }       /**         * Returns the elements of this [[Js.Value]], fails if it is not         * a [[Js.Arr]]         */       def arr = this match{         case Arr(value @ _*) => value         case _ => throw Invalid.Data(this, ""Expected Js.Arr"")       }       /**         * Returns the `Double` value of this [[Js.Value]], fails if it is not         * a [[Js.Num]]         */       def num = this match{         case Num(value) => value         case _ => throw Invalid.Data(this, ""Expected Js.Num"")       }        /**         * Looks up the [[Js.Value]] as a [[Js.Arr]] using an index, throws         * otherwise if it's not a [[Js.Arr]]         */       def apply(i: Int): Value = this.arr(i)       /**         * Looks up the [[Js.Value]] as a [[Js.Obj]] using an index, throws         * otherwise if it's not a [[Js.Obj]]         */       def apply(s: java.lang.String): Value = this.obj(s)     }     case class Str(value: java.lang.String) extends AnyVal with Value     case class Obj(value: (java.lang.String, Value)*) extends AnyVal with Value     case class Arr(value: Value*) extends AnyVal with Value     case class Num(value: Double) extends AnyVal with Value     case object False extends Value{       def value = false     }     case object True extends Value{       def value = true     }     case object Null extends Value{       def value = null     }   }      def apply(i: Int): Value = this.asInstanceOf[Arr].value(i)     def apply(s: java.lang.String): Value = this.asInstanceOf[Obj].value.find(_._1 == s).get._2   }   case class Str(value: java.lang.String) extends AnyVal with Value   case class Obj(value: (java.lang.String, Value)*) extends AnyVal with Value   case class Arr(value: Value*) extends AnyVal with Value   case class Num(value: Double) extends AnyVal with Value   case object False extends Value{     def value = false   }   case object True extends Value{     def value = true   }   case object Null extends Value{     def value = null   } } As well as in the upickle.json.read and upickle.json.write functions: def read(s: String): Js.Value def write(v: Js.Value): String Which you use to convert between structured Js.* trees and unstructured Strings. As described earlier, the implementation of these functions differs between ScalaJVM/ScalaJS. You can use this JSON data structure for simple tasks: json.read(""[1]"").arr        ==> Seq(Js.Num(1)) json.read(""1"").num          ==> 1 json.read(""\""1\"""").str      ==> ""1"" json.read(""{\""1\"": 1}"").obj ==> Map(""1"" -> Js.Num(1)) val unparsed = json.write(parsed) val reparsed = json.read(unparsed) for (json <- Seq(parsed, reparsed)){   assert(     json(0).value == ""JSON Test Pattern pass1"",     json(8)(""real"").value == -9876.54321,     json(8)(""comment"").value == ""// /* <!-- --"",     json(8)(""jsontext"").value == ""{\""object with 1 member\"":[\""array with 1 element\""]}"",     json(19).value == ""rosebud""   ) } (parsed(19), reparsed(19)) uPickle does not provide any other utilities are JSON that other libraries do (zippers, lenses, combinators, ...). If you're looking for a compact JSON AST to construct or pattern match on, together with fast serializing and deserializing, it may do the trick. Caching Picklers Synthesizing case class picklers at every callsite where you're pickling something can get expensive: at compile-time the compiler is deriving the correct combination of readers of writers over and over at every callsite, and at runtime the JVM is instantiating that same combination over and over. To speed things up both at compile time and runtime, you can pre-generate picklers inside the case classes companion objects. This should greatly speed up compilation in large codebases with lots of different case-classes being pickled, and improve runtime performance. Simply add a macroRW implicit into your case class's companion object: case class CachedCaseClass(b: String, a: Double) object CachedCaseClass {   implicit val pkl = upickle.default.macroRW[CachedCaseClass] } And then the next time you read and write the case class, things should work as expected: import upickle.default.{read, write} val res = read[CachedCaseClass](write(CachedCaseClass(""aaa"", 42.0))) assert(res == CachedCaseClass(""aaa"", 42.0)) Except that the Reader and Writer is simply taken from the companion object each time instead of being synthesized again and again at every callside import upickle.default.{Reader, Writer} // Each time you ask for an implicit reader or writer, it's the same one assert(implicitly[Reader[CachedCaseClass]] eq implicitly[Reader[CachedCaseClass]]) assert(implicitly[Writer[CachedCaseClass]] eq implicitly[Writer[CachedCaseClass]]) Why uPickle I wrote uPickle because I needed a transparent serialization library that worked both in Scala-JVM and Scala-JS, and my dissatisfaction with existing solutions: None of the libraries I could find were pure Scala: spray-json uses parboiled1 which is written in Java, play-json uses Jackson, which uses Java/reflection. scala-pickling silently falls back to reflection. This makes them difficult to port to ScalaJS, which supports neither Java nor reflection. Those libraries also typically have non-trivial dependency chains. That also makes them hard to port to ScalaJS, since I'd need to port all of their dependencies too. Lastly, many aim for a very ambitious target: untyped serialization of arbitrary object graphs. That forces them to use reflection, and makes their internals and semantics much more complex. uPickle on the other hand aims much lower: by limiting the scope of the problem to statically-typed, tree-like, immutable data structures, it greatly simplifies both the internal implementation and the external API and behavior of the library. uPickle serializes objects using a very simple set of rules (""Does it have an implicit? Is it a class with apply/unapply on the companion?"") that makes its behavior predictable and simple to understand. Version History 0.4.0 Allow custom handling for JSON nulls via a Custom Configuration Made derive.key a case class Remove unnecessary dependency of derive on default arguments #143 Fixed derivation allowing Caching Picklers in a case class's companion object, potentially speeding up compilation times and runtimes 0.3.9 Add .arr: Seq[Js.Value], .obj: Map[String, Js.Value], .str: String and .num: Double helper methods on Js.Value to simplify usage as a simple JSON tree. Invalid.Json and Invalid.Data now have better exception messages by default, which should simplify debugging Some usages should compile faster due to fiddling with implicits (#138) 0.3.8 Tweaks to PPrint 0.3.7 You can now pass in an indent parameter to upickle.default.write in order to format/indent the JSON nicely across multiple lines Derivation based on sealed abstract classes works, in addition to traits (#104), thanks to Voltir Fix non-deterministic failure due to improperly implemented equals/hashCode in macro (#124), thanks to Voltir Slightly improve hygiene of uPickle/PPrint macro expansion uPickle de-serialization failures should no longer throw MatchErrors (#101) Using case-class-derived Readers/Writers should no longer fail in class extends clauses (#108) Float.NaN and Double.NaN are now properly handled (#123) Provided an example of a Custom Configuration being used to snake_case case-class fields during serialization/de-serialization (#120) 0.3.6 Fix more bugs in PPrint derivation 0.3.5 Fix some bugs in PPrint derivation 0.3.4 Remove unnecessary shapeless dependency 0.3.3 Fix more edge cases to avoid diverging implicits 0.3.2 Fix more edge cases around typeclass derivation: #94, #95, #96 Don't get tripped up by custom Apply methods: #48 0.3.1 Fixed edge cases around typeclass derivation 0.3.0 Top-to-bottom rewrite of type-class derivation macros. Much faster, more reliable, etc.. Still one or two cases where it misbehaves, but much fewer than before. Extracted it into the derive subproject Force users to choose between import upickle.default._ which now renders sealed trait hierarchies as dictionaries with a $type attribute, and import upickle.legacy._ which does the old-style array-wrapper. You can also now create your own custom subclass of upickle.Api if you wish to customize things further, e.g. changing the type-attribute or changing the rendering of case classes. 0.2.8 Support for java.util.UUID, which are serialized as strings in the standard format 0.2.7 Re-published for Scala.js 0.6.1 0.2.6 'Symbols are now read/write-able by default Added lots of warnings for common issues Map[String, V] now pickles to a JSON dictionary ""key"": ""value"", ...}. Map[K, V] for all other K != String are unchanged Source maps now point towards a reasonabel place on Github 0.2.5 Fixed [#23](https://github.com/lihaoyi/upickle/issues/23): self-recursive data structures are now supported. Fixed [#18](https://github.com/lihaoyi/upickle/issues/18): you can now auto-pickle classes in objects that originated from traits that were mixed in. 0.2.4 Support reading and writing null Fixed Reader/Writer macros for single-class sealed hierarchies Used CanBuildFrom to serialize a broader range of collections 0.2.3 Added a pickler for Unit/() 0.2.2 Swapped over from the hand-rolled parser to using Jawn/JSON.parse on the two platforms, resulting in a 10-15x speedup for JSON handling. Renamed Js.{String, Object, Array, Number} into Js.{Str, Obj, Arr, Num}, and made Js.Arr and Js.Obj use varargs, to allow for better direct-use. Documented and exposed JSON API for direct use by users of the library. 0.2.1 Improved error messages for unpickle-able types ScalaJS version now built against 0.5.3 0.2.0 Members of sealed trait/class hierarchies are now keyed with the fully-qualified name of their class, rather than an index, as it is less likely to change due to adding or removing classes Members of sealed hierarchies and parameters now support a upickle.key(""..."") annotation, which allows you to override the default key used (which is the class/parameter name) with a custom one, allowing you to change the class/param name in your code while maintaining compatibility with serialized structures Default parameters are now supported: they are used to substitute missing keys when reading, and cause the key/value pair to be omitted if the serialized value matches the default when writing Missing keys when deserializing case classes now throws a proper Invalid.Data exception objects are now serialized as {} rather than [], better matching the style of case classes 0-argument case classes, previously unsupported, now serialize to {} the same way as objects Fixed a bug that was preventing multi-level sealed class hierarchies from being serialized due to a compilation error Fixed a bug causing case classes nested in other packages/objects and referred to by their qualified paths to fail pickling Tightened up error handling semantics, swapping out several MatchErrors with Invalid.Data errors 0.1.7 Cleaned up the external API, marking lots of things which should have been private private or stuffing them in the Internals namespace Organized things such that only a single import import upickle._ is necessary to use the library 0.1.6 Tuples and case classes now have implicit picklers up to an arity limit of 22. Case classes now serialize as JSON dictionaries rather than as lists. 0.1.5 Simple case classes and case class hierarchies are now auto-serializable view Macros. No need to define your own implicit using Case0ReadWriter anymore! 0.1.4 Serialize numbers as JSON numbers instead of Strings. 0.1.3 Specification of the exception-throwing behavior: instead of failing with random MatchErrors or similar, parse failures now are restricted to subclasses upickle.Invalid which define different failure modes."	"null"	"null"	"A lightweight serialization library for Scala that works in ScalaJS, allowing transfer of structured data between the JVM and JavaScript."	"true"
"Serialization"	"avro-codegen ★ 16 ⧗ 120"	"https://github.com/Nitro/avro-codegen"	"Code generation from avro schemas to serialize/deserialize avro messages, no runtime reflection."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"21"	"9"	"7"	"GitHub - Nitro/avro-codegen: Scala code generator for Avro schemas Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 9 Star 21 Fork 7 Nitro/avro-codegen Code Issues 6 Pull requests 0 Pulse Graphs Scala code generator for Avro schemas 45 commits 1 branch 3 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 0.3.4 0.3.3 0.3.2 Nothing to show New pull request Latest commit fa0d31d Jul 12, 2016 malcolmgreaves committed on GitHub 0.3.4 release Permalink Failed to load latest commit information. codegen Fix for constantly growing schema cache #20 Jul 12, 2016 project 0.3.4 release Jul 12, 2016 runtime .gitignore Moved e2e to a scripted test Dec 22, 2015 .travis.yml CONTRIBUTORS.md README.md TODO build.sbt fix broken build, downgrade scalacheck (#19) Jun 22, 2016 README.md avro-codegen Scala code generation from Avro schemas. Generates code similar to ScalaPB. Notably, the generated code for data encapsulation is in the form of case classes. The data representation is completely immutable. All generated code adheres to the GeneratedMessageCompanion type class. This ensures that there exists decode and encode functionality. working code generation for types: enums records scalar fields (for all types except for byte arrays) union fields via shapeless coproducts (for unions not containing NULL) optional union fields via shapeless coproducts (for unions containing more than 2 types, one of which is NULL) optional fields (for unions containing 2 types, one of which is NULL) arrays maps byte arrays features: generated org.scalacheck.Gen[_] instances for generated enums, records not implemented: fixed-length fields Generating code from inter-dependent schemas defined in multiple files (currently only handles this case when everything is in the same file). Subprojects runtime: runtime dependencies codegen: generates .scala files from avro schemas. Currently hardcoded to generate Out.scala in sandbox from the avro schemas in the example directory. proptest: generates random schemas, generates scala code for the schemas, tests the generated scala code by serializing and deserializing random message instances To Use There are two ways to use this project. The first is in creating case class instances from Avro schemas. The second is in interacting with the generated code. In practice, it's common for a project to do a little bit of both. For the first use case: add this to project/plugins.sbt addSbtPlugin(""com.gonitro"" % ""avro-codegen-compiler"" % ""X.Y.Z"")  where X.Y.Z is the most recent version. Add your avro schemas to src/main/avro with the .avsc extension. Generated scala classes will be created in target/scala-2.11/src_managed/main/generated_avro_classes/. For the second use case: When using code generated by this plugin, it is necessary to include the runtime dependency (which includes lots of goodies -- notably the serialization type class GeneratedMessage). Therefore, include the following in your build.sbt libraryDependencies ++= Seq(""com.gonitro"" %% ""avro-codegen-runtime"" % ""X.Y.Z"")  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Nitro/avro-codegen"	"Code generation from avro schemas to serialize/deserialize avro messages, no runtime reflection."	"true"
"Science and Data Analysis"	"Algebird ★ 1195 ⧗ 0"	"https://github.com/twitter/algebird"	"Abstract Algebra for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1276"	"270"	"202"	"GitHub - twitter/algebird: Abstract Algebra for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 270 Star 1,276 Fork 202 twitter/algebird Code Issues 84 Pull requests 22 Wiki Pulse Graphs Abstract Algebra for Scala https://twitter.com/scalding 1,357 commits 26 branches 31 releases 61 contributors Scala 95.3% Java 1.8% Shell 1.6% Ruby 1.3% Scala Java Shell Ruby Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags 0.7-develop 0.7.1 0.8-develop add-coverage-aggregate develop erik-oscar/batched_semigroup feature/hash fixAlgebirdBaseProperties gh-pages ianoc/SketchMapBenchmarking ianoc/twitterBackPortTestVersions ianoc/0.11.0_with_qtree_fix intersection_test ioconnell/MakeDevelopVersionHaveSnapshotSuffix jco/0.5.0rc5 jnievelt-291 klin_qtree_query master more-types-interval oscar-interval-tree oscar/non-algebra piyush-test release/0.9.0-scalacheck-1.10.0 release/0.10.1 remove_unused_seed takeSemigroup Nothing to show v0.12.1 v0.12.0 0.11.0 0.10.2 0.10.1 0.10.0 0.9.0 0.8.2 0.8.1 0.8.0 0.7.2 0.7.1 0.7.0 0.6.0 0.5.0 0.4.0 0.3.1 0.3.0 0.2.0 0.1.13 0.1.12 0.1.11 0.1.10 0.1.9 0.1.8 0.1.7 0.1.6 0.1.5 0.1.4 0.1.3 0.1.1 Nothing to show New pull request Latest commit b742c52 Jul 7, 2016 johnynek committed on GitHub Merge pull request #540 from VEINHORN/develop … Move to sbt 0.13.11 Permalink Failed to load latest commit information. algebird-benchmark Minor fixer for backwards compatibility (proposed 0.12.1) (#535) Jun 25, 2016 algebird-bijection/src fixed tests Feb 24, 2015 algebird-core/src Add sumOption to Short,Int,Long,Float,Double (#538) Jun 27, 2016 algebird-spark/src Minor fixer for backwards compatibility (proposed 0.12.1) (#535) Jun 25, 2016 algebird-test/src Minor fixer for backwards compatibility (proposed 0.12.1) (#535) Jun 25, 2016 algebird-util/src Merge remote-tracking branch 'upstream/develop' into feature/insertFail Mar 17, 2015 project move to sbt 0.13.11 Jul 7, 2016 scripts Update product_generators.rb Jun 25, 2015 .gitignore Update CHANGES.md Feb 1, 2016 .travis.yml Minor fixer for backwards compatibility (proposed 0.12.1) (#535) Jun 25, 2016 CHANGES.md Update CHANGES.md Feb 2, 2016 CONTRIBUTING.md Add CONTRIBUTING.md Mar 20, 2013 LICENSE strut out sbt project. Aug 2, 2012 NOTICE copy in cassandra's MurmurHash Oct 23, 2012 README.md Update Apache Storm link Jun 1, 2016 build.sbt Minor fixer for backwards compatibility (proposed 0.12.1) (#535) Jun 25, 2016 sbt Update the build Feb 1, 2016 version.sbt Setting version to 0.12.2-SNAPSHOT Jun 25, 2016 README.md Algebird Abstract algebra for Scala. This code is targeted at building aggregation systems (via Scalding or Apache Storm). It was originally developed as part of Scalding's Matrix API, where Matrices had values which are elements of Monoids, Groups, or Rings. Subsequently, it was clear that the code had broader application within Scalding and on other projects within Twitter. See the current API documentation for more information. What can you do with this code? > ./sbt algebird-core/console  Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_40). Type in expressions to have them evaluated. Type :help for more information.  scala> import com.twitter.algebird._ import com.twitter.algebird._  scala> import com.twitter.algebird.Operators._ import com.twitter.algebird.Operators._  scala> Map(1 -> Max(2)) + Map(1 -> Max(3)) + Map(2 -> Max(4)) res0: scala.collection.immutable.Map[Int,com.twitter.algebird.Max[Int]] = Map(2 -> Max(4), 1 -> Max(3)) In the above, the class Max[T] signifies that the + operator should actually be max (this is accomplished by providing an implicit instance of a typeclass for Max that handles +). Model a wide class of ""reductions"" as a sum on some iterator of a particular value type. For example, average, moving average, max/min, set union, approximate set size (in much less memory with HyperLogLog), approximate item counting (using CountMinSketch). All of these combine naturally in tuples, vectors, maps, options and more standard scala classes. Implementations of Monoids for interesting approximation algorithms, such as Bloom filter, HyperLogLog and CountMinSketch. These allow you to think of these sophisticated operations like you might numbers, and add them up in hadoop or online to produce powerful statistics and analytics. Community and Documentation This, and all github.com/twitter projects, are under the Twitter Open Source Code of Conduct. Additionally, see the Typelevel Code of Conduct for specific examples of harassing behavior that are not tolerated. To learn more and find links to tutorials and information around the web, check out the Algebird Wiki. The latest ScalaDocs are hosted on Algebird's Github Project Page. Discussion occurs primarily on the Algebird mailing list. Issues should be reported on the GitHub issue tracker. Maven Algebird modules are available on maven central. The current groupid and version for all modules is, respectively, ""com.twitter"" and 0.11.0. Current published artifacts are algebird-core_2.11 algebird-core_2.10 algebird-test_2.11 algebird-test_2.10 algebird-util_2.11 algebird-util_2.10 algebird-bijection_2.11 algebird-bijection_2.10 The suffix denotes the scala version. Questions Why not use spire? We didn't know about it when we started this code, but it seems like we're more focused on large scale analytics. Why not use Scalaz's Monoid trait? The answer is a mix of the following: The trait itself is tiny, we just need zero and plus, it is the implementations for all the types that are important. We wrote a code generator to derive instances for all the tuples, and by hand wrote monoids for List, Set, Option, Map, and several other objects used for counting (DecayedValue for exponential decay, AveragedValue for averaging, HyperLogLog for approximate cardinality counting). It's the instances that are useful in scalding and elsewhere. We needed this to work in scala 2.8, and it appeared that Scalaz 7 didn't support 2.8. We've since moved to 2.9, though. We also needed Ring and Field, and those are not (as of the writing of the code) in Scalaz. If you want to interop, it is trivial to define implicit conversions to and from Scalaz Monoid. Authors Oscar Boykin http://twitter.com/posco Avi Bryant http://twitter.com/avibryant Edwin Chen http://twitter.com/echen ellchow http://github.com/ellchow Mike Gagnon https://twitter.com/gmike Moses Nakamura https://twitter.com/mnnakamura Steven Noble http://twitter.com/snoble Sam Ritchie http://twitter.com/sritchie Ashutosh Singhal http://twitter.com/daashu Argyris Zymnis http://twitter.com/argyris License Copyright 2015 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/algebird"	"Abstract Algebra for Scala."	"true"
"Science and Data Analysis"	"Axle"	"http://axle-lang.org"	"Spire-based DSL for scientific cloud computing."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"46"	"7"	"8"	"GitHub - axlelang/axle: Axle Domain Specific Language for Scientific Cloud Computing and Visualization Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 7 Star 46 Fork 8 axlelang/axle Code Issues 3 Pull requests 0 Wiki Pulse Graphs Axle Domain Specific Language for Scientific Cloud Computing and Visualization http://axle-lang.org 2,371 commits 1 branch 18 releases Fetching contributors Scala 88.6% Java 8.0% Python 3.4% Scala Java Python Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.1.17 v0.1.16 v0.1.15 v0.1.14 v0.1.13 v0.1-M12 v0.1-M11 v0.1-M10 v0.1-M9 v0.1-M8 v0.1-M7 v0.1-M6 v0.1-M5 v0.1-M4 v0.1-M3 v0.1-M2 v0.1-M1 predehigherkinding Nothing to show New pull request Latest commit 1beb43b Jun 12, 2016 adampingel committed on GitHub Merge pull request #102 from adampingel/versions-2016-06-12 … update versions of dependencies Permalink Failed to load latest commit information. axle-algorithms/src axle-core axle-figaro/src/main/scala/axle/figaro axle-games/src axle-hbase/src/main/scala/axle/hbase axle-jblas/src axle-jcublas/src/main/scala/axle/jcublas axle-joda/src/main/scala/axle/joda axle-jung/src axle-languages axle-mtj/src/main/scala/axle/mtj axle-repl/src/main/scala/axle/repl axle-scalding/src/main/scala/axle/scalding axle-spark axle-test/src/test/scala/axle axle-visualize/src project .gitignore .travis.yml LICENSE.txt convert to Apache 2.0 license Apr 13, 2015 README.md axle publishing.txt scalastyle-config.xml version.sbt README.md See axle-lang.org Run sbt ""project axle-test"" console to check out some of the code in a scala REPL. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/axlelang/axle"	"Spire-based DSL for scientific cloud computing."	"true"
"Science and Data Analysis"	"Breeze ★ 1451 ⧗ 1"	"https://github.com/scalanlp/breeze"	"Breeze is a numerical processing library for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1618"	"171"	"424"	"GitHub - scalanlp/breeze: Breeze is a numerical processing library for Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 171 Star 1,618 Fork 424 scalanlp/breeze Code Issues 84 Pull requests 10 Wiki Pulse Graphs Breeze is a numerical processing library for Scala. http://www.scalanlp.org 3,222 commits 6 branches 25 releases 68 contributors Scala 99.8% Java 0.2% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags fusion lp master scala-2.10.0RC1 signal upstream/master Nothing to show v0.2 scalanlp-core-0.3.1 scalanlp-core-0.2.1 scalanlp-core-0.1 releases/0.3 releases/v0.12 releases/v0.11.2 releases/v0.11.1 releases/v0.11 releases/v0.10 releases/v0.9 releases/v0.8 releases/v0.7 releases/v0.6.1 releases/v0.6 releases/v0.5.2 releases/v0.5.1 releases/v0.5 releases/v0.4 releases/v0.2.1 releases/v0.1 release/v0.8.1 killldcounters LAST_SEMIRING LAST_GRAPHS Nothing to show New pull request Latest commit d2400fe Jul 14, 2016 dlwh committed on GitHub Merge pull request #562 from yanboliang/argtopk … Fix bug of argtopk applying to DenseVector and k == length of vector Permalink Failed to load latest commit information. benchmark centralize configuration Jun 17, 2016 macros centralize configuration Jun 17, 2016 math Fix bug of argtopk applying for DenseVector and k == length of vector Jul 14, 2016 natives centralize configuration Jun 17, 2016 project adjust snapshot polciy Jun 17, 2016 viz disable the new max impl because of 2.10 Jul 16, 2015 .gitignore .gitignore from other machine... we'll merge. Aug 9, 2013 .travis.yml stop using openjdk in test builds Jan 16, 2016 LICENSE Refactor ADMM based ProximalMinimizer to breeze.optimize.quadratic fo… Oct 31, 2014 NOTICE infiniteIteration API in FirstOrderMinimizer takes initialState;PQN b… Mar 11, 2015 README.md Update README.md Jun 17, 2016 build.sbt centralize configuration Jun 17, 2016 version.sbt whoop Jun 17, 2016 README.md Breeze Breeze is a library for numerical processing. It aims to be generic, clean, and powerful without sacrificing (much) efficiency. The current snapshot version is 0.13-0598e003cfa7f00f76919aa556009ad6d4fc1332. The latest release is 0.12. Documentation https://github.com/scalanlp/breeze/wiki/Quickstart https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet Scaladoc (Scaladoc is typically horribly out of date, and not a good way to learn Breeze.) There is also the scala-breeze google group for general questions and discussion. Using Breeze Building it yourself. This project can be built with sbt 0.13 SBT For SBT, Add these lines to your SBT project definition: For SBT versions 0.13.x or later libraryDependencies  ++= Seq(   // other dependencies here   ""org.scalanlp"" %% ""breeze"" % ""0.12"",   // native libraries are not included by default. add this if you want them (as of 0.7)   // native libraries greatly improve performance, but increase jar sizes.    // It also packages various blas implementations, which have licenses that may or may not   // be compatible with the Apache License. No GPL code, as best I know.   ""org.scalanlp"" %% ""breeze-natives"" % ""0.12"",   // the visualization library is distributed separately as well.    // It depends on LGPL code.     ""org.scalanlp"" %% ""breeze-viz"" % ""0.12""      // To depend on snapshot versions, use:       ""org.scalanlp"" %% ""breeze"" % ""latest.integration"", )    resolvers ++= Seq(   // other resolvers here   // if you want to use snapshot builds (currently 0.13-0598e003cfa7f00f76919aa556009ad6d4fc1332-SNAPSHOT), use this.   ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/"",   ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" )  // or 2.11.5 scalaVersion := ""2.10.4"" For more details on the optional breeze-natives module, please watch Sam Halliday's talk at Scala eXchange 2014 High Performance Linear Algebra in Scala (follow along with high-res slides). Maven Maven looks like this: <dependency>   <groupId>org.scalanlp</groupId>   <artifactId>breeze_2.10</artifactId> <!-- or 2.11 -->   <version>0.12</version> </dependency> Other build tools http://mvnrepository.com/artifact/org.scalanlp/breeze_2.10/0.12 (as an example) is a great resource for finding other configuration examples for other build tools. See documentation (linked above!) for more information on using Breeze. History Breeze is the merger of the ScalaNLP and Scalala projects, because one of the original maintainers is unable to continue development. The Scalala parts are largely rewritten. (c) David Hall, 2009 - Portions (c) Daniel Ramage, 2009 - 2011 Contributions from: Jason Zaugg (@retronym) Alexander Lehmann (@afwlehmann) Jonathan Merritt (@lancelet) Keith Stevens (@fozziethebeat) Jason Baldridge (@jasonbaldridge) Timothy Hunter (@tjhunter) Dave DeCaprio (@DaveDeCaprio) Daniel Duckworth (@duckworthd) Eric Christiansen (@emchristiansen) Marc Millstone (@splittingfield) Mérő László (@laci37) Alexey Noskov (@alno) Devon Bryant (@devonbryant) Kentaroh Takagaki (@ktakagaki) Sam Halliday (@fommil) Chris Stucchio (@stucchio) Xiangrui Meng (@mengxr) Gabriel Schubiner (@gabeos) Debasish Das (@debasish83) Julien Dumazert (@DumazertJulien) Matthias Langer (@bashimao) Corporate (Code) Contributors: Semantic Machines (@semanticmachines) ContentSquare Big Data Analytics, Verizon Lab, Palo Alto crealytics GmbH, Berlin/Passau, Germany And others (contact David Hall if you've contributed and aren't listed). Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalanlp/breeze"	"Breeze is a numerical processing library for Scala."	"true"
"Science and Data Analysis"	"Chalk ★ 200 ⧗ 0"	"https://github.com/scalanlp/chalk"	"Chalk is a natural language processing library."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"206"	"29"	"37"	"GitHub - scalanlp/chalk: Chalk is a natural language processing library. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 29 Star 206 Fork 37 scalanlp/chalk Code Issues 6 Pull requests 1 Wiki Pulse Graphs Chalk is a natural language processing library. 102 commits 2 branches 8 releases Fetching contributors Scala 99.2% Other 0.8% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show v1.3.0 v1.2.0 v1.1.3 v1.1.2 v1.1.1 v1.1.0 releases/v1.3.2 releases/v1.3.1 Nothing to show New pull request Latest commit 312a245 Oct 16, 2014 jasonbaldridge Merge pull request #26 from Schibsted-ADA/build … Make build work for scala 2.11 && Bump versions Permalink Failed to load latest commit information. bin Updated version of Scala and SBT, plus other minor cleanups. Jul 20, 2013 project Make build work for scala 2.11 && Bump versions Oct 16, 2014 src 1.3.1 May 28, 2014 .gitignore #11 progress: Now compiles with Nak reorg. Mar 13, 2013 LICENSE Added Apache 2.0 license. Dec 2, 2012 NOTICE Added notice regarding MASC data. Nov 3, 2013 README.md Update README.md May 29, 2014 build Set default encoding to UTF8 so Chalk compiles effortlessly on Macs. Dec 18, 2012 build.sbt Make build work for scala 2.11 && Bump versions Oct 16, 2014 sbt-launch.jar Up to version 0.13 of SBT. Nov 3, 2013 README.md Chalk Introduction Chalk is a library for natural language processing (NLP). What's inside The latest stable version is 1.3.2. Changes from the previous release include: Initial implementation of functional pipelines for NLP applications, and actor-based pipelines based on those components. See the CHANGELOG for changes in previous versions. Using Chalk In SBT: libraryDependencies += ""org.scalanlp"" %% ""chalk"" % ""1.3.0""  In Maven: <dependency>    <groupId>org.scalanlp</groupId>    <artifactId>chalk</artifactId>    <version>1.3.0</version> </dependency>  Requirements Version 1.6 of the Java 2 SDK (http://java.sun.com) Configuring your environment variables Set JAVA_HOME to match the top level directory containing the Java installation you want to use. If you want to be able to use the chalk command line executable, set CHALK_DIR to where you put Chalk, and then add the directory CHALK_DIR/bin to your path. Building the system from source Chalk uses SBT (Simple Build Tool) with a standard directory structure. To build Chalk, type (in the CHALK_DIR directory): $ ./build update compile  This will compile the source files and put them in ./target/classes. If this is your first time running it, you will see messages about Scala being downloaded -- this is fine and expected. Once that is over, the Chalk code will be compiled. To try out other build targets, do: $ ./build  To make sure all the tests pass, do: $ ./build test  Documentation for SBT is at http://www.scala-sbt.org/ Questions or suggestions? Email Jason Baldridge: jasonbaldridge@gmail.com Or, create an issue: https://github.com/scalanlp/chalk/issues Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalanlp/chalk"	"Chalk is a natural language processing library."	"true"
"Science and Data Analysis"	"FACTORIE ★ 393 ⧗ 1"	"https://github.com/factorie/factorie"	"A toolkit for deployable probabilistic modeling, implemented as a software library in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"423"	"70"	"131"	"GitHub - factorie/factorie: FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 70 Star 423 Fork 131 factorie/factorie Code Issues 13 Pull requests 5 Pulse Graphs FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference. 5,120 commits 5 branches 23 releases 56 contributors Scala 96.6% Emacs Lisp 2.2% Other 1.2% Scala Emacs Lisp Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master tackbp2014 tackbp2015 tackbp2016 Nothing to show withAnnotations factorie_2.11-1.2 factorie_2.11-1.1.1 factorie_2.11-1.1 factorie_2.10-1.1 factorie-1.1 factorie-1.0.0-RC2 factorie-1.0.0-RC1 factorie-1.0.0-M7 factorie-1.0.0-M6 factorie-1.0.0-M5 factorie-1.0.0-M4 factorie-1.0.0-M3 factorie-1.0.0-M2 factorie-1.0.0-M1 factorie-1.0 factorie-0.10.2 factorie-0.10.1 factorie-0.9.2 factorie-0.9.1 factorie-0.9.0 factorie-0.8.1 factorie-0.8.0 Nothing to show New pull request Latest commit e848f78 Jun 13, 2016 strubell committed on GitHub Merge pull request #371 from JamesSullivan/classify … Fix for --write-classifications see this thread: Permalink Failed to load latest commit information. bin Changed factorie version capture format in bin/fac Apr 28, 2016 doc Update copyright year and add notice to some files Apr 7, 2016 lib/emacs Add Makefile and emacs scala-mode files, for use in an indentation-fi… Nov 24, 2009 project Fixes - File name too long during build. This can happen on encrypted… May 18, 2016 src Fix for --write-classifications see this thread: Jun 13, 2016 .gitignore use java nextGaussian to get Gaussian random numbers, because the imp… May 6, 2015 .hgignore undo hgignore that was removed, and factorie.iml that was added Jan 25, 2013 .travis.yml try to fix travis without turning off tests Apr 12, 2016 CHANGELOG.txt update changelog Nov 17, 2014 HACKING.txt Numerous spelling corrections and a couple of style fixes. Jan 11, 2015 LICENSE.txt split LICENSE/NOTICE files, update licensing on AhoCorasick and friends Apr 22, 2015 Makefile Fixed chatty echo in makefile default rule Aug 8, 2014 NOTICE.txt split LICENSE/NOTICE files, update licensing on AhoCorasick and friends Apr 22, 2015 README.md fix of example of using classify --read-text-dirs May 11, 2015 TODO.txt Additional comments. Oct 6, 2013 pom.xml Fixes - File name too long during build. This can happen on encrypted… May 18, 2016 sbt reverted irrelelvant sbt change Sep 2, 2015 README.md FACTORIE This directory contains the source of FACTORIE, a toolkit for probabilistic modeling based on imperatively-defined factor graphs. More information, see the FACTORIE webpage. Installation Installation relies on Maven, version 3. If you don't already have maven, install it from http://maven.apache.org/download.html. Alternatively, you can use sbt as outlined below (a script for running sbt comes bundled with Factorie). To compile type $ mvn compile  To accomplish the same with sbt, type $ ./sbt compile  You might need additional memory. If so, for sbt type export SBT_OPTS=""$SBT_OPTS -Xmx1g""  and for Maven type: export MAVEN_OPTS=""$MAVEN_OPTS -Xmx1g -XX:MaxPermSize=128m""  To create a self-contained .jar, that contains FACTORIE plus all its dependencies, including the Scala runtime, type $ mvn -Dmaven.test.skip=true package -Pjar-with-dependencies  To accomplish the same with sbt, type $ ./sbt assembly  To create a similar self-contained .jar that also contains all resources needed for NLP (including our lexicons and pre-trained model parameters), type $ mvn -Dmaven.test.skip=true package -Pnlp-jar-with-dependencies  To accomplish the same with sbt, type $ ./sbt -J-Xmx2G with-nlp-resources:assembly   Try out a simple example To get an idea what a simple FACTORIE program might look like, open one of the class files in the tutorial package $ ls src/main/scala/cc/factorie/tutorial  To run one of these examples using maven type $ mvn scala:run -DmainClass=cc.factorie.tutorial.Grid  Try out implemented NLP models Then you can run some FACTORIE tools from the command-line. For example, you can run many natural language processing tools. $ bin/fac nlp --wsj-forward-pos --conll-chain-ner  will launch an NLP server that will perform part-of-speech tagging and named entity recognition in its input. The server listens for text on a socket, and spawns a parallel document processor on each request. To feed it input, type in a separate shell $ echo ""I told Mr. Smith to take a job at IBM in Raleigh."" | nc localhost 3228  You can also run a latent Dirichlet allocation (LDA) topic model. Assume that ""mytextdir"" is a directory name containing many plain text documents each in its own file. Then typing $ bin/fac lda --read-dirs mytextdir --num-topics 20 --num-iterations 100  will run 100 iterations of a sparse collapsed Gibbs sampling on all the documents, and print out the results every 10 iterations. FACTORIE's LDA implementation is faster than MALLET's. You can also train a document classifier. Assume that ""sportsdir"" and ""politicsdir"" are each directories that contain plan text files in the categories sports and politics. Typing $ bin/fac classify --read-text-dirs sportsdir,politicsdir --write-classifier mymodel.factorie  will train a log-linear by maximum likelihood (MaxEnt) and save it in the file ""mymodel.factorie"". The above are simply a few simple command-line options. Internally the FACTORIE library contains extensive and general facilities for factor graphs: data representation, model structure, inference, learning. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/factorie/factorie"	"A toolkit for deployable probabilistic modeling, implemented as a software library in Scala."	"true"
"Science and Data Analysis"	"Figaro ★ 289 ⧗ 0"	"https://github.com/p2t2/figaro"	"Figaro is a probabilistic programming language that supports development of very rich probabilistic models."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"352"	"59"	"66"	"GitHub - p2t2/figaro: Figaro Programming Language and Core Libraries Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 59 Star 352 Fork 66 p2t2/figaro Code Issues 52 Pull requests 0 Wiki Pulse Graphs Figaro Programming Language and Core Libraries 1,064 commits 19 branches 15 releases Fetching contributors Scala 90.9% TeX 9.1% Scala TeX Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags Alex_DEV2.2 BR_3.0.0 DEV2.1 DEV2.2 DEV2.2.1 DEV2.2.2 DEV2.3.0.0 DEV2.4.0 DEV2.5.0 DEV3.0.0 DEV3.1.0 DEV3.2.0 DEV3.2.1 DEV3.2.1.1 DEV3.3 DEV3.3.0.1 DEV4.0 DEV4.1 master Nothing to show 4.0.0.0 3.3.0.0 3.2.1.1 3.2.1.0 3.2.0.0 3.1.0.0 3.0.0.0 2.5.0.0 2.4.0.0 2.3.0.0 2.2.2.0 2.2.1.0 2.2.0.0 2.1.0.0 2.0.0.0 Nothing to show New pull request Latest commit 290abb5 Mar 14, 2016 mreposa Merge pull request #555 from p2t2/DEV4.0 … Dev4.0 Permalink Failed to load latest commit information. Figaro Fixes to unit test Mar 11, 2016 FigaroExamples Remove references to Figaro package values Feb 25, 2016 FigaroLaTeX Tutorial updates Mar 8, 2016 doc Updated Release Notes for 4.0 Mar 11, 2016 project More updates to book unit tests Mar 10, 2016 .gitignore Containers and variable size arrays. Also has some bug fixes to impor… Nov 27, 2014 LICENSE For use in Figaro 2.0 Distribution Sep 23, 2013 README.md Fix broken links Jul 22, 2015 README.md Figaro Programming Language & Core Libraries Figaro is a probabilistic programming language that supports development of very rich probabilistic models and provides reasoning algorithms that can be applied to models to draw useful conclusions from evidence. Both model representation and reasoning algorithm development can be challenging tasks. Figaro makes it possible to express probabilistic models using the power of programming languages, giving the modeler the expressive tools to create a wide variety of models. Figaro comes with a number of built-in reasoning algorithms that can be applied automatically to new models. In addition, Figaro models are data structures in the Scala programming language, which is interoperable with Java, and can be constructed, manipulated, and used directly within any Scala or Java program. Figaro is free and is released under an open-source license. The current, stable binary release of Figaro can be found here. For more information please see the Figaro Release Notes and Figaro Tutorial. Documentation of the Figaro library interface can be found here. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/p2t2/figaro"	"Figaro is a probabilistic programming language that supports development of very rich probabilistic models."	"true"
"Science and Data Analysis"	"MGO ★ 12 ⧗ 85"	"https://github.com/openmole/mgo"	"Modular multi-objective evolutionary algorithm optimization library enforcing immutability."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"12"	"4"	"2"	"GitHub - openmole/mgo: A cake for evolutionary algorithm in scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 12 Fork 2 openmole/mgo Code Issues 3 Pull requests 0 Pulse Graphs A cake for evolutionary algorithm in scala 903 commits 5 branches 73 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags debugCrowding gh-pages hitmap master rjmcmc Nothing to show v2.0 v1.71 v1.70 v1.69 v1.68 v1.67 v1.66 v1.65 v1.64 v1.63 v1.62 v1.61 v1.60 v1.59 v1.58 v1.57 v1.56 v1.55 v1.54 v1.53 v1.52 v1.51 v1.50 v1.49 v1.48 v1.47 v1.46 v1.45 v1.44 v1.43 v1.42 v1.41 v1.40 v1.39 v1.38 v1.37 v1.36 v1.35 v1.34 mgo-1.33 mgo-1.32 mgo-1.31 mgo-1.30 mgo-1.29 mgo-1.28 mgo-1.27 mgo-1.26 mgo-1.25 mgo-1.24 mgo-1.23 mgo-1.22 mgo-1.21 mgo-1.20 mgo-1.19 mgo-1.18 mgo-1.17 mgo-1.16 mgo-1.15 mgo-1.14 mgo-1.13 mgo-1.12 mgo-1.11 mgo-1.10 mgo-1.9 mgo-1.8 mgo-1.7 mgo-1.6 mgo-1.5 mgo-1.4 mgo-1.3 mgo-1.2 mgo-1.1 mgo-1.0 Nothing to show New pull request Latest commit ee2d2fb Jun 30, 2016 romainreuillon Use field to access the hit map. Permalink Failed to load latest commit information. project src .gitignore README.md build.sbt version.sbt README.md MGO MGO is a purely functionnal scala library based for evolutionary / genetic algorithms: enforcing immutability, exposes a modular and extensible architecture, implements state of the art algorithms, handles noisy (stochastic) fitness functions, implements auto-adaptatative algortihms. MGO implements NGSAII, CP (Calibration Profile), PSE (Pattern Search Experiment). Licence MGO is licenced under the GNU Affero GPLv3 software licence.  Example Define a problem, for instance ZDT4: MGO is being refactored  Define the optimisation algorithm, for instance NSGAII: MGO is being refactored  Run the optimisation: MGO is being refactored  For more examples, have a look at the main/scala/fr/iscpif/mgo/test directory in the repository. SBT dependency libraryDependencies += ""fr.iscpif"" %% ""mgo"" % ""version""  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/openmole/mgo"	"Modular multi-objective evolutionary algorithm optimization library enforcing immutability."	"true"
"Science and Data Analysis"	"MLLib"	"https://spark.apache.org/mllib/"	"Machine Learning framework for Spark"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"MLlib | Apache Spark        MLlib Toggle navigation Download Libraries SQL and DataFrames Spark Streaming MLlib (machine learning) GraphX (graph) Third-Party Packages Documentation Latest Release (Spark 1.6.2) Other Resources Examples Community Mailing Lists Events and Meetups Project History Powered By Project Committers Issue Tracker FAQ Apache Software Foundation Apache Homepage License Sponsorship Thanks Security Latest News Spark 1.6.2 released (Jun 25, 2016) Call for Presentations for Spark Summit EU is Open (Jun 16, 2016) Preview release of Spark 2.0 (May 26, 2016) Spark Summit (June 6, 2016, San Francisco) agenda posted (Apr 17, 2016) Archive Download Spark Built-in Libraries: SQL and DataFrames Spark Streaming MLlib (machine learning) GraphX (graph) Third-Party Packages MLlib is Apache Spark's scalable machine learning library. Ease of Use Usable in Java, Scala, Python, and SparkR. MLlib fits into Spark's APIs and interoperates with NumPy in Python (starting in Spark 0.9). You can use any Hadoop data source (e.g. HDFS, HBase, or local files), making it easy to plug into Hadoop workflows. points = spark.textFile(""hdfs://..."")               .map(parsePoint) model = KMeans.train(points, k=10) Calling MLlib in Python Performance High-quality algorithms, 100x faster than MapReduce. Spark excels at iterative computation, enabling MLlib to run fast. At the same time, we care about algorithmic performance: MLlib contains high-quality algorithms that leverage iteration, and can yield better results than the one-pass approximations sometimes used on MapReduce. Logistic regression in Hadoop and Spark Easy to Deploy Runs on existing Hadoop clusters and data. If you have a Hadoop 2 cluster, you can run Spark and MLlib without any pre-installation. Otherwise, Spark is easy to run standalone or on EC2 or Mesos. You can read from HDFS, HBase, or any Hadoop data source. Algorithms MLlib contains the following algorithms and utilities: logistic regression and linear support vector machine (SVM) classification and regression tree random forest and gradient-boosted trees recommendation via alternating least squares (ALS) clustering via k-means, bisecting k-means, Gaussian mixtures (GMM), and power iteration clustering topic modeling via latent Dirichlet allocation (LDA) survival analysis via accelerated failure time model singular value decomposition (SVD) and QR decomposition principal component analysis (PCA) linear regression with L1, L2, and elastic-net regularization isotonic regression multinomial/binomial naive Bayes frequent itemset mining via FP-growth and association rules sequential pattern mining via PrefixSpan summary statistics and hypothesis testing feature transformations model evaluation and hyper-parameter tuning Refer to the MLlib guide for usage examples. Community MLlib is developed as part of the Apache Spark project. It thus gets tested and updated with each Spark release. If you have questions about the library, ask on the Spark mailing lists. MLlib is still a young project and welcomes contributions. If you'd like to submit an algorithm to MLlib, read how to contribute to Spark and send us a patch! Getting Started To get started with MLlib: Download Spark. MLlib is included as a module. Read the MLlib guide, which includes various usage examples. Learn how to deploy Spark on a cluster if you'd like to run in distributed mode. You can also run locally on a multicore machine without any setup. Download Apache Spark Includes MLlib Apache Spark, Spark, Apache, and the Spark logo are trademarks of The Apache Software Foundation."	"null"	"null"	"Machine Learning framework for Spark"	"true"
"Science and Data Analysis"	"ND4S"	"https://github.com/deeplearning4j/nd4s"	"N-Dimensional arrays and linear algebra for Scala with an API similar to Numpy. ND4S is a scala wrapper around."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"134"	"26"	"23"	"GitHub - deeplearning4j/nd4s: ND4S: N-Dimensional Arrays for Scala. Scientific Computing a la Numpy. Based on ND4J. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 26 Star 134 Fork 23 deeplearning4j/nd4s Code Issues 5 Pull requests 0 Wiki Pulse Graphs ND4S: N-Dimensional Arrays for Scala. Scientific Computing a la Numpy. Based on ND4J. http://nd4j.org/ 151 commits 2 branches 0 releases 6 contributors Scala 99.8% Shell 0.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/0.4-rc3.9 master Nothing to show Nothing to show New pull request Latest commit 8192dfc May 31, 2016 agibsonccc update nd4j versions Permalink Failed to load latest commit information. project Correcting implicit class names May 19, 2016 src Use Scalaxy/loops to optimize Range integer macros May 19, 2016 .gitignore moved nd4j-scala-api from nd4j repo to nd4s repo Jul 14, 2015 .travis.yml updated to rc3.8 / support Scala 2.12.0-M3 Jan 1, 2016 LICENSE Initial commit Jul 13, 2015 README.md Update README.md Feb 11, 2016 build.sbt update nd4j versions May 31, 2016 nd4j_install.sh install latest nd4j snapshot as option Dec 22, 2015 README.md ND4S: Scala bindings for ND4J ND4S is open-source Scala bindings for ND4J. Released under an Apache 2.0 license. Main Features NDArray manipulation syntax sugar with safer type. NDArray slicing syntax, similar with NumPy. Installation Install via Maven ND4S is already included in official Maven repositories. With IntelliJ, incorporation of ND4S is easy: just create a new Scala project, go to ""Project Settings""/Libraries, add ""From Maven..."", and search for nd4s. No need for git-cloning & compiling! Clone from the GitHub Repo ND4S is actively developed. You can clone the repository, compile it, and reference it in your project. Clone the repository: $ git clone https://github.com/deeplearning4j/nd4s.git  Compile the project: $ cd nd4s $ sbt +publish-local  Try ND4S in REPL The easiest way to play ND4S around is cloning this repository and run the following command. $ cd nd4s $ sbt test:console  It starts REPL with importing org.nd4s.Implicits._ and org.nd4j.linalg.factory.Nd4j automatically. It uses jblas backend at default. scala> val arr = (1 to 9).asNDArray(3,3)  arr: org.nd4j.linalg.api.ndarray.INDArray = [[1.00,2.00,3.00]  [4.00,5.00,6.00]  [7.00,8.00,9.00]]  scala> val sub = arr(0->2,1->3) sub: org.nd4j.linalg.api.ndarray.INDArray = [[2.00,3.00]  [5.00,6.00]] CheatSheet(WIP) ND4S syntax Equivalent NumPy syntax Result Array(Array(1,2,3),Array(4,5,6)).toNDArray np.array([[1, 2 , 3], [4, 5, 6]]) [[1.0, 2.0, 3.0] [4.0, 5.0, 6.0]] val arr = (1 to 9).asNDArray(3,3) arr = np.array([[1, 2 , 3], [4, 5, 6],[7, 8, 9]]) [[1.0, 2.0, 3.0] [4.0, 5.0, 6.0] ,[7.0, 8.0, 9.0]] arr(0,0) arr[0,0] 1.0 arr(0,->) arr[0,:] [1.0, 2.0, 3.0] arr(--->) arr[...] [[1.0, 2.0, 3.0] [4.0, 5.0, 6.0] ,[7.0, 8.0, 9.0]] arr(0 -> 3 by 2, ->) arr[0:3:2,:] [[1.0, 2.0, 3.0] [7.0, 8.0, 9.0]] arr(0 to 2 by 2, ->) arr[0:3:2,:] [[1.0, 2.0, 3.0] [7.0, 8.0, 9.0]] arr.filter(_ > 3) [[0.0, 0.0, 0.0] [4.0, 5.0, 6.0] ,[7.0, 8.0, 9.0]] arr.map(_ % 3) [[1.0, 2.0, 0.0] [1.0, 2.0, 0.0] ,[1.0, 2.0, 0.0]] arr.filterBit(_ < 4) [[1.0, 1.0, 1.0] [0.0, 0.0, 0.0] ,[0.0, 0.0, 0.0]] arr + arr arr + arr [[2.0, 4.0, 6.0] [8.0, 10.0, 12.0] ,[14.0, 16.0, 18.0]] arr * arr arr * arr [[1.0, 4.0, 9.0] [16.0, 25.0, 36.0] ,[49.0, 64.0, 81.0]] arr dot arr np.dot(arr, arr) [[30.0, 36.0, 42.0] [66.0, 81.0, 96.0] ,[102.0, 126.0, 150.0]] arr.sumT np.sum(arr) 45.0 //returns Double value val comp = Array(1 + i, 1 + 2 * i).toNDArray comp = np.array([1 + 1j, 1 + 2j]) [1.0 + 1.0i ,1.0 + 2.0i] comp.sumT np.sum(comp) 2.0 + 3.0i //returns IComplexNumber value for(row <- arr.rowP if row.get(0) > 1) yield row*2 [[8.00,10.00,12.00] [14.00,16.00,18.00]] val tensor = (1 to 8).asNDArray(2,2,2) tensor = np.array([[[1, 2], [3, 4]],[[5,6],[7,8]]]) [[[1.00,2.00] [3.00,4.00]] [[5.00,6.00] [7.00,8.00]]] for(slice <- tensor.sliceP if slice.get(0) > 1) yield slice*2 [[[10.00,12.00][14.00,16.00]]] arr(0 -> 3 by 2, ->) = 0 [[0.00,0.00,0.00] [4.00,5.00,6.00] [0.00,0.00,0.00]] Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/deeplearning4j/nd4s"	"N-Dimensional arrays and linear algebra for Scala with an API similar to Numpy. ND4S is a scala wrapper around."	"true"
"Science and Data Analysis"	"ND4J"	"http://nd4j.org/"	"N-Dimensional arrays and linear algebra for Scala with an API similar to Numpy. ND4S is a scala wrapper around."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"24"	"4"	"1"	"GitHub - whilo/nd4clj: An implementation of core.matrix protocols with nd4j. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 24 Fork 1 whilo/nd4clj Code Issues 0 Pull requests 0 Pulse Graphs An implementation of core.matrix protocols with nd4j. 10 commits 1 branch 0 releases Fetching contributors Clojure 100.0% Clojure Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 10937fe Oct 17, 2015 whilo Add requirements and try to line out a roadmap (TODOs). Permalink Failed to load latest commit information. doc Add requirements and try to line out a roadmap (TODOs). Oct 17, 2015 src/nd4clj Add requirements and try to line out a roadmap (TODOs). Oct 17, 2015 test/nd4clj .gitignore Ignore files for CCW Sep 8, 2015 LICENSE Check in initial partial implementation. Aug 16, 2015 README.md Add requirements and try to line out a roadmap (TODOs). Oct 17, 2015 project.clj README.md nd4clj An implementation of core.matrix protocols with nd4j. Most importantly this is supposed to allow the usage of the jcublas backend of nd4j and the integration of deeplearning4j code in Clojure projects. Requirements be core.matrix compliant don't introduce significant performance overhead make implementation of machine learning algorithms in Clojure more feasible avoid wrapping INDArray Matrices for direct interop with deeplearning4j TODO make shaping of Nd4j compatible with core.matrix, e.g. introduce Vector type, see https://github.com/mikera/core.matrix/wiki/Vectors-vs.-matrices fix other outstanding issues in compliance tests evaluate the GPU backend in comparison to theano, e.g. with boltzmann implement example with deeplearning4j and core.matrix dependent code, e.g. boltzmann, incanter 2.0 Usage Not really usable yet, as it is not compliant to core.matrix. License Copyright © 2015 Christian Weilbach Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/whilo/clj-nd4j"	"N-Dimensional arrays and linear algebra for Scala with an API similar to Numpy. ND4S is a scala wrapper around."	"true"
"Science and Data Analysis"	"OpenMOLE ★ 11 ⧗ 137"	"https://github.com/ISCPIF/openmole"	"OpenMOLE (Open MOdeL Experiment) is a workflow engine designed to leverage the computing power of distributed execution environments for naturally parallel processes."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"https://github.com/ISCPIF/openmole"	"OpenMOLE (Open MOdeL Experiment) is a workflow engine designed to leverage the computing power of distributed execution environments for naturally parallel processes."	"false"
"Science and Data Analysis"	"OscaR"	"https://bitbucket.org/oscarlib/oscar/wiki/Home"	"a Scala toolkit for solving Operations Research problems"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"oscarlib / OscaR / wiki / Home — Bitbucket  Bitbucket Features Pricing owner/repository English English 日本語 Sign up Log in OscaR Actions Clone Compare Fork Navigation Overview Source Commits Branches 2 Pull requests 8 Issues Wiki Downloads Settings HTTPS HTTPS SSH Need help cloning? Learn how to clone a repository. Clone in SourceTree Atlassian SourceTree is a free Git and Mercurial client for Windows. Atlassian SourceTree is a free Git and Mercurial client for Mac. oscar oscar OscaR Wiki Clone wiki HTTPS HTTPS SSH Need help cloning? Learn how to clone a repository. Clone in SourceTree Atlassian SourceTree is a free Git and Mercurial client for Windows. Atlassian SourceTree is a free Git and Mercurial client for Mac. OscaR / Home View History oscarlib.org OscaR is a Scala toolkit for solving Operations Research problems. The techniques currently available in OscaR are: Constraint Programming Constrained Based Local Search Linear (Integer) Programming Discrete Event Simulation Derivative Free Optimization Visualization Getting OscaR Installation instructions The nightly build of the development branch can be downloaded from our CI server (requires scala 2.11.0) OscaR Scaladoc of the development branch (also generated nightly) The OscaR user group (where you can ask your modeling/installation questions) The OscaR developer group (for OscaR developers' discussions) The Oscar blog Some statistics: Ohloh Developers' Corner Some programming guidelines and good practice for developers of the project Who is behind OscaR? This project is kindly supported by companies/institutions: UCLouvain and the BeCool research lab. CETIC who develop and maintain the CBLS package and its satellite engines n-Side who use OscaR and allocate resources to improve it. YourKit is kindly supporting this open source projects with its full-featured Java Profiler. Want to cite OscaR? Please use this reference: @Misc{oscar,   author = ""{OscaR Team}"",   title = ""{O}sca{R}: {S}cala in {O}{R}"",   year = {2012},   note = {Available from \texttt{https://bitbucket.org/oscarlib/oscar}}, }  Inspiration and Acknowledgments: Many of the ideas implemented in OscaR come from the pioneering system/language Comet developed by Pascal Van Hentenryck and Laurent Michel. This system was the first to allow users full control to create complex hybridizations between CBLS/CP/LP components. A particularly desirable feature of Comet was that the models always looked nice, and consequently were easy to read and understand. Another solver and API that inspired us, because we used it in the past, is Ilog-Solver. Ilog-Solver was the first C++ library for CP with a neat API designed by J.-F. Puget. All the researchers there inspired the whole community and continue to do so (kind of a dream team for CP ;-)). This company published many important papers for the field that were implemented in many solvers. Updated 2016-05-04 Blog Support Plans & pricing Documentation API Site status Version info Terms of service Privacy policy English Git 2.7.4.1.g5468f9e Mercurial 3.6.3 Django 1.7.11 Python 2.7.3 b7520e3d05e9 / b7520e3d05e9 @ app-107 JIRA Software Confluence Bamboo SourceTree HipChat Atlassian Help Online help Learn Git Keyboard shortcuts Latest features Bitbucket tutorials Site status Support"	"null"	"null"	"a Scala toolkit for solving Operations Research problems"	"true"
"Science and Data Analysis"	"Persist-Units ★ 6 ⧗ 29"	"https://github.com/nestorpersist/units"	"Type check units of measure in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"7"	"1"	"1"	"GitHub - nestorpersist/units: Scala Units of Measure Types Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 7 Fork 1 nestorpersist/units Code Issues 0 Pull requests 0 Pulse Graphs Scala Units of Measure Types 10 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 0a3ffb8 Mar 9, 2016 nestorpersist minor improvements Permalink Failed to load latest commit information. license add license Sep 1, 2015 project add sbt-pgp plugin Sep 1, 2015 src minor improvements Mar 9, 2016 .gitignore add worksheet Sep 1, 2015 README.md Update README.md Sep 1, 2015 build.sbt fixed up build.sbt Sep 1, 2015 README.md Units Scala Unit of Measure Types See Demo.sc for examples of use. It can be included from Maven Central via ""com.persist"" % ""scala-units_2.11"" % ""1.0.0""  This work has been supported by 47 Degrees, an international Scala and Spark training and consulting company. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nestorpersist/units"	"Type check units of measure in Scala."	"true"
"Science and Data Analysis"	"PredictionIO ★ 8944 ⧗ 0"	"https://github.com/PredictionIO/PredictionIO"	"machine learning server for developers and data scientists. Built on Apache Spark, HBase and Spray"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"9399"	"756"	"1397"	"GitHub - apache/incubator-predictionio: PredictionIO, a machine learning server for developers and ML engineers. Built on Apache Spark, HBase and Spray. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 756 Star 9,399 Fork 1,397 apache/incubator-predictionio Code Issues 29 Pull requests 9 Pulse Graphs PredictionIO, a machine learning server for developers and ML engineers. Built on Apache Spark, HBase and Spray. http://prediction.io/ 4,018 commits 2 branches 0 releases 82 contributors Scala 89.3% Shell 9.1% Other 1.6% Scala Shell Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags develop master Nothing to show Nothing to show New pull request Latest commit ec79f20 Apr 14, 2016 dszeto Show build status for develop branch Permalink Failed to load latest commit information. bin Follow redirects Apr 11, 2016 common Scalastyle fixes Mar 8, 2016 conf Merge branch 'ng' into develop Mar 8, 2016 core Merge pull request #219 from HyukjinKwon/ISSUE-206 Apr 4, 2016 data Merge pull request #219 from HyukjinKwon/ISSUE-206 Apr 4, 2016 docs Merge livedoc changes Apr 4, 2016 e2 Merge pull request #219 from HyukjinKwon/ISSUE-206 Apr 4, 2016 examples Remove useless val in case classes Mar 29, 2016 project Update artifact publishing related plugins Mar 9, 2016 sbt Change repo order, and follow redirect with curl Sep 20, 2015 tools Merge pull request #219 from HyukjinKwon/ISSUE-206 Apr 4, 2016 .gitattributes Prevent .travis.yml from being overwritten Jul 16, 2015 .gitignore Enabling unit test in travis CI May 3, 2015 .project fixed CLI of classification engine template Nov 3, 2014 .travis.yml Migrate deployment away from Travis Apr 11, 2016 CONTRIBUTING.md Add pointers to main documentation Jan 13, 2015 LICENSE.txt updating license file Mar 16, 2015 NOTICE.txt Add notice for Kryo Serializers Mar 23, 2015 README.md Show build status for develop branch Apr 14, 2016 RELEASE.md more cleanup Nov 19, 2015 build.sbt Bump to 0.9.7-SNAPSHOT Apr 14, 2016 make-distribution.sh Ignore error from not being able to copy subdirectories in bin May 20, 2015 manifest.json added the ability to grab the auto-generated config data for a storag… Sep 17, 2015 scalastyle-config.xml Remove an additional newline in scalastyle-config.xml Apr 3, 2016 README.md PredictionIO PredictionIO is an open source machine learning framework for developers and data scientists. It supports event collection, deployment of algorithms, evaluation, querying predictive results via REST APIs. To get started, check out http://prediction.io! Table of contents Installation Quick Start Bugs and Feature Requests Documentation Contributing Community Installation Five installation options available. Installing PredictionIO on Linux / Mac OS X Installing PredictionIO from Source Code If you are installing from source code, it's recommended that you clone the master branch. Launching PredictionIO on AWS Installing PredictionIO with Docker (Community contributed) Installing PredictionIO with Vagrant Quick Start Recommendation Engine Template Quick Start Guide Similiar Product Engine Template Quick Start Guide Classification Engine Template Quick Start Guide Bugs and Feature Requests Have a bug or a feature request? Please search for existing and closed issues on the Community Forum. If your problem or idea is not addressed yet, please open a new issue. Documentation PredictionIO's documentation, included in this repo in the docs/manual directory, is built with Middleman and publicly hosted at docs.prediction.io. Interested in helping with our documentation? Read Contributing Documentation. Community Keep track of development and community news. Follow @predictionio on Twitter. Read and subscribe to the Newsletter. Join the Community Forum. Contributing Please read and sign the Contributor Agreement. If you have any questions, you can post on the Contributor Forum. You can also list your projects on the Community Project page. License PredictionIO is under Apache 2 license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/PredictionIO/PredictionIO"	"machine learning server for developers and data scientists. Built on Apache Spark, HBase and Spray"	"true"
"Science and Data Analysis"	"Saddle ★ 373 ⧗ 4"	"https://github.com/saddle/saddle"	"A minimalist port of Pandas to Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"392"	"48"	"47"	"GitHub - saddle/saddle: SADDLE: Scala Data Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 48 Star 392 Fork 47 saddle/saddle Code Issues 16 Pull requests 5 Pulse Graphs SADDLE: Scala Data Library http://saddle.github.com 168 commits 17 branches 0 releases Fetching contributors HTML 96.9% JavaScript 1.5% Scala 1.4% CSS 0.2% HTML JavaScript Scala CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0.0 1.0.1 1.0.2 1.0.3 1.0.4 1.0.5 1.1.0 1.2.0 1.3.0-1 1.3.0 1.3.1 1.3.2 1.3.3 1.3.4 gh-pages master scala211x Nothing to show Nothing to show New pull request Latest commit 5acef9f May 31, 2015 adamklein Update readme a bit Permalink Failed to load latest commit information. project Release 1.3.4, update to 1.3.5-SNAPSHOT May 31, 2015 saddle-core Some javadoc updates May 31, 2015 saddle-hdf5 Some javadoc updates May 31, 2015 saddle-test-framework/src/main/scala/org/saddle/framework Consolidated test framework code into dedicated module Nov 1, 2013 .gitignore Updated .gitignore for api docs Oct 27, 2013 .travis.yml Bump travis versions Aug 24, 2014 CONTRIBUTORS First commit Mar 25, 2013 LICENSE NOTICE First commit Mar 25, 2013 README.md Update readme a bit May 31, 2015 README.md Saddle: Scala Data Library Introduction Saddle is a data manipulation library for Scala that provides array-backed, indexed, one- and two-dimensional data structures that are judiciously specialized on JVM primitives to avoid the overhead of boxing and unboxing. Saddle offers vectorized numerical calculations, automatic alignment of data along indices, robustness to missing (N/A) values, and facilities for I/O. Saddle draws inspiration from several sources, among them the R programming language & statistical environment, the numpy and pandas Python libraries, and the Scala collections library. Documentation Docs Quick Start Guide scaladoc License Saddle is distributed under the Apache License Version 2.0 (see LICENSE file). Copyright Copyright (c) 2013-2015 Novus Partners, Inc. Copyright (c) 2013-2015 The Saddle Development Team All rights reserved. Saddle is subject to a shared copyright. Each contributor retains copyright to his or her contributions to Saddle, and is free to annotate these contributions via code repository commit messages. The copyright to the entirety of the code base is shared among the Saddle Development Team, comprised of the developers who have made such contributions. The copyright and license of each file shall read as follows: Copyright (c) 2013-2015 Saddle Development Team Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Individual contributors may, if they so desire, append their names to the CONTRIBUTORS file. About the Copyright Holders Adam Klein began Saddle development in 2012 while an employee of Novus Partners, Inc. The code was released by Novus under this license in 2013. Adam Klein is lead developer. Saddle was inspired by earlier prototypes developed by Chris Lewis, Cheng Peng, & David Cru. Saddle was also inspired by previous work with pandas, a data analysis library written in Python. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/saddle/saddle"	"A minimalist port of Pandas to Scala"	"true"
"Science and Data Analysis"	"Smile"	"http://haifengl.github.io/smile/"	"Statistical Machine Intelligence and Learning Engine. Smile is a fast and comprehensive machine learning system."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2601"	"173"	"346"	"GitHub - haifengl/smile: Statistical Machine Intelligence & Learning Engine Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 173 Star 2,601 Fork 346 haifengl/smile Code Issues 9 Pull requests 1 Wiki Pulse Graphs Statistical Machine Intelligence & Learning Engine http://haifengl.github.io/smile/ 586 commits 2 branches 1 release 12 contributors Java 85.8% HTML 7.1% Scala 6.9% CSS 0.1% JavaScript 0.1% Shell 0.0% Java HTML Scala CSS JavaScript Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show v1.1.0 Nothing to show New pull request Latest commit 4b23aff Jul 13, 2016 haifengl committed on GitHub Merge pull request #106 from cranst0n/master … Update README.md Permalink Failed to load latest commit information. benchmark remove unnecessary import May 11, 2016 core support SoftClassifier by Platt Scaling Jun 30, 2016 data Fix: squid:RedundantThrowsDeclarationCheck, Throws declarations shoul… Jun 27, 2016 demo Merge branch 'master' into release/replace-synchronized-classes-fix-1 Jun 27, 2016 graph Fix: squid:RedundantThrowsDeclarationCheck, Throws declarations shoul… Jun 27, 2016 interpolation remove pom.xml Apr 29, 2016 math add abstract interface DenseMatrix Jun 29, 2016 nlp Merge branch 'master' into release/replace-synchronized-classes-fix-1 Jun 27, 2016 plot Fix: squid:RedundantThrowsDeclarationCheck, Throws declarations shoul… Jun 27, 2016 project make pgp plugin local May 11, 2016 scala provide SVD-based least square to handle rank deficient Jun 16, 2016 shell slf4j 1.7.21 Jun 24, 2016 .gitignore move src/universal to shell/src/universal Dec 28, 2015 LICENSE Initial commit Nov 20, 2014 README.md Update README.md Jul 13, 2016 build.sbt add slf4j-simple for test Jun 24, 2016 docker.sh clean up images and exit container automatically Jun 23, 2016 pkg.sh publish signed Mar 17, 2016 smile.sh run smile from stage directory May 2, 2016 smile.snb smile notebook for spark-notebook Jul 1, 2016 README.md Smile Smile (Statistical Machine Intelligence and Learning Engine) is a fast and comprehensive machine learning system. With advanced data structures and algorithms, Smile delivers state-of-art performance. Smile covers every aspect of machine learning, including classification, regression, clustering, association rule mining, feature selection, manifold learning, multidimensional scaling, genetic algorithms, missing value imputation, efficient nearest neighbor search, etc. The core algorithms are implemented in Java and self contained. Core The core machine learning library. Scala The high level operators in Scala. Math Linear algebra, statistical distribution, hypothesis tests, random number generators, sorting, special functions, various kernels, distance and rbf functions. Data Parsers for arff, libsvm, delimited text, sparse matrix, microarray gene expression data. Graph Graph algorithms on adjacency list and matrix. Interpolation One and two dimensional interpolation. NLP Natural language processing. Plot Swing-based data visualization library. Smile is well documented and please check out the project website for programming guides and more information. You can use the libraries through Maven central repository by adding the following to your project pom.xml file.     <dependency>       <groupId>com.github.haifengl</groupId>       <artifactId>smile-core</artifactId>       <version>1.1.0</version>     </dependency>  For NLP, use the artifactId smile-nlp. For Scala API, please use     <dependency>       <groupId>com.github.haifengl</groupId>       <artifactId>smile-scala_2.11</artifactId>       <version>1.1.0</version>     </dependency>  Smile comes with an interactive shell. Download pre-packaged Smile from the releases page. In the home directory of Smile, type     ./bin/smile  to enter the shell, which is based on Scala interpreter. So you can run any valid Scala expressions in the shell. In the simplest case, you can use it as a calculator. Besides, all high-level Smile operators are predefined in the shell. Be default, the shell uses up to 4GB memory. If you need more memory to handle large data, use the option -J-Xmx. For example,     ./bin/smile -J-Xmx8192M  You can also modify the configuration file ./conf/application.ini for the memory and other JVM settings. For detailed helps, checkout the project website. Smile implements the following major machine learning algorithms: Classification Support Vector Machines, Decision Trees, AdaBoost, Gradient Boosting, Random Forest, Logistic Regression, Neural Networks, RBF Networks, Maximum Entropy Classifier, KNN, Naïve Bayesian, Fisher/Linear/Quadratic/Regularized Discriminant Analysis. Regression Support Vector Regression, Gaussian Process, Regression Trees, Gradient Boosting, Random Forest, RBF Networks, OLS, LASSO, Ridge Regression. Feature Selection Genetic Algorithm based Feature Selection, Ensemble Learning based Feature Selection, Signal Noise ratio, Sum Squares ratio. Clustering BIRCH, CLARANS, DBScan, DENCLUE, Deterministic Annealing, K-Means, X-Means, G-Means, Neural Gas, Growing Neural Gas, Hierarchical Clustering, Sequential Information Bottleneck, Self-Organizing Maps, Spectral Clustering, Minimum Entropy Clustering. Association Rule & Frequent Itemset Mining FP-growth mining algorithm Manifold learning IsoMap, LLE, Laplacian Eigenmap, PCA, Kernel PCA, Probabilistic PCA, GHA, Random Projection Multi-Dimensional Scaling Classical MDS, Isotonic MDS, Sammon Mapping Nearest Neighbor Search BK-Tree, Cover Tree, KD-Tree, LSH Sequence Learning Hidden Markov Model, Conditional Random Field. Natural Language Processing Sentence Splitter and Tokenizer, Bigram Statistical Test, Phrase Extractor, Keyword Extractor, Stemmer, POS Tagging, Relevance Ranking Model Serialization You may notice that most models support the Java Serializable interface (all classifiers do support Serializable interface). It is because the exact format is hard to keep stable, class changes can easily make your serialized data unreadable, reading/writing the data in non-Java code is almost impossible. Currently, we suggest XStream to serialize the trained models. XStream is a simple library to serialize objects to XML and back again. XStream is easy to use and doesn't require mappings (actually requires no modifications to objects). Protostuff is a nice alternative that supports forward-backward compatibility (schema evolution) and validation. Beyond XML, Protostuff supports many other formats such as JSON, YAML, protobuf, etc. For some predictive models, we look forward to supporting PMML (Predictive Model Markup Language), an XML-based file format developed by the Data Mining Group. Smile Scala API provides read and write functions in package smile.io to read and save models by XStream. SmilePlot Smile also has a Swing-based data visualization library SmilePlot, which provides scatter plot, line plot, staircase plot, bar plot, box plot, histogram, 3D histogram, dendrogram, heatmap, hexmap, QQ plot, contour plot, surface, and wireframe. The class PlotCanvas provides builtin functions such as zoom in/out, export, print, customization, etc. SmilePlot requires SwingX library for JXTable. But if your environment cannot use SwingX, it is easy to remove this dependency by using JTable. To use SmilePlot, add the following to dependencies     <dependency>       <groupId>com.github.haifengl</groupId>       <artifactId>smile-plot</artifactId>       <version>1.1.0</version>     </dependency>  Demo Gallery Kernel PCA IsoMap Multi-Dimensional Scaling SOM Neural Network SVM Agglomerative Clustering X-Means DBScan Neural Gas Wavelet Exponential Family Mixture Tutorial This tutorial shows how to use the Smile Java API for predictive modeling (classification and regression). It includes loading data, training and testing the model, and applying the model. If you use Scala, we strongly recommend the new high level Scala API, which is similar to R and Matlab. The programming guide with Scala API is available at project website. Load Data Most Smile algorithms take simple double[] as input so you can use your favorite methods or library to import the data as long as the samples are in double arrays. To make life easier, Smile does provide a couple of parsers for popular data formats, such as Weka's ARFF files, LibSVM's file format, delimited text files, and binary sparse data. These classes are in the package smile.data.parser. The package smile.data.parser.microarray also provides several parsers for microarray gene expression datasets, including GCT, PCL, RES, and TXT files. In the following example, we use the ARFF parser to load the weather dataset: ArffParser arffParser = new ArffParser(); arffParser.setResponseIndex(4); AttributeDataset weather = arffParser.parse(new FileInputStream(""data/weka/weather.nominal.arff"")); double[][] x = weather.toArray(new double[weather.size()][]); int[] y = weather.toArray(new int[weather.size()]); Note that the data file weather.nominal.arff is in Smile distribution package. After unpacking the package, there is a lot of testing data in the directory of $smile/data, where $smile is the the root of Smile package. In the second line, we use setResponseIndex to set the column index (starting at 0) of the dependent/response variable. In supervised learning, we need a response variable for each sample to train the model. Basically, it is the y in the mathematical model. For classification, it is the class label. For regression, it is of real value. Without setting it, the data assumes no response variable. In that case, the data can be used for testing or unsupervised learning. The parse method can take a URI, File, path string, or InputStream as an input argument. And it returns an AttributeDataset object, which is a dataset of a number of attributes. All attribute values are stored as double even if the attribute may be nominal, ordinal, string, or date. The first call of toArray taking a double[][] argument fills the array with all the parsed data and returns it, of which each row is a sample/object. The second call of toArray taking an int array fills it with the class labels of the samples and then returns it. The AttributeDataset.attributes method returns the list of Attribute objects in the dataset. The Attribute object contains the type information (and optional weight), which is needed in some algorithms (e.g. decision trees). The Attribute object also contains variable name and description, which are useful in the output or UI. Similar to ArffParser, we can also use the DelimitedTextParser class to parse plain delimited text files. By default, the parser expects a white-space-separated-values file. Each line in the file corresponds to a row in the table. Within a line, fields are separated by white spaces, each field belonging to one table column. This class can also be used to read other text tabular files by setting the delimiter character such as ','. The file may contain comment lines (starting with '%') and missing values (indicated by placeholder '?'), which can both be parameterized. DelimitedTextParser parser = new DelimitedTextParser(); parser.setResponseIndex(new NominalAttribute(""class""), 0); AttributeDataset usps = parser.parse(""USPS Train"", new FileInputStream(""data/usps/zip.train"")); where the setResponseIndex also takes an extra parameter about the attribute of the response variable. Because this is a classification problem, we set it to a NominalAttribute with name ""class"". In case of regression, we should use NumericAttribute instead. If your input data contains different types of attributes (e.g. NumericAttribute, NominalAttribute, StringAttribute, DateAttribute, etc), you should pass an array of Attribute[] to the constructor of DelimitedTextParser to indicate the data types of each column. By default, DelimitedTextParser assumes all columns as NumericAttribute. Train The Model Smile implements a variety of classification and regression algorithms. In what follows, we train a support vector machine (SVM) on the USPS zip code handwriting dataset. The SVM employs a Gaussian kernel and one-to-one strategy as this is a multi-class problem. Different from LibSVM or other popular SVM library, Smile implements an online learning algorithm for training SVM. The method learn trains the SVM with the given dataset for one epoch. The caller may call this method multiple times to obtain better accuracy although one epoch is usually sufficient. Note that after calling learn, we need to call the finish method, which processes support vectors until they converge. As it is an online algorithm, the user may update the model anytime by calling learn even after calling the finish method. In the example, we show another way of learning by working on single sample. As shown in the example, we simply call the predict method on a testing sample. Both learn and predict methods are generic for all classification and regression algorithms. DelimitedTextParser parser = new DelimitedTextParser(); parser.setResponseIndex(new NominalAttribute(""class""), 0); try {     AttributeDataset train = parser.parse(""USPS Train"", new FileInputStream(""/data/usps/zip.train""));     AttributeDataset test = parser.parse(""USPS Test"", new FileInputStream(""/data/usps/zip.test""));      double[][] x = train.toArray(new double[train.size()][]);     int[] y = train.toArray(new int[train.size()]);     double[][] testx = test.toArray(new double[test.size()][]);     int[] testy = test.toArray(new int[test.size()]);      SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(8.0), 5.0, Math.max(y)+1, SVM.Multiclass.ONE_VS_ONE);     svm.learn(x, y);     svm.finish();      int error = 0;     for (int i = 0; i < testx.length; i++) {         if (svm.predict(testx[i]) != testy[i]) {             error++;         }     }      System.out.format(""USPS error rate = %.2f%%\n"", 100.0 * error / testx.length);      System.out.println(""USPS one more epoch..."");     for (int i = 0; i < x.length; i++) {         int j = Math.randomInt(x.length);         svm.learn(x[j], y[j]);     }      svm.finish();      error = 0;     for (int i = 0; i < testx.length; i++) {         if (svm.predict(testx[i]) != testy[i]) {             error++;         }     }     System.out.format(""USPS error rate = %.2f%%\n"", 100.0 * error / testx.length); } catch (Exception ex) {     System.err.println(ex); } As aforementioned, tree based methods need the type information of attributes. In the next example, we train an AdaBoost model on the weather dataset. ArffParser arffParser = new ArffParser(); arffParser.setResponseIndex(4); AttributeDataset weather = arffParser.parse(new FileInputStream(""/data/weka/weather.nominal.arff"")); double[][] x = weather.toArray(new double[weather.size()][]); int[] y = weather.toArray(new int[weather.size()]);  AdaBoost forest = new AdaBoost(weather.attributes(), x, y, 200, 4); In the example, we set the number of trees to 200 and the maximum number of leaf nodes in the trees to 4, which works as a regularization control. Model Validation In the example of USPS, we have both training and test datasets. However, we frequently have only a single dataset for building models. For model validation, Smile provide LOOCV (leave-one-out cross validation), cross validation, and bootstrap in the package smile.validation. Additionally, the package also has various measures to evaluate classification, regression, and clustering. For example, we have accuracy, fallout, FDR, F-measure (F1 score or F-score), precision, recall, sensitivity, specificity for classification; absolute deviation, MSE, RMSE, RSS for regression; rand index, adjust rand index for clustering. The following is an example how to use LOOCV. double[][] x = weather.toArray(new double[weather.size()][]); int[] y = weather.toArray(new int[weather.size()]);  int n = x.length; LOOCV loocv = new LOOCV(n); int error = 0; for (int i = 0; i < n; i++) {     double[][] trainx = Math.slice(x, loocv.train[i]);     int[] trainy = Math.slice(y, loocv.train[i]);      AdaBoost forest = new AdaBoost(weather.attributes(), trainx, trainy, 200, 4);     if (y[loocv.test[i]] != forest.predict(x[loocv.test[i]]))         error++; }  System.out.println(""Decision Tree error = "" + error); Use The Trained Model All classifiers in Smile implement the following interface. public interface Classifier<T> {     public int predict(T x);     public int predict(T x, double[] posteriori); } To use the trained model, we can apply the method predict on a new sample. Besides just returning the class label, many methods (e.g. neural networks) can also output the posteriori probabilities of each class. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/haifengl/smile"	"Statistical Machine Intelligence and Learning Engine. Smile is a fast and comprehensive machine learning system."	"true"
"Science and Data Analysis"	"Spire ★ 919 ⧗ 0"	"https://github.com/non/spire"	"Powerful new number types and numeric abstractions for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"977"	"77"	"124"	"GitHub - non/spire: Powerful new number types and numeric abstractions for Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 77 Star 977 Fork 124 non/spire Code Issues 139 Pull requests 22 Pulse Graphs Powerful new number types and numeric abstractions for Scala. 1,712 commits 21 branches 18 releases 49 contributors Scala 99.8% Other 0.2% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bug/fix-491-2 bug/fix-491 bug/fix-dec-syntax feature/securerandom jetdim_compile_time_check master topic/add_=== topic/algebra-integration-2 topic/algebra-integration topic/continued-fractions topic/conversions2 topic/error-handling-etc topic/lattices topic/matrix-wip topic/multivariate topic/primes topic/random2 topic/remove-arb-filter topic/typeclass-optimizations topic/0.11.0-changelog wip/interval2 Nothing to show v0.11.0 v0.10.1 v0.10.0 v0.9.1 v0.9.0 v0.8.2 v0.8.1 v0.8.0 v0.7.5 v0.7.4 v0.7.3 v0.7.2 v0.7.1 v0.7.0 v0.6.1 v0.6.0 v0.5.1 rm Nothing to show New pull request Latest commit 9922460 Jun 15, 2016 non committed on GitHub Merge pull request #567 from kazzna/topic/readme-fix … Fix command in README.md Permalink Failed to load latest commit information. benchmark-jmh/src/main/scala/spire/benchmark/jmh Rewrite all remaining package declarations. Nov 5, 2015 benchmark/src/main Rewrite all remaining package declarations. Nov 5, 2015 core make Interval extend Serializable Jun 15, 2016 doc refactor numerics -> spire Mar 15, 2012 examples/src/main Rewrite all remaining package declarations. Nov 5, 2015 extras/src/main/scala/spire/math Merge remote-tracking branch 'upstream/master' into feature/intervalset Nov 11, 2015 laws/src/main/scala/spire/laws Merge remote-tracking branch 'upstream/master' into feature/intervalset Nov 11, 2015 macros/src Rewrite all remaining package declarations. Nov 5, 2015 project Update to Scala.js 0.6.8 (for Scala 2.11.8) Apr 8, 2016 scripts Fix #502: Request: Access to Snaphot Nov 15, 2015 tests/src/test/scala/spire Fix bug in Eq.by May 14, 2016 .gitignore Ignore .idea project files. Oct 7, 2014 .jvmopts Add scala-js support Aug 2, 2015 .travis.yml Update to Scala 2.10.6 & 2.11.8 Apr 8, 2016 AUTHORS.md Add new authors, create 0.11.0 change log, updated README Oct 28, 2015 CHANGES.md Add new authors, create 0.11.0 change log, updated README Oct 28, 2015 CONTRIBUTING.md fix trailing words May 3, 2014 COPYING release code under the MIT license Mar 22, 2012 DESIGN.md Update DESIGN.md Feb 27, 2015 GUIDE.md Fix typos in GUIDE.md Jan 17, 2016 README.md Merge pull request #567 from kazzna/topic/readme-fix Jun 15, 2016 TODO Start to rationalize support for Fractional[A]. Jan 4, 2013 build.sbt Update to Scala 2.10.6 & 2.11.8 Apr 8, 2016 scalastyle-config.xml Made equals/hashcode a warning for now May 27, 2015 version.sbt Setting version to 0.12.0-SNAPSHOT Oct 27, 2015 README.md Spire Overview Spire is a numeric library for Scala which is intended to be generic, fast, and precise. Using features such as specialization, macros, type classes, and implicits, Spire works hard to defy conventional wisdom around performance and precision trade-offs. A major goal is to allow developers to write efficient numeric code without having to ""bake in"" particular numeric representations. In most cases, generic implementations using Spire's specialized type classes perform identically to corresponding direct implementations. Scaladoc Spire is provided to you as free software under the MIT license. Organization The Spire mailing list is shared with other Typelevel projects. It is the place to go for announcements and discussions around Spire. When posting, place the word [spire] at the begining of your subject. We also have a guide on contributing to Spire as well as a guide that provides information on Spire's design. Spire has maintainers who are responsible for signing-off on and merging pull requests, and for helping to guide the direction of Spire: Erik Osheim (erik@osheim.org) Tom Switzer (thomas.switzer@gmail.com) Rüdiger Klaehn (rklaehn@gmail.com) Denis Rosset (denis.rosset@unige.ch) People are expected to follow the Typelevel Code of Conduct when discussing Spire on the Github page, in Gitter, the IRC channel, mailing list, and other official venues. Concerns or issues can be sent to any of Spire's maintainers, or to the Typelevel organization. Set up Spire is currently available for Scala 2.10 and 2.11 (and supports scala-js for both versions). To get started with SBT, simply add the following to your build.sbt file: libraryDependencies += ""org.spire-math"" %% ""spire"" % ""0.11.0""  For Maven instructions, and to download the jars directly, visit the Central Maven repository. Here is a list of all of Spire's modules: spire-macros: macros and compile-time code (required by spire) spire: the core Spire library, the types and type classes spire-laws: optional support for law-checking and testing spire-extras: extra types which are more specific or esoteric Playing Around If you clone the Spire repo, you can get a taste of what Spire can do using SBT's console. Launch sbt and at the prompt, type coreJVM/console: > coreJVM/console [info] Generating spire/std/tuples.scala [info] Starting scala interpreter... [info] Welcome to Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_51). Type in expressions to have them evaluated. Type :help for more information.  scala> import spire.implicits._ import spire.implicits._  scala> import spire.math._ import spire.math._  scala> Complex(3.0, 5.0).sin res0: spire.math.Complex[Double] = (10.472508533940392 + -73.46062169567367i)  Number Types In addition to supporting all of Scala's built-in number types, Spire introduces several new ones, all of which can be found in spire.math: Natural unsigned, immutable, arbitrary precision integer Rational fractions of integers with perfect precision Algebraic lazily-computed, arbitrary precision algebraic numbers Real computable real number implementation Complex[A] complex numbers, points on the complex plane Jet[A] N-dimensional dual numbers, for automatic differentiation Quaternion[A] extension of complex numbers into 4D space UByte through ULong value classes supporting unsigned operations SafeLong fast, overflow-proof integer type Number boxed type supporting a traditional numeric tower Interval[A] arithmetic on open, closed, and unbound intervals Polynomial[A] univariate (single-variable) polynomial expressions Trilean value class supporting three-valued logic FixedPoint fractions with Long numerator and implicit denominator (in extras) Detailed treatment of these types can be found in the guide. Type Classes Spire provides type classes to support a wide range of unary and binary operations on numbers. The type classes are specialized, do no boxing, and use implicits to provide convenient infix syntax. The general-purpose type classes can be found in spire.math and consist of: Numeric[A] all number types, makes ""best effort"" to support operators Fractional[A] fractional number types, where / is true division Integral[A] integral number types, where / is floor division Some of the general-purpose type classes are built in terms of a set of more fundamental type classes defined in spire.algebra. Many of these correspond to concepts from abstract algebra: Eq[A] types that can be compared for equality Order[A] types that can be compared and ordered PartialOrder[A] types that can be compared for equality, and for which certain pairs are ordered Semigroup[A] types with an associative binary operator |+| Monoid[A] semigroups that have an identity element Group[A] monoids that have an inverse operator (Left/Right/)Action[P, G] left/right/ actions of semigroups/monoids/groups Semiring[A] types that form semigroups under + and * Rng[A] types that form a group under + and a semigroup under * Rig[A] types that form monoids under + and * Ring[A] types that form a group under + and a monoid under * EuclideanRing[A] rings with quotients and remainders (euclidean division) Field[A] euclidean rings with multiplicative inverses (reciprocals) Signed[A] types that have a sign (negative, zero, positive) NRoot[A] types that support k-roots, logs, and fractional powers Module[V,R] types that form a left R-module VectorSpace[V,F] types that form a vector space NormedVectorSpace[V,F] types with an associated norm InnerProductSpace[V,F] types with an inner product MetricSpace[V,R] types with an associated metric Trig[A] types that support trigonometric functions Bool[A] types that form a Boolean algebra Heyting[A] types that form a Heyting algebra Variants of Semigroup/Monoid/Group/Action with partial operations are defined in the spire.algebra.partial subpackage. In addition to the type classes themselves, spire.implicits defines many implicits which provide unary and infix operators for the type classes. The easiest way to use these is via a wildcard import of spire.implicits._. Detailed treatment of these type classes can be found in the guide. Getting Started Spire contains a lot of types, as well as other machinery to provide a nice user experience. The easiest way to use spire is via wildcard imports: import spire.algebra._   // provides algebraic type classes import spire.math._      // provides functions, types, and type classes import spire.implicits._ // provides infix operators, instances and conversions Of course, you can still productively use Spire without wildcard imports, but it may require a bit more work to figure out which functionality you want and where it's coming from. Operators by Type Class The following is an outline in more detail of the type classes provided by Spire, as well as the operators that they use. While Spire avoids introducing novel operators when possible, in a few cases it was unavoidable. Eq, Order and PartialOrder The type classes provide type-safe equivalence and comparison functions. Orderings can be total (Order) or partial (PartialOrder); although undefined elements like NaN or null will cause problems in the default implementations [1]. Eq eqv (===): equivalence neqv (=!=): non-equivalence Order compare: less-than (-1), equivalent (0), or greater-than (1) gt (>): greater-than gteqv (>=): greater-than-or-equivalent lt (<): less-than lteqv (<=): less-than-or-equivalent min: find least value max: find greatest value PartialOrder partialCompare: less-than (-1.0), equivalent (0.0), greater-than (1.0) or incomparable (NaN) tryCompare: less-than (Some(-1)), equivalent (Some(0)), greater-than (Some(1)) or incomparable (None) pmin: find the least value if the elements are comparable; returns an Option pmax: find the greated value if the elements are comparable; returns an Option gt (>), gteqv (>=), lt (<) and lteqv (<=) return false if the elements are incomparable, or the result of their comparison [1] For floating-point numbers, alternate implementations that take NaN into account can be imported from spire.optional.totalfloat._. Semigroup, Monoid, and Group These general type classes constitute very general operations. The operations range from addition and multiplication to concatenating strings or lists, and beyond! Semigroup op (|+|): associative binary operator Monoid id: an identity element isId: checks (together with Eq) for identity Group inverse: an unary operator There are Additive and Multiplicative refinements of these general type classes, which are used in the Ring-family of type classes. Rings &co The Ring family of type classes provides the typical arithmetic operations most users will expect. Semiring plus (+): addition times (*): multiplication pow (**): exponentiation (integral exponent) Rng negate (-): additive inverse minus (-): subtraction zero: additive identity Rig zero: additive identity one: multiplicative identity Ring (Rng + Rig) EuclideanRing quot (/~): quotient (floor division) mod (%): remainder quotmod (/%): quotient and mod gcd: greatest-common-divisor lcm: least-common-multiple Field reciprocal: multiplicative inverse div (/): division ceil: round up floor: round down round: round to nearest NRoot nroot: k-roots (k: Int) sqrt: square root log: natural logarithm fpow (**): exponentiation (fractional exponent) VectorSpaces &co The vector space family of type classes provide basic vector operations. They are parameterized on 2 types: the vector type and the scalar type. Module plus (+): vector addition minus (-): vector subtraction timesl (*:): scalar multiplication VectorSpace divr (:/): scalar division NormedVectorSpace norm: vector norm normalize: normalizes vector (so norm is 1) InnerProductSpace dot (⋅, dot): vector inner product Numeric, Integral, and Fractional These high-level type classes will pull in all of the relevant algebraic type classes. Users who aren't concerned with algebraic properties directly, or who wish for more permissiveness, should prefer these type classes. Integral: whole number types (e.g. Int, BigInt) Fractional: fractional/decimal types (e.g. Double, Rational) Numeric: any number type, making ""best effort"" to support ops The Numeric type class is unique in that it provides the same functionality as Fractional for all number types. Each type will attempt to ""do the right thing"" as far as possible, and throw errors otherwise. Users who are leery of this behavior are encouraged to use more precise type classes. Bool Bool supports Boolean algebras, an abstraction of the familiar bitwise boolean operators. Bool complement (unary ~): logical negation and (&): conjunction or (|): disjunction xor (^): exclusive-disjunction imp: implicitation, equivalent to ~a | b nand: ""not-and,"" equivalent to ~(a & b) nor: ""not-or,"" equivalent to ~(a | b) nxor: ""not-xor,"" equivalent to ~(a ^ b) Bool instances exist not just for Boolean, but also for Byte, Short, Int, Long, UByte, UShort, UInt, and ULong. Trig Trig provides an abstraction for any type which defines trigonometric functions. To do this, types should be able to reasonably approximate real values. Trig e: Euler's number, 2.71828... pi: Ratio of circle's circumference to diameter, 3.14159... exp: exponential function, e^x expm1: e^x - 1 log: natural logarithm log1p: log(x + 1) sin, cos, tan: sine, cosine, and tangent, the standard functions of angles asin, acos, atan, atan2: inverse functions sinh, cosh, tanh: hyperbolic functions toRadians, toDegrees: convert between angle units Syntax Using string interpolation and macros, Spire provides convenient syntax for number types. These macros are evaluated at compile-time, and any errors they encounter will occur at compile-time. For example: import spire.syntax.literals._  // bytes and shorts val x = b""100"" // without type annotation! val y = h""999"" val mask = b""255"" // unsigned constant converted to signed (-1)  // rationals val n1 = r""1/3"" val n2 = r""1599/115866"" // simplified at compile-time to 13/942  // support different radix literals import spire.syntax.literals.radix._  // representations of the number 23 val a = x2""10111"" // binary val b = x8""27"" // octal val c = x16""17"" // hex  // SI notation for large numbers import spire.syntax.literals.si._ // .us and .eu also available  val w = i""1 944 234 123"" // Int val x = j""89 234 614 123 234 772"" // Long val y = big""123 234 435 456 567 678 234 123 112 234 345"" // BigInt val z = dec""1 234 456 789.123456789098765"" // BigDecimal Spire also provides a loop macro called cfor whose syntax bears a slight resemblance to a traditional for-loop from C or Java. This macro expands to a tail-recursive function, which will inline literal function arguments. The macro can be nested in itself and compares favorably with other looping constructs in Scala such as for and while: import spire.syntax.cfor._  // print numbers 1 through 10 cfor(0)(_ < 10, _ + 1) { i =>   println(i) }  // naive sorting algorithm def selectionSort(ns: Array[Int]) {   val limit = ns.length -1   cfor(0)(_ < limit, _ + 1) { i =>     var k = i     val n = ns(i)     cfor(i + 1)(_ <= limit, _ + 1) { j =>       if (ns(j) < ns(k)) k = j     }     ns(i) = ns(k)     ns(k) = n   } } Sorting, Selection, and Searching Since Spire provides a specialized ordering type class, it makes sense that it also provides its own methods for doing operations based on order. These methods are defined on arrays and occur in-place, mutating the array. Other collections can take advantage of sorting by converting to an array, sorting, and converting back (which is what the Scala collections framework already does in most cases). Thus, Spire supports both mutable arrays and immutable collections. Sorting methods can be found in the spire.math.Sorting object. They are: quickSort fastest, nlog(n), not stable with potential n^2 worst-case mergeSort also fast, nlog(n), stable but allocates extra temporary space insertionSort n^2 but stable and fast for small arrays sort alias for quickSort Both mergeSort and quickSort delegate to insertionSort when dealing with arrays (or slices) below a certain length. So, it would be more accurate to describe them as hybrid sorts. Selection methods can be found in an analagous spire.math.Selection object. Given an array and an index k these methods put the kth largest element at position k, ensuring that all preceeding elements are less-than or equal-to, and all succeeding elements are greater-than or equal-to, the kth element. There are two methods defined: quickSelect usually faster, not stable, potentially bad worst-case linearSelect usually slower, but with guaranteed linear complexity select alias for quickSelect Searching methods are located in the spire.math.Searching object. Given a sorted array (or indexed sequence), these methods will locate the index of the desired element (or return -1 if it is not found). search(array, item) finds the index of item in array search(array, item, lower, upper) only searches between lower and upper. Searching also supports a more esoteric method: minimalElements. This method returns the minimal elements of a partially-ordered set. Pseudo-Random Number Generators Spire comes with many different PRNG implementations, which extends the spire.random.Generator interface. Generators are mutable RNGs that support basic operations like nextInt. Unlike Java, generators are not threadsafe by default; synchronous instances can be attained by calling the .sync method. Spire supports generating random instances of arbitrary types using the spire.random.Dist[A] type class. These instances represent a strategy for getting random values using a Generator instance. For instance: import spire.implicits._ import spire.math._ import spire.random._  val rng = Cmwc5()  // produces a double in [0.0, 1.0) val n = rng.next[Double]  // produces a complex number, with real and imaginary parts in [0.0, 1.0) val c = rng.next[Complex[Double]]  // produces a map with ~10-20 entries implicit val nextmap = Dist.map[Int, Complex[Double]](10, 20) val m = rng.next[Map[Int, Complex[Double]]] Unlike generators, Dist[A] instances are immutable and composable, supporting operations like map, flatMap, and filter. Many default instances are provided, and it's easy to create custom instances for user-defined types. Miscellany In addition, Spire provides many other methods which are ""missing"" from java.Math (and scala.math), such as: log(BigDecimal): BigDecimal exp(BigDecimal): BigDecimal pow(BigDecimal): BigDecimal pow(Long): Long gcd(Long, Long): Long and so on... Benchmarks In addition to unit tests, Spire comes with a relatively fleshed-out set of micro-benchmarks written against Caliper. To run the benchmarks from within SBT, change to the benchmark subproject and then run to see a list of benchmarks: $ sbt [info] Set current project to spire (in build file:/Users/erik/w/spire/) > project benchmark [info] Set current project to benchmark (in build file:/Users/erik/w/spire/) > run  Multiple main classes detected, select one to run:   [1] spire.benchmark.AnyValAddBenchmarks  [2] spire.benchmark.AnyValSubtractBenchmarks  [3] spire.benchmark.AddBenchmarks  [4] spire.benchmark.GcdBenchmarks  [5] spire.benchmark.RationalBenchmarks  [6] spire.benchmark.JuliaBenchmarks  [7] spire.benchmark.ComplexAddBenchmarks  [8] spire.benchmark.CForBenchmarks  [9] spire.benchmark.SelectionBenchmarks  [10] spire.benchmark.Mo5Benchmarks  [11] spire.benchmark.SortingBenchmarks  [12] spire.benchmark.ScalaVsSpireBenchmarks  [13] spire.benchmark.MaybeAddBenchmarks  You can also run a particular benchmark with run-main, for instance: > run-main spire.benchmark.JuliaBenchmarks  If you plan to contribute to Spire, please make sure to run the relevant benchmarks to be sure that your changes don't impact performance. Benchmarks usually include comparisons against equivalent Scala or Java classes to try to measure relative as well as absolute performance. Caveats Code is offered as-is, with no implied warranty of any kind. Comments, criticisms, and/or praise are welcome, especially from numerical analysts! ;) Copyright 2011-2015 Erik Osheim, Tom Switzer A full list of contributors can be found in AUTHORS.md. The MIT software license is attached in the COPYING file. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/non/spire"	"Powerful new number types and numeric abstractions for Scala."	"true"
"Science and Data Analysis"	"Squants ★ 259 ⧗ 8"	"https://github.com/garyKeorkunian/squants"	"The Scala API for Quantities, Units of Measure and Dimensional Analysis."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"281"	"19"	"35"	"GitHub - garyKeorkunian/squants: The Scala API for Quantities, Units of Measure and Dimensional Analysis Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 19 Star 281 Fork 35 garyKeorkunian/squants Code Issues 8 Pull requests 1 Wiki Pulse Graphs The Scala API for Quantities, Units of Measure and Dimensional Analysis http://www.squants.com 221 commits 21 branches 5 releases 10 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.3.3 WIP-20150521 gh-pages master release-0.4 release-0.5 release-0.6 snapshot-0.6.0 wip-0.6.1-snapshot wip-20150426 wip-20150427 wip-20150610 wip-add-nanoseconds-improve-duration-conversion wip-area-square-root wip-generic-october-2015 wip-generic-value wip-gitter-integrations wip-release-0.6.2 wip-remove-bogus-conversions wip-vector-improvements wip-vector-polar Nothing to show v0.6.2 v0.5.3 v0.4.2 v0.3.3 v0.2.3 Nothing to show New pull request Latest commit e19d11a May 13, 2016 garyKeorkunian Merge pull request #131 from garyKeorkunian/wip-gitter-integrations … Add Gitter Integrations and Badge Permalink Failed to load latest commit information. project Update version to 0.7.1-SNAPSHOT and update README May 10, 2016 shared/src Added unit tests for SpectralIrradiance Apr 23, 2016 .gitignore add .gitignore Mar 6, 2014 .travis.yml Add Gitter Integrations and Badge May 13, 2016 LICENSE Update LICENSE Apr 5, 2014 README.md Add Gitter Integrations and Badge May 13, 2016 README.md Squants The Scala API for Quantities, Units of Measure and Dimensional Analysis Squants is a framework of data types and a domain specific language (DSL) for representing Quantities, their Units of Measure, and their Dimensional relationships. The API supports typesafe dimensional analysis, improved domain models and more. All types are immutable and thread-safe. Website | GitHub | User Forum | Wiki Current Versions Current Release: 0.6.2 (API Docs) Development Build: 0.7.1-SNAPSHOT (API Docs) Release History Build services provided by Travis CI NOTE - This README reflects the feature set in the branch it can be found. For more information on feature availability of a specific version see the Release History or the README for a that version Installation Repository hosting for Squants is provided by Sonatype. To use Squants in your SBT project add the following dependency to your build. ""com.squants""  %% ""squants""  % ""0.6.2""  or ""com.squants""  %% ""squants""  % ""0.7.1-SNAPSHOT""  To use Squants in your Maven project add the following dependency <dependency>     <groupId>com.squants</groupId>     <artifactId>squants_2.11</artifactId>     <version>0.6.2</version> </dependency> Beginning with Squants 0.4.x series, both Scala 2.10 and 2.11 builds are available. To use Squants interactively in the Scala REPL, clone the git repo and run sbt squantsJVM/console git clone https://github.com/garyKeorkunian/squants cd squants sbt squantsJVM/console  Type Safe Dimensional Analysis The Trouble with Doubles When building programs that perform dimensional analysis, developers are quick to declare quantities using a basic numeric type, usually Double. While this may be satisfactory in some situations, it can often lead to semantic and other logic issues. For example, when using a Double to describe quantities of Energy (kWh) and Power (kW), it is possible to compile a program that adds these two values together. This is not appropriate as kW and kWh measure quantities of two different dimensions. The unit kWh is used to measure an amount of Energy used or produced. The unit kW is used to measure Power/Load, the rate at which Energy is being used or produced, that is, Power is the first time derivative of Energy. Power = Energy / Time Consider the following code val loadKw = 1.2                    // Double: 1.2 val energyMwh = 24.2                // Double: 24.2         val sumKw = loadKw + energyMwh      // Double: 25.4 which not only adds quantities of different dimensions (Power vs Energy), it also fails to convert the scales implied in the val names (Mega vs Kilo). Because this code compiles, detection of these errors is pushed further into the development cycle. Dimensional Type Safety Only quantities with the same dimensions may be compared, equated, added, or subtracted. Squants helps prevent errors like these by type checking operations at compile time and automatically applying scale and type conversions at run-time. For example, val load1: Power = Kilowatts(12)        // returns Power(12, Kilowatts) or 12 kW val load2: Power = Megawatts(0.023)     // Power: 0.023 MW val sum = load1 + load2                 // Power: 35 kW - unit on left side is preserved sum should be(Kilowatts(35))             sum should be(Megawatts(0.035))         // comparisons automatically convert scale works because Kilowatts and Megawatts are both units of Power. Only the scale is different and the library applies an appropriate conversion. Also, notice that keeping track of the scale within the value name is no longer needed. val load: Power = Kilowatts(1.2)            // Power: 1.2 kW val energy: Energy = KilowattHours(23.0)    // Energy: 23 kWH val sum = load + energy                     // Invalid operation - does not compile The unsupported operation in this expression prevents the code from compiling, catching the error made when using Double in the example above. Dimensionally Correct Type Conversions One may take quantities with different dimensions, and multiply or divide them. Dimensionally correct type conversions are a key feature of Squants. Conversions are implemented by defining relationships between Quantity types using the * and / operators. The following code demonstrates creating ratio between two quantities of the same dimension, resulting in a dimensionless value: val ratio = Days(1) / Hours(3)  // Double: 8.0 This code demonstrates use of the Power.* method that takes a Time and returns an Energy: val load = Kilowatts(1.2)                   // Power: 1.2 kW val time = Hours(2)                         // Time: 2 h val energyUsed = load * time                // Energy: 2.4 kWh This code demonstrates use of the Energy./ method that takes a Time and returns a Power: val aveLoad: Power = energyUsed / time      // Power: 1.2 kW Unit Conversions Quantity values are based in the units used to create them. val loadA: Power = Kilowatts(1200)  // Power: 1200.0 kW val loadB: Power = Megawatts(1200)  // Power: 1200.0 MW Since Squants properly equates values of a like dimension, regardless of the unit, there is usually no reason to explicitly convert from one to the other. This is especially true if the user code is primarily performing dimensional analysis. However, there are times when you may need to set a Quantity value to a specific unit (eg, for proper JSON encoding). When necessary, a quantity can be converted to another unit using the in method. val loadA = Kilowatts(1200)    // Power: 1200.0 kW val loadB = loadA in Megawatts // Power: 1.2 MW val loadC = loadA in Gigawatts // Power: 0.0012 GW Sometimes you need to get the numeric value of the quantity in a specific unit (eg, for submission to an external service that requires a numeric in a specified unit or to perform analysis beyond Squant's domain) When necessary, the value can be extracted in the desired unit with the to method. val load: Power = Kilowatts(1200) val kw: Double = load to Kilowatts // Double: 1200.0 val mw: Double = load to Megawatts // Double: 1.2 val gw: Double = load to Gigawatts // Double: 0.0012 Most types include methods with convenient aliases for the to methods. val kw: Double = load toKilowatts // Double: 1200.0 val mw: Double = load toMegawatts // Double: 1.20 val gw: Double = load toGigawatts // Double: 0.0012 NOTE - It is important to use the to method for extracting the numeric value, as this ensures you will be getting the numeric value for the desired unit. Quantity.value should not be accessed directly. To prevent improper usage, direct access to the Quantity.value field may be deprecated in a future version. Creating strings formatted in the desired unit: val kw: String = load toString Kilowatts // String: “1200.0 kW” val mw: String = load toString Megawatts // String: “1.2 MW” val gw: String = load toString Gigawatts // String: “0.0012 GW” Creating Tuple2(Double, String) that includes a numeric value and unit symbol: val load: Power = Kilowatts(1200) val kw: Tuple2 = load toTuple               // Tuple2: (1200, ""kW"") val mw: Tuple2 = load toTuple Megawatts     // Tuple2: (1.2, ""MW) val gw: Tuple2 = load toTuple Gigawatts     // Tuple2: (0.0012, ""GW"") This can be useful for passing properly scaled quantities to other processes that do not use Squants, or require use of more basic types (Double, String) Simple console based conversions (using DSL described below) 1.kilograms to Pounds       // Double: 2.2046226218487757  kilogram / pound            // Double: 2.2046226218487757  2.1.pounds to Kilograms     // Double: 0.952543977  2.1.pounds / kilogram       // Double: 0.952543977  100.C to Fahrenheit         // Double: 212.0 Mapping over Quantity values Apply a Double => Double operation to the underlying value of a quantity, while preserving its type and unit. val load = Kilowatts(2.0)                   // 2.0 kW val newLoad = load.map(v => v * 2 + 10)     // Power: 14.0 kW The q.map(f) method effectively expands to q.unit(f(q.to(q.unit)) Approximations Create an implicit Quantity value to be used as a tolerance in approximations. Then use the approx method (or =~, ~=, ≈ operators) like you would use the equals method (== operator). implicit val tolerance = Watts(.1)      // implicit Power: 0.1 W  val load = Kilowatts(2.0)               // Power: 2.0 kW val reading = Kilowatts(1.9999)         // Power: 1.9999 kW   // uses implicit tolerance load =~ reading should be(true) load ≈ reading should be(true) load approx reading should be(true) The =~ and ≈ are the preferred operators as they have the correct precedence for equality operations. The ~= is provided for those who wish to use a more natural looking approx operator using standard characters. However, because of its lower precedence, user code may require parenthesis around these comparisons. Vectors All Quantity types in Squants represent the scalar value of a quantity. That is, there is no direction information encoded in any of the Quantity types. This is true even for Quantities which are normally vector quantities (ie. Velocity, Acceleration, etc). Vector quantities in Squants are implemented as case classes that takes a variable parameter list of like quantities representing a set of point coordinates in Cartesian space. The SVector object is a factory for creating DoubleVectors and QuantityVectors. The dimensionality of the vector is determined by the number of arguments. Most basic vector operations are currently supported (addition, subtraction, scaling, cross and dot products) val vector: QuantityVector[Length] = SVector(Kilometers(1.2), Kilometers(4.3), Kilometers(2.3)) val magnitude: Length = vector.magnitude        // returns the scalar value of the vector val normalized = vector.normalize(Kilometers)   // returns a corresponding vector scaled to 1 of the given unit  val vector2: QuantityVector[Length] = SVector(Kilometers(1.2), Kilometers(4.3), Kilometers(2.3)) val vectorSum = vector + vector2        // returns the sum of two vectors val vectorDiff = vector - vector2       // return the difference of two vectors val vectorScaled = vector * 5           // returns vector scaled 5 times val vectorReduced = vector / 5          // returns vector reduced 5 time val vectorDouble = vector / 5.meters    // returns vector reduced and converted to DoubleVector val dotProduct = vector * vectorDouble  // returns the Dot Product of vector and vectorDouble  val crossProduct = vector crossProduct vectorDouble  // currently only supported for 3-dimensional vectors Simple non-quantity (Double based) vectors are also supported. val vector: DoubleVector = SVector(1.2, 4.3, 2.3, 5.4)   // a Four-dimensional vector Dimensional conversions within Vector operations. NOTE - This feature is currently under development and the final implementation being evaluated. The following type of operation is the goal. val vectorLength = QuantityVector(Kilometers(1.2), Kilometers(4.3), Kilometers(2.3)) val vectorArea = vectorLength * Kilometers(2)   // QuantityVector(2.4 km², 8.6 km², 4.6 km²) val vectorVelocity = vectorLength / Seconds(1)  // QuantityVector(1200.0 m/s, 4300.0 m/s, 2300.0 m/s)  val vectorDouble = DoubleVector(1.2, 4.3, 2.3) val vectorLength = vectorDouble.to(Kilometers)  // QuantityVector(1.2 km, 4.3 km, 2.3 km) Currently dimensional conversions are supported by using the slightly verbose, but flexible map method. val vectorLength = QuantityVector(Kilometers(1.2), Kilometers(4.3), Kilometers(2.3)) val vectorArea = vectorLength.map[Area](_ * Kilometers(2))      // QuantityVector(2.4 km², 8.6 km², 4.6 km²) val vectorVelocity = vectorLength.map[Velocity](_ / Seconds(1)) // QuantityVector(1200.0 m/s, 4300.0 m/s, 2300.0 m/s)  val vectorDouble = DoubleVector(1.2, 4.3, 2.3) val vectorLength = vectorDouble.map[Length](Kilometers(_))      // QuantityVector(1.2 km, 4.3 km, 2.3 km) Convert QuantityVectors to specific units using the to or in method - much like Quantities. val vectorLength = QuantityVector(Kilometers(1.2), Kilometers(4.3), Kilometers(2.3)) val vectorMetersNum = vectorLength.to(Meters)   // DoubleVector(1200.0, 4300.0, 2300.0) val vectorMeters = vectorLength.in(Meters)      // QuantityVector(1200.0 m, 4300.0 m, 2300.0 m) Market Package Market Types are similar but not quite the same as other quantities in the library. The primary type, Money, is a Dimensional Quantity, and its Units of Measure are Currencies. However, because the conversion multipliers between currency units can not be predefined, many of the behaviors have been overridden and augmented to realize correct behavior. Money A Quantity of purchasing power measured in Currency units. Like other quantities, the Unit of Measures are used to create Money values. val tenBucks = USD(10)      // Money: 10 USD val someYen = JPY(1200)     // Money: 1200 JPY val goldStash = XAU(50)     // Money: 50 XAU val digitalCache = BTC(50)  // Money: 50 BTC Price A Ratio between Money and another Quantity. A Price value is typed on a Quantity and can be denominated in any defined Currency. Price = Money / Quantity val threeForADollar = USD(1) / Each(3)              // Price[Dimensionless]: 1 USD / 3 ea val energyPrice = USD(102.20) / MegawattHours(1)    // Price[Energy]: 102.20 USD / megawattHour val milkPrice = USD(4) / UsGallons(1)               // Price[Volume]: 4 USD / gallon  val costForABunch = threeForADollar * Dozen(10) // Money: 40 USD val energyCost = energyPrice * MegawattHours(4) // Money: 408.80 USD val milkQuota = milkPrice * USD(20)             // Volume: 5 gal FX Support Currency Exchange Rates are used to define the conversion factors between currencies // create an exchange rate val rate = CurrencyExchangeRate(USD(1), JPY(100)) // OR val rate = USD / JPY(100) // OR val rate = JPY(100) -> USD(1) // OR val rate = JPY(100) toThe USD(1)  val someYen: Money = JPY(350) val someBucks: Money = USD(23.50)  // Use the convert method which automatically converts the money to the 'other' currency val dollarAmount: Money = rate.convert(someYen) // Money: 3.5 USD val yenAmount: Money = rate.convert(someBucks)  // Money: 2360 JPY  // or just use the * operator in either direction (money * rate, or rate * money) val dollarAmount2: Money = rate * someYen       // Money: 3.5 USD val yenAmount2: Money = someBucks * rate        // Money: 2360 JPY Money Context A MoneyContext can be implicitly declared to define default settings and applicable exchange rates within its scope. This allows your application to work with a default currency based on an application configuration or other dynamic source. It also provides support for updating exchange rates and using those rates for automatic conversions between currencies. The technique and frequency chosen for exchange rate updates is completely in control of the application. val exchangeRates = List(USD / CAD(1.05), USD / MXN(12.50), USD / JPY(100)) implicit val moneyContext = defaultMoneyContext withExchangeRates exchangeRates  val someMoney = Money(350) // 350 in the default Cur val usdMoney: Money = someMoney in USD val usdBigDecimal: BigDecimal = someMoney to USD val yenCost: Money = (energyPrice * MegawattHours(5)) in JPY val northAmericanSales: Money = (CAD(275) + USD(350) + MXN(290)) in USD Quantity Ranges Used to represent a range of Quantity values between an upper and lower bound val load1: Power = Kilowatts(1000) val load2: Power = Kilowatts(5000) val range: QuantityRange[Power] = QuantityRange(load1, load2) Use multiplication and division to create a Seq of ranges from the original // Create a Seq of 10 sequential ranges starting with the original and each the same size as the original val rs1 = range * 10 // Create a Seq of 10 sequential ranges each 1/10th of the original size val rs2 = range / 10 // Create a Seq of 10 sequential ranges each with a size of 400 kilowatts val rs3 = range / Kilowatts(400) Apply foreach, map and foldLeft/foldRight directly to QuantityRanges with a divisor // foreach over each of the 400 kilometer ranges within the range range.foreach(Kilometers(400)) {r => ???} // map over each of 10 even parts of the range range.map(10) {r => ???} // fold over each 10 even parts of the range range.foldLeft(10)(0) {(z, r) => ???} NOTE - Because these implementations of foreach, map and fold* take a parameter (the divisor), these methods are not directly compatible with Scala's for comprehensions. To use in a for comprehension, apply the * or / operators as described above to create a Seq from the Range. for {     interval <- (0.seconds to 1.seconds) * 60  // 60 time ranges, 0s to 1s, 1s to 2s, ...., 59s to 60s     ... } yield ... Natural Language DSL Implicit conversions give the DSL some features that allows user code to express quantities in a more naturally expressive and readable way. Create Quantities using Unit Of Measure Factory objects (no implicits required) val load = Kilowatts(100) val time = Hours(3.75) val money = USD(112.50) val price = Price(money, MegawattHours(1)) Create Quantities using Unit of Measure names and/or symbols (uses implicits) val load1 = 100 kW                  // Simple expressions don’t need dots val load2 = 100 megaWatts val time = 3.hours + 45.minutes     // Compound expressions may need dots Create Quantities using operations between other Quantities val energyUsed = 100.kilowatts * (3.hours + 45.minutes) val price = 112.50.USD / 1.megawattHours val speed = 55.miles / 1.hours Create Quantities using formatted Strings val load = Power(""40 MW"")       // 40 MW Create Quantities using Tuples val load = Power((40, ""MW""))    // 40 MW Use single unit values to simplify expressions // Hours(1) == 1.hours == hour val ramp = 100.kilowatts / hour val speed = 100.kilometers / hour  // MegawattHours(1) == 1.megawattHours == megawattHour == MWh val hi = 100.dollars / MWh val low = 40.dollars / megawattHour Implicit conversion support for using Double on the left side of operations val price = 10 / dollar     // 1 USD / 10 ea val freq = 60 / second      // 60 Hz val load = 10 * 4.MW        // 40 MW Create Quantity Ranges using to or plusOrMinus (+-) operators val range1 = 1000.kW to 5000.kW              // 1000.kW to 5000.kW val range2 = 5000.kW plusOrMinus 1000.kW     // 4000.kW to 6000.kW val range2 = 5000.kW +- 1000.kW              // 4000.kW to 6000.kW Numeric Support Most Quantities that support implicit conversions also include an implicit Numeric object that can be imported to your code where Numeric support is required. These follow the following pattern: import squants.mass.MassConversions.MassNumeric  val sum = List(Kilograms(100), Grams(34510)).sum NOTE - Because a quantity can not be multiplied by a like quantity and return a like quantity, the Numeric.times operation of numeric is implemented to throw an UnsupportedOperationException for all types except Dimensionless. The MoneyNumeric implementation is a bit different than the implementations for other quantity types in a few important ways. MoneyNumeric is a class, not an object like the others. To create a MoneyNumeric value there must be an implicit MoneyContext in scope. The MoneyContext must contain applicable exchange rates if you will be applying cross-currency Numeric ops. The following code provides a basic example for creating a MoneyNumeric: import MoneyConversions._ implicit val moneyContext = defaultMoneyContext implicit val moneyNum = new MoneyNumeric()  val sum = List(USD(100), USD(10)).sum Type Hierarchy The type hierarchy includes the following core types: Quantity, Dimension, and UnitOfMeasure Quantity and Dimension A Dimension represents a type of Quantity. For example: Mass, Length, Time, etc. A Quantity represents a dimensional value or measurement. A Quantity is a combination of a numeric value and a unit. For example: 2 lb, 10 km, 3.4 hr. Squants has built in support for 54 quantity dimensions. Unit of Measure UnitOfMeasure is the scale or multiplier in which the Quantity is being measured. Squants has built in support for over 257 units of measure For each Dimension a set of UOM objects implement a primary UOM trait typed to that Quantity. The UOM objects define the unit symbols, conversion factors, and factory methods for creating Quantities in that unit. Quantity Implementations The code for specific implementations include A class representing the Quantity including cross-dimensional operations A companion object representing the Dimension and set of available units A base trait for its Units A set of objects defining specific units, their symbols and conversion factors This is an abbreviated example of how a Quantity type is constructed: class Length(val value: Double, val unit: LengthUnit) extends Quantity[Length]  { ... } object Length extends Dimension[Length]  { ... } trait LengthUnit extends UnitOfMeasure[Length]  { ... } object Meters extends LengthUnit { ... } object Yards extends LengthUnit { ... } The apply method of the UOM objects are implemented as factories for creating Quantity values. val len1: Length = Meters(4.3) val len2: Length = Yards(5) Squants currently supports 257 units of measure Time Derivatives Special traits are used to establish a time derivative relationship between quantities. For example Velocity is the 1st Time Derivative of Length (Distance), Acceleration is the 2nd Time Derivative. class Length( ... ) extends Quantity[Length] with TimeIntegral[Velocity] ... class Velocity( ... ) extends Quantity[Velocity] with TimeDerivative[Length] with TimeIntegral[Acceleration] ... class Acceleration( ... ) extends Quantity[Acceleration] with TimeDerivative[Velocity] These traits provide operations with time operands which result in correct dimensional transformations. val distance: Length = Kilometers(100) val time: Time = Hours(2) val velocity: Velocity = distance / time val acc: Acceleration = velocity / Seconds(1)  val gravity = 32.feet / second.squared // Power is the 1st Time Derivative of Energy, PowerRamp is the 2nd val power = Kilowatts(100) val time: Time = Hours(2) val energy = power * time val ramp = Kilowatt(50) / Hours(1) Use Cases Dimensional Analysis The primary use case for Squants, as described above, is to produce code that is typesafe within domains that perform dimensional analysis. val energyPrice: Price[Energy] = 45.25.money / megawattHour val energyUsage: Energy = 345.kilowatts * 5.4.hours val energyCost: Money = energyPrice * energyUsage  val dodgeViper: Acceleration = 60.miles / hour / 3.9.seconds val speedAfter5Seconds: Velocity = dodgeViper * 5.seconds val timeTo100MPH: Time = 100.miles / hour / dodgeViper  val density: Density = 1200.kilograms / cubicMeter val volFlowRate: VolumeFlowRate = 10.gallons / minute val flowTime: Time = 30.minutes val totalMassFlow: Mass = volFlowRate * flowTime * density Domain Modeling Another excellent use case for Squants is stronger typing for fields in your domain model. This is OK ... case class Generator(   id: String,   maxLoadKW: Double,   rampRateKWph: Double,   operatingCostPerMWh: Double,   currency: String,   maintenanceTimeHours: Double) ... val gen1 = Generator(""Gen1"", 5000, 7500, 75.4, ""USD"", 1.5) val gen2 = Generator(""Gen2"", 100, 250, 2944.5, ""JPY"", 0.5) ... but this is much better case class Generator(   id: String,   maxLoad: Power,   rampRate: PowerRamp,   operatingCost: Price[Energy],   maintenanceTime: Time) ... val gen1 = Generator(""Gen1"", 5 MW, 7.5.MW/hour, 75.4.USD/MWh, 1.5 hours) val gen2 = Generator(""Gen2"", 100 kW, 250 kWph, 2944.5.JPY/MWh, 30 minutes) Anticorruption Layers Create wrappers around external services that use basic types to represent quantities. Your application code then uses the ACL to communicate with that system thus eliminating the need to deal with type and scale conversions in multiple places throughout your application logic. class ScadaServiceAnticorruption(val service: ScadaService) {   // ScadaService returns meter load as Double representing Megawatts   def getLoad: Power = Megawatts(service.getLoad(meterId))   }   // ScadaService.sendTempBias requires a Double representing Fahrenheit   def sendTempBias(temp: Temperature) =     service.sendTempBias(temp.to(Fahrenheit)) } Implement the ACL as a trait and mix in to the application's services where needed. trait WeatherServiceAntiCorruption {   val service: WeatherService   def getTemperature: Temperature = Celsius(service.getTemperature)   def getIrradiance: Irradiance = WattsPerSquareMeter(service.getIrradiance) } Extend the pattern to provide multi-currency support class MarketServiceAnticorruption(val service: MarketService)      (implicit val moneyContext: = MoneyContext) {    // MarketService.getPrice returns a Double representing $/MegawattHour   def getPrice: Price[Energy] =     (USD(service.getPrice) in moneyContext.defaultCurrency) / megawattHour    // MarketService.sendBid requires a Double representing $/MegawattHour   // and another Double representing the max amount of energy in MegawattHours   def sendBid(bid: Price[Energy], limit: Energy) =     service.sendBid((bid * megawattHour) to USD, limit to MegawattHours) } Build Anticorruption into Akka routers // LoadReading message used within a Squants enabled application context case class LoadReading(meterId: String, time: Long, load: Power) class ScadaLoadListener(router: Router) extends Actor {   def receive = {    // ScadaLoadReading - from an external service - sends load as a string    // eg, “10.3 MW”, “345 kW”    case msg @ ScadaLoadReading(meterId, time, loadString) ⇒     // Parse the string and on success emit the Squants enabled event to routees     Power(loadString) match {       case Success(p) => router.route(LoadReading(meterId, time, p), sender())       case Failure(e) => // react to QuantityStringParseException     }   } } ... and REST API's with contracts that require basic types trait LoadRoute extends HttpService {   def repo: LoadRepository   val loadRoute = {     path(""meter-reading"") {       // REST API contract requires load value and units in different fields       // Units are string values that may be 'kW' or 'MW'       post {         parameters(meterId, time, loadDouble, unit) { (meterId, time, loadDouble, unit) =>           complete {             val load = unit match {               case ""kW"" => Kilowatts(loadDouble)               case ""MW"" => Megawatts(loadDouble)             }             repo.saveLoad(meterId, time, load)           }         }       } ~       // REST API contract requires load returned as a number representing megawatts       get {         parameters(meterId, time) { (meterId, time) =>           complete {             repo.getLoad(meterId, time) to Megawatts           }         }       }     }   } } Contributors Gary Keorkunian (garyKeorkunian) Jeremy Apthorp (nornagon) Steve Barham (stevebarham) Derek Morr (derekmorr) rmihael (rmihael) Florian Nussberger (fnussber) Ajay Chandran (ajaychandran) Gia Bảo (giabao) Josh Lemer (joshlemer) Dave DeCarpio (DaveDeCaprio) Carlos Quiroz (cquiroz) Code of Conduct Squants is a Typelevel Incubator Project and, as such, supports the Typelevel Code of Conduct. Caveats Code is offered as-is, with no implied warranty of any kind. Comments, criticisms, and/or praise are welcome, especially from scientists, engineers and the like. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/garyKeorkunian/squants"	"The Scala API for Quantities, Units of Measure and Dimensional Analysis."	"true"
"Science and Data Analysis"	"Zeppelin"	"http://zeppelin-project.org/"	"Scala and Spark Notebook (like IPython Notebook)"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Zeppelin Toggle navigation zeppelin-project Try Apache Zeppelin Start Live demo powered by Apache Spark and Apache Zeppelin on Amazon EMR Apache Zeppelin is a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. Official Website for Apache Zeppelin is http://zeppelin.apache.org Apache Zeppelin 0.5.6 Official release | 0.6.0 Snapshot ZeppelinHub Hub | Viewer | Learn more Apache Zeppelin and the Zeppelin logo are trademarks of the Apache Software Foundation. Apache Zeppelin snapshot binary is provided for development and verification purpose only. ZeppelinHub is neither part of Apache Software Foundation nor Apache Zeppelin community work. 31 May 2016 zeppelin, ApacheCon, and 2016 Zeppelin at ApacheCon BigData North America 2016 Apache Zeppelin at ApacheCon BigData North America 2016 A small followup on Zeppelin project taking part in ApacheCon in Vancouver. Read More... 18 May 2016 apache, zeppelin, toplevel, graduation, spark, notebook, and visualization Apache Zeppelin As A Top Level Project English The Apache Software Foundation Announces Apache® Zeppelin™ as a Top-Level Project Original announcement can be found here. Congrats team! Open Source Big Data analytics and visualization tool for distributed, interactive, and collaborative systems using Apache Flink, Apache Hadoop, Apache Spark, and more. Forest Hill, MD –25 May 2016– The Apache Software Foundation (ASF), the all-volunteer developers, stewards, and incubators of more than 350 Open Source projects... Read More... 22 Jan 2016 apache, zeppelin, release, hadoop, spark, and visualization Zeppelin 0.5.6 Released Zeppelin-0.5.6-incubating released The Apache Zeppelin (incubating) community is pleased to announce the availability of the 0.5.6-incubating release. The community puts significant effort into improving Apache Zeppelin since the last release, focusing on having new backend support, improvements on stability and simplifying the configuration. More than 38 contributors provided new features, improvements and verifying release. More than 110 issues has been resolved. Read More... 23 Dec 2015 r, zeppelin, data science, and ggplot2 Apache Zeppelin 2015 Year In Review Apache Zeppelin (incubating): 2015 year in review Read More... Featured Links Tweets from https://twitter.com/ApacheZeppelin/lists/people"	"null"	"null"	"Scala and Spark Notebook (like IPython Notebook)"	"true"
"Big Data"	"BIDMach ★ 552 ⧗ 1"	"https://github.com/BIDData/BIDMach"	"CPU and GPU machine learning library, using JNI for GPU computation."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"622"	"96"	"122"	"GitHub - BIDData/BIDMach: CPU and GPU-accelerated Machine Learning Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 96 Star 622 Fork 122 BIDData/BIDMach Code Issues 30 Pull requests 12 Wiki Pulse Graphs CPU and GPU-accelerated Machine Learning Library 1,873 commits 11 branches 6 releases Fetching contributors Jupyter Notebook 44.6% Scala 30.8% Cuda 8.2% C++ 7.0% C 5.0% Shell 1.8% Other 2.6% Jupyter Notebook Scala Cuda C++ C Shell Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags MH_test bayesnet_2016 caffe gibbs haoyu_factorGraph logging master maven newgibbs scala2.9 tmat Nothing to show v1.0.3 v1.0.2 v1.0.0 v0.9.6 v0.9.5 v0.9.0 Nothing to show New pull request Latest commit df67b88 Jun 28, 2016 John Canny removed putback default in LDA Gibbs Permalink Failed to load latest commit information. data revise the MHTest to make it run with temp and var sd >1.2 May 17, 2016 jni hashmultadagrad working? Jun 15, 2016 lib updates to Net powerShape May 4, 2016 scripts tweaks to Gibbs LDA Jun 28, 2016 src/main removed putback default in LDA Gibbs Jun 28, 2016 tutorials Fix Higgs script Apr 27, 2016 .classpath fix RandomForest learner Jun 23, 2015 .project added eclipse data Nov 3, 2012 Copyright.txt added copyright notices Oct 21, 2012 LICENSE added license file Sep 20, 2014 README.md Update README.md Mar 23, 2016 benchmarks.txt Update benchmarks.txt Jun 25, 2014 bidmach Switching to new bayesnet branch for continued work on this (for CS 2… Apr 15, 2016 bidmach.cmd fixed bidmat.cmd Nov 20, 2014 bidmach65 added bidach65 Jan 31, 2016 bidmach_full fixes to startup scripts Jun 19, 2014 build.sbt added build scripts for 2.10 and 2.11 Mar 18, 2016 build_scala_2_10_sbt added both build files Mar 18, 2016 build_scala_2_11_sbt added build scripts for 2.10 and 2.11 Mar 18, 2016 getdevlibs.sh fix lib link May 27, 2016 getlibs.sh trying to remove libiomp5 dependence May 28, 2015 getnativepath.class fixes to pairembed Jun 9, 2016 getnativepath.java fixes to pairembed Jun 10, 2016 pom.xml fixes to pairembed Jun 10, 2016 sbt add sbt Jan 24, 2014 shortpath.bat added shortpath.bat Apr 9, 2014 README.md BIDMach is a very fast machine learning library. Check the latest benchmarks The github distribution contains source code only. To get the libraries for your platform, run ./getdevlibs.sh from this directory. Then you can run bidmach with ./bidmach. You can build the Java/Scala main jar with sbt (included). There are build scripts for both Scala 2.10 and 2.11. Copy the appropriate one into build.sbt, and then do ""./sbt package"". You can then run bidmach with ./bidmach (you still need to download the libraries some of which are native). You can also download an executable bundle from here. You will need the libs from there in order to build from a git branch. We use a lot of native code which isn't all available from repos, and you will save a lot of time and headaches by grabbing compiled versions. The main project page is here. Documentation is here in the wiki New BIDMach has a discussion group on Google Groups. BIDMach is a sister project of BIDMat, a matrix library, which is also on github Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/BIDData/BIDMach"	"CPU and GPU machine learning library, using JNI for GPU computation."	"true"
"Big Data"	"Gearpump ★ 538 ⧗ 0"	"https://github.com/gearpump/gearpump"	"Lightweight real-time big data streaming engine"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"558"	"91"	"123"	"GitHub - gearpump/gearpump: Lightweight real-time big data streaming engine over Akka Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 91 Star 558 Fork 123 gearpump/gearpump Code Issues 0 Pull requests 0 Wiki Pulse Graphs Lightweight real-time big data streaming engine over Akka http://gearpump.io 2,063 commits 2 branches 34 releases 17 contributors Scala 84.1% JavaScript 9.1% Java 4.0% HTML 2.6% Other 0.2% Scala JavaScript Java HTML Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.8.0 master Nothing to show v0.1 cdh 0.8.0 0.7.6 0.7.5 0.7.4 0.7.3 0.7.2 0.7.1 0.7.0 0.6.1.4 0.6.1.3 0.6.1.2 0.6.1.1 0.6.1 0.6.0 0.5.0 0.4.4 0.4.2 0.4.1 0.4 0.3.7 0.3.6 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0-rc1 0.2.3 0.2.2 0.2.1 0.2 0.2-rc1 Nothing to show New pull request Latest commit ab1df6d Apr 27, 2016 manuzhang add notice for moving to Apache repo Permalink Failed to load latest commit information. conf GEARPUMP-11, fix code style Apr 11, 2016 core/src GEARPUMP-11, fix code style Apr 11, 2016 daemon/src GEARPUMP-11, fix code style Apr 11, 2016 docs GEARPUMP-11, fix code style Apr 11, 2016 examples GEARPUMP-11, fix code style Apr 11, 2016 experiments GEARPUMP-11, fix code style Apr 11, 2016 external GEARPUMP-11, fix code style Apr 11, 2016 integrationtest GEARPUMP-11, fix code style Apr 11, 2016 notes GEARPUMP-11, fix code style Apr 11, 2016 project GEARPUMP-11, fix code style Apr 11, 2016 services GEARPUMP-11, fix code style Apr 11, 2016 streaming/src GEARPUMP-11, fix code style Apr 11, 2016 unmanagedlibs GEARPUMP-11, fix code style Apr 11, 2016 yarnconf GEARPUMP-11, fix code style Apr 11, 2016 .coveragerc fix #132 add codecov config Dec 25, 2014 .gitignore GEARPUMP-8, fix ""two machines can possibly have same worker id for si… Apr 2, 2016 .sbtopts GEARPUMP-9, Clean and fix integration test Apr 4, 2016 .travis.yml GEARPUMP-11, fix code style Apr 11, 2016 CHANGELOG.md GEARPUMP-11, fix code style Apr 11, 2016 CONTRIBUTING.md GEARPUMP-11, fix code style Apr 11, 2016 LICENSE GEARPUMP-11, fix code style Apr 11, 2016 README.md add notice for moving to Apache repo Apr 27, 2016 ReleaseProcess.md GEARPUMP-11, fix code style Apr 11, 2016 jvm.sbt GEARPUMP-11, fix code style Apr 11, 2016 pubring.asc fix #359, Allow travis to auto publish signed artifacts to nexus sona… Jan 27, 2015 scalastyle-config.xml GEARPUMP-11, fix code style Apr 11, 2016 secring.asc.enc fix #359, Allow travis to auto publish signed artifacts to nexus sona… Jan 27, 2015 version.sbt GEARPUMP-11, fix code style Apr 11, 2016 README.md Gearpump has moved to Apache. The official Gearpump repo is now hosted by Apache and mirrored to https://github.com/apache/incubator-gearpump. Issues are tracked at https://issues.apache.org/jira/browse/GEARPUMP Gearpump Online Demo Site: http://demo.gearpump.io/ Gearpump is a lightweight real-time big data streaming engine. It is inspired by recent advances in the Akka framework and a desire to improve on existing streaming frameworks. The name Gearpump is a reference to the engineering term ""gear pump"", which is a super simple pump that consists of only two gears, but is very powerful at streaming water. We model streaming within the Akka actor hierarchy. Per initial benchmarks we are able to process near 18 million messages/second (100 bytes per message) with a 8ms latency on a 4-node cluster. For steps to reproduce the performance test, please check Performance benchmark. Useful Resources Read the Introduction on TypeSafe's Blog Learn the Basic Concepts How to Develop your first application How to Submit your first application Explore the Maven dependencies Explore the Document site Explore the User List Report an issue How to Build 1). Clone the Gearpump repository   git clone https://github.com/gearpump/gearpump.git   cd gearpump 2). Build package   ## Please use scala 2.11 or 2.10   ## The target package path: output/target/gearpump-${version}.zip   sbt clean +assembly +packArchiveZip After the build, there will be a package file gearpump-${version}.zip generated under output/target/ folder. To build scala document, use    ## Will generate the scala doc under target/scala_2.xx/unidoc/    sbt unidoc NOTE: The build requires network connection. If you are behind an enterprise proxy, make sure you have set the proxy in your env before running the build commands. For windows: set HTTP_PROXY=http://host:port set HTTPS_PROXY= http://host:port For Linux: export HTTP_PROXY=http://host:port export HTTPS_PROXY= http://host:port How to run Gearpump integration test Gearpump has an integration test system which is based on Docker. Please check the instructions. How to do style check before submitting a pull request? Before submitting a PR, you should always run style check first:   ## Run style check for compile, test, and integration test.   sbt scalastyle test:scalastyle it:scalastyle  Contributors (time order) Sean Zhong Kam Kasravi Manu Zhang Huafeng Wang Weihua Jiang Suneel Marthi Stanley Xu Tomasz Targonski Sun Kewei Gong Yu Contacts Please use the google user list if possible. For things that are not OK to be shared in mailing list, please contact: Sean Zhong, Kam Kasravi or Weihua Jiang. License Gearpump itself is licensed under the Apache License (2.0). For library it used, please see LICENSE. Acknowledgement The netty transport code work is based on Apache Storm. Thanks Apache Storm contributors. The cgroup code work is based on JStorm. Thanks JStorm contributors. Thanks to Jetbrains for providing a IntelliJ IDEA Free Open Source License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/gearpump/gearpump"	"Lightweight real-time big data streaming engine"	"true"
"Big Data"	"GridScale ★ 12 ⧗ 79"	"https://github.com/openmole/gridscale"	"A Scala API for computing clusters and grids."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"12"	"4"	"2"	"GitHub - openmole/gridscale: Scala library for accessing various file, batch systems, job schedulers and grid middlewares. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 12 Fork 2 openmole/gridscale Code Issues 0 Pull requests 0 Pulse Graphs Scala library for accessing various file, batch systems, job schedulers and grid middlewares. 718 commits 3 branches 0 releases Fetching contributors Java 82.1% Scala 17.9% Java Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bc147 benchmark master Nothing to show Nothing to show New pull request Latest commit 84159c2 Jul 15, 2016 romainreuillon Make group status refresh asynchronous in DIRAC. Permalink Failed to load latest commit information. examples libraries Use duration for proxy renewal. Jul 7, 2016 modules Make group status refresh asynchronous in DIRAC. Jul 15, 2016 project .gitignore Reformat poms. Dec 4, 2012 README.md build.sbt enh: optimise imports across project Nov 17, 2015 version.sbt README.md GridScale GridScale is a scala library for accessing various file and batch system. For the time being it supports: Glite / EMI, the European grid middleware, Remote SSH server, PBS clusters, SLURM clusters, SGE clusters, OAR clusters, Condor flocks HTTP file lists, DIRAC job pilot system. Licence GridScale is licenced under the GNU Affero GPLv3 software licence.  Build GridScale builds with sbt. Use the compile and/or package task to build all the modules. Imports In order to use gridscale you should import the folowing namespaces: import fr.iscpif.gridscale._  SBT GridScale is cross compiled against serveral versions of scala. To use on of its modules add a dependency like: libraryDependencies += ""fr.iscpif.gridscale"" %% ""gridscalepbs"" % version  Examples Up to date examples are available in the example directory. Standalone runnable jars can be generated for each example with the one-jar task (please note that this can only be done after the task package has been run. Development GridScale can be generated locally using: sbt publish-local To release in one step, use: sbt 'release with-defaults' Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/openmole/gridscale"	"A Scala API for computing clusters and grids."	"true"
"Big Data"	"Kafka ★ 2917 ⧗ 0"	"https://github.com/apache/kafka"	"Kafka is a message broker project and aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"3441"	"476"	"2115"	"GitHub - apache/kafka: Mirror of Apache Kafka Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 476 Star 3,441 Fork 2,115 apache/kafka mirrored from git://git.apache.org/kafka.git Code Pull requests 216 Pulse Graphs Mirror of Apache Kafka 2,526 commits 14 branches 28 releases 203 contributors Java 57.8% Scala 36.4% Python 4.8% Shell 0.6% Batchfile 0.2% XSLT 0.1% HTML 0.1% Java Scala Python Shell Batchfile XSLT HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: trunk Switch branches/tags Branches Tags 0.7 0.7.0 0.7.1 0.7.2 0.8 0.8.0-beta1-candidate1 0.8.1 0.8.2 0.9.0 0.10.0 consumer_redesign legacy_client_libraries transactional_messaging trunk Nothing to show show kafka-0.7.2-incubating-candidate-5 kafka-0.7.2-incubating-candidate-4 kafka-0.7.2-incubating-candidate-3 kafka-0.7.2-incubating-candidate-2 kafka-0.7.2-incubating-candidate-1 kafka-0.7.1-incubating-candidate-3 kafka-0.7.1-incubating-candidate-2 kafka-0.7.1-incubating-candidate-1 kafka-0.7.0-incubating-candidate-9 0.10.0.0 0.10.0.0-rc6 0.10.0.0-rc5 0.10.0.0-rc4 0.10.0.0-rc3 0.10.0.0-rc2 0.10.0.0-rc1 0.9.0.1 0.9.0.0 0.8.2.2 0.8.2.1 0.8.2.0 0.8.2-beta 0.8.1.1 0.8.1.0 0.8.0 0.8.0-beta1 0.8.0-beta1-candidate1 Nothing to show New pull request Latest commit 7a70c1a Jul 15, 2016 swwl1992 committed with guozhangwang KAFKA-3952: Consumer rebalance verifier never succeed due to type mis… … …match  Author: Wan Wenli <wwl.990@hotmail.com>  Reviewers: Guozhang Wang <wangguoz@gmail.com>  Closes #1612 from swwl1992/ticket-KAFKA-3952-fix-consumer-rebalance-verifier Permalink Failed to load latest commit information. bin KAFKA-3692; Add quotes to variables in kafka-run-class.sh May 13, 2016 checkstyle KAFKA-3561: Auto create through topic for KStream aggregation and join Jun 16, 2016 clients/src KAFKA-3905: Handling null/empty topics and collections, patterns when… Jul 14, 2016 config KAFKA-3421: Connect developer guide update and several fixes May 12, 2016 connect KAFKA-2941: Clarify docs for key and value Converters Jul 10, 2016 core/src KAFKA-3952: Consumer rebalance verifier never succeed due to type mis… Jul 15, 2016 docs KAFKA-3725; Update documentation with regards to XFS Jul 11, 2016 examples KAFKA-3548: Use root locale for case transformation of constant strings Apr 20, 2016 gradle KAFKA-3838; Update zkClient to 0.9 and Zookeeper to 3.4.8 Jun 17, 2016 log4j-appender/src KAFKA-3077: Enable KafkaLog4jAppender to work with SASL enabled brokers Jan 11, 2016 streams KAFKA-3941: Delay eviction listener in InMemoryKeyValueLoggedStore af… Jul 13, 2016 tests MINOR: bug fixes to ducktape services Jun 30, 2016 tools/src/main/java/org/apache/kafka/tools MINOR: Fix a variable name semantically correct. Jun 11, 2016 vagrant MINOR: fix Bash shebang on vagrant/ scripts Jul 1, 2016 .gitignore KAFKA-3592: System test - configurable paths May 6, 2016 CONTRIBUTING.md KAFKA-2321; Introduce CONTRIBUTING.md Jul 27, 2015 HEADER trivial fix to add missing license header using .gradlew licenseForma… Feb 7, 2014 LICENSE KAFKA-3314: Add CDDL license to LICENSE and NOTICE file Mar 3, 2016 NOTICE KAFKA-3314: Add CDDL license to LICENSE and NOTICE file Mar 3, 2016 README.md MINOR: Add user overridden test logging events Jun 3, 2016 Vagrantfile MINOR: Add vagrant up wrapper for simple parallel bringup on aws Mar 20, 2016 build.gradle KAFKA-3562; Handle topic deletion during a send Jul 11, 2016 doap_Kafka.rdf MINOR: Remove <release> tag from doap file May 12, 2016 gradle.properties Changing version to 0.10.1.0-SNAPSHOT Mar 21, 2016 gradlew MINOR: Upgrade to Gradle 2.13 Apr 26, 2016 gradlew.bat MINOR: Upgrade to Gradle 2.13 Apr 26, 2016 kafka-merge-pr.py MINOR: update new version in additional places Mar 21, 2016 settings.gradle KAFKA-3066: Demo Examples for Kafka Streams Jan 22, 2016 wrapper.gradle KAFKA-1490 remove gradlew initial setup output from source distributi… Sep 23, 2014 README.md Apache Kafka See our web site for details on the project. You need to have Gradle and Java installed. Kafka requires Gradle 2.0 or higher. Java 7 should be used for building in order to support both Java 7 and Java 8 at runtime. First bootstrap and download the wrapper cd kafka_source_dir gradle  Now everything else will work. Building a jar and running it ./gradlew jar    Follow instructions in http://kafka.apache.org/documentation.html#quickstart Building source jar ./gradlew srcJar  Building aggregated javadoc ./gradlew aggregatedJavadoc  Building javadoc and scaladoc ./gradlew javadoc ./gradlew javadocJar # builds a javadoc jar for each module ./gradlew scaladoc ./gradlew scaladocJar # builds a scaladoc jar for each module ./gradlew docsJar # builds both (if applicable) javadoc and scaladoc jars for each module  Running unit tests ./gradlew test  Forcing re-running unit tests w/o code change ./gradlew cleanTest test  Running a particular unit test ./gradlew -Dtest.single=RequestResponseSerializationTest core:test  Running a particular test method within a unit test ./gradlew core:test --tests kafka.api.ProducerFailureHandlingTest.testCannotSendToInternalTopic ./gradlew clients:test --tests org.apache.kafka.clients.MetadataTest.testMetadataUpdateWaitTime  Running a particular unit test with log4j output Change the log4j setting in either clients/src/test/resources/log4j.properties or core/src/test/resources/log4j.properties ./gradlew -i -Dtest.single=RequestResponseSerializationTest core:test  Generating test coverage reports ./gradlew reportCoverage  Building a binary release gzipped tar ball ./gradlew clean ./gradlew releaseTarGz  The above command will fail if you haven't set up the signing key. To bypass signing the artifact, you can run: ./gradlew releaseTarGz -x signArchives  The release file can be found inside ./core/build/distributions/. Cleaning the build ./gradlew clean  Running a task on a particular version of Scala (either 2.10.6 or 2.11.8) Note that if building the jars with a version other than 2.10, you need to set the SCALA_BINARY_VERSION variable or change it in bin/kafka-run-class.sh to run the quick start. You can pass either the major version (eg 2.11) or the full version (eg 2.11.8): ./gradlew -PscalaVersion=2.11 jar ./gradlew -PscalaVersion=2.11 test ./gradlew -PscalaVersion=2.11 releaseTarGz  Running a task for a specific project This is for core, examples and clients ./gradlew core:jar ./gradlew core:test  Listing all gradle tasks ./gradlew tasks  Building IDE project Note that this is not strictly necessary (IntelliJ IDEA has good built-in support for Gradle projects, for example). ./gradlew eclipse ./gradlew idea  Building the jar for all scala versions and for all projects ./gradlew jarAll  Running unit tests for all scala versions and for all projects ./gradlew testAll  Building a binary release gzipped tar ball for all scala versions ./gradlew releaseTarGzAll  Publishing the jar for all version of Scala and for all projects to maven ./gradlew uploadArchivesAll  Please note for this to work you should create/update ~/.gradle/gradle.properties and assign the following variables mavenUrl= mavenUsername= mavenPassword= signing.keyId= signing.password= signing.secretKeyRingFile=  Installing the jars to the local Maven repository ./gradlew installAll  Building the test jar ./gradlew testJar  Determining how transitive dependencies are added ./gradlew core:dependencies --configuration runtime  Determining if any dependencies could be updated ./gradlew dependencyUpdates  Running checkstyle on the java code ./gradlew checkstyleMain checkstyleTest  This will most commonly be useful for automated builds where the full resources of the host running the build and tests may not be dedicated to Kafka's build. Common build options The following options should be set with a -D switch, for example ./gradlew -Dorg.gradle.project.maxParallelForms=1 test. org.gradle.project.mavenUrl: sets the URL of the maven deployment repository (file://path/to/repo can be used to point to a local repository). org.gradle.project.maxParallelForks: limits the maximum number of processes for each task. org.gradle.project.showStandardStreams: shows standard out and standard error of the test JVM(s) on the console. org.gradle.project.skipSigning: skips signing of artifacts. org.gradle.project.testLoggingEvents: unit test events to be logged, separated by comma. For example ./gradlew -Dorg.gradle.project.testLoggingEvents=started,passed,skipped,failed test Running in Vagrant See vagrant/README.md. Contribution Apache Kafka is interested in building the community; we would welcome any thoughts or patches. You can reach us on the Apache mailing lists. To contribute follow the instructions here: http://kafka.apache.org/contributing.html Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/apache/kafka"	"Kafka is a message broker project and aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds."	"true"
"Big Data"	"Reactive-kafka ★ 467 ⧗ 0"	"https://github.com/akka/reactive-kafka"	"Reactive Streams API for Apache Kafka."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"570"	"67"	"120"	"GitHub - akka/reactive-kafka: Reactive Streams API for Apache Kafka Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 67 Star 570 Fork 120 akka/reactive-kafka Code Issues 14 Pull requests 1 Pulse Graphs Reactive Streams API for Apache Kafka 485 commits 3 branches 26 releases 23 contributors Scala 92.2% Java 7.8% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.8 0.10 master Nothing to show v0.11-M4 v0.11-M3 v0.11-M2 v0.11-M1 v0.10.1 v0.10.0 v0.9.0 v0.8.8 v0.8.7 v0.8.6 v0.8.5 v0.8.4 v0.8.3 v0.8.2 v0.8.1 v0.8.0 v0.7.2 v0.7.1 v0.7.0 v0.6.0 v0.5.0 v0.4.0 v0.3.0 v0.2.0 v0.1.0 0.11-M3 Nothing to show New pull request Latest commit d06ab5c Jul 13, 2016 kciesielski committed on GitHub Merge pull request #179 from 13h3r/176-first-poll-problem-13h3r … Pause partition after assignment Permalink Failed to load latest commit information. benchmarks/src/main/resources remove old api Apr 1, 2016 core/src Pause partition after assignment Jul 13, 2016 docs images moved to a subfolder Feb 23, 2016 project enable header plugin, copyright Mar 16, 2016 .gitignore gitignore .tmpBin Mar 16, 2016 .travis-jvmopts sbt jvmopts for travis Mar 11, 2016 .travis.yml Use Embedded Kafka for integration tests May 19, 2016 CONTRIBUTING.md add CONTRIBUTING.md Mar 16, 2016 LICENSE enable header plugin, copyright Mar 16, 2016 README.md Fix link syntax Jul 11, 2016 build.sbt update to Akka 2.4.8 Jul 8, 2016 version.sbt Setting version to 0.11-SNAPSHOT Jul 8, 2016 README.md Reactive Streams for Kafka If you have questions or are working on a pull request or just curious, please feel welcome to join the chat room: Akka Streams connector for Apache Kafka. Created and maintained by New API: 0.11-M4 Supports Kafka 0.10.0.x This version of akka-stream-kafka depends on Akka 2.4.8, Scala 2.11.8. Available at Maven Central for Scala 2.11: libraryDependencies += ""com.typesafe.akka"" %% ""akka-stream-kafka"" % ""0.11-M4"" Changes 0.11-M4 Consumer implementation is completely rewritten. Now we support partition assignment for consumer groups. More details about it here. Also we may reuse one kafka connection in case of manual topic-partition assignment. Consumer API slightly changed. In 0.11-M3 you set topic-partition to subscribe in ConsumerSettings. Now ConsumerSettings represents a kafka connection. The connection potentially can serve multiple Sources and you specify topic-partition for every source. Example usage Scala Producer Settings: import akka.kafka._ import akka.kafka.scaladsl._ import org.apache.kafka.common.serialization.StringSerializer import org.apache.kafka.common.serialization.ByteArraySerializer    val producerSettings = ProducerSettings(system, new ByteArraySerializer, new StringSerializer)     .withBootstrapServers(""localhost:9092"") Produce messages:   Source(1 to 10000)     .map(_.toString)     .map(elem => new ProducerRecord[Array[Byte], String](""topic1"", elem))     .to(Producer.plainSink(producerSettings)) Produce messages in a flow:   Source(1 to 10000)     .map(elem => ProducerMessage.Message(new ProducerRecord[Array[Byte], String](""topic1"", elem.toString), elem))     .via(Producer.flow(producerSettings))     .map { result =>       val record = result.message.record       println(s""${record.topic}/${record.partition} ${result.offset}: ${record.value} (${result.message.passThrough}"")       result     } Consumer Settings: import akka.kafka._ import akka.kafka.scaladsl._ import org.apache.kafka.common.serialization.StringDeserializer import org.apache.kafka.common.serialization.ByteArrayDeserializer import org.apache.kafka.clients.consumer.ConsumerConfig  val consumerSettings = ConsumerSettings(system, new ByteArrayDeserializer, new StringDeserializer)   .withBootstrapServers(""localhost:9092"")   .withGroupId(""group1"")   .withProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"") Consume messages and store a representation, including offset, in DB:   db.loadOffset().foreach { fromOffset =>     val subscription = Subscriptions.assignmentWithOffset(new TopicPartition(""topic1"", 1) -> fromOffset)     Consumer.plainSource(consumerSettings, subscription)       .mapAsync(1)(db.save)   } Consume messages at-most-once:   Consumer.atMostOnceSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))     .mapAsync(1) { record =>       rocket.launch(record.value)     } Consume messages at-least-once:   Consumer.committableSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))     .mapAsync(1) { msg =>       db.update(msg.value).flatMap(_ => msg.committableOffset.commitScaladsl())     } Connect a Consumer to Producer:   Consumer.committableSource(consumerSettings.withClientId(""client1""))     .map(msg =>       ProducerMessage.Message(new ProducerRecord[Array[Byte], String](""topic2"", msg.value), msg.committableOffset))     .to(Producer.commitableSink(producerSettings)) Consume messages at-least-once, and commit in batches:   Consumer.committableSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))     .mapAsync(1) { msg =>       db.update(msg.value).map(_ => msg.committableOffset)     }     .batch(max = 10, first => CommittableOffsetBatch.empty.updated(first)) { (batch, elem) =>       batch.updated(elem)     }     .mapAsync(1)(_.commitScaladsl()) Reusable kafka consumer:   //Consumer is represented by actor   //Create new consumer   val consumer: ActorRef = system.actorOf(KafkaConsumerActor.props(consumerSettings))    //Manually assign topic partition to it   val stream1 = Consumer     .plainExternalSource[Array[Byte], String](consumer, Subscriptions.assignment(new TopicPartition(""topic1"", 1)))     .via(business)     .to(Sink.ignore)    //Manually assign another topic partition   val stream2 = Consumer     .plainExternalSource[Array[Byte], String](consumer, Subscriptions.assignment(new TopicPartition(""topic1"", 2)))     .via(business)     .to(Sink.ignore) Consumer group:   //Consumer group represented as Source[(TopicPartition, Source[Messages])]   val consumerGroup = Consumer.committablePartitionedSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))   //Process each assigned partition separately   consumerGroup.map {     case (topicPartition, source) =>       source         .via(business)         .toMat(Sink.ignore)(Keep.both)         .run()   }   .mapAsyncUnordered(maxPartitions)(_._2) Additional examples are available in ConsumerExamples.scala Java Producer Settings: import akka.kafka.*; import akka.kafka.javadsl.*; import org.apache.kafka.common.serialization.StringSerializer; import org.apache.kafka.common.serialization.ByteArraySerializer;  final ProducerSettings<byte[], String> producerSettings = ProducerSettings   .create(system, new ByteArraySerializer(), new StringSerializer())   .withBootstrapServers(""localhost:9092""); Produce messages: Source.range(1, 10000)   .map(n -> n.toString()).map(elem -> new ProducerRecord<byte[], String>(""topic1"", elem))   .to(Producer.plainSink(producerSettings)); Produce messages in a flow: Source.range(1, 10000)   .map(n -> new ProducerMessage.Message<byte[], String, Integer>(     new ProducerRecord<>(""topic1"", n.toString()), n))   .via(Producer.flow(producerSettings))   .map(result -> {     ProducerRecord record = result.message().record();     System.out.println(record);     return result;   }); Consumer Settings: import akka.kafka.*; import akka.kafka.javadsl.*; import org.apache.kafka.common.serialization.StringSerializer; import org.apache.kafka.common.serialization.ByteArraySerializer; import org.apache.kafka.clients.consumer.ConsumerConfig;  final ConsumerSettings<byte[], String> consumerSettings =     ConsumerSettings.create(system, new ByteArrayDeserializer(), new StringDeserializer())   .withBootstrapServers(""localhost:9092"")   .withGroupId(""group1"")   .withProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest""); Consume messages and store a representation, including offset, in DB: db.loadOffset().thenAccept(fromOffset -> {   Consumer.plainSource(           consumerSettings,           Subscriptions.assignmentWithOffset(new TopicPartition(""topic1"", 1), fromOffset)   ).mapAsync(1, record -> db.save(record)); }); Consume messages at-most-once: Consumer.atMostOnceSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))   .mapAsync(1, record -> rocket.launch(record.value())); Consume messages at-least-once: Consumer.committableSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))   .mapAsync(1, msg -> db.update(msg.value())     .thenCompose(done -> msg.committableOffset().commitJavadsl())); Connect a Consumer to Producer: Consumer.committableSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))   .map(msg ->     new ProducerMessage.Message<byte[], String, ConsumerMessage.Committable>(         new ProducerRecord<>(""topic2"", msg.value()), msg.committableOffset()))   .to(Producer.commitableSink(producerSettings)); Consume messages at-least-once, and commit in batches: Consumer.committableSource(consumerSettings.withClientId(""client1""), Subscriptions.topics(""topic1""))   .mapAsync(1, msg ->     db.update(msg.value()).thenApply(done -> msg.committableOffset()))   .batch(10,     first -> ConsumerMessage.emptyCommittableOffsetBatch().updated(first),     (batch, elem) -> batch.updated(elem))   .mapAsync(1, c -> c.commitJavadsl()); Additional examples are available in ConsumerExamples.java Configuration The configuration properties are defined in reference.conf Testing Tests require Apache Kafka and Zookeeper to be available on localhost:9092 and localhost:2181. Note that auto.create.topics.enable should be true. Old API: 0.10.0 Supports Kafka 0.9.0.x For Kafka 0.8 see this branch. Available at Maven Central for Scala 2.11: libraryDependencies += ""com.softwaremill.reactivekafka"" %% ""reactive-kafka-core"" % ""0.10.0"" Example usage Scala import akka.actor.ActorSystem import akka.stream.ActorMaterializer import akka.stream.scaladsl.{Sink, Source} import com.softwaremill.react.kafka.KafkaMessages._ import org.apache.kafka.common.serialization.{StringSerializer, StringDeserializer} import com.softwaremill.react.kafka.{ProducerMessage, ConsumerProperties, ProducerProperties, ReactiveKafka} import org.reactivestreams.{ Publisher, Subscriber }  implicit val actorSystem = ActorSystem(""ReactiveKafka"") implicit val materializer = ActorMaterializer()  val kafka = new ReactiveKafka() val publisher: Publisher[StringConsumerRecord] = kafka.consume(ConsumerProperties(  bootstrapServers = ""localhost:9092"",  topic = ""lowercaseStrings"",  groupId = ""groupName"",  valueDeserializer = new StringDeserializer() )) val subscriber: Subscriber[StringProducerMessage] = kafka.publish(ProducerProperties(   bootstrapServers = ""localhost:9092"",   topic = ""uppercaseStrings"",   valueSerializer = new StringSerializer() ))  Source.fromPublisher(publisher).map(m => ProducerMessage(m.value().toUpperCase))   .to(Sink.fromSubscriber(subscriber)).run() Java import akka.actor.ActorSystem; import akka.stream.ActorMaterializer; import akka.stream.javadsl.Sink; import akka.stream.javadsl.Source; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.common.serialization.StringDeserializer; import org.apache.kafka.common.serialization.StringSerializer; import org.reactivestreams.Publisher; import org.reactivestreams.Subscriber;  public void run() { String brokerList = ""localhost:9092"";  ReactiveKafka kafka = new ReactiveKafka(); ActorSystem system = ActorSystem.create(""ReactiveKafka""); ActorMaterializer materializer = ActorMaterializer.create(system);  StringDeserializer deserializer = new StringDeserializer(); ConsumerProperties<String> cp =    new PropertiesBuilder.Consumer(brokerList, ""topic"", ""groupId"", deserializer)       .build();  Publisher<ConsumerRecord<String, String>> publisher = kafka.consume(cp, system);  StringSerializer serializer = new StringSerializer(); ProducerProperties<String, String> pp = new PropertiesBuilder.Producer(    brokerList,    ""topic"",    serializer,    serializer).build();  Subscriber<ProducerMessage<String, String>> subscriber = kafka.publish(pp, system); Source.fromPublisher(publisher).map(this::toProdMessage)   .to(Sink.fromSubscriber(subscriber)).run(materializer); }  private ProducerMessage<String, String> toProdMessage(ConsumerRecord<String, String> record) {   return KeyValueProducerMessage.apply(record.key(), record.value()); } Passing configuration properties to Kafka In order to set your own custom Kafka parameters, you can construct ConsumerProperties and ProducerProperties using some of their provided methods in a builder-pattern-style DSL, for example: import org.apache.kafka.common.serialization.StringDeserializer import com.softwaremill.react.kafka.ConsumerProperties  val consumerProperties = ConsumerProperties(   ""localhost:2181"",   ""topic"",   ""groupId"",   new StringDeserializer() )   .readFromEndOfStream()   .consumerTimeoutMs(300)   .commitInterval(2 seconds)   .setProperty(""some.kafka.property"", ""value"") The ProducerProperties class offers a similar API. Controlling consumer start offset By default a new consumer will start reading from the beginning of a topic, fetching all uncommitted messages. If you want to start reading from the end, you can specify this on your ConsumerProperties:   val consumerProperties = ConsumerProperties(...).readFromEndOfStream() Working with actors Since we are based upon akka-stream, the best way to handle errors is to leverage Akka's error handling and lifecycle management capabilities. Producers and consumers are in fact actors. Obtaining actor references ReactiveKafka comes with a few methods allowing working on the actor level. You can let it create Props to let your own supervisor create these actors as children, or you can directly create actors at the top level of supervision. Here are some examples: import akka.actor.{Props, ActorRef, Actor, ActorSystem} import akka.stream.ActorMaterializer import org.apache.kafka.common.serialization.{StringSerializer, StringDeserializer} import com.softwaremill.react.kafka.{ReactiveKafka, ProducerProperties, ConsumerProperties}  // inside an Actor: implicit val materializer = ActorMaterializer()  val kafka = new ReactiveKafka() // consumer val consumerProperties = ConsumerProperties(   bootstrapServers = ""localhost:9092"",   topic = ""lowercaseStrings"",   groupId = ""groupName"",   valueDeserializer = new StringDeserializer() ) val consumerActorProps: Props = kafka.consumerActorProps(consumerProperties) val publisherActor: ActorRef = context.actorOf(consumerActorProps) // or: val topLevelPublisherActor: ActorRef = kafka.consumerActor(consumerActorProps)  // subscriber val producerProperties = ProducerProperties(   bootstrapServers = ""localhost:9092"",   topic = ""uppercaseStrings"",   new StringSerializer() ) val producerActorProps: Props = kafka.producerActorProps(producerProperties) val subscriberActor: ActorRef = context.actorOf(producerActorProps) // or: val topLevelSubscriberActor: ActorRef = kafka.producerActor(producerProperties) Handling errors When a consumer or a producer fails to read/write from Kafka, the error is unrecoverable and requires that the connection be terminated. This will be performed automatically and the KafkaActorSubscriber / KafkaActorPublisher which failed will be stopped. You can use DeathWatch to detect such failures in order to restart your stream. Additionally, when a producer fails, it will signal onError() to stop the rest of stream. Example of monitoring routine: import akka.actor.{Actor, ActorRef, ActorSystem, Props} import akka.stream.ActorMaterializer import com.softwaremill.react.kafka.KafkaMessages._ import com.softwaremill.react.kafka.{ConsumerProperties, ProducerProperties, ReactiveKafka}  class Handler extends Actor {   implicit val materializer = ActorMaterializer()    def createSupervisedSubscriberActor() = {     val kafka = new ReactiveKafka()      // subscriber     val subscriberProperties = ProducerProperties(       bootstrapServers = ""localhost:9092"",       topic = ""uppercaseStrings"",       valueSerializer = new StringSerializer()     )     val subscriberActorProps: Props = kafka.producerActorProps(subscriberProperties)     val subscriberActor = context.actorOf(subscriberActorProps)     context.watch(subscriberActor)   }    override def receive: Receive = {     case Terminated(actorRef) => // your custom handling   }    // Rest of the Actor's body } Cleaning up If you want to manually stop a publisher or a subscriber, you have to send an appropriate message to the underlying actor. KafkaActorPublisher must receive a KafkaActorPublisher.Stop, whereas KafkaActorSubscriber must receive a ActorSubscriberMessage.OnComplete. If you're using a PublisherWithCommitSink returned from ReactiveKafka.consumeWithOffsetSink(), you must call its cancel() method in order to gracefully close all underlying resources. Manual Commit (version 0.8 and above) In order to be able to achieve ""at-least-once"" delivery, you can use following API to obtain an additional Sink, where you can stream back messages that you processed. An underlying actor will periodically flush offsets of these messages as committed. Example: import scala.concurrent.duration._ import akka.actor.ActorSystem import akka.stream.ActorMaterializer import com.softwaremill.react.kafka.KafkaMessages._ import akka.stream.scaladsl.Source import com.softwaremill.react.kafka.{ConsumerProperties, ReactiveKafka}  implicit val actorSystem = ActorSystem(""ReactiveKafka"") implicit val materializer = ActorMaterializer()  val kafka = new ReactiveKafka() val consumerProperties = ConsumerProperties(   bootstrapServers = ""localhost:9092"",   topic = ""lowercaseStrings"",   groupId = ""groupName"",   valueDeserializer = new StringDeserializer()) .commitInterval(5 seconds) // flush interval  val consumerWithOffsetSink = kafka.consumeWithOffsetSink(consumerProperties) Source.fromPublisher(consumerWithOffsetSink.publisher)   .map(processMessage(_)) // your message processing   .to(consumerWithOffsetSink.offsetCommitSink) // stream back for commit   .run() Tuning KafkaActorSubscriber and KafkaActorPublisher have their own thread pools, configured in reference.conf. You can tune them by overriding kafka-publisher-dispatcher.thread-pool-executor and kafka-subscriber-dispatcher.thread-pool-executor in your application.conf file. Alternatively, you can provide your own dispatcher name. It can be passed to appropriate variants of factory methods in ReactiveKafka: publish(), producerActor(), producerActorProps() or consume(), consumerActor(), consumerActorProps(). Testing Tests require Apache Kafka and Zookeeper to be available on localhost:9092 and localhost:2181. Note that auto.create.topics.enable should be true. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/akka/reactive-kafka"	"Reactive Streams API for Apache Kafka."	"true"
"Big Data"	"Scalding ★ 2478 ⧗ 0"	"https://github.com/twitter/scalding"	"A Scala binding for the Cascading abstraction of Hadoop MapReduce."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2579"	"357"	"606"	"GitHub - twitter/scalding: A Scala API for Cascading Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 357 Star 2,579 Fork 606 twitter/scalding Code Issues 237 Pull requests 33 Wiki Pulse Graphs A Scala API for Cascading http://twitter.com/scalding 3,844 commits 63 branches 70 releases 122 contributors Scala 89.3% Java 6.4% Ruby 2.4% Shell 1.2% Thrift 0.7% Scala Java Ruby Shell Thrift Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags 0.16.0 alexlevenson/parquet-1.8.1 alexlevenson/sbt-launcher-upgrade alexlevenson/typed-pipe-diff-fix alexlevenson/1541 bazel-build benpence-patch-1 benpence-patch-2 cascading3 dcl/exec_fun dcl/exec_listener dependent-mode develop exie/1068 exie/1135_try_another_branch fbs-staging-macros-scrooge-tbase fbs-staging-macros-scrooge fix_to_iterable_execution ggonzalez/fewer_gets ggonzalez/fix_readPathsFor ggonzalez/safer_box ggonzalez/0.15.1-rc1 gh-pages ianoc/MoveDefaultBuildsTo211 ianoc/addSketchOrderedSerializationInterface ianoc/genericSource ianoc/housekeeping ianoc/jdbcMacros ianoc/lui ianoc/removeUnneededTypeParameter ianoc/scaldingViz ioconnell/SeparateFS ioconnell/addMacroDescriptions ioconnell/sparkScalding isnotinvain-patch-1 isnotinvain-patch-2-1 isnotinvain-patch-2 mashraf/scalding_RC15 mashraf/scalding_rc16 mashraf/sequential_build mashraf/0.15.1-rc1 master oscar/execution-caching-fix oscar_fix_bdd-typedtext oscar/no-mode-tpinst oscar/weaken-the-cache oscar/0.16.0-RC1 oscar/0.16.0-RC2 releases/0.15 revertUnitOrderedSerialization rubanm/cascading3/tez_partitions_config rubanm/deprecate_jobtest_reflection rubanm/move_cascading_schemes_to_subprojects rubanm/parquet_cascading3 rubanm/0.16.1-RC1_typed_mapside_reduce_revert rubanm/0.16.1-RC1 rubanm/0.16.1-RC3 sbt_0_13_7 scoverage simple-source typed-transient typed_jdbc_source unitOrderedSerialization Nothing to show v0.16.1-RC3 v0.16.1-RC2 v0.16.1-RC1 v0.16.0 v0.16.0-RC6 v0.16.0-RC5 v0.16.0-RC4 v0.16.0-RC3 v0.16.0-RC2 v0.16.0-RC1 list 0.15.1-RC16 0.15.1-RC15 0.15.1-RC14 0.15.1-RC13 0.15.1-RC12 0.15.1-RC11 0.15.1-RC10 0.15.1-RC9 0.15.1-RC8 0.15.1-RC4 0.15.1-RC3 0.15.1-RC2 0.15.0 0.14.0 0.13.1 0.13.0 0.12.4 0.12.0 0.12.0rc5 0.12.0rc4 0.12.0rc3 0.12.0.rc2 0.12.0rc1 0.11.3rc1 0.11.2 0.11.1 0.11.0 0.10.0 0.9.1 0.9.0 0.9.0rc17 0.9.0rc16 0.9.0rc15 0.9.0rc4 0.9.0rc1 0.8.11 0.8.8 0.8.7 0.8.6 0.8.5 0.8.4 0.8.3 0.8.2 0.8.1 0.7.0 0.6.0 0.5.4 0.5.3 0.5.2 0.5.1 0.5.0 0.4.1 0.4.0 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 Nothing to show New pull request Latest commit ede6974 Jul 13, 2016 johnynek committed on GitHub Merge pull request #1579 from jbalogh/list-type-deprecation … [parquet tuple macros] listType was deprecated in favor of listOfElements. Permalink Failed to load latest commit information. logo Add scalding logo Oct 25, 2013 maple/src/main/java/com Maple fix for HBaseTap, Issue 781 Mar 2, 2016 project Switch to https based remote repo for DocGen May 3, 2016 scalding-args/src Convert procedures to methods Jun 5, 2016 scalding-avro Move Scalding to 2.10/2.11 Dec 5, 2014 scalding-benchmarks/src/test/scala/com/twitter/scalding import hygiene: remove unused imports Apr 1, 2015 scalding-commons Call side-effecting functions with () Jun 5, 2016 scalding-core Merge pull request #1575 from twitter/oscar/batched-cms Jun 28, 2016 scalding-date/src Convert procedures to methods Jun 5, 2016 scalding-db Convert procedures to methods Jun 5, 2016 scalding-hadoop-test/src Call side-effecting functions with () Jun 5, 2016 scalding-hraven refactor toFlowStepHistory Jan 14, 2016 scalding-jdbc/src Fixes Dec 5, 2014 scalding-json/src Making side-effecting functions non-nullary May 12, 2016 scalding-parquet-fixtures/src/test/resources Copy over code from parquet-cascading, drop dep Feb 1, 2016 scalding-parquet-scrooge-fixtures/src/test/resources Move the scrooge fixtures into their own build targets so the local b… Aug 3, 2015 scalding-parquet-scrooge Fix pig exclude, unused imports, parquet-scrooge test Feb 1, 2016 scalding-parquet listType was deprecated in favor of listOfElements. Jul 9, 2016 scalding-repl Make side-effecting functions non-nullary and call side-effecting fun… Jun 5, 2016 scalding-serialization/src Little cleanup Jun 10, 2016 scalding-thrift-macros-fixtures/src/test/resources Because, because... fun, the scala compiler has special naming rules … Mar 3, 2016 scalding-thrift-macros Kill dead code Jun 10, 2016 scripts Fix references to Build.scala in other places Feb 1, 2016 tutorial Prepare for release 0.16.0 May 3, 2016 .gitignore Update the build Feb 1, 2016 .travis.blacklist Add scalding-parquet-fixtures to travis blacklist Feb 1, 2016 .travis.yml Update Scala version to 2.10.6 Dec 27, 2015 CHANGES.md Remove two extra changes May 3, 2016 CONTRIBUTING.md Add CONTRIBUTING.md Sep 19, 2012 LICENSE Initial Import Jan 10, 2012 NOTICE Initial Import Jan 10, 2012 README.md Prepare for release 0.16.0 May 3, 2016 build.sbt Use Batched to speed up CMS summing on mappers Jun 25, 2016 sbt Update Scala version to 2.10.6 Dec 27, 2015 version.sbt Setting version to 0.16.1-SNAPSHOT Jun 6, 2016 README.md Scalding Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs. Scalding is built on top of Cascading, a Java library that abstracts away low-level Hadoop details. Scalding is comparable to Pig, but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs. Current version: 0.16.0 Word Count Hadoop is a distributed system for counting words. Here is how it's done in Scalding. package com.twitter.scalding.examples  import com.twitter.scalding._ import com.twitter.scalding.source.TypedText  class WordCountJob(args: Args) extends Job(args) {   TypedPipe.from(TextLine(args(""input"")))     .flatMap { line => tokenize(line) }     .groupBy { word => word } // use each word for a key     .size // in each group, get the size     .write(TypedText.tsv[(String, Long)](args(""output"")))    // Split a piece of text into individual words.   def tokenize(text: String): Array[String] = {     // Lowercase each word and remove punctuation.     text.toLowerCase.replaceAll(""[^a-zA-Z0-9\\s]"", """").split(""\\s+"")   } } Notice that the tokenize function, which is standard Scala, integrates naturally with the rest of the MapReduce job. This is a very powerful feature of Scalding. (Compare it to the use of UDFs in Pig.) You can find more example code under examples/. If you're interested in comparing Scalding to other languages, see our Rosetta Code page, which has several MapReduce tasks in Scalding and other frameworks (e.g., Pig and Hadoop Streaming). Documentation and Getting Started Getting Started page on the Scalding Wiki Scalding Scaladocs provide details beyond the API References. Prefer using this as it's always up to date. REPL in Wonderland a hands-on tour of the scalding REPL requiring only git and java installed. Runnable tutorials in the source. The API Reference, including many example Scalding snippets: Type-safe API Reference Fields-based API Reference The Matrix Library provides a way of working with key-attribute-value scalding pipes: The Introduction to Matrix Library contains an overview and a ""getting started"" example The Matrix API Reference contains the Matrix Library API reference with examples Introduction to Scalding Execution contains general rules and examples of calling Scalding from inside another application. Please feel free to use the beautiful Scalding logo artwork anywhere. Code of Conduct This, and all github.com/twitter projects, are under the Twitter Open Source Code of Conduct. Additionally, see the Typelevel Code of Conduct for specific examples of harassing behavior that are not tolerated. Building There is a script (called sbt) in the root that loads the correct sbt version to build: ./sbt update (takes 2 minutes or more) ./sbt test ./sbt assembly (needed to make the jar used by the scald.rb script) The test suite takes a while to run. When you're in sbt, here's a shortcut to run just one test: > test-only com.twitter.scalding.FileSourceTest Please refer to FAQ page if you encounter problems when using sbt. We use Travis CI to verify the build: We use Coveralls for code coverage results: Scalding modules are available from maven central. The current groupid and version for all modules is, respectively, ""com.twitter"" and 0.16.0-RC1. Current published artifacts are scalding-core_2.10 scalding-args_2.10 scalding-date_2.10 scalding-commons_2.10 scalding-avro_2.10 scalding-parquet_2.10 scalding-repl_2.10 The suffix denotes the scala version. Adopters Ebay Etsy Sharethrough Snowplow Analytics Soundcloud Twitter To see a full list of users or to add yourself, see the wiki Contact For user questions or scalding development (internals, extending, release planning): https://groups.google.com/forum/#!forum/scalding-dev (Google search also works as a first step) In the remote possibility that there exist bugs in this code, please report them to: https://github.com/twitter/scalding/issues Follow @Scalding on Twitter for updates. Chat: Authors: Avi Bryant http://twitter.com/avibryant Oscar Boykin http://twitter.com/posco Argyris Zymnis http://twitter.com/argyris Thanks for assistance and contributions: Sam Ritchie http://twitter.com/sritchie Aaron Siegel: http://twitter.com/asiegel Ian O'Connell http://twitter.com/0x138 Alex Levenson http://twitter.com/THISWILLWORK Jonathan Coveney http://twitter.com/jco Kevin Lin http://twitter.com/reconditesea Brad Greenlee: http://twitter.com/bgreenlee Edwin Chen http://twitter.com/edchedch Arkajit Dey: http://twitter.com/arkajit Krishnan Raman: http://twitter.com/dxbydt_jasq Flavian Vasile http://twitter.com/flavianv Chris Wensel http://twitter.com/cwensel Ning Liang http://twitter.com/ningliang Dmitriy Ryaboy http://twitter.com/squarecog Dong Wang http://twitter.com/dongwang218 Josh Attenberg http://twitter.com/jattenberg Juliet Hougland https://twitter.com/j_houg Eddie Xie https://twitter.com/eddiex A full list of contributors can be found on GitHub. License Copyright 2013 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/scalding"	"A Scala binding for the Cascading abstraction of Hadoop MapReduce."	"true"
"Big Data"	"Scoobi ★ 480 ⧗ 12"	"https://github.com/nicta/scoobi"	"Write type-safe Hadoop programs in idiomatic Scala way"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"484"	"52"	"97"	"GitHub - NICTA/scoobi: A Scala productivity framework for Hadoop. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 484 Fork 97 NICTA/scoobi Code Issues 19 Pull requests 2 Wiki Pulse Graphs A Scala productivity framework for Hadoop. http://nicta.github.com/scoobi/ 1,915 commits 13 branches 35 releases Fetching contributors Scala 97.4% Shell 1.8% Other 0.8% Scala Shell Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.6.2 cdh4.3.0-trial checkpoint-expiry gh-pages kiama-1.6.0 master new-mscr-algo new-new-mscr-algo-on-sbt-0.13 new-new-mscr-algo-refactoring-before-merge scoobi-on-scala-2.11 topic/persist-unused-type-param topic/thrift-serialiser-thread-safe topic/thrift-wire-format Nothing to show v0.7.0 suaothuaoe SCOOBI-0.9.1 SCOOBI-0.9.1-SNAPSHOT SCOOBI-0.8.5 SCOOBI-0.8.4 SCOOBI-0.8.3 SCOOBI-0.8.2 SCOOBI-0.8.1 SCOOBI-0.8.0 SCOOBI-0.7.3 SCOOBI-0.7.2 SCOOBI-0.7.1 SCOOBI-0.7.0 SCOOBI-0.7.0-cdh4 SCOOBI-0.7.0-RELEASE-TRIAL-cdh4 SCOOBI-0.7.0-RC2-cdh4 SCOOBI-0.7.0-RC1-cdh4 SCOOBI-0.6.2-cdh4 SCOOBI-0.6.1-cdh4 SCOOBI-0.6.1-cdh3 SCOOBI-0.6.0 SCOOBI-0.6.0-cdh4 SCOOBI-0.6.0-cdh4-RC1 SCOOBI-0.6.0-cdh3 SCOOBI-0.6.0-cdh3-RC1 SCOOBI-0.5.0-cdh4 SCOOBI-0.5.0-cdh3 SCOOBI-0.4.0 SCOOBI-0.4.0-cdh4 0.6.0-RC2 0.3.1 0.3.0 0.2.0 0.1.0 Nothing to show New pull request Latest commit 01d2701 Oct 13, 2015 etorreborre Merge pull request #361 from olorin/topic/update-sbt … Update sbt script Permalink Failed to load latest commit information. bin made the ci-repl-snapshot script executable Jul 14, 2015 examples Fixed the WireFormat bug (getSimpleName->getName), got AvroExample to… Sep 19, 2014 notes added notes for 0.9.1 Nov 26, 2014 project Update build.scala Feb 26, 2015 src Merge pull request #360 from NICTA/topic/fix-counter-issue Oct 12, 2015 .gitignore added *.iml to the gitignore file Jul 26, 2013 .travis.yml updated the travis script to cache sbt artefacts Oct 8, 2015 NOTICE.txt adding correct years to the notice Feb 20, 2014 README.md updated the README file Jul 14, 2015 pom.xml re-introduced the pom file with an inside comment explaining that it … May 26, 2013 sbt Update sbt script Oct 13, 2015 version.sbt Setting version to 0.9.2 Jul 14, 2015 README.md Welcome! Hadoop MapReduce is awesome, but it seems a little bit crazy when you have to write this to count words. Wouldn't it be nicer if you could simply write what you want to do: import Scoobi._, Reduction._  val lines = fromTextFile(""hdfs://in/..."")  val counts = lines.mapFlatten(_.split("" ""))                .map(word => (word, 1))                .groupByKey                .combine(Sum.int)  counts.toTextFile(""hdfs://out/..."", overwrite=true).persist(ScoobiConfiguration())  This is what Scoobi is all about. Scoobi is a Scala library that focuses on making you more productive at building Hadoop applications. It stands on the functional programming shoulders of Scala and allows you to just write what you want rather than how to do it. Scoobi is a library that leverages the Scala programming language to provide a programmer friendly abstraction around Hadoop's MapReduce to facilitate rapid development of analytics and machine-learning algorithms. Install See the install instructions in the QuickStart section of the User Guide. Features Familiar APIs - the DList API is very similar to the standard Scala List API Strong typing - the APIs are strongly typed so as to catch more errors at compile time, a major improvement over standard Hadoop MapReduce where type-based run-time errors often occur Ability to parameterise with rich data types - unlike Hadoop MapReduce, which requires that you go off implementing a myriad of classes that implement the Writable interface, Scoobi allows DList objects to be parameterised by normal Scala types including value types (e.g. Int, String, Double), tuple types (with arbitrary nesting) as well as case classes Support for multiple types of I/O - currently built-in support for text, Sequence and Avro files with the ability to implement support for custom sources/sinks Optimization across library boundaries - the optimiser and execution engine will assemble Scoobi code spread across multiple software components so you still keep the benefits of modularity It's Scala - being a Scala library, Scoobi applications still have access to those precious Java libraries plus all the functional programming and concise syntax that makes developing Hadoop applications very productive Apache V2 licence - just like the rest of Hadoop Getting Started To get started, read the getting started steps and the section on distributed lists. The remaining sections in the User Guide provide further detail on various aspects of Scoobi's functionality. The user mailing list is at http://groups.google.com/group/scoobi-users. Please use it for questions and comments! Community Issues Change history Source code (github) API Documentation Examples User Guide for the SNAPSHOT version (latest api) Mailing Lists: scoobi-users, scoobi-dev Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nicta/scoobi"	"Write type-safe Hadoop programs in idiomatic Scala way"	"true"
"Big Data"	"Scoozie ★ 61 ⧗ 18"	"https://github.com/klout/scoozie"	"Scala DSL on top of Oozie XML"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"65"	"48"	"19"	"GitHub - klout/scoozie: Scala DSL on top of Oozie XML Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 48 Star 65 Fork 19 klout/scoozie Code Issues 5 Pull requests 0 Pulse Graphs Scala DSL on top of Oozie XML 27 commits 3 branches 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags drj-ooziereply dyr-fix-decision master Nothing to show Nothing to show New pull request Latest commit 05c23ef May 6, 2015 trevor.dsouza change fs.default.name to fs.defaultFs to work with cdh5 Permalink Failed to load latest commit information. project Upgrade cdh to 4.7.0 Nov 19, 2014 src change fs.default.name to fs.defaultFs to work with cdh5 May 6, 2015 .gitignore Upgrade cdh to 4.7.0 Nov 19, 2014 LICENSE copy code from old repo Jul 31, 2013 NOTICE copy code from old repo Jul 31, 2013 README.md upgrade cdh libraries to cdh5.3.3 and bump scoozie version to 0.5.6 May 5, 2015 build.sbt upgrade cdh libraries to cdh5.3.3 and bump scoozie version to 0.5.6 May 5, 2015 README.md scoozie Scala DSL on top of Oozie XML Why? Scoozie is designed to solve problems concerning developer productivity when creating and running oozie workflow jobs. Problems that Scoozie solves: Job hierarchy and re-use: No more copy/pasting XML to create a new job Type and syntax checking: No more errors from typos in XML Safety / Verification: Scala provides a type-safe environment. Various other verificaitons and sanity checks are made. For example, it is impossible to create a cyclic graph or provide a fork/join structure that is not supported by oozie. Developer Overhead: The Scoozie DSL code makes it easy to read and write workflows - no more messy XML. Multiple sources of truth: The Scoozie specification of a job will be the source of truth for any workflow Version: As of version 0.5.6, scoozie has been upgraded to cdh5.3.3. Please depend on an earlier scoozie version to support lower versions of CDH libraries. How it works: There are three main layers to Scoozie: Specification of job in DSL. This encapsulates the syntactic sugar that makes it easy to read and write Scoozie workflows. The general format for a Scoozie workflow is as follows: val job1 = JobType(params) dependsOn Start val job2 = JobType(params) dependsOn job2 val job3 = JobType(params) dependsOn job3, etc This is the code that a worflow developer will actually be writing. The intent is to make it easy to read and to see the dependency path between jobs. Scoozie will automatically figure out forks/joins, etc. Specific samples of this can be found in src/main/scala/samples.scala Logic / conversion from DSL spec to workflow graph. This is the real workhorse of Scoozie, as it converts the programmer-specified workflow into an intermediate graph of workflow nodes. Forks, joins, and decisions are figured out and made ready for the final step - conversion to XML. This step also includes verification on the result graph. Conversion from intermediate graph to XML. This step makes use of scalaxb, an XML data-binding tool for scala. In this step each node in the intermediate graph is converted to an oozie-specific type. Finally, these nodes are converted to XML. This XML can be run in oozie by a scala command, examples of which can be found in src/main/scala/runSamples.scala DSL Class Structure: Node: Contains work and a list of dependencies. A node is also a dependency. Dependency: The parent of everything that a workflow job could depend on. (nodes or decisions) Work: Any work that a workflow node performs. This could be End, Kill, Jobs, sub workflows, etc. Job: Extends Work. This is the parent of the typical oozie jobs. (HiveJob, MapReduceJob, DistCpJob, etc). Specific jobs are easy to extend and customize in order to fit the client's needs. In practice, each line of developer-specified Scoozie code will return a Node, which other Nodes will depend on (Decisions are a special case of this, which will simply return a Dependency). Tutorial: Hello World! The following workflow will create an Oozie workflow that will create a Hello World directory on HDFS. def HelloWorldWorkflow = {     val mkHelloWorld = FsJob(         name = ""make-hello-world"",         tasks = List(             MkDir(""${nameNode}/users/tmp/oozie-fun/hello-world_${wf:id()}"")         )     ) dependsOn Start     val done = End dependsOn mkHelloWorld     Workflow(""hello-world-wf"", done) } This is an example of the basic structure of every scoozie workflow. Likely every scoozie workflow you will write will begin with a job that depends on Start and ends with an ""End"" job. The dependency chain makes it possible to specify the entire workflow by only referencing the end node in the Workflow object creation. Getting more complicated - using forks & joins. Scoozie is smart enough to figure out fork and join structures by looking at the dependencies between workflow nodes. This reduces cognitive load as for each node the developer only needs to think about what that node depends on. For example, consider the following workflow: def SimpleForkJoin = {     val first = MapReduceJob(""first"") dependsOn Start     val secondA = MapReduceJob(""secondA"") dependsOn first     val secondB = MapReduceJob(""secondB"") dependsOn first     val third = MapReduceJob(""third"") dependsOn (secondA, secondB)     Workflow(""simple-fork-join"", third) } Scoozie will automatically create a workflow structured as first -> fork -> secondA/secondB -> join -> third In addition, Scoozie will alert you by throwing an error if the workflow you have specified is not allowed by Oozie (for example, two nodes from different threads being joined) Making things interesting - Decisions Often a developer needs to specify variable paths in a workflow that are run conditionally. This is allowed in Scoozie via Decision and OneOf. def DecisionExample = {     val first = MapReduceJob(""first"") dependsOn Start     val decision = Decision(         ""route1"" -> Predicates.BooleanProperty(""${doRoute1}"")     ) dependsOn first      val route1Start = MapReduceJob(""r1Start"") dependsOn (decision option ""route1"")     val route1End = MapReduceJob(""r1End"") dependsOn route1Start     val route2Start = MapReduceJob(""r2Start"") dependsOn (decision default)     val route2End = MapReduceJob(""r2End"") dependsOn route2Start     val last = MapReduceJob(""last"") dependsOn OneOf(route1End, route2End)     val done = End dependsOn last     Workflow(""simple-decision"", done) } In this example, route1 (route1Start -> route1End) will be run if ${doRoute1} is evaluated to true. Otherwise, route2 (route2Start -> route2End) will be run. Again, this reduces developer cognitive overhead as the developer only needs to consider the dependencies for one node at a time. Clearly ""last"" depends on only one of route1 and route2. More complex decision structures are supported by Oozie as well. Verification will be automatically performed on the decisions to make sure that the developer has included all routes and default routes for each decision. Pour some sugar on me - ""doIf"" and ""Optional"" A common design pattern for decisions is to have an ""optional"" route that may or may not be inserted into the workflow. Scoozie has provided some sugar for making this special case easy to think about and specify in code. def SugarOption = {     val first = MapReduceJob(""first"") dependsOn Start     val option = MapReduceJob(""option"") dependsOn first doIf ""doOption""     val second = MapReduceJob(""second"") dependsOn Optional(option)     val done = End dependsOn second     Workflow(""sugar-option-decision"", done) } This syntax makes the semantics behind the optional node more clear than thinking about a separate decision. In this example, ""option"" is run if the ""${doOption}"" argument is evaluated to true (scoozie will figure out the brackets), and ""second"" must come after ""first"" and optionally ""option"". Scalability, Modularity - Sub Workflows Scoozie allows the developer to control the granularity of his project by using and defining sub-workflows at will. Using the previous SugarOption workflow as an example, we can easily create a new, more complex workflow without worrying about its specifics. def SubWfExample = {     val begin = MapReduceJob(""begin"") dependsOn Start     val someWork = MapReduceJob(""someWork"") dependsOn begin     val subwf = SugarOption dependsOn someWork     val end = End dependsOn subwf     Workflow(""sub-wf-example"", end) } Misc. Cool Things Specifying custom error-to paths on nodes. Scoozie also makes it easy to provide customizations such as error-to paths on nodes (rather than erroring to ""kill"" or ""fail"") def CustomErrorTo = {     val first = MapReduceJob(""first"") dependsOn Start     val errorPath = MapReduceJob(""error"") dependsOn (first error)     val second = MapReduceJob(""second"") dependsOn first     val end = End dependsOn OneOf(second, errorPath) } This scoozie code will create a workflow containing three nodes. The ""first"" node will proceed to ""second"" in the optimal case, but will now proceed to ""errorPath"" in the case of an error. Additionally, the OneOf clause is used again here and has an intuitive meaning. Creating and Running Workflows in Scoozie Scoozie makes running workflows easy. Simply create an object that extends ScoozieApp, pass in your workflow and any parameters, and you've got a runable main class that will generate and run an oozie workflow. object RunExample extends ScoozieApp(SubWfExample) Scoozie uses sbt-assembly in order to create an executable jar. Simply type > assembly  from sbt and all of the necessary dependencies and code will be assembled into a single jar: ""scoozie-assembly-0.x.jar"" You can run your workflow by typing java -cp scoozie-assembly-0.x.jar RunExample -arg1=foo -arg2=bar ...  More examples of creating runnable scoozie objects can be found in runSamples.scala Additional Help More example scoozie code can be found in samples.scala. Available job definitions can be found in jobs.scala. These jobs can be easily extended to fit your specific use cases. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/klout/scoozie"	"Scala DSL on top of Oozie XML"	"true"
"Big Data"	"Scrunch"	"http://crunch.apache.org/scrunch.html"	"A Scala wrapper for which provides a framework for writing, testing, and running MapReduce pipelines."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Apache Crunch - Scrunch Apache » Crunch Apache Crunch Overview Getting Started User Guide Download API Documentation Development Source Code Mailing Lists Issue Tracking Wiki Project About Bylaws License Scrunch A Scala Wrapper for the Apache Crunch Java API Introduction¶ Scrunch is an experimental Scala wrapper for the Apache Crunch Java API, based on the same ideas as the Cascade project at Google, which created a Scala wrapper for FlumeJava. Why Scala?¶ In many ways, Scala is the perfect language for writing MapReduce pipelines. Scala supports a mixture of functional and object-oriented programming styles and has powerful type-inference capabilities, allowing us to create complex pipelines using very few keystrokes. Here is an implementation of the classic WordCount problem using the Scrunch API: import org.apache.crunch.io.{From => from} import org.apache.crunch.scrunch._ import org.apache.crunch.scrunch.Conversions_  # For implicit type conversions  class WordCountExample {   val pipeline = new Pipeline[WordCountExample]    def wordCount(fileName: String) = {     pipeline.read(from.textFile(fileName))       .flatMap(_.toLowerCase.split(""\\W+""))       .filter(!_.isEmpty())       .count   } }  The Scala compiler can infer the return type of the flatMap function as an Array[String], and the Scrunch wrapper code uses the type inference mechanism to figure out how to serialize the data between the Map and Reduce stages. Here's a slightly more complex example, in which we get the word counts for two different files and compute the deltas of how often different words occur, and then only returns the words where the first file had more occurrences then the second: class WordCountExample {   def wordGt(firstFile: String, secondFile: String) = {     wordCount(firstFile).cogroup(wordCount(secondFile))       .map((k, v) => (k, (v._1.sum - v._2.sum)))       .filter((k, v) => v > 0).map((k, v) => k)   } }  Materializing Job Outputs¶ The Scrunch API also incorporates the Java library's materialize functionality, which allows us to easily read the output of a MapReduce pipeline into the client: class WordCountExample {   def hasHamlet = wordGt(""shakespeare.txt"", ""maugham.txt"").materialize.exists(_ == ""hamlet"") }  Notes and Thanks¶ Scrunch emerged out of conversations with Dmitriy Ryaboy, Oscar Boykin, and Avi Bryant from Twitter. Many thanks to them for their feedback, guidance, and encouragement. We are also grateful to Matei Zaharia, whose Spark Project inspired much of the original Scrunch API implementation. Copyright © 2013 The Apache Software Foundation, licensed under the Apache License, Version 2.0. Apache Crunch, Apache Hadoop, Hadoop, Apache, and the Apache feather logo are trademarks of The Apache Software Foundation. Other names appearing on the site may be trademarks of their respective owners."	"null"	"null"	"A Scala wrapper for which provides a framework for writing, testing, and running MapReduce pipelines."	"true"
"Big Data"	"Apache Crunch"	"http://crunch.apache.org/index.html"	"A Scala wrapper for which provides a framework for writing, testing, and running MapReduce pipelines."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Apache Crunch - Apache Crunch Apache » Crunch Apache Crunch Overview Getting Started User Guide Download API Documentation Development Source Code Mailing Lists Issue Tracking Wiki Project About Bylaws License Apache Crunch Simple and Efficient MapReduce Pipelines The Apache Crunch Java library provides a framework for writing, testing, and running MapReduce pipelines. Its goal is to make pipelines that are composed of many user-defined functions simple to write, easy to test, and efficient to run. Running on top of Hadoop MapReduce and Apache Spark, the Apache Crunch™ library is a simple Java API for tasks like joining and data aggregation that are tedious to implement on plain MapReduce. The APIs are especially useful when processing data that does not fit naturally into relational model, such as time series, serialized object formats like protocol buffers or Avro records, and HBase rows and columns. For Scala users, there is the Scrunch API, which is built on top of the Java APIs and includes a REPL (read-eval-print loop) for creating MapReduce pipelines. Copyright © 2013 The Apache Software Foundation, licensed under the Apache License, Version 2.0. Apache Crunch, Apache Hadoop, Hadoop, Apache, and the Apache feather logo are trademarks of The Apache Software Foundation. Other names appearing on the site may be trademarks of their respective owners."	"null"	"null"	"A Scala wrapper for which provides a framework for writing, testing, and running MapReduce pipelines."	"true"
"Big Data"	"Shadoop ★ 8 ⧗ 219"	"https://github.com/adamretter/shadoop"	"A Scala DSL for Hadoop MapReduce."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"8"	"2"	"16"	"GitHub - adamretter/Shadoop: A wrapper for Hadoop in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 8 Fork 16 adamretter/Shadoop forked from bsdfish/ScalaHadoop Code Issues 5 Pull requests 0 Pulse Graphs A wrapper for Hadoop in Scala 114 commits 2 branches 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags configuration-modifier-inheritance master Nothing to show shadoop-1.0 Nothing to show New pull request Pull request Compare This branch is 112 commits ahead of bsdfish:master. Latest commit e23471e Sep 16, 2014 adamretter Merge pull request #11 from rwalpole/master … [feature] moved to Scala version 2.11.2 Permalink Failed to load latest commit information. src .gitignore .travis.yml LICENSE NOTICES README.md pom.xml [feature] moved to Scala version 2.11.2 Sep 16, 2014 README.md Shadoop A Hadoop DSL and lightweight wrapper for Scala This fork of ScalaHadop is mostly just cherry-picked commits from the forks by @hito-asa, @ivmaykov and @oscarrenalis, of the original work by @bsdfish. In addition there are a few extra features and a cleaned up Maven build. This code provides some syntactic sugar on top of Hadoop in order to make it more usable from Scala. Take a look at src/main/scala/net/renalias/scoop/examples/WordCount.scala for more details. License Apache License, Version 2.0 Usage Basic Usage A basic mapper looks like: val mapper = new Mapper[LongWritable, Text, Text, LongWritable] {     mapWith {         (k, v) =>             (v split "" |\t"").map(x => (new Text(x), new LongWritable(1L))).toList     } } a reducer looks like this: val reducer = new Reducer[Text, LongWritable, Text, LongWritable] {     reduceWith {         (k, v) =>             List((k, (0L /: v)((total, next) => total + next)))     } } and, the pipeline to bind them together may look like this: TextInput[LongWritable, Text](""/tmp/input.txt"") --> MapReduceTask(mapper, reducer, ""Word Count"")    --> TextOutput[Text, LongWritable](""/tmp/output"")   execute The key difference here between standard mappers and reducers is that the map and reduce parts are written as side-effect free functions that accept a key and a value, and return an iterable; code behind the scenes will take care of updating Hadoop's Context object. Some note still remains to be done to polish the current interface, to remove things like .toList from the mapper and the creation of Hadoop's specific Text and LongWritable objects. Note that implicit conversion is used to convert between LongWritable and longs, as well as Text and Strings. The types of the input and output parameters only need to be stated as the generic specializers of the class it extends. These mappers and reducers can be chained together with the --> operator: object WordCount extends ScalaHadoop {   def run(args: Array[String]) : Int = {     TextInput[LongWritable, Text](args(0)) -->     MapReduceTask(mapper, reducer, ""Main task"") -->     TextOutput[Text, LongWritable](args(1)) execute      0 //result code   } } Multiple map/reduce Multiple map/reduce runs may be chained together: object WordsWithSameCount extends ScalaHadoop {   def run(args: Array[String]) : Int = {     TextInput[LongWritable, Text](args(0)) -->     MapReduceTask(tokenizerMap1, sumReducer, ""Sum"") -->     MapReduceTask(flipKeyValueMap, wordListReducer, ""Reduce"") -->     TextOutput[LongWritable, Text](args(1)) execute      0 //result code   } } Contributors Alex Simma: Developer of original version of ScalaHadoop. https://github.com/bsdfish/ScalaHadoop ASAI Hitoshi: Cherry-picked - Code re-organisation and initial Maven build. https://github.com/hiti-asa/ScalaHadoop Ilya Maykov: Cherry-picked - Various fixes, and support for Multiple Input Paths. https://github.com/ivmaykov/ScalaHadoop Oscar Renalias: Cherry-picked - Scala Syntax improvements. https://github.com/oscarrenalias/ScalaHadoop Rob Walpole: Various bug fixes: https://github.com/rwalpole/ScalaHadoop Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/adamretter/shadoop"	"A Scala DSL for Hadoop MapReduce."	"true"
"Big Data"	"Spark"	"http://spark.apache.org/"	"Lightning fast cluster computing — up to 100x faster than Hadoop for iterative algorithms (memory caching) and up to 10x faster than Hadoop for single-pass MapReduce jobs. Compatible with YARN-enabled Hadoop clusters, can run on Mesos and in stand-alone mode as well."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Apache Spark™ - Lightning-Fast Cluster Computing        Lightning-fast cluster computing Toggle navigation Download Libraries SQL and DataFrames Spark Streaming MLlib (machine learning) GraphX (graph) Third-Party Packages Documentation Latest Release (Spark 1.6.2) Other Resources Examples Community Mailing Lists Events and Meetups Project History Powered By Project Committers Issue Tracker FAQ Apache Software Foundation Apache Homepage License Sponsorship Thanks Security Latest News Spark 1.6.2 released (Jun 25, 2016) Call for Presentations for Spark Summit EU is Open (Jun 16, 2016) Preview release of Spark 2.0 (May 26, 2016) Spark Summit (June 6, 2016, San Francisco) agenda posted (Apr 17, 2016) Archive Download Spark Built-in Libraries: SQL and DataFrames Spark Streaming MLlib (machine learning) GraphX (graph) Third-Party Packages Apache Spark™ is a fast and general engine for large-scale data processing. Speed Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk. Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing. Logistic regression in Hadoop and Spark Ease of Use Write applications quickly in Java, Scala, Python, R. Spark offers over 80 high-level operators that make it easy to build parallel apps. And you can use it interactively from the Scala, Python and R shells. text_file = spark.textFile(""hdfs://..."")   text_file.flatMap(lambda line: line.split())     .map(lambda word: (word, 1))     .reduceByKey(lambda a, b: a+b) Word count in Spark's Python API Generality Combine SQL, streaming, and complex analytics. Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application. Runs Everywhere Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3. You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. Access data in HDFS, Cassandra, HBase, Hive, Tachyon, and any Hadoop data source. Community Spark is used at a wide range of organizations to process large datasets. You can find example use cases at the Spark Summit conference, or on the Powered By page. There are many ways to reach the community: Use the mailing lists to ask questions. In-person events include numerous meetup groups and Spark Summit. We use JIRA for issue tracking. Contributors Apache Spark is built by a wide set of developers from over 200 companies. Since 2009, more than 1000 developers have contributed to Spark! The project's committers come from 19 organizations. If you'd like to participate in Spark, or contribute to the libraries on top of it, learn how to contribute. Getting Started Learning Spark is easy whether you come from a Java or Python background: Download the latest release — you can run Spark locally on your laptop. Read the quick start guide. Spark Summit 2014 contained free training videos and exercises. Learn how to deploy Spark on a cluster. Download Apache Spark Apache Spark, Spark, Apache, and the Spark logo are trademarks of The Apache Software Foundation."	"null"	"null"	"Lightning fast cluster computing — up to 100x faster than Hadoop for iterative algorithms (memory caching) and up to 10x faster than Hadoop for single-pass MapReduce jobs. Compatible with YARN-enabled Hadoop clusters, can run on Mesos and in stand-alone mode as well."	"true"
"Big Data"	"spark-deployer ★ 45 ⧗ 0"	"https://github.com/KKBOX/spark-deployer"	"A sbt plugin which helps deploying Apache Spark stand-alone cluster and submitting job on cloud system like AWS EC2."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"61"	"12"	"14"	"GitHub - KKBOX/spark-deployer: Deploy Spark cluster in an easy way. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 12 Star 61 Fork 14 KKBOX/spark-deployer Code Issues 1 Pull requests 0 Pulse Graphs Deploy Spark cluster in an easy way. 218 commits 4 branches 42 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags dev master spark-sql-hive-only spark-sql Nothing to show v2.8.2 v2.8.1 v2.8.0 v2.7.1 v2.7.0 v2.6.0 v2.5.0 v2.4.0 v2.3.0 v2.2.0 v2.1.0 v2.0.1 v2.0.0 v1.3.0 v1.2.0 v1.1.1 v1.1.0 v1.0.1 v1.0.0 v0.13.0 v0.12.0 v0.11.1 v0.11.0 v0.10.1 v0.10.0 v0.9.2 v0.9.1 v0.9.0 v0.8.0 v0.7.4 v0.7.3 v0.7.2 v0.7.1 v0.7.0 v0.6.1 v0.6.0 v0.5.2 v0.5.1 v0.5.0 v0.3.0 v0.2.0 v0.1.0 Nothing to show New pull request Latest commit 88ecb84 May 18, 2016 pishen bugfix for local runMain Permalink Failed to load latest commit information. cmd/src/main/scala/sparkdeployer refine the usage of ClusterConf for integration with different Machin… Mar 18, 2016 core/src/main/scala/sparkdeployer disable setting for thread-pool, reconstruct SparkDeployer for each T… May 5, 2016 plugin/src/main bugfix for local runMain May 18, 2016 project remove build.properties, terminate all instances that's not terminate… Nov 18, 2015 .gitignore add gitignore Apr 25, 2016 LICENSE init May 15, 2015 README.md bugfix for local runMain May 18, 2016 build.sbt bugfix for local runMain May 18, 2016 configurations.md make local mode an option May 10, 2016 README.md spark-deployer A Scala tool which helps deploying Apache Spark stand-alone cluster and submitting job. Currently supports Amazon EC2 and OpenStack with Spark 1.4.1+. There are two modes when using spark-deployer, SBT plugin and embedded mode. SBT plugin mode Set the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY if you are deploying Spark on AWS EC2 (or if you want to access AWS S3 from your sbt or Spark cluster). Prepare a project with structure like below: project-root ├── build.sbt ├── project │   └── plugins.sbt ├── spark-deployer.conf └── src     └── main         └── scala             └── mypackage                 └── Main.scala  Write one line in project/plugins.sbt: addSbtPlugin(""net.pishen"" % ""spark-deployer-sbt"" % ""2.8.2"")  Write your cluster configuration in spark-deployer.conf (See the examples for more details). If you want to use another configuration file name, please set the environment variable SPARK_DEPLOYER_CONF when starting sbt (e.g. $ SPARK_DEPLOYER_CONF=./my-spark-deployer.conf sbt). Write your Spark project's build.sbt (Here we give a simple example): lazy val root = (project in file("".""))   .settings(     name := ""my-project-name"",     version := ""0.1"",     scalaVersion := ""2.10.6"",     libraryDependencies ++= Seq(       ""org.apache.spark"" %% ""spark-core"" % ""1.6.0"" % ""provided""     )   )  Write your job's algorithm in src/main/scala/mypackage/Main.scala: package mypackage  import org.apache.spark._  object Main {   def main(args: Array[String]) {     //setup spark     val sc = new SparkContext(new SparkConf())     //your algorithm     val n = 10000000     val count = sc.parallelize(1 to n).map { i =>       val x = scala.math.random       val y = scala.math.random       if (x * x + y * y < 1) 1 else 0     }.reduce(_ + _)     println(""Pi is roughly "" + 4.0 * count / n)   } } Create the cluster by sbt ""sparkCreateCluster <number-of-workers>"". You can also execute sbt first and type sparkCreateCluster <number-of-workers> in the sbt console. You may first type spark and hit TAB to see all the available commands. Once the cluster is created, submit your job by sparkSubmitJob <arg0> <arg1> ... When your job is done, destroy your cluster by sparkDestroyCluster All available commands sparkCreateMaster creates only the master node. sparkAddWorkers <number-of-workers> supports dynamically add more workers to an existing cluster. sparkCreateCluster <number-of-workers> shortcut command for the above two commands. sparkRemoveWorkers <number-of-workers> supports dynamically remove workers to scale down the cluster. sparkDestroyCluster terminates all the nodes in the cluster. sparkRestartCluster restart the cluster with new settings from spark-env without recreating the machines. sparkShowMachines shows the machine addresses with commands to login master and execute spark-shell on it. sparkUploadFile <local-path> <remote-path> upload a file to master, for example, sparkUploadFile ./settings.conf ~/ will upload settings.conf to master's home folder. sparkUploadJar uploads the job's jar file to master node. sparkSubmitJob <arg0> <arg1> ... uploads and runs the job. sparkSubmitJobWithMain uploads and runs the job (with main class provided, e.g. sparkSubmitJobWithMain mypackage.Main <args>). sparkRemoveS3Dir <dir-name> remove the s3 directory with the _$folder$ folder file (e.g. sparkRemoveS3Dir s3://bucket_name/middle_folder/target_folder). sparkChangeConfig <config-key> see the configuration page for more details. sparkRunCommand <command> run a command on master, e.g. sparkRunCommand df -h will show the disk usage on master. sparkRunCommands <config-key> see the configuration page for more details. Execute Spark locally Besides running your job with sparkCreateCluster and sparkSubmitJob, you can also test your job locally (e.g. on your laptop) without creating any machine. To enable this feature, add the following line in your build.sbt (add it to the .settings(...) block in the example above) SparkDeployerPlugin.localModeSettings  Then, just type run or runMain in your sbt, spark-deployer will set the master url and app name for your SparkContext. You don't have to download the Spark tarball or assembly the project into jar file. Embedded mode If you don't want to use sbt, or if you would like to trigger the cluster creation from within your Scala application, you can include the library of spark-deployer directly: libraryDependencies += ""net.pishen"" %% ""spark-deployer-core"" % ""2.8.2""  Then, from your Scala code, you can do something like this: import sparkdeployer._  val sparkDeployer = SparkDeployer.fromFile(""path/to/spark-deployer.conf"")  val numOfWorkers = 2 val jobJar = new File(""path/to/job.jar"") val args = Seq(""arg0"", ""arg1"")  sparkDeployer.createCluster(numOfWorkers) sparkDeployer.submitJob(jobJar, args) sparkDeployer.destroyCluster() Environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY should also be set if you need it. You may prepare the job.jar by sbt-assembly from other sbt project with Spark. For other available functions, check SparkDeployer.scala in our source code. spark-deployer uses slf4j, remember to add your own backend to see the log. For example, to print the log on screen, add libraryDependencies += ""org.slf4j"" % ""slf4j-simple"" % ""1.7.14""  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/KKBOX/spark-deployer"	"A sbt plugin which helps deploying Apache Spark stand-alone cluster and submitting job on cloud system like AWS EC2."	"true"
"Big Data"	"Sparkta ★ 208 ⧗ 2"	"https://github.com/Stratio/sparkta"	"Real Time Aggregation based on Spark Streaming."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"234"	"107"	"76"	"GitHub - Stratio/Sparta: Real Time Aggregation based on Spark Streaming Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 107 Star 234 Fork 76 Stratio/Sparta Code Issues 303 Pull requests 5 Wiki Pulse Graphs Real Time Aggregation based on Spark Streaming http://www.stratio.com 3,408 commits 36 branches 42 releases 22 contributors Scala 38.9% JavaScript 31.6% Cucumber 12.0% CSS 7.0% HTML 5.2% Java 3.7% Other 1.6% Scala JavaScript Cucumber CSS HTML Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags branch-0.1 branch-0.4 branch-0.6 branch-0.7 branch-0.8 branch-0.9 branch/benchmark bug-fest bugfixing/ignore-test-sube demo/client1 demo/shopping-centers demo/spark-summit dev/front feature/client1 feature/client1.1 feature/cluster-execution feature/docs feature/edit-policy feature/ingestion-parser feature/new-doc feature/policy-refactor feature/refactor-unnecesary-parsers feature/spark-1.3 feature/spark-1.5.1 feature/test-coverage fix/elastic-search-clustername fix_zk_connectionString_entrypoint issue/#519-Solr-output master release/sparkta-0.5.0_spark-1.3 smola/scala-cross-build test-cdh version/cdh-5.4.2 version/spark-cdh-5.4.2 version/spark-cdh-5.6.0 versions/sparkta-with-spark-1.3 Nothing to show 0.9.5 0.9.5-RC1 0.9.4 0.9.4-RC1 0.9.3 0.9.3-RC1 0.9.2 0.9.2-RC1 0.9.1 0.9.1-RC1 0.9.0 0.9.0-RC7 0.9.0-RC6 0.9.0-RC5 0.9.0-RC4 0.9.0-RC3 0.9.0-RC2 0.9.0-RC1 0.8.1 0.8.0 0.8.0-RC3 0.8.0-RC2 0.8.0-RC1 0.7.0 0.7.0-RC2 0.7.0-RC1 0.6.2 0.6.2-RC4 0.6.2-RC3 0.6.2-RC2 0.6.1 0.6.1-RC2 0.6.0 0.6.0-spark-1.3-RC2 0.6.0-spark-1.3-RC1 0.6.0-RC2 0.6.0-RC1 0.5.1 0.5.1-spark-1.3 0.5.0 0.5.0-spark-1.3 0.4.0-RC1 Nothing to show New pull request Latest commit a58c9a4 Jul 12, 2016 witokondoria committed on GitHub removed deprecated doc module Permalink Failed to load latest commit information. Docker update sparta compose to point to docker hub May 3, 2016 benchmark [SPARTA-616]Update parent to 0.7.2 and add new license May 5, 2016 dist Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 doc [RELEASE] Prepare for next development iteration Mar 4, 2016 driver Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 examples Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 lib Added library as system lib Nov 30, 2015 plugins [SPARTA-657] [PLUGINS] Updated Spark and plugins to spark version 1.6… Jun 30, 2016 project SPARTA-623 Error trace is not giving enought information (#1599) Jun 1, 2016 sandbox Renamed from sparkta to sparta Mar 4, 2016 sdk SPARTA-638 updated policies when one fragment are modified (#1608) Jun 27, 2016 serving-api Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 serving-core Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 test-at SPARTA-655 Corrected bugs related to policy statuses and actors (#1614) Jun 30, 2016 testsAT Added new enviroment variable tu HDFS0 Jul 7, 2016 web Sparta 457 checkpoint hdfs (#1601) Jul 4, 2016 .gitignore Renamed from sparkta to sparta Mar 4, 2016 .gitmodules integration through mavenrepository dependency May 20, 2015 .jenkins.yml Added new enviroment variable tu HDFS0 Jul 7, 2016 CHANGELOG.md added coveralls badge (#1618) Jul 11, 2016 README.md added coveralls badge (#1618) Jul 11, 2016 pom.xml removed deprecated doc module Jul 12, 2016 run_before_push.sh Coverage plugin Jun 19, 2015 README.md About Stratio Sparta Since Aryabhatta invented zero, Mathematicians such as John von Neuman have been in pursuit of efficient counting and architects have constantly built systems that computes counts quicker. In this age of social media, where 100s of 1000s events take place every second, we were inspired by twitter's Rainbird project to develop a distributed aggregation engine with this high level features: Pure Spark No need of coding, only declarative aggregation workflows Data continuously streamed in & processed in near real-time Ready to use, plug&play Flexible workflows (input, output, parsers, etc...) High performance Scalable Business Activity Monitoring Visualization Strataconf London 2015 slideshare Introduction Social media and networking sites are part of the fabric of everyday life, changing the way the world shares and accesses information. The overwhelming amount of information gathered not only from messages, updates and images but also readings from sensors, GPS signals and many other sources was the origin of a (big) technological revolution. This vast amount of data allows us to learn from the users and explore our own world. We can follow in real-time the evolution of a topic, an event or even an incident just by exploring aggregated data. But beyond cool visualizations, there are some core services delivered in real-time, using aggregated data to answer common questions in the fastest way. These services are the heart of the business behind their nice logos. Site traffic, user engagement monitoring, service health, APIs, internal monitoring platforms, real-time dashboards… Aggregated data feeds directly to end users, publishers, and advertisers, among others. In Sparta we want to start delivering real-time services. Real-time monitoring could be really nice, but your company needs to work in the same way as digital companies: Rethinking existing processes to deliver them faster, better. Creating new opportunities for competitive advantages. Features Highly business-project oriented Multiple application Cubes Time-based Secondly, minutely, hourly, daily, monthly, yearly... Hierarchical GeoRange: Areas with different sizes (rectangles) Flexible definition of aggregation policies (json, web app) Operators: Max, min, count, sum, range Average, median Stdev, variance, count distinct Last value Full-text search Architecture Sparta overview Key technologies Spark Streaming & Spark SparkSQL Akka MongoDB Apache Cassandra ElasticSearch Redis Apache Parquet HDFS Apache Kafka Apache Flume RabbitMQ Spray KiteSDK (morphlines) Inputs Twitter Kafka Flume RabbitMQ Socket Outputs MongoDB Cassandra ElasticSearch Redis Spark's DataFrames Outputs PrintOut CSV Parquet Build You can generate rpm and deb packages by running: mvn clean package -Ppackage Note: you need to have installed the following programs in order to build these packages: In a debian distribution: fakeroot dpkg-dev rpm In a centOS distribution: fakeroot dpkg-dev rpmdevtools Sandbox All about how to start running the sandbox Documentation Stratio Sparta Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Stratio/sparkta"	"Real Time Aggregation based on Spark Streaming."	"true"
"Big Data"	"Summingbird ★ 1697 ⧗ 0"	"https://github.com/twitter/summingbird"	"An implementation of the “lambda architecture” as a software abstraction — a single API for Hadoop and Storm."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1756"	"338"	"231"	"GitHub - twitter/summingbird: Streaming MapReduce with Scalding and Storm Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 338 Star 1,756 Fork 231 twitter/summingbird Code Issues 126 Pull requests 8 Wiki Pulse Graphs Streaming MapReduce with Scalding and Storm https://twitter.com/summingbird 1,730 commits 32 branches 38 releases 27 contributors Scala 96.9% Shell 2.5% Java 0.6% Scala Shell Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags add-composed-keyed-fns aggregator alexlevenson/merge-conflict dcl/clientproxy develop feature/add_left_join_store feature/akka_planner2 feature/downgrade_scalacheck feature/mutateFlow feature/parallel-tests feature/summingbird_stat fixSB gh-pages ianoc/updateSbt jco/test_scald090 jco/0.3.4rc2 kgonina/0.6.0rc1 kgonina/042_add_leftjoin_fix master memoryCounterBump new-travisci oscar/factor-dag-opt oscar/0.10.0-RC1 pg/memory_fix pg/options_break release/0.5.2rc1 release/0.5.2rc3 removeScala2.9.3 revert-587-removestripnames spout_counters store-join-1 try-travis-caching Nothing to show v0.11.0-RC1 v0.10.0-RC2 v0.10.0-RC1 0.9.1 0.9.0 0.8.0 0.7.0 0.6.0 0.5.2rc6 0.5.2rc5 0.5.2rc4 0.5.2rc3 0.5.1 0.4.2 0.4.1 0.4.0 0.3.3 0.3.2 0.3.1 0.3.0 0.2.5 0.2.4 0.2.3 0.2.2 0.2.1 0.2.0 0.1.5 0.1.4 0.1.3 0.1.2 0.1.1 0.1.0 0.0.7 0.0.6 0.0.4 0.0.3 0.0.2 0.0.1 Nothing to show New pull request Latest commit 424f2b6 Jul 15, 2016 jnievelt committed on GitHub Merge pull request #668 from twitter/jnievelt/externalize-storm-platf… … …orm-functions  Externalizing optionMap collapse and metrics registration for StormPlatform Permalink Failed to load latest commit information. logo Add official summingbird logo May 20, 2014 project Update the build to .sbt style Feb 1, 2016 scripts added run_tests.sh Feb 17, 2015 summingbird-batch-hadoop/src Fix FileSystem.get issue Jan 29, 2016 summingbird-batch/src Merge issue Oct 20, 2015 summingbird-builder/src Mixup test for Config access instead of hadoop config Oct 20, 2015 summingbird-chill/src Do not use set references to more closely match our other Kryo usages Oct 19, 2015 summingbird-client/src Incorporate latest storehaus release. Jun 14, 2016 summingbird-core-test/src Add another dependantsAfterMerge test Dec 11, 2015 summingbird-core/src Remove unused imports. Jun 21, 2016 summingbird-example/src Incorporate latest storehaus release. Jun 14, 2016 summingbird-online/src Remove space in import, running tests adds them back. Jul 14, 2016 summingbird-scalding-test/src Merge branch 'Lookup' of github.com:adamkozuch/summingbird into adamk… Jan 29, 2016 summingbird-scalding/src/main/scala/com/twitter/summingbird/scalding Fix FileSystem.get issue Jan 30, 2016 summingbird-storm-test/src Fix storm tests Aug 13, 2015 summingbird-storm/src/main Externalizing OptionMap collapse and metrics registration for StormPl… Jul 15, 2016 .gitignore Update the build to .sbt style Feb 2, 2016 .travis.yml Merge branch 'develop' of github.com:twitter/summingbird into ianoc/m… Jul 30, 2015 CHANGES.md Updates for the 0.9.1 release Nov 16, 2015 CONTRIBUTING.md link. Aug 26, 2013 LICENSE Project template. Oct 23, 2012 NOTICE Project template. Oct 23, 2012 README.md Updates for the 0.9.1 release Nov 16, 2015 build.sbt Update versions of finagle and util. Storehaus depends on them. Jun 14, 2016 sbt Update the build to .sbt style Feb 2, 2016 version.sbt Setting version to 0.11.0-SNAPSHOT Jun 23, 2016 README.md Summingbird Summingbird is a library that lets you write MapReduce programs that look like native Scala or Java collection transformations and execute them on a number of well-known distributed MapReduce platforms, including Storm and Scalding. While a word-counting aggregation in pure Scala might look like this:   def wordCount(source: Iterable[String], store: MutableMap[String, Long]) =     source.flatMap { sentence =>       toWords(sentence).map(_ -> 1L)     }.foreach { case (k, v) => store.update(k, store.get(k) + v) } Counting words in Summingbird looks like this:   def wordCount[P <: Platform[P]]     (source: Producer[P, String], store: P#Store[String, Long]) =       source.flatMap { sentence =>         toWords(sentence).map(_ -> 1L)       }.sumByKey(store) The logic is exactly the same, and the code is almost the same. The main difference is that you can execute the Summingbird program in ""batch mode"" (using Scalding), in ""realtime mode"" (using Storm), or on both Scalding and Storm in a hybrid batch/realtime mode that offers your application very attractive fault-tolerance properties. Summingbird provides you with the primitives you need to build rock solid production systems. Getting Started: Word Count with Twitter The summingbird-example project allows you to run the wordcount program above on a sample of Twitter data using a local Storm topology and memcache instance. You can find the actual job definition in ExampleJob.scala. First, make sure you have memcached installed locally. If not, if you're on OS X, you can get it by installing Homebrew and running this command in a shell: brew install memcached When this is finished, run the memcached command in a separate terminal. Now you'll need to set up access to the Twitter Streaming API. This blog post has a great walkthrough, so open that page, head over to https://dev.twitter.com/ and get your various keys and tokens. Once you have these, clone the Summingbird repository: git clone https://github.com/twitter/summingbird.git cd summingbird And open StormRunner.scala in your editor. Replace the dummy variables under config variable with your auth tokens: lazy val config = new ConfigurationBuilder()     .setOAuthConsumerKey(""mykey"")     .setOAuthConsumerSecret(""mysecret"")     .setOAuthAccessToken(""token"")     .setOAuthAccessTokenSecret(""tokensecret"")     .setJSONStoreEnabled(true) // required for JSON serialization     .build You're all ready to go! Now it's time to unleash Storm on your Twitter stream. Make sure the memcached terminal is still open, then start Storm from the summingbird directory: ./sbt ""summingbird-example/run --local"" Storm should puke out a bunch of output, then stabilize and hang. This means that Storm is updating your local memcache instance with counts of every word that it sees in each tweet. To query the aggregate results in Memcached, you'll need to open an SBT repl in a new terminal: ./sbt summingbird-example/console At the launched repl, run the following: scala> import com.twitter.summingbird.example._ import com.twitter.summingbird.example._  scala> StormRunner.lookup(""i"") <memcache store loading elided> res0: Option[Long] = Some(5)  scala> StormRunner.lookup(""i"") res1: Option[Long] = Some(52) Boom. Counts for the word ""i"" are growing in realtime. See the wiki page for a more detailed explanation of the configuration required to get this job up and running and some ideas for where to go next. Community and Documentation This, and all github.com/twitter projects, are under the Twitter Open Source Code of Conduct. Additionally, see the Typelevel Code of Conduct for specific examples of harassing behavior that are not tolerated. To learn more and find links to tutorials and information around the web, check out the Summingbird Wiki. The latest ScalaDocs are hosted on Summingbird's Github Project Page. Discussion occurs primarily on the Summingbird mailing list. Issues should be reported on the GitHub issue tracker. Simpler issues appropriate for first-time contributors looking to help out are tagged ""newbie"". IRC: freenode channel #summingbird Follow @summingbird on Twitter for updates. Please feel free to use the beautiful Summingbird logo artwork anywhere. Maven Summingbird modules are published on maven central. The current groupid and version for all modules is, respectively, ""com.twitter"" and 0.9.1. Current published artifacts are summingbird-core_2.11 summingbird-core_2.10 summingbird-batch_2.11 summingbird-batch_2.10 summingbird-client_2.11 summingbird-client_2.10 summingbird-storm_2.11 summingbird-storm_2.10 summingbird-scalding_2.11 summingbird-scalding_2.10 summingbird-builder_2.11 summingbird-builder_2.10 The suffix denotes the scala version. Authors (alphabetically) Oscar Boykin https://twitter.com/posco Ian O'Connell https://twitter.com/0x138 Sam Ritchie https://twitter.com/sritchie Ashutosh Singhal https://twitter.com/daashu License Copyright 2013 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/summingbird"	"An implementation of the “lambda architecture” as a software abstraction — a single API for Hadoop and Storm."	"true"
"Image processing and image analysis"	"scalismo ★ 25 ⧗ 12"	"https://github.com/unibas-gravis/scalismo"	"Shape modelling and model-based image analysis."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"30"	"12"	"7"	"GitHub - unibas-gravis/scalismo: Scalable Image Analysis and Shape Modelling Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 12 Star 30 Fork 7 unibas-gravis/scalismo Code Issues 8 Pull requests 0 Wiki Pulse Graphs Scalable Image Analysis and Shape Modelling 1,415 commits 13 branches 9 releases Fetching contributors Scala 99.4% Java 0.6% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags breeze0.12 bugfix-in-release-0.13-triangulation-access develop fix-mapped-property-ame gh-pages linemesh master polyline procrustesCenter release-0.12 release-0.13 rotationCenterInASM vantage-point-tree Nothing to show v0.12.0 v0.11.0 v0.10.0 v0.9.0 v0.8.0 v0.8.0-RC2 v0.8.0-RC1 v0.7.1 v0.7.0 Nothing to show New pull request Latest commit 4b2430b Jun 28, 2016 marcelluethi Merge branch 'release-0.12' … Conflicts: 	project/Build.scala Permalink Failed to load latest commit information. project version 0.12.0 Jun 28, 2016 src Merge branch 'release-0.12' Jun 28, 2016 .gitattributes added .gitattributes Feb 13, 2015 .gitignore updated .gitignore to exclude idea files Mar 21, 2014 .travis.yml travis: split build process into two sbt calls (update/test) Apr 17, 2015 LICENSE added License and headers Feb 11, 2015 README.md clearer Readme page Feb 5, 2016 scalariform.sbt added scalarifom plugin Feb 11, 2015 scalastyle-config.xml better formatting for plugins declaration Feb 13, 2015 README.md Scalismo - Scalable Image Analysis and Shape Modelling Scalismo is a library for statistical shape modeling and model-based image analysis in Scala. It has its origin in the research done at the Graphics and Vision Research Group at the University of Basel. The vision of the project is to provide an environment for modelling and image analysis which makes it easy and fun to try out ideas and build research prototypes is powerful enough to build full-scale industrial applications makes it feasible to deploy it in complex, distributed imaging pipelines. We aim to achieve these properties by leveraging two core technologies: A simple but versatile mathematical approach to shape modeling and registration, based on the theory of Gaussian processes. The Scala and Java ecosystem for reducing the software complexity. Documentation Quickstart Setup a project using Scalismo Tutorial (Graphical tool: Scalismo Lab) API Doc There is also a scalismo google group for general questions and discussions. Getting involved While scalismo is already fully usable for shape modeling and simple image processing task, its functionality is currently targeted to support the needs that arise in the research at the Gravis and Vision research group. In particular, many standard image and mesh processing algorithms are missing. If you find scalismo useful for your work, you can help us to make it more complete by implementing missing features, in particular filters for image and mesh processing, and support for more image formats. We are also always grateful if you report bugs or if give us feedback on how you use scalismo in your work and how you think we can improve it. Maintainers The project is currently developed and maintained by the Graphics and Vision Research group, University of Basel. The current maintainers of the project (people who can merge pull requests) are: Ghazi Bouabene Thomas Gerig Christoph Langguth Marcel Luethi Related Projects Scalismo is closely related to the statismo project, and some of the scalismo developers are also actively working on statismo. In fact, scalismo had been started as an attempt to provide the core functionality of Statismo and the ITK registration toolkit, but without the complexity that is induced by these toolkits. The design of the registration approach used in scalismo is strongly influenced by ITK and Elastix. Copyright and License All code is available to you under the Apache license, version 2, available at http://www.apache.org/licenses/LICENSE-2.0. Copyright, University of Basel, 2015. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/unibas-gravis/scalismo"	"Shape modelling and model-based image analysis."	"true"
"Image processing and image analysis"	"scrimage ★ 335 ⧗ 8"	"https://github.com/sksamuel/scrimage"	"Image io, resize, manipulation and thumbnails."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"381"	"42"	"62"	"GitHub - sksamuel/scrimage: Scala image processing library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 42 Star 381 Fork 62 sksamuel/scrimage Code Issues 3 Pull requests 0 Pulse Graphs Scala image processing library 808 commits 6 branches 11 releases 19 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master metadata parimage release/2.0.0 stable v2.0 Nothing to show v2.1.6 v2.1.5 v2.1.4 v2.1.3 v2.1.2 2.1.1 2.1.0 2.0.2 2.0.1 2.0.0 2.0.0-M1 Nothing to show New pull request Latest commit c754c82 Jul 4, 2016 sksamuel Fixed failing tests Permalink Failed to load latest commit information. examples Updated composite examples Jan 5, 2014 guide Update autocrop.md May 1, 2015 project Bumped some versions Jul 4, 2016 scrimage-core Fixed failing tests Jul 4, 2016 scrimage-filters Added basic par image Jun 21, 2015 scrimage-io-extra/src/main/scala/com/sksamuel/scrimage/nio Renamed io module to io-extra Jul 5, 2015 .coveralls.yml Added coveralls Jul 5, 2013 .gitignore updated gitignore May 23, 2014 .jvmopts Added .jvmopts for travis Mar 31, 2014 .sbtopts added sbt opts Feb 21, 2016 .travis.yml Fixed failing tests Jul 4, 2016 LICENSE Updated license file Aug 5, 2013 README.md Update README.md Mar 7, 2016 scalastyle-config.xml Added scalastyle plugin Jan 19, 2014 version.sbt Set snapshot. Added font bounds and new contexts. Jun 27, 2016 README.md Scrimage This readme is for the 2.0.x versions. For 1.4.x please see the old README1.4.md. Scrimage is a consistent, idiomatic, and immutable scala library for manipulating and processing of images. The aim of the this library is to provide a quick and easy way to do the kinds of image operations that are most common, such as scaling, rotating, converting between formats and applying filters. It is not intended to provide functionality that might be required by a more ""serious"" image processing application - such as face recognition or movement tracking. A typical use case for this library would be creating thumbnails of images uploaded by users in a web app, or resizing a set of images to have a consistent size, or optimizing PNG uploads by users to apply maximum compression, or applying a grayscale filter in a print application. Scrimage mostly builds on the functionality provided by java.awt.* along with selected other third party libraries. Latest Release: 2.1.5 Image Operations These operations all operate on an existing image, returning a copy of that image. The more complicated operations have a link to more detailed documentation. Operation Description autocrop Removes any ""excess"" background, returning just the image proper blank Creates a new image but without initializing the data buffer to any specific values. bound Ensures that the image is no larger than specified dimensions. If the original is bigger, it will be scaled down, otherwise the original is returned. This is useful when you want to ensure images do need exceed a certain size but you don't want to scale up if smaller. copy Creates a new clone of this image with a new pixel buffer. Any operations on the copy do not write back to the original. cover Resizes the canvas to the given dimensions and scales the original image so that it is the minimum size needed to cover the new dimensions without leaving any background visible. This operation is useful if you want to generate an avatar/thumbnail style image from a larger image where having no background is more important than cropping part of the image. Think a facebook style profile thumbnail. fill Creates a new image and initializes the data buffer to the given color. filter Returns a new image with the given filter applied. See the filters section for examples of the filters available. Filters can be chained and are applied in sequence. fit Resizes the canvas to the given dimensions and scales the original image so that it is the maximum possible size inside the canvas while maintaining aspect ratio. This operation is useful if you want a group of images to all have the same canvas dimensions while maintaining the original aspect ratios. Think thumbnails on a site like amazon where they are padded with white background. flip Flips the image either horizontally or vertically. max Returns an image that is as large as possible to fit into the specified dimensions whilst maintaining aspect ratio and without adding padding. Note: The difference between this and fit, is that fit will pad out the canvas to the specified dimensions, whereas max will not overlay Returns a new image which is the original image plus a specified one overlaid on top pad Resizes the canvas by adding a number of pixels around the edges in a given color. resize Resizes the canvas to the given dimensions. This does not scale the image but simply changes the dimensions of the canvas on which the image is sitting. Specifying a larger size will pad the image with a background color and specifying a smaller size will crop the image. This is the operation most people want when they think of crop. rotate Rotates the image clockwise or anti-clockwise. scale Scales the image to given dimensions. This operation will change both the canvas and the image. This is what most people think of when they want a ""resize"" operation. translate Returns a new image with the original image translated (moved) the specified number of pixels trim Removes a specified amount of pixels from each edge, essentially sugar to a crop operation. underlay Returns a new image which is the original image overload on top of the specified image Quick Examples Reading an image, scaling it to 50% using the Bicubic method, and writing out as PNG val in = ... // input stream val out = ... // output stream Image.fromStream(in).scale(0.5, Bicubic).output(out) // an implicit PNG writer is in scope by default Reading an image from a java File, applying a blur filter, then flipping it on the horizontal axis, then writing out as a Jpeg val inFile = ... // input File val outFile = ... // output File Image.fromFile(inFile).filter(BlurFilter).flipX.output(outFile)(JpegWriter()) // specified Jpeg Padding an image with a 20 pixel border around the edges in red val in = ... // input stream val out = ... // output stream Image.fromStream(in).pad(20, Color.Red) Enlarging the canvas of an image without scaling the image. Note: the resize methods change the canvas size, and the scale methods are used to scale/resize the actual image. This terminology is consistent with Photoshop. val in = ... // input stream val out = ... // output stream Image.fromStream(in).resize(600,400) Scaling an image to a specific size using a fast non-smoothed scale val in = ... // input stream val out = ... // output stream Image.fromStream(in).scaleTo(300, 200, FastScale) Writing out a heavily compressed Jpeg thumbnail implicit val writer = JpegWriter().withCompression(50) val in = ... // input stream val out = ... // output stream Image.fromStream(in).fit(180,120).output(new File(""image.jpeg"")) Printing the sizes and ratio of the image val in = ... // input stream val out = ... // output stream val image = Image.fromStream(in) println(s""Width: ${image.width} Height: ${image.height} Ratio: ${image.ratio}"") Converting a byte array in JPEG to a byte array in PNG val in : Array[Byte] = ... // array of bytes in JPEG say val out = Image(in).write // default is PNG val out2 = Image(in).bytes) // an implicit PNG writer is in scope by default with max compression  Coverting an input stream to a PNG with no compression implicit val writer = PngWriter.NoCompression val in : InputStream = ... // some input stream val out = Image.fromStream(in).stream  Input / Output Scrimage supports loading and saving of images in the common web formats (currently png, jpeg, gif, tiff). In addition it extends javas image.io support by giving you an easy way to compress / optimize / interlace the images when saving. To load an image simply use the Image companion methods on an input stream, file, filepath (String) or a byte array. The format does not matter as the underlying reader will determine that. Eg, val in = ... // a handle to an input stream val image = Image.fromInputStream(in) To save a method, Scrimage requires an ImageWriter. You can use this implicitly or explicitly. A PngWriter is in scope by default. val image = ... // some image image.output(new File(""/home/sam/spaghetti.png"")) // use implicit writer image.output(new File(""/home/sam/spaghetti.png""))(writer) // use explicit writer To set your own implicit writer, just define it in scope and it will override the default: implicit val writer = PngWriter.NoCompression val image = ... // some image image.output(new File(""/home/sam/spaghetti.png"")) // use custom implicit writer instead of default If you want to override the configuration for a writer then you can do this when you create the writer. Eg: implicit val writer = JpegWriter().withCompression(50).withProgressive(true) val image = ... // some image image.output(new File(""/home/sam/compressed_spahgetti.png"")) Metadata Scrimage builds on the metadata-extractor project to provide the ability to read metadata. This can be done in two ways. Firstly, the metadata is attached to the image if it was available when you loaded the image from the Image.fromStream, Image.fromResource, or Image.fromFile methods. Then you can call image.metadata to get a handle to the metadata object. Secondly, the metadata can be loaded without an Image being needed, by using the methods on ImageMetadata. Once you have the metadata object, you can invoke directories or tags to see the information. Format Detection If you are interested in detecting the format of an image (which you don't need to do when simply loading an image, as Scrimage will figure it out for you) then you can use the FormatDetector. The detector recognises PNG, JPEG and GIF. FormatDetector.detect(bytes) // returns an Option[Format] with the detected format if any FormatDetector.detect(in) // same thing from an input stream IPhone Orientation Apple iPhone's have this annoying ""feature"" where an image taken when the phone is rotated is not saved as a rotated file. Instead the image is always saved as landscape with a flag set to whether it was portrait or not. Scrimage will detect this flag, if it is present on the file, and correct the orientation for you automatically. Most image readers do this, such as web browsers, but you might have noticed some things do not, such as intellij. Note: This will only work if you use Image.fromStream, Image.fromResource, or Image.fromFile, as otherwise the metadata will not be available. X11 Colors There is a full list of X11 defined colors in the X11Colorlist class. This can be imported import X11Colorlist._ and used when you want to programatically specify colours, and gives more options than the standard 20 or so that are built into java.awt.Colo. Migration from 1.4.x to 2.0.0 The major difference in 2.0.0 is the way the outputting works. See the earlier input/output section on how to update your code to use the new writers. Changelist: Changed output methods to use typeclass approach Removal of Mutableimage and replacement of AsyncImage with ParImage Introduction of ""Pixel"" abstraction for methods that operate directly on pixels Addition of metadata Addition of io packag Benchmarks Some noddy benchmarks comparing the speed of rescaling an image. I've compared the basic getScaledInstance method in java.awt.Image with ImgScalr and Scrimage. ImgScalr delegates to awt.Graphics2D for its rendering. Scrimage adapts the methods implemented by Morten Nobel. The code is inside src/test/scala/com/sksamuel/scrimage/ScalingBenchmark.scala. The results are for 100 runs of a resize to a fixed width / height. Library Fast High Quality (Method) java.awt.Image.getScaledInstance 11006ms 17134ms (Area Averaging) ImgScalr 57ms 5018ms (ImgScalr.Quality) Scrimage 113ms 2730ms (Bicubic) As you can see, ImgScalr is the fastest for a simple rescale, but Scrimage is much faster than the rest for a high quality scale. Including Scrimage in your project Scrimage is available on maven central. There are several dependencies. One is the scrimage-core library which is required. The others are scrimage-filters and scrimage-io-extra. They are split because the image filters is a large jar, and most people just want the basic resize/scale/load/save functionality. Thescrimage-io-extra package brings in readers/writers for less common formats such as BMP Tiff or PCX. Note: The canvas operations are now part of the core library since 2.0.1 Scrimage is cross compiled for scala 2.11 and 2.10. If using SBT then you want: libraryDependencies += ""com.sksamuel.scrimage"" %% ""scrimage-core"" % ""2.1.0""  libraryDependencies += ""com.sksamuel.scrimage"" %% ""scrimage-io-extra"" % ""2.1.0""  libraryDependencies += ""com.sksamuel.scrimage"" %% ""scrimage-filters"" % ""2.1.0"" Maven: <dependency>     <groupId>com.sksamuel.scrimage</groupId>     <artifactId>scrimage-core_2.11</artifactId>     <version>2.1.0</version> </dependency> <dependency>     <groupId>com.sksamuel.scrimage</groupId>     <artifactId>scrimage-io-extra_2.11</artifactId>     <version>2.1.0</version> </dependency> <dependency>     <groupId>com.sksamuel.scrimage</groupId>     <artifactId>scrimage-filters_2.11</artifactId>     <version>2.1.0</version> </dependency> If you're using maven you'll have to adjust the artifact id with the correct scala version. SBT will do this automatically when you use %% like in the example above. Filters Scrimage comes with a wide array (or Iterable ;) of filters. Most of these filters I have not written myself, but rather collected from other open source imaging libraries (for compliance with licenses and / or attribution - see file headers), and either re-written them in Scala, wrapped them in Scala, fixed bugs or improved them. Some filters have options which can be set when creating the filters. All filters are immutable. Most filters have sensible default options as default parameters. Click on the small images to see an enlarged example. Filter Example 1 Example 2 Example 3 blur border brightness bump chrome color_halftone contour contrast despeckle diffuse dither edge emboss errordiffusion gamma gaussian glow grayscale hsb invert lensblur lensflare minimum maximum motionblur noise offset oil pixelate pointillize_square posterize prewitt quantize rays ripple roberts rylanders sepia smear_circles snow sobels solarize sparkle summer swim television threshold tritone twirl unsharp vignette vintage Composites Scrimage comes with the usual composites built in. This grid shows the effect of compositing palm trees over a US mailbox. The first column is the composite with a value of 0.5f, and the second column with 1f. Note, if you reverse the order of the images then the effects would be reversed. The code required to perform a composite is simply. val composed = image1.composite(new XYZComposite(alpha), image2) Click on an example to see it full screen. Composite Alpha 0.5f Alpha 1f average blue color colorburn colordodge diff green grow hue hard heat lighten negation luminosity multiply negation normal overlay red reflect saturation screen subtract License This software is licensed under the Apache 2 license, quoted below.  Copyright 2013-2015 Stephen Samuel  Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sksamuel/scrimage"	"Image io, resize, manipulation and thumbnails."	"true"
"Sound processing and music"	"ScalaCollider ★ 95 ⧗ 3"	"https://github.com/Sciss/ScalaCollider"	"Sound synthesis and signal processing client for SuperCollider."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"100"	"13"	"8"	"GitHub - Sciss/ScalaCollider: Base platform for Scala based SuperCollider client Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 13 Star 100 Fork 8 Sciss/ScalaCollider Code Issues 10 Pull requests 0 Wiki Pulse Graphs Base platform for Scala based SuperCollider client http://www.sciss.de/scalaCollider 474 commits 3 branches 31 releases 1 contributor Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master work Nothing to show v1.18.0 v1.17.3 v1.17.2 v1.17.1 v1.16.0 v1.15.0 v1.14.0 v1.13.0 v1.12.0 v1.11.0 v1.10.0 v1.9.0 v1.8.0 v1.7.0 v1.6.0 v1.5.2 v1.5.0 v1.4.0 v1.3.1 v1.3.0 v1.2.0 v1.1.1 v1.1.0 v1.0.0 v0.34 v0.33 v0.32 v0.30 v0.24 v0.23 v0.21 Nothing to show New pull request Latest commit ef5a4b3 Mar 13, 2016 Sciss can't run sbt under jdk 8 with scala 2.10.0 -- remove that from travis Permalink Failed to load latest commit information. lib sbt'fied. REPL now via ""sbt demo"" May 5, 2010 project v1.18.1 - fixes #56 Mar 13, 2016 site site update Feb 2, 2016 src v1.18.1 - fixes #56 Mar 13, 2016 .gitignore ... Dec 27, 2014 .travis.yml can't run sbt under jdk 8 with scala 2.10.0 -- remove that from travis Mar 13, 2016 CHANGES.md missing changes Oct 1, 2014 CONTRIBUTING.md add contrib, update gitter Feb 2, 2016 ExampleCmd.sc add `Server.nrt` function (not yet tested). Fixes #48 Jan 9, 2015 LICENSE v1.18.1 - fixes #56 Mar 13, 2016 README.md v1.18.1 - fixes #56 Mar 13, 2016 build.sbt v1.18.1 - fixes #56 Mar 13, 2016 README.md ScalaCollider statement ScalaCollider is a SuperCollider client for the Scala programming language. It is (C)opyright 2008–2016 by Hanns Holger Rutz. All rights reserved. ScalaCollider is released under the GNU General Public License and comes with absolutely no warranties. To contact the author, send an email to contact at sciss.de SuperCollider is one of the most elaborate open source sound synthesis frameworks. It comes with its own language 'SCLang' that controls the sound synthesis processes on a server, 'scsynth'. ScalaCollider is an alternative to 'SCLang', giving you the (perhaps) familiar Scala language to express these sound synthesis processes, and letting you hook up any other Scala, Java or JVM-based libraries. ScalaCollider's function is more reduced than 'SCLang', focusing on UGen graphs and server-side resources such as buses and buffers. Other functionality is part of the standard Scala library, e.g. collections and GUI. Other functionality, such as plotting, MIDI, client-side sequencing (Pdefs, Routines, etc.) must be added through dedicated libraries (see section 'packages' below). While ScalaCollider itself is in the form of a library (although you can use it from the REPL with sbt console), you may want to have a look at the ScalaCollider-Swing project that adds an easy-to-use standalone application or mini-IDE. On the ScalaCollider-Swing page, you'll find a link to download a readily compiled binary for this standalone version. A still experimental system on top of ScalaCollider, providing higher level abstractions, is SoundProcesses and its graphical front-end Mellite. Please get in touch if you intend to use these, as the documentation is still sparse, and the system and API is still a moving target. download and resources The current version of ScalaCollider (the library) can be downloaded from github.com/Sciss/ScalaCollider. More information is available from the wiki at github.com/Sciss/ScalaCollider/wiki. The API documentation is available at sciss.github.io/ScalaCollider/latest/api. The best way to ask questions, no matter if newbie or expert, is to use the mailing list at groups.google.com/group/scalacollider. To subscribe, simply send a mail to ScalaCollider+subscribe@googlegroups.com (you will receive a mail asking for confirmation). The early architectural design of ScalaCollider is documented in the SuperCollider 2010 symposium proceedings: H.H.Rutz, Rethinking the SuperCollider Client.... However, many design decisions have been revised or refined in the meantime. The file ExampleCmd.sc is a good starting point for understanding how UGen graphs are written in ScalaCollider. You can directly copy and paste these examples into the ScalaCollider-Swing application's interpreter window. See the section 'starting a SuperCollider server' below, for another simple example of running a server (possibly from your own application code). building ScalaCollider currently builds with sbt 0.13 against Scala 2.11, 2.10. It requires SuperCollider 3.5 or higher. Note that the UGens are provided by the separate ScalaColliderUGens project. A simple Swing front end is provided by the ScalaColliderSwing project. Targets for sbt: clean – removes previous build artefacts compile – compiles classes into target/scala-version/classes doc – generates api in target/scala-version/api/index.html package – packages jar in target/scala-version console – opens a Scala REPL with ScalaCollider on the classpath Note: Due to SI-7436, the project must be currently compiled against Scala 2.10.0 and not 2.10.1 through 2.10.4. It can be used, however, with any Scala 2.10 version. Another implication is that you cannot build against 2.10.0 using JDK 8, because is sbt is broken here. To build for 2.10.0, you must use JDK 6 or JDK 7. linking To use this project as a library, use the following artifact: libraryDependencies += ""de.sciss"" %% ""scalacollider"" % v  The current version v is ""1.18.1"" contributing Please see the file CONTRIBUTING.md starting a SuperCollider server The following short example illustrates how a server can be launched and a synth played: import de.sciss.synth._ import ugen._ import Ops._  val cfg = Server.Config() cfg.program = ""/path/to/scsynth"" // runs a server and executes the function // when the server is booted, with the // server as its argument  Server.run(cfg) { s =>   // play is imported from package de.sciss.synth.   // it provides a convenience method for wrapping   // a synth graph function in an `Out` element   // and playing it back.   play {     val f = LFSaw.kr(0.4).madd(24, LFSaw.kr(Seq(8, 7.23)).madd(3, 80)).midicps     CombN.ar(SinOsc.ar(f) * 0.04, 0.2, 0.2, 4)   } }    Specifying SC_HOME Note: This section is mostly irrelevant on Linux, where scsynth is normally found on $PATH, and thus no further customisation is needed. You might omit to set the program of the server's configuration, as ScalaCollider will by default read the system property SC_HOME, and if that is not set, the environment variable SC_HOME. Environment variables are stored depending on your operating system. On OS X, if you use the app-bundle of ScalaCollider-Swing, you can access them from the terminal: $ mkdir ~/.MacOSX $ touch ~/.MacOSX/environment.plist $ open ~/.MacOSX/environment.plist  Here, open should launch the PropertyEditor. Otherwise you can edit this file using a text editor. The content will be like this: {   ""SC_HOME"" = ""/Applications/SuperCollider_3.6.5/SuperCollider.app/Contents/Resources/""; }  On the other hand, if you run ScalaCollider from a Bash terminal, you edit ~/.bash_profile instead. The entry is something like: export SC_HOME=/path/to/folder-of-scsynth  On linux, the environment variables probably go in ~/.profile. For more sound examples, see ExampleCmd.sc. There is also an introductory video for the Swing frontend at www.screencast.com/t/YjUwNDZjMT. packages ScalaCollider's core functionality may be extended by other libraries I or other people wrote. The following two libraries are dependencies and therefore always available in ScalaCollider: Audio file functionality is provided by the ScalaAudioFile library. Open Sound Control functionality is provided by the ScalaOSC library. Here are some examples for libraries not included: MIDI functionality is not included, but can be added with the ScalaMIDI library. Plotting is most easily achieved through Scala-Chart, which is conveniently included in ScalaCollider-Swing. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Sciss/ScalaCollider"	"Sound synthesis and signal processing client for SuperCollider."	"true"
"Functional Reactive Programming"	"Monix ★ 373 ⧗ 0"	"https://github.com/monifu/monix"	"Extensions to Scala’s standard library for multi-threading primitives and functional reactive programming. Scala.js compatible."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"458"	"38"	"35"	"GitHub - monixio/monix: Reactive Programming for Scala and Scala.js (former Monifu). Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 38 Star 458 Fork 35 monixio/monix Code Issues 9 Pull requests 2 Pulse Graphs Reactive Programming for Scala and Scala.js (former Monifu). https://monix.io 749 commits 14 branches 62 releases Fetching contributors Scala 95.2% Java 4.8% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.x 2.x cats-integration cats-stage-2 docs gh-pages issue-89-cats-integration issue-181 master protocol refactor-ops scala.2.12 subjectImplicitSchMissing task Nothing to show v2.0-RC8 v2.0-RC7 v2.0-RC6 v2.0-RC5 v2.0-RC4 v2.0-RC3 v2.0-RC2 v2.0-M2 v2.0-M1 v1.2 v1.1 v1.0 v1.0-RC4 v1.0-RC3 v1.0-RC1 v1.0-M11 v1.0-M10 v1.0-M9 v1.0-M8 v1.0-M7 v1.0-M6 v1.0-M5 v1.0-M4 v1.0-M3 v1.0-M2 v1.0-M1 v0.15.0 v0.14.1 v0.14.0 v0.14.0.RC2 v0.14.0.RC1 v0.14.0.M4 v0.14.0.M3 v0.14.0.M2 v0.13.0 v0.13.0-RC5 v0.13.0-RC4 v0.13.0-RC3 v0.13.0-RC2 v0.13.0-RC1 v0.13.0-M3 v0.13.0-M2 v0.13.0-M1 v0.12.2 v0.12.1 v0.12.0 v0.11.0 v0.10.1 v0.9.5 v0.9.4 v0.9.3 v0.9.2 v0.9.1 v0.9.0-alpha1 v0.8.0 v0.7.1 v0.7.0 v0.6.1 v0.6 v0.5 v0.4 v0.3 Nothing to show New pull request Latest commit 7e91779 Jul 1, 2016 alexandru Issue #193 - running instances in parallel Permalink Failed to load latest commit information. benchmarks/src/main/scala/monix Issues #160, #161 - fix buffer operators, subject builders Jun 15, 2016 monix-cats/shared/src Issue #193 - running instances in parallel Jul 1, 2016 monix-eval Fixes #189, fixes #190, fixes #191 - Task producing operators, Coeval… Jun 29, 2016 monix-execution Issue #85 - clarify the ScalaDoc of RefCountCancelable Jun 29, 2016 monix-reactive Release 2.0-RC8 Jun 29, 2016 monix-scalaz/series-7.2/shared/src Issue #193 - running instances in parallel Jul 1, 2016 monix-types/shared/src/main/scala/monix/types Fix #187, #177 - make interfaces serializable, integrate with Cats Mo… Jun 29, 2016 project Release 2.0-RC8 Jun 29, 2016 tckTests/src/test/scala/monix/tckTests Issue #184: Introduce the Consumer type, Observer.Sync and Subscriber… Jun 29, 2016 .gitignore Refactoring Scheduler Feb 16, 2015 .java-version Issue #111 - improving coverage for base Jan 20, 2016 .jvmopts Add jvmopts for SBT with bigger memory limits (file taken from Cats) Aug 31, 2015 .travis.yml Fix travis.yml May 25, 2016 AUTHORS Issue #91 - rename project, Monifu to Monix Dec 30, 2015 CHANGES.md Release 2.0-RC8 Jun 29, 2016 CONTRIBUTING.md Issue #91 - rename project, Monifu to Monix Dec 30, 2015 LICENSE.txt Issue #58 - Update copyright notice, again :-) Aug 20, 2015 README.md Release 2.0-RC8 Jun 29, 2016 build.sbt Issue #184: Introduce the Consumer type, Observer.Sync and Subscriber… Jun 29, 2016 rootdoc.txt Fix #175 - Reinclude all of project Sincron in Monix-execution Jun 21, 2016 version.sbt Release 2.0-RC8 Jun 29, 2016 README.md Monix Reactive Programming for Scala and Scala.js. NOTE: renamed from Monifu, see issue #91 for details. Overview Monix is a high-performance Scala / Scala.js library for composing asynchronous and event-based programs using observable sequences that are exposed as asynchronous streams, expanding on the observer pattern, strongly inspired by ReactiveX, but designed from the ground up for back-pressure and made to cleanly interact with Scala's standard library and compatible out-of-the-box with the Reactive Streams protocol. Highlights: exposes the kick-ass Observable, Task and Coeval modular, only use what you need the core has no third-party dependencies strives to be idiomatic Scala and encourages referential transparency, but is built to be faster than alternatives accepted in the Typelevel incubator designed for true asynchronicity, running on both the JVM and Scala.js, really good test coverage and API documentation as a project policy Usage See monix-sample for a project exemplifying Monix used both on the server and on the client. Dependencies The packages are published on Maven Central. Current stable release: 1.2 Current beta release: 2.0-RC8 For the stable release (use the %%% for Scala.js): libraryDependencies += ""org.monifu"" %% ""monifu"" % ""1.2"" For the beta/preview release (use at your own risk): libraryDependencies += ""io.monix"" %% ""monix"" % ""2.0-RC8"" Sub-projects Monix 2.0 is modular by design, so you can pick and choose: monix-types exposes type-classes and shims needed for integration with other FP libraries (Cats, Scalaz) monix-execution exposes the low-level execution environment, or more precisely Scheduler, Cancelable, Atomic and CancelableFuture monix-eval exposes Task, Coeval and depends on monix-execution and monix-types monix-reactive exposes Observable streams and depends on monix-eval and monix-types monix provides all of the above Optional packages: monix-cats provides integration with Cats and depends on monix-types and org.typelevel.cats-core monix-scalaz-72 provides integration with Scalaz and depends on monix-types and org.scalaz.scalaz-core version 7.2.x Documentation NOTE: The documentation is a work in progress. All documentation is hosted at, contributions are welcome: Monix.io API Documentation: 1.2 2.0-RC8 Presentations: Monix Task: Lazy, Async & Awesome, flatMap(Oslo), 2016 Akka & Monix: Controlling Power Plants, Typelevel Summit, Oslo, 2016 Maintainers The current maintainers (people who can help you) are: Alexandru Nedelcu (@alexandru) Andrei Oprisan (@aoprisan) Contributing The Monix project welcomes contributions from anybody wishing to participate. All code or documentation that is provided must be licensed with the same license that Monix is licensed with (Apache 2.0, see LICENSE.txt). People are expected to follow the Typelevel Code of Conduct when discussing Monix on the Github page, Gitter channel, or other venues. Feel free to open an issue if you notice a bug, have an idea for a feature, or have a question about the code. Pull requests are also gladly accepted. For more information, check out the contributor guide. License All code in this repository is licensed under the Apache License, Version 2.0. See LICENCE.txt. Acknowledgements YourKit supports the Monix project with its full-featured Java Profiler. YourKit, LLC is the creator YourKit Java Profiler and YourKit .NET Profiler, innovative and intelligent tools for profiling Java and .NET applications. Development of Monix has been initiated by Eloquentix engineers, with Monix being introduced at E.ON Connecting Energies, powering the next generation energy grid solutions. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/monifu/monix"	"Extensions to Scala’s standard library for multi-threading primitives and functional reactive programming. Scala.js compatible."	"true"
"Functional Reactive Programming"	"Reactive Collections ★ 1 ⧗ 81"	"https://github.com/storm-enroute/reactors"	"A library that incorporates event streams and signals with specialized collections called reactive containers, and expresses concurrency using isolates and channels."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2"	"3"	"7"	"GitHub - storm-enroute/reactors: A concurrent reactive programming framework. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 2 Fork 7 storm-enroute/reactors forked from reactors-io/reactors Code Pull requests 0 Pulse Graphs A concurrent reactive programming framework. http://reactors.io 1,180 commits 13 branches 0 releases Fetching contributors Scala 48.3% JavaScript 46.9% HTML 2.1% Java 2.0% CSS 0.7% Scala JavaScript HTML Java CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags dev master topic/channel-refactoring topic/exceptions topic/isolate-refactoring topic/isolate-system topic/lazy-rope topic/snap-queue-leaks topic/snapq-experimentation v0.2 v0.3 v0.4 v0.5 Nothing to show Nothing to show New pull request Pull request Compare This branch is even with reactors-io:master. Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. project Remove debugger from 2.10. Jul 14, 2016 reactors-common Eliminate deadlocks in repl and implement asynchronous repl updates. Jul 15, 2016 reactors-container/src Remove lint errors. May 4, 2016 reactors-core/src Embed REPL updates into `state` API calls. Jul 14, 2016 reactors-debugger/src/main Add ok status glow. Jul 15, 2016 reactors-extra/src Fix flaky test. May 15, 2016 reactors-protocols/src Add additional doc comment. Jul 7, 2016 reactors-remote/src Rename `find` to `get`. May 21, 2016 src/test Add `drop` to `japi`. Jun 29, 2016 tools Fix condition. Jul 9, 2016 .drone.sec Add drone build. Dec 9, 2015 .gitignore Minor cleanups. Jan 31, 2016 .gitlab-ci.yml Add gitlab runner. May 2, 2016 .travis.yml Add linter to travis. May 14, 2016 LICENSE Update copyright. Jan 16, 2016 README.md Fix Maven link. Jul 7, 2016 build.sbt Add missing quotes. Apr 15, 2016 cross.conf Update to latest mecha, set 2.11.4 as default. Dec 26, 2014 dependencies.conf Add the debugger project. Jul 8, 2016 reactress-title-96.png Renaming package. May 29, 2014 version.conf Bump up snapshot version. Jun 23, 2016 README.md CI service Status Description Travis Linux container tests Maven Artifact on Maven Reactors is a concurrent, distributed programming framework based on asynchronous event streams. documentation download page Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/storm-enroute/reactors"	"A library that incorporates event streams and signals with specialized collections called reactive containers, and expresses concurrency using isolates and channels."	"true"
"Functional Reactive Programming"	"RxScala ★ 447 ⧗ 7"	"https://github.com/ReactiveX/RxScala"	"Reactive Extensions for Scala – a library for composing asynchronous and event-based programs using observable sequences"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"505"	"52"	"85"	"GitHub - ReactiveX/RxScala: RxScala – Reactive Extensions for Scala – a library for composing asynchronous and event-based programs using observable sequences Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 505 Fork 85 ReactiveX/RxScala Code Issues 16 Pull requests 0 Pulse Graphs RxScala – Reactive Extensions for Scala – a library for composing asynchronous and event-based programs using observable sequences 796 commits 1 branch 11 releases 34 contributors Scala 99.8% Shell 0.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 0.x Switch branches/tags Branches Tags 0.x Nothing to show 0.26.2 0.26.1 0.26.0 0.25.1 0.25.0 0.24.1 0.24.0 0.23.1 0.23.0 0.22.0 0.21.1 Nothing to show New pull request Latest commit c17b434 Jul 14, 2016 jking-roar committed with zsxwing take with scheduler returns an Observable (#199) … fixes signature of method with erroneous Unit return type Permalink Failed to load latest commit information. examples/src Bump to RxJava 1.1.6 Jun 16, 2016 project Upgrade sbt to 0.13.8; move completeness package to scala-2.11; enabl… Sep 20, 2015 src take with scheduler returns an Observable (#199) Jul 14, 2016 .gitignore Copy .gitignore from RxJava Aug 21, 2014 .travis.yml Scala 2.12 support Apr 12, 2016 CHANGES.md Update the release date Jun 17, 2016 CONTRIBUTING.md Doc improvements Oct 22, 2014 LICENSE Upgrade to Nebula Build Aug 29, 2014 README.md 0.26.2 Release Note Jun 15, 2016 Rationale.md Doc improvements Oct 22, 2014 build.sbt Bump to RxJava 1.1.6 Jun 17, 2016 key.private.F984A085.asc.enc Private and Public PGP Key Sep 27, 2014 key.public.F984A085.asc.enc Private and Public PGP Key Sep 27, 2014 publish.sbt Remove the GPG plugin because Bintray will sign files automatically Jan 21, 2015 publishViaTravis.sh Scala 2.12 support Apr 13, 2016 version.sbt Prepare for signed publishing via sbt-pgp. Sep 27, 2014 README.md RxScala: Reactive Extensions for Scala This is a Scala adapter to RxJava. Example usage: val o = Observable.interval(200 millis).take(5) o.subscribe(n => println(""n = "" + n)) Observable.just(1, 2, 3, 4).reduce(_ + _) For-comprehensions are also supported: val first = Observable.just(10, 11, 12) val second = Observable.just(10, 11, 12) val booleans = for ((n1, n2) <- (first zip second)) yield (n1 == n2) Further, this adaptor attempts to expose an API which is as Scala-idiomatic as possible. This means that certain methods have been renamed, their signature was changed, or static methods were changed to instance methods. Some examples:  // instead of concat: def ++[U >: T](that: Observable[U]): Observable[U]  // instance method instead of static: def zip[U](that: Observable[U]): Observable[(T, U)]   // the implicit evidence argument ensures that dematerialize can only be called on Observables of Notifications: def dematerialize[U](implicit evidence: T <:< Notification[U]): Observable[U]   // additional type parameter U with lower bound to get covariance right: def onErrorResumeNext[U >: T](resumeFunction: Throwable => Observable[U]): Observable[U]   // curried in Scala collections, so curry fold also here: def fold[R](initialValue: R)(accumulator: (R, T) => R): Observable[R]   // using Duration instead of (long timepan, TimeUnit duration): def sample(duration: Duration): Observable[T]   // called skip in Java, but drop in Scala def drop(n: Int): Observable[T]   // there's only mapWithIndex in Java, because Java doesn't have tuples: def zipWithIndex: Observable[(T, Int)]   // corresponds to Java's toList: def toSeq: Observable[Seq[T]]   // the implicit evidence argument ensures that switch can only be called on Observables of Observables: def switch[U](implicit evidence: Observable[T] <:< Observable[Observable[U]]): Observable[U]  // Java's from becomes apply, and we use Scala Range def apply(range: Range): Observable[Int]  // use Bottom type: def never: Observable[Nothing] Also, the Scala Observable is fully covariant in its type parameter, whereas the Java Observable only achieves partial covariance due to limitations of Java's type system (or if you can fix this, your suggestions are very welcome). For more examples, see RxScalaDemo.scala. Scala code using Rx should only import members from rx.lang.scala and below. Master Build Status Communication Since RxScala is part of the RxJava family the communication channels are similar: Google Group: RxJava Twitter: @RxJava GitHub Issues Versioning RxScala version Compatible RxJava version 0.26.* 1.0.* 0.25.* 1.0.* 0.24.* 1.0.* 0.23.*[1] 1.0.* 0.22.0 1.0.0-rc.5 0.21.1 1.0.0-rc.3 0.X.Y (X < 21)[2] 0.X.Y [1] You can use any release of RxScala 0.23 with any release of RxJava 1.0. E.g, use RxScala 0.23.0 with RxJava 1.0.1 [2] You should use the same version of RxScala with RxJava. E.g, use RxScala 0.20.1 with RxJava 0.20.1 If you are using APIs labeled with Experimental/Beta, or ExperimentalAPIs (deprecated since 0.25.0), which uses RxJava Beta/Experimental APIs, you should use the corresponding version of RxJava as the following table: RxScala version Compatible RxJava version 0.26.2 1.1.6+ 0.26.1 1.1.1+ 0.26.0 1.1.0+ 0.25.1 1.0.17+ 0.25.0 1.0.11+ 0.24.1 1.0.8+ 0.24.0 1.0.7+ Full Documentation RxScala: The API documentation can be found here. Note that starting from version 0.15, rx.lang.scala.Observable is not a value class any more. ./Rationale.md explains why. You can build the API documentation yourself by running sbt doc in the RxScala root directory. Open target/scala-2.11/api/index.html to display it. RxJava: Wiki Javadoc Binaries Binaries and dependency information for Maven, Ivy, Gradle and others can be found at http://search.maven.org. Example for sbt/activator: libraryDependencies += ""io.reactivex"" %% ""rxscala"" % ""x.y.z"" and for Maven: <dependency>     <groupId>io.reactivex</groupId>     <artifactId>rxscala_${scala.compat.version}</artifactId>     <version>x.y.z</version> </dependency> and for Ivy: <dependency org=""io.reactivex"" name=""rxscala_${scala.compat.version}"" rev=""x.y.z"" /> Build To build you need sbt: $ git clone git@github.com:ReactiveX/RxScala.git $ cd RxScala $ TRAVIS_TAG=1.0.0-RC1 sbt package  Use TRAVIS_TAG to set the version of the package. You can also run the examples from within sbt: $ sbt examples/run  When you see the list of available App objects pick the one you want to execute. Multiple main classes detected, select one to run:   [1] AsyncWikiErrorHandling  [2] SyncObservable  [3] AsyncObservable  [4] AsyncWiki  [5] Transforming  Enter number:  Bugs and Feedback For bugs, questions and discussions please use the Github Issues. LICENSE Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ReactiveX/RxScala"	"Reactive Extensions for Scala – a library for composing asynchronous and event-based programs using observable sequences"	"true"
"Functional Reactive Programming"	"scala.frp ★ 18 ⧗ 240"	"https://github.com/dylemma/scala.frp"	"Functional Reactive Programming for Scala (event streams)."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"18"	"8"	"4"	"GitHub - dylemma/scala.frp: Functional Reactive Programming for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 8 Star 18 Fork 4 dylemma/scala.frp Code Issues 0 Pull requests 1 Pulse Graphs Functional Reactive Programming for Scala http://dylemma.github.com/scala.frp/ 44 commits 2 branches 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. project src .gitignore LICENSE readme.md readme.md Scala FRP This library replaces the idea of Publishers and Subscribers with EventStreams. An EventStream can be treated like a collection of events; it can be transformed similarly to any other Scala collection, and instead of needing to call publisher.addEventListener(new EventListener(){...}), you simply call eventStream.foreach{...} Background Scala FRP (stands for Functional Reactive Programming) is a library inspired by Ingo Maier's paper, Deprecating the Observer Pattern. Ingo Maier made an implementation of his ""scala.react"" framework which is available on Github in its original form. I also made a version of the same library that works with SBT to manage its dependencies, available here. Getting it Scala FRP is built for Scala 2.10.3; as long as your project is using Scala 2.10, you can simply add the following to your sbt settings: libraryDependencies += ""io.dylemma"" %% ""scala-frp"" % ""1.1""  Example Usage import io.dylemma.frp._  // Mix in `Observer` for free memory management. object Example extends App with Observer {      // Create a source of events.     val ints = EventSource[Int]      // You can derive new event streams any other event stream.     val evenInts = ints.filter{ _ % 2 == 0 }     val intsWithIndices = ints.zipWithIndex     val soonInts = ints.before(2 seconds fromNow)      // Attach event handlers.     ints.foreach{ x => println(s""An int: $x"") }     evenInts.foreach { x => println(s""Even int: $x"") }     intsWithIndices.foreach { case (x, i) => println(s""$ith int was $x"") }     soonInts.foreach { x => println(s""$x came soon enough"") }      // Fire events!     ints fire 1     ints fire 2     ints fire 3  }  FRP-101 Attaching event handlers requires an implicit Observer. The observer helps make sure that no cyclical references are made between the EventStream and the handler function; it keeps a weak reference to the handler so that the handler function may be garbage-collected once the Observer is garbage-collected. You can get an implicit Observer one of two ways: Make one yourself: implicit object myObserver extends Observer Mix it into the containing class, like in the example above. The Observer trait has an implicit reference to itself. With that requirement out of the way, you're ready to start! There are two main classes that you will want to interact with. EventStream is a read-only class that you can attach event handlers to, and create mappings and combinations with. EventSource is a concrete implementation of EventStream that also exposes fire and stop methods. FRP-102 EventStreams are finite. At a low level, they emit events as an Event[A], which can either be a Fire(item) or a Stop. When you call foreach on a stream, it will receive all Fire events until the stream emits a Stop. You can attach to these lower-level events with the sink(handler: Event[A] => Boolean) method. The handler will remain attached until it returns false in response to an event. Because of the concept of finite EventStreams, streams are able to be concatenated; handlers may be attached that simply wait for the last event from the stream, or the eventual Stop. For further details and a full list of capabilities, check out the docs Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/dylemma/scala.frp"	"Functional Reactive Programming for Scala (event streams)."	"true"
"Functional Reactive Programming"	"Scala.Rx ★ 638 ⧗ 0"	"https://github.com/lihaoyi/scala.rx"	"An experimental library for Functional Reactive Programming in Scala (reactive variables). Scala.js compatible."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"681"	"73"	"49"	"GitHub - lihaoyi/scala.rx: An experimental library for Functional Reactive Programming in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 73 Star 681 Fork 49 lihaoyi/scala.rx Code Issues 8 Pull requests 2 Pulse Graphs An experimental library for Functional Reactive Programming in Scala 239 commits 5 branches 2 releases 5 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.3.0-owner-data 0.3.0-reduce 0.3.0 gh-pages master Nothing to show v0.2.8 0.3.0 Nothing to show New pull request Latest commit 801757f Mar 10, 2016 lihaoyi Merge pull request #62 from stewSquared/patch-1 … Update first readme example to work with 0.3.1 Permalink Failed to load latest commit information. media thinner images May 7, 2013 project bumped scala.js version Feb 12, 2016 scalarx ignoring async tests until issue with travis.ci can be solved Feb 9, 2016 .gitignore Major Reshufflign for reals Jan 22, 2014 .travis.yml tweak travis Jan 22, 2016 build.sbt 0.3.1 Feb 18, 2016 readme.md Update first readme example to work with 0.3.1 Mar 9, 2016 readme.md Scala.Rx 0.3.1 Scala.Rx is an experimental change propagation library for Scala. Scala.Rx gives you Reactive variables (Rxs), which are smart variables who auto-update themselves when the values they depend on change. The underlying implementation is push-based FRP based on the ideas in Deprecating the Observer Pattern. A simple example which demonstrates the behavior is: import rx._ val a = Var(1); val b = Var(2) val c = Rx{ a() + b() } println(c.now)) // 3 a() = 4 println(c.now)) // 6 The idea being that 99% of the time, when you re-calculate a variable, you re-calculate it the same way you initially calculated it. Furthermore, you only re-calculate it when one of the values it depends on changes. Scala.Rx does this for you automatically, and handles all the tedious update logic for you so you can focus on other, more interesting things! Apart from basic change-propagation, Scala.Rx provides a host of other functionality, such as a set of combinators for easily constructing the dataflow graph, compile time checks for a high degree of correctness, and seamless interop with existing Scala code. This means it can be easily embedded in an existing Scala application. Contents Getting Started ScalaJS Using Scala.Rx Basic Usage Ownership Context Data Dependency Additional Operations Asynchronous Combinators Graph Inspection Logging and Debugging Design Considerations Related Work Scaladoc Getting Started Scala.Rx is available on Maven Central. In order to get started, simply add the following to your build.sbt: libraryDependencies += ""com.lihaoyi"" %% ""scalarx"" % ""0.3.1"" After that, opening up the sbt console and pasting the above example into the console should work! You can proceed through the examples in the Basic Usage page to get a feel for what Scala.Rx can do. ScalaJS In addition to running on the JVM, Scala.Rx also compiles to Scala-Js! This artifact is currently on Maven Central and an be used via the following SBT snippet: libraryDependencies += ""com.lihaoyi"" %%% ""scalarx"" % ""0.3.1"" There are some minor differences between running Scala.Rx on the JVM and in Javascript particularly around asynchronous operations, the parallelism model and memory model. In general, though, all the examples given in the documentation below will work perfectly when cross-compiled to javascript and run in the browser! Scala.rx 0.3.1 is only compatible with ScalaJS 0.6.5+. Using Scala.Rx The primary operations only need a import rx._ before being used, with addtional operations also needing a import rx.ops._. Some of the examples below also use various imports from scala.concurrent or scalatest aswell. Basic Usage import rx._  val a = Var(1); val b = Var(2) val c = Rx{ a() + b() } println(c.now) // 3 a() = 4 println(c.now) // 6 The above example is an executable program. In general, import rx._ is enough to get you started with Scala.Rx, and it will be assumed in all further examples. These examples are all taken from the unit tests. The basic entities you have to care about are Var, Rx and Obs: Var: a smart variable which you can get using a() and set using a() = .... Whenever its value changes, it pings any downstream entity which needs to be recalculated. Rx: a reactive definition which automatically captures any Vars or other Rxs which get called in its body, flagging them as dependencies and re-calculating whenever one of them changes. Like a Var, you can use the a() syntax to retrieve its value, and it also pings downstream entities when the value changes. Obs: an observer on one or more Var s or Rx s, performing some side-effect when the observed node changes value and sends it a ping. Using these components, you can easily construct a dataflow graph, and have the various values within the dataflow graph be kept up to date when the inputs to the graph change: val a = Var(1) // 1  val b = Var(2) // 2  val c = Rx{ a() + b() } // 3 val d = Rx{ c() * 5 } // 15 val e = Rx{ c() + 4 } // 7 val f = Rx{ d() + e() + 4 } // 26  println(f.now) // 26 a() = 3 println(f.now) // 38 The dataflow graph for this program looks like this: Where the Vars are represented by squares, the Rxs by circles and the dependencies by arrows. Each Rx is labelled with its name, its body and its value. Modifying the value of a causes the changes the propagate through the dataflow graph As can be seen above, changing the value of a causes the change to propagate all the way through c d e to f. You can use a Var and Rx anywhere you an use a normal variable. The changes propagate through the dataflow graph in waves. Each update to a Var touches off a propagation, which pushes the changes from that Var to any Rx which is (directly or indirectly) dependent on its value. In the process, it is possible for a Rx to be re-calculated more than once. Observers As mentioned, Obs s can be created from Rx s or Var s and be used to perform side effects when they change: val a = Var(1) var count = 0 val o = a.trigger {   count = a.now + 1 } println(count) // 2 a() = 4 println(count) // 5 This creates a dataflow graph that looks like: When a is modified, the observer o will perform the side effect: The body of Rxs should be side effect free, as they may be run more than once per propagation. You should use Obs s to perform your side effects, as they are guaranteed to run only once per propagation after the values for all Rxs have stabilized. Scala.Rx provides a convenient .foreach() combinator, which provides an alternate way of creating an Obs from an Rx: val a = Var(1) var count = 0 val o = a.foreach{ x =>   count = x + 1 } println(count) // 2 a() = 4 println(count) // 5 This example does the same thing as the code above. Note that the body of the Obs is run once initially when it is declared. This matches the way each Rx is calculated once when it is initially declared. but it is conceivable that you want an Obs which fires for the first time only when the Rx it is listening to changes. You can do this by using the alternate triggerLater syntax: val a = Var(1) var count = 0 val o = a.triggerLater {   count = count + 1 } println(count) // 0 a() = 2 println(count) // 1 An Obs acts to encapsulate the callback that it runs. They can be passed around, stored in variables, etc.. When the Obs gets garbage collected, the callback will stop triggering. Thus, an Obs should be stored in the object it affects: if the callback only affects that object, it doesn't matter when the Obs itself gets garbage collected, as it will only happen after that object holding it becomes unreachable, in which case its effects cannot be observed anyway. An Obs can also be actively shut off, if a stronger guarantee is needed: val a = Var(1) val b = Rx{ 2 * a() } var target = 0 val o = b.trigger {   target = b.now } println(target) // 2 a() = 2 println(target) // 4 o.kill() a() = 3 println(target) // 4 After manually calling .kill(), the Obs no longer triggers. Apart from .kill()ing Obss, you can also kill Rxs, which prevents further updates. In general, Scala.Rx revolves around constructing dataflow graphs which automatically keep things in sync, which you can easily interact with from external, imperative code. This involves using: Vars as inputs to the dataflow graph from the imperative world Rxs as the intermediate nodes in the dataflow graphs Obss as the output from the dataflow graph back into the imperative world Complex Reactives Rxs are not limited to Ints. Strings, Seq[Int]s, Seq[String]s, anything can go inside an Rx: val a = Var(Seq(1, 2, 3)) val b = Var(3) val c = Rx{ b() +: a() } val d = Rx{ c().map(""omg"" * _) } val e = Var(""wtf"") val f = Rx{ (d() :+ e()).mkString }  println(f.now) // ""omgomgomgomgomgomgomgomgomgwtf"" a() = Nil println(f.now) // ""omgomgomgwtf"" e() = ""wtfbbq"" println(f.now) // ""omgomgomgwtfbbq"" As shown, you can use Scala.Rx's reactive variables to model problems of arbitrary complexity, not just trivial ones which involve primitive numbers. Error Handling Since the body of an Rx can be any arbitrary Scala code, it can throw exceptions. Propagating the exception up the call stack would not make much sense, as the code evaluating the Rx is probably not in control of the reason it failed. Instead, any exceptions are caught by the Rx itself and stored internally as a Try. This can be seen in the following unit test: val a = Var(1) val b = Rx{ 1 / a() } println(b.now) // 1 println(b.toTry) // Success(1) a() = 0 intercept[ArithmeticException]{   b() } assert(b.toTry.isInstanceOf[Failure]) Initially, the value of a is 1 and so the value of b also is 1. You can also extract the internal Try using b.toTry, which at first is Success(1). However, when the value of a becomes 0, the body of b throws an ArithmeticException. This is caught by b and re-thrown if you try to extract the value from b using b(). You can extract the entire Try using toTry and pattern match on it to handle both the Success case as well as the Failure case. When you have many Rxs chained together, exceptions propagate forward following the dependency graph, as you would expect. The following code: val a = Var(1) val b = Var(2)  val c = Rx{ a() / b() } val d = Rx{ a() * 5 } val e = Rx{ 5 / b() } val f = Rx{ a() + b() + 2 } val g = Rx{ f() + c() }  inside(c.toTry){case Success(0) => () } inside(d.toTry){case Success(5) => () } inside(e.toTry){case Success(2) => () } inside(f.toTry){case Success(5) => () } inside(g.toTry){case Success(5) => () }  b() = 0  inside(c.toTry){case Failure(_) => () } inside(d.toTry){case Success(5) => () } inside(e.toTry){case Failure(_) => () } inside(f.toTry){case Success(3) => () } inside(g.toTry){case Failure(_) => () } Creates a dependency graph that looks like the follows: In this example, initially all the values for a, b, c, d, e, f and g are well defined. However, when b is set to 0: c and e both result in exceptions, and the exception from c propagates to g. Attempting to extract the value from g using g.now, for example, will re-throw the ArithmeticException. Again, using toTry works too. Nesting Rxs can contain other Rxs, arbitrarily deeply. This example shows the Rxs nested two levels deep: val a = Var(1) val b = Rx{     (Rx{ a() }, Rx{ math.random }) } val r = b.now._2.now a() = 2 println(b.now._2.now) // r In this example, we can see that although we modified a, this only affects the left-inner Rx, neither the right-inner Rx (which takes on a different, random value each time it gets re-calculated) or the outer Rx (which would cause the whole thing to re-calculate) are affected. A slightly less contrived example may be: var fakeTime = 123 trait WebPage{     def fTime = fakeTime     val time = Var(fTime)     def update(): Unit  = time() = fTime     val html: Rx[String] } class HomePage(implicit ctx: Ctx.Owner) extends WebPage {     val html = Rx{""Home Page! time: "" + time()} } class AboutPage(implicit ctx: Ctx.Owner) extends WebPage {     val html = Rx{""About Me, time: "" + time()} }  val url = Var(""www.mysite.com/home"") val page = Rx{     url() match{         case ""www.mysite.com/home"" => new HomePage()         case ""www.mysite.com/about"" => new AboutPage()     } }  println(page.now.html.now) // ""Home Page! time: 123""  fakeTime = 234 page.now.update() println(page.now.html.now) // ""Home Page! time: 234""  fakeTime = 345 url() = ""www.mysite.com/about"" println(page.now.html.now) // ""About Me, time: 345""  fakeTime = 456 page.now.update() println(page.now.html.now) // ""About Me, time: 456"" In this case, we define a web page which has a html value (a Rx[String]). However, depending on the url, it could be either a HomePage or an AboutPage, and so our page object is a Rx[WebPage]. Having a Rx[WebPage], where the WebPage has an Rx[String] inside, seems natural and obvious, and Scala.Rx lets you do it simply and naturally. This kind of objects-within-objects situation arises very naturally when modelling a problem in an object-oriented way. The ability of Scala.Rx to gracefully handle the corresponding Rxs within Rxs allows it to gracefully fit into this paradigm, something I found lacking in most of the Related Work I surveyed. Most of the examples here are taken from the unit tests, which provide more examples on guidance on how to use this library. Ownership Context In the last example above, we had to introduce the concept of Ownership where Ctx.Owner is used. In fact, if we leave out (implicit ctx: Ctx.Owner), we would get the following compile time error: error: This Rx might leak! Either explicitly mark it unsafe (Rx.unsafe) or ensure an implicit RxCtx is in scope!            val html = Rx{""Home Page! time: "" + time()} To understand ownership it is important to understand the problem it fixes: leaks. As an example, consider this slight modification to the first example: var count = 0 val a = Var(1); val b = Var(2) def mkRx(i: Int) = Rx.unsafe { count += 1; i + b() } val c = Rx{    val newRx = mkRx(a())    newRx()  } println(c.now, count) //(3,1) In this version, the function mkRx was added, but otherwise the computed value of c remains unchanged. And modfying a appears to behave as expected: a() = 4 println(c.now, count) //(6,2) But if we modify b we might start to notice something not quite right: b() = 3  println(c.now, count) //(7,5) -- 5??  (0 to 100).foreach { i => a() = i } println(c.now, count) //(103,106)  b() = 4 println(c.now, count) //(104,211) -- 211!!! In this example, even though b is only updated a few times, the count value starts to soar as a is modified. This is mkRx leaking! That is, every time c is recomputed, it builds a whole new Rx that sticks around and keeps on evaluating, even after it is no longer reachable as a data dependency and forgotten. So after running that (0 to 100).foreach statment, there are over 100 Rxs that all fire every time b is changed. This clearly is not desirable. However, by adding an explicit owner (and removing unsafe), we can fix the leak: var count = 0 val a = Var(1); val b = Var(2) def mkRx(i: Int)(implicit ctx: Ctx.Owner) = Rx { count += 1; i + b() } val c = Rx{    val newRx = mkRx(a())    newRx()  } println(c.now,count) // (3,1) a() = 4 println(c.now,count) // (6,2) b() = 3 println(c.now,count) // (7,4) (0 to 100).foreach { i => a() = i } println(c.now,count) //(103,105) b() = 4 println(c.now,count) //(104,107) Ownership fixes leaks by keeping allowing a parent Rx to track its ""owned"" nested Rx. That is whenever an Rx recaculates, it first kills all of its owned dependencies, ensuring they do not leak. In this example, c is the owner of all the Rxs which are created in mkRx and kills them automatically every time c recalculates. Data Context Given either a Rx or a Var using () (aka apply) unwraps the current value and adds itself as a dependency to whatever Rx that is currently evaluating. Alternatively, .now can be used to simply unwrap the value and skips over becoming a data dependency: val a = Var(1); val b = Var(2) val c = Rx{ a.now + b.now } //not a very useful `Rx` println(c.now) // 3 a() = 4 println(c.now) // 3  b() = 5 println(c.now) // 3 To understand the need for a Data context and how Data contexts differ from Owner contexts, consider the following example: def foo()(implicit ctx: Ctx.Owner) = {   val a = rx.Var(1)   a()   a }  val x = rx.Rx{val y = foo(); y() = y() + 1; println(""done!"") } With the concept of ownership, if a() is allowed to create a data dependency on its owner, it would enter infinite recursion and blow up the stack! Instead, the above code gives this compile time error: <console>:17: error: No implicit Ctx.Data is available here!         a() We can ""fix"" the error by explicitly allowing the data dependencies (and see that the stack blows up): def foo()(implicit ctx: Ctx.Owner, data: Ctx.Data) = {   val a = rx.Var(1)   a()   a } val x = rx.Rx{val y = foo(); y() = y() + 1; println(""done!"") } ... at rx.Rx$Dynamic$Internal$$anonfun$calc$2.apply(Core.scala:180)   at scala.util.Try$.apply(Try.scala:192)   at rx.Rx$Dynamic$Internal$.calc(Core.scala:180)   at rx.Rx$Dynamic$Internal$.update(Core.scala:184)   at rx.Rx$.doRecalc(Core.scala:130)   at rx.Var.update(Core.scala:280)   at $anonfun$1.apply(<console>:15)   at $anonfun$1.apply(<console>:15)   at rx.Rx$Dynamic$Internal$$anonfun$calc$2.apply(Core.scala:180)   at scala.util.Try$.apply(Try.scala:192) ... The Data context is the mechanism that an Rx uses to decide when to recaculate. Ownership fixes the problem of leaking. Mixing the two can lead to infinite recursion: when something is both owned and a data dependency of the same parent Rx. Luckily though it is almost always the case that only one or the other context is needed. when dealing with dynamic graphs, it is almost always the case that only the ownership context is needed, ie functions most often have the form: def f(...)(implicit ctx: Ctx.Owner) = Rx { ... } The Data context is needed less often and is useful in, as an example, the case where it is desirable to DRY up some repeated Rx code. Such a funtion would have this form: def f(...)(implicit data: Ctx.Data) = ... This would allow some shared data dependency to be pulled out of the body of each Rx and into the shared function. By splitting up the orthogonal concepts of ownership and data dependencies the problem of infinite recursion as outlined above is greatly limited. Explicit data dependencies also make it more clear when the use of a Var or Rx is meant to be a data dependency, and not just a simple read of the current value (ie .now). Without this distiction, it is easier to introduce ""accidental"" data dependencies that are unexpected and unintended. Additional Operations Apart from the basic building blocks of Var/Rx/Obs, Scala.Rx also provides a set of combinators which allow your to easily transform your Rxs; this allows the programmer to avoid constantly re-writing logic for the common ways of constructing the dataflow graph. The five basic combinators: map(), flatMap, filter(), reduce() and fold() are all modelled after the scala collections library, and provide an easy way of transforming the values coming out of an Rx. Map val a = Var(10) val b = Rx{ a() + 2 } val c = a.map(_*2) val d = b.map(_+3) println(c.now) // 20 println(d.now) // 15 a() = 1 println(c.now) // 2 println(d.now) // 6 map does what you would expect, creating a new Rx with the value of the old Rx transformed by some function. For example, a.map(_*2) is essentially equivalent to Rx{ a() * 2 }, but somewhat more convenient to write. FlatMap val a = Var(10) val b = Var(1) val c = a.flatMap(a => Rx { a*b() }) println(c.now) // 10 b() = 2 println(c.now) // 20 flatMap is analogous to flatMap from the collections library in that it allows for merging nested Rx s of type Rx[Rx[_]] into a single Rx[_]. This in conjunction with the map combinator allow for scala's for comprehension syntax to work with Rx s and Var s: val a = Var(10) val b = for {   aa <- a   bb <- Rx { a() + 5}   cc <- Var(1).map(_*2) } yield {   aa + bb + cc } Filter val a = Var(10) val b = a.filter(_ > 5) a() = 1 println(b.now) // 10 a() = 6 println(b.now) // 6 a() = 2 println(b.now) // 6 a() = 19 println(b.now) // 19 filter ignores changes to the value of the Rx that fail the predicate. Note that none of the filter methods is able to filter out the first, initial value of a Rx, as there is no ""older"" value to fall back to. Hence this: val a = Var(2) val b = a.filter(_ > 5) println(b.now) will print out ""2"". Reduce val a = Var(1) val b = a.reduce(_ * _) a() = 2 println(b.now) // 2 a() = 3 println(b.now) // 6 a() = 4 println(b.now) // 24 The reduce operator combines subsequent values of an Rx together, starting from the initial value. Every change to the original Rx is combined with the previously-stored value and becomes the new value of the reduced Rx. Fold val a = Var(1) val b = a.fold(List.empty[Int])((acc,elem) => elem :: acc) a() = 2 println(b.now) // List(2,1) a() = 3 println(b.now) // List(3,2,1) a() = 4 println(b.now) // List(4,3,2,1) Fold enables accumulation in a similar way to reduce, but can accumulate to some other type than that of the source Rx. Each of these five combinators has a counterpart in the .all namespace which operates on Try[T]s rather than Ts, in the case where you need the added flexibility to handle Failures in some special way. Asynchronous Combinators These are combinators which do more than simply transforming a value from one to another. These have asynchronous effects, and can spontaneously modify the dataflow graph and begin propagation cycles without any external trigger. Although this may sound somewhat unsettling, the functionality provided by these combinators is often necessary, and manually writing the logic around something like Debouncing, for example, is far more error prone than simply using the combinators provided. Note that none of these combinators are doing anything that cannot be done via a combination of Obss and Vars; they simply encapsulate the common patterns, saving you manually writing them over and over, and reducing the potential for bugs. Future import scala.concurrent.Promise import scala.concurrent.ExecutionContext.Implicits.global import rx.async._  val p = Promise[Int]() val a = p.future.toRx(10) println(a.now) //10 p.success(5) println(a.now) //5 The toRx combinator only applies to Future[_]s. It takes an initial value, which will be the value of the Rx until the Future completes, at which point the the value will become the value of the Future. This async can create Futures as many times as necessary. This example shows it creating two distinct Futures: import scala.concurrent.Promise import scala.concurrent.ExecutionContext.Implicits.global import rx.async._  var p = Promise[Int]() val a = Var(1)  val b: Rx[Int] = Rx {   val f =  p.future.toRx(10)   f() + a() } println(b.now) //11 p.success(5) println(b.now) //6  p = Promise[Int]() a() = 2 println(b.now) //12  p.success(7) println(b.now) //9 The value of b() updates as you would expect as the series of Futures complete (in this case, manually using Promises). This is handy if your dependency graph contains some asynchronous elements. For example, you could have a Rx which depends on another Rx, but requires an asynchronous web request to calculate its final value. With async, the results from the asynchronous web request will be pushed back into the dataflow graph automatically when the Future completes, starting off another propagation run and conveniently updating the rest of the graph which depends on the new result. Timer import rx.async._ import rx.async.Platform._ import scala.concurrent.duration._  val t = Timer(100 millis) var count = 0 val o = t.trigger {     count = count + 1 }  println(count) // 3 println(count) // 8 println(count) // 13 A Timer is a Rx that generates events on a regular basis. In the example above, using println in the console show that the value t() has increased over time. The scheduled task is cancelled automatically when the Timer object becomes unreachable, so it can be garbage collected. This means you do not have to worry about managing the life-cycle of the Timer. On the other hand, this means the programmer should ensure that the reference to the Timer is held by the same object as that holding any Rx listening to it. This will ensure that the exact moment at which the Timer is garbage collected will not matter, since by then the object holding it (and any Rx it could possibly affect) are both unreachable. Delay import rx.async._ import rx.async.Platform._ import scala.concurrent.duration._  val a = Var(10) val b = a.delay(250 millis)  a() = 5 println(b.now) // 10 eventually{   println(b.now) // 5 }  a() = 4 println(b.now) // 5 eventually{   println(b.now) // 4 } The delay(t) combinator creates a delayed version of an Rx whose value lags the original by a duration t. When the Rx changes, the delayed version will not change until the delay t has passed. This example shows the delay being applied to a Var, but it could easily be applied to an Rx as well. Debounce import rx.async._ import rx.async.Platform._ import scala.concurrent.duration._  val a = Var(10) val b = a.debounce(200 millis) a() = 5 println(b.now) // 5  a() = 2 println(b.now) // 5  eventually{   println(b.now) // 2 }  a() = 1 println(b.now) // 2  eventually{   println(b.now) // 1 } The debounce(t) combinator creates a version of an Rx which will not update more than once every time period t. If multiple updates happen with a short span of time (less than t apart), the first update will take place immediately, and a second update will take place only after the time t has passed. For example, this may be used to limit the rate at which an expensive result is re-calculated: you may be willing to let the calculated value be a few seconds stale if it lets you save on performing the expensive calculation more than once every few seconds. Design Considerations Simple to Use This meant that the syntax to write programs in a dependency-tracking way had to be as light weight as possible, that programs written using FRP had to look like their normal, old-fashioned, imperative counterparts. This meant using DynamicVariable instead of implicits to automatically pass arguments, sacrificing proper lexical scoping for nice syntax. I ruled out using a purely monadic style (like reactive-web), as although it would be far easier to implement the library in that way, it would be a far greater pain to actually use it. I also didn't want to have to manually declare dependencies, as this violates DRY when you are declaring your dependencies twice: once in the header of the Rx, and once more when you use it in the body. The goal was to be able to write code, sprinkle a few Rx{}s around and have the dependency tracking and change propagation just work. Overall, I believe it has been quite successful at that! Simple to Reason About This means many things, but most of all it means having no globals. This greatly simplifies many things for someone using the library, as you no longer need to reason about different parts of your program interacting through the library. Using Scala.Rx in different parts of a large program is completely fine; they are completely independent. Another design decision in this area was to have the parallelism and propagation-scheduling be left mainly to an implicit ExecutionContext, and have the default to simply run the propagation wave on whatever thread made the update to the dataflow graph. The former means that anyone who is used to writing parallel programs in Scala/Akka is already familiar with how to deal with parallelizing Scala.Rx The latter makes it far easier to reason about when propagations happen, at least in the default case: it simply happens right away, and by the time that Var.update() function has returned, the propagation has completed. Overall, limiting the range of side effects and removing global state makes Scala.Rx easy to reason about, and means a developer can focus on using Scala.Rx to construct dataflow graphs rather than worry about un-predictable far-reaching interactions or performance bottlenecks. Simple to Interop This meant that it had to be easy for a programmer to drop in and out of the FRP world. Many of the papers I read in preparing for Scala.Rx described systems that worked brilliantly on their own, and had some amazing properties, but required that the entire program be written in an obscure variant of an obscure language. No thought at all was given to inter-operability with existing languages or paradigms, which makes it impossible to incrementally introduce FRP into an existing codebase. With Scala.Rx, I resolved to do things differently. Hence, Scala.Rx: Is written in Scala: an uncommon, but probably less-obscure language than Haskell or Scheme Is a library: it is plain-old-scala. There is no source-to-source transformation, no special runtime necessary to use Scala.Rx. You download the source code into your Scala project, and start using it Allows you to use any programming language construct or library functionality within your Rxs: Scala.Rx will figure out the dependencies without the programmer having to worry about it, without limiting yourself to some inconvenient subset of the language Allows you to use Scala.Rx within a larger project without much pain. You can easily embed dataflow graphs within a larger object-oriented universe and interact with them via setting Vars and listening to Obss Many of the papers reviewed show a beautiful new FRP universe that we could be programming in, if only you ported all your code to FRP-Haskell and limited yourself to the small set of combinators used to create dataflow graphs. On the other hand, by letting you embed FRP snippets anywhere within existing code, using FRP ideas in existing projects without full commitment, and allowing you easy interop between your FRP and non-FRP code, Scala.Rx aims to bring the benefits FRP into the dirty, messy universe which we are programming in today. Limitations Scala.Rx has a number of significant limitations, some of which arise from trade-offs in the design, others from the limitations of the underlying platform. No ""Empty"" Reactives The API of Rxs in Scala.Rx tries to follow the collections API as far as possible: you can map, filter and reduce over the Rxs, just as you can over collections. However, it is currently impossible to have a Rx which is empty in the way a collection can be empty: filtering out all values in a Rx will still leave at least the initial value (even if it fails the predicate) and Async Rxs need to be given an initial value to start. This limitation arises from the difficulty in joining together possibly empty Rxs with good user experience. For example, if I have a dataflow graph: val a = Var() val b = Var() var c = Rx{     .... a() ...     ... some computation ...     ... b() ...     result } Where a and b are initially empty, I have basically two options: Block the current thread which is computing c, waiting for a and then b to become available. Throw an exception when a() and b() are requested, aborting the computation of c but registering it to be restarted when a() or b() become available. Re-write this in a monadic style using for-comprehensions. Use the delimited continuations plugin to transform the above code to monadic code automatically. The first option is a performance problem: threads are generally extremely heavy weight on most operation systems. You cannot reasonably make more than a few thousand threads, which is a tiny number compared to the amount of objects you can create. Hence, although blocking would be the easiest, it is frowned upon in many systems (e.g. in Akka, which Scala.Rx is built upon) and does not seem like a good solution. The second option is a performance problem in a different way: with n different dependencies, all of which may start off empty, the computation of c may need to be started and aborted n times even before completing even once. Although this does not block any threads, it does seem extremely expensive. The third option is a no-go from a user experience perspective: it would require far reaching changes in the code base and coding style in order to benefit from the change propagation, which I'm not willing to require. The last option is problematic due to the bugginess of the delimited continuations plugin. Although in theory it should be able to solve everything, a large number of small bugs (messing up type inferencing, interfering with implicit resolution) combined with a few fundamental problems meant that even on a small scale project (less than 1000 lines of reactive code) it was getting painful to use. No Automatic Parallelization at the Start As mentioned earlier, Scala.Rx can perform automatic parallelization of updates occurring in the dataflow graph: simply provide an appropriate ExecutionContext, and independent Rxs will have their updates spread out over multiple cores. However, this only works for updates, and not when the dataflow graph is being initially defined: in that case, every Rx evaluates its body once in order to get its default value, and it all happens serially on the same thread. This limitation arises from the fact that we do not have a good way to work with ""empty"" Rxs, and we do not know what an Rxs dependencies are before the first time we evaluate it. Hence, we cannot start all our Rxs evaluating in parallel as some may finish before others they depend on, which would then be empty, their initial value still being computed. We also cannot choose to parallelize those which do not have dependencies on each other, as before execution we do not know what the dependencies are! Thus, we have no choice but to have the initial definitions of Rxs happen serially. If necessary, a programmer can manually create independent Rxs in parallel using Futures. Glitchiness and Redundant Computation In the context of FRP, a glitch is a temporary inconsistency in the dataflow graph. Due to the fact that updates do not happen instantaneously, but instead take time to computer, the values within an FRP system may be transiently out of sync during the update process. Furthermore, depending on the nature of the FRP system, it is possible to have nodes be updated more than once in a propagation. This may or may not be a problem, depending on how tolerant the application is of occasional stale inconsistent data. In a single-threaded system, it can be avoided in a number of ways Make the dataflow graph static, and perform a topological sort to rank nodes in the order they are to be updated. This means that a node always is updated after its dependencies, meaning they will never see any stale data Pause the updating of a node when it tries to call upon a dependency which has not been updated. This could be done by blocking the thread, for example, and only resuming after the dependency has been updated. However, both of these approaches have problems. The first approach is extremely constrictive: a static dataflow graph means that a large amount of useful behavior, e.g. creating and destroying sections of the graph dynamically at run-time, is prohibited. This goes against Scala.Rx's goal of allowing the programmer to write code ""normally"" without limits, and letting the FRP system figure it out. The second case is a problem for languages which do not easily allow computations to be paused. In Java, and by extension Scala, the threads used are operating system (OS) threads which are extremely expensive. Hence, blocking an OS thread is frowned upon. Coroutines and continuations could also be used for this, but Scala lacks both of these facilities. The last problem is that both these models only make sense in the case of single threaded, sequential code. As mentioned on the section on Concurrency and Parallelism, Scala.Rx allows you to use multiple threads to parallelize the propagation, and allows propagations to be started by multiple threads simultaneously. That means that a strict prohibition of glitches is impossible. Scala.Rx maintains somewhat looser model: the body of each Rx may be evaluated more than once per propagation, and Scala.Rx only promises to make a ""best-effort"" attempt to reduce the number of redundant updates. Assuming the body of each Rx is pure, this means that the redundant updates should only affect the time taken and computation required for the propagation to complete, but not affect the value of each node once the propagation has finished. In addition, Scala.Rx provides the Obss, which are special terminal-nodes guaranteed to update only once per propagation, intended to produce some side effect. This means that although a propagation may cause the values of the Rxs within the dataflow graph to be transiently out of sync, the final side-effects of the propagation will only happen once the entire propagation is complete and the Obss all fire their side effects. If multiple propagations are happening in parallel, Scala.Rx guarantees that each Obs will fire at most once per propagation, and at least once overall. Furthermore, each Obs will fire at least once after the entire dataflow graph has stabilized and the propagations are complete. This means that if you are relying on Obs to, for example, send updates over the network to a remote client, you can be sure that you don't have any unnecessary chatter being transmitted over the network, and when the system is quiescent the remote client will have the updates representing the most up-to-date version of the dataflow graph. Related Work Scala.Rx was not created in a vacuum, and borrows ideas and inspiration from a range of existing projects. Scala.React Scala.React, as described in Deprecating the Observer Pattern, contains a reactive change propagation portion (there called Signals) which is similar to what Scala.Rx does. However, it does much more than that: It contains implementations for using event-streams, and multiple DSLs using delimited continuations in order to make it easy to write asynchronous workflows. I have used this library, and my experience is that it is extremely difficult to set up and get started. It requires a fair amount of global configuration, with a global engine doing the scheduling and propagation, even running its own thread pools. This made it extremely difficult to reason about interactions between parts of the programs: would completely-separate dataflow graphs be able to affect each other through this global engine? Would the performance of multithreaded code start to slow down as the number of threads rises, as the engine becomes a bottleneck? I never found answers to many of these questions, and had did not manage to contact the author. The global propagation engine also makes it difficult to get started. It took several days to get a basic dataflow graph (similar to the example at the top of this document) working. That is after a great deal of struggling, reading the relevant papers dozens of times and hacking the source in ways I didn't understand. Needless to say, these were not foundations that I would feel confident building upon. reactive-web reactive-web was another inspiration. It is somewhat orthogonal to Scala.Rx, focusing more on event streams and integration with the Lift web framework, while Scala.Rx focuses purely on time-varying values. Nevertheless, reactive-web comes with its own time-varying values (called Signals), which are manipulated using combinators similar to those in Scala.Rx (map, filter, flatMap, etc.). However, reactive-web does not provide an easy way to compose these Signals: the programmer has to rely entirely on map and flatMap, possibly using Scala's for-comprehensions. I did not like the fact that you had to program in a monadic style (i.e. living in .map() and .flatMap() and for{} comprehensions all the time) in order to take advantage of the change propagation. This is particularly cumbersome in the case of nested Rxs, where Scala.Rx's // a b and c are Rxs x = Rx{ a() + b().c() } becomes x = for {   va <- a   vb <- b   vc <- vb.c } yield (va + vc) As you can see, using for-comprehensions as in reactive-web results in the code being significantly longer and much more obfuscated. Knockout.js Knockout.js does something similar for Javascript, along with some other extra goodies like DOM-binding. In fact, the design and implementation and developer experience of the automatic-dependency-tracking is virtually identical. This: this.firstName = ko.observable('Bob'); this.lastName = ko.observable('Smith'); fullName = ko.computed(function() {     return this.firstName() + "" "" + this.lastName(); }, this); is semantically equivalent to the following Scala.Rx code: val firstName = Var(""Bob"") val lastName = Var(""Smith"") fullName = Rx{ firstName() + "" "" + lastName() } a ko.observable maps directly onto a Var, and a kocomputed maps directly onto an Rx. Apart from the longer variable names and the added verbosity of Javascript, the semantics are almost identical. Apart from providing an equivalent of Var and Rx, Knockout.js focuses its efforts in a different direction. It lacks the majority of the useful combinators that Scala.Rx provides, but provides a great deal of other functionality, for example integration with the browser's DOM, that Scala.Rx lacks. Others This idea of change propagation, with time-varying values which notify any value which depends on them when something changes, part of the field of Functional Reactive Programming. This is a well studied field with a lot of research already done. Scala.Rx builds upon this research, and incorporates ideas from the following projects: FlapJax Frappe Fran All of these projects are filled with good ideas. However, generally they are generally very much research projects: in exchange for the benefits of FRP, they require you to write your entire program in an obscure variant of an obscure language, with little hope inter-operating with existing, non-FRP code. Writing production software in an unfamiliar paradigm such as FRP is already a significant risk. On top of that, writing production software in an unfamiliar language is an additional variable, and writing production software in an unfamiliar paradigm in an unfamiliar language with no inter-operability with existing code is downright reckless. Hence it is not surprising that these libraries have not seen significant usage. Scala.Rx aims to solve these problems by providing the benefits of FRP in a familiar language, with seamless interop between FRP and more traditional imperative or object-oriented code. Version History 0.3.1 Fixed leak with observers (they also require an owning context). Fixed type issue with flatMap 0.3.0 Introduced Owner and Data context. This is a completely different implementation of dependency and lifetime managment that allows for safe construction of runtime dynamic graphs. More default combinators: fold and flatMap are now implemented by default. Credits Copyright (c) 2013, Li Haoyi (haoyi.sg at gmail.com) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/scala.rx"	"An experimental library for Functional Reactive Programming in Scala (reactive variables). Scala.js compatible."	"true"
"Functional Reactive Programming"	"SynapseGrid ★ 89 ⧗ 24"	"https://github.com/Primetalk/SynapseGrid"	"an FRP framework for constructing reactive real-time immutable data flow systems. It implements an original way of running and organizing event-driven systems based on Petri nets. The topology can be viewed as a.dot graph. The library is compatible with Akka and can seamlessly communicate with other actors."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"90"	"16"	"6"	"GitHub - Primetalk/SynapseGrid: SynapseGrid is a framework for constructing reactive real-time immutable data flow systems. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 16 Star 90 Fork 6 Primetalk/SynapseGrid Code Issues 1 Pull requests 0 Wiki Pulse Graphs SynapseGrid is a framework for constructing reactive real-time immutable data flow systems. 219 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 4b81625 Sep 11, 2015 Primetalk pr status Permalink Failed to load latest commit information. .idea synapse frames started. Apr 25, 2014 docs synapse-grid documentation Apr 11, 2015 gradle/wrapper gradle 2.5 Aug 6, 2015 lib Actor composition implemented Jul 1, 2013 synapse-grid-akka test and API fixed Aug 26, 2015 synapse-grid-concurrent/src dot label quotes Aug 11, 2015 synapse-grid-core test and API fixed Aug 26, 2015 synapse-grid-examples refactoring Aug 5, 2015 synapse-grid-rx/src API modularization: Apr 10, 2015 synapse-grid-shapeless/src/main/scala/ru/primetalk/synapse/shapeless Contact -> components._ Apr 11, 2015 synapse-grid-slf4j API modularization: SystemBuilder -> SystemBuilderApi Apr 5, 2015 .gitignore gitignore added Nov 21, 2013 .travis.yml travis.yml added Aug 26, 2015 LICENSE.md English documentation review Jul 17, 2013 README.md pr status Sep 11, 2015 SynapseGrid.iml docs minor changes Jan 28, 2014 build.gradle snapshot version (without signing) Aug 26, 2015 gradlew gradle wrapper added Oct 9, 2014 gradlew.bat gradle wrapper added Oct 9, 2014 settings.gradle Separation of other projects Apr 11, 2015 synapse-grid-concurrent.iml synapse frames started. Apr 24, 2014 synapse-grid-rx.iml synapse frames started. Apr 24, 2014 synapse-grid-shapeless.iml synapse frames started. Apr 24, 2014 README.md SynapseGrid SynapseGrid is an original approach to implement functional reactive programming paradigm in Scala. The library is based on a solid foundation of Petri nets. A few words about what SynapseGrid is: framework for constructing systems that are: reactive event-driven resilent it resembles other modern event-driven architectures (ScalaRx, Akka Streams, Spark, etc.). Blog about SynapseGrid Feature highlights SynapseGrid allows function composition of ""multifunctions"" (functions with a few inputs and outputs). It is more flexible than monads composition of Haskell Arrows. Strictly typed message handling in Akka actors (more natural than in Typed actors or Typed Channels). Multi input/multi output functions (multifunctions). Systems process portions of information ASAP. The grid can be the base of real time systems. It is possible to nest subsystems (like matreshkas) creating modular systems. Declarative composition in the form of DataFlow diagram. Easy to use DSL: val a = contact[String](""a"") val b = contact[String](""b"") val c = contact[Char](""c"") a -> b flatMap (.split(""\s+"")) a -> c flatMap (.toCharArray) inputs(a) outputs(b,c)  Dependency injection replacement (accompanied with Scala traits). DataFlow diagram for a system can be created easily — system.toDot(): For details see README in English. Getting started Add a dependency to your build: gradle: compile ['ru.primetalk:synapse-grid-core_2.11:1.4.5', 'ru.primetalk:synapse-grid-akka_2.11:1.4.5'] sbt: libraryDependencies += ""ru.primetalk"" % ""synapse-grid-core_2.11"" % ""1.4.5"" libraryDependencies += ""ru.primetalk"" % ""synapse-grid-akka_2.11"" % ""1.4.5"" (or any other build system: group: ru.primetalk, artifactId: synapse-grid-core, version: 1.4.5) Travis build status PR status: See also (English) Walkthrough. Motivation for SynapseGrid. License (BSD-like). Subsystems. Blog about SynapseGrid Distrubuted systems Typed frames. См. также (See also in Russian) README по-русски. Actors support (in Russian). Лицензия. Работа с подсистемами. Blog about SynapseGrid (en) Распределённые системы. Строго типизированные фреймы. Хабрахабр: Строго типизированное представление неполных данных Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Primetalk/SynapseGrid"	"an FRP framework for constructing reactive real-time immutable data flow systems. It implements an original way of running and organizing event-driven systems based on Petri nets. The topology can be viewed as a.dot graph. The library is compatible with Akka and can seamlessly communicate with other actors."	"true"
"Functional Reactive Programming"	"Vertx.io"	"http://vertx.io/"	"A polyglot reactive application platform for the JVM which aims to be an alternative to node.js. Its concurrency model resembles actors. It supports, Clojure, Java, Javascript, Ruby, Groovy and Python."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"472"	"98"	"542"	"GitHub - vert-x3/vertx-examples Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 98 Star 472 Fork 542 vert-x3/vertx-examples Code Issues 8 Pull requests 1 Pulse Graphs No description or website provided. 649 commits 14 branches 7 releases 38 contributors Java 40.2% JavaScript 19.9% HTML 16.1% Groovy 7.1% Ruby 6.9% CSS 6.1% Other 3.7% Java JavaScript HTML Groovy Ruby CSS Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 3.1.0-SNAPSHOT 3.1.0-staging 3.2.0-SNAPSHOT 3.2.0-staging 3.2.1-SNAPSHOT 3.2.1-staging 3.3.0-SNAPSHOT 3.3.0-staging 3.3.0.CR1 3.3.2 codetrans-config master sync vertx-sync Nothing to show 3.3.2 3.3.0 3.2.1 3.2.0 3.0.0 3.0.0-milestone4 3.0.0-milestone3 Nothing to show New pull request Latest commit f3a65b9 Jul 12, 2016 cescoffier Bump version to 3.3.2 Permalink Failed to load latest commit information. amqp-bridge-examples Bump version to 3.3.2 Jul 10, 2016 camel-bridge-examples Bump version to 3.3.2 Jul 10, 2016 ceylon Cleanup Jun 24, 2016 core-examples Bump version to 3.3.2 Jul 10, 2016 docker-examples Bump version to 3.3.2 Jul 12, 2016 examples-utils Bump version to 3.3.2 Jul 10, 2016 fatjar-examples Bump version to 3.3.2 Jul 12, 2016 gradle-redeploy Bump version to 3.3.2 Jul 12, 2016 gradle-simplest Bump version to 3.3.2 Jul 12, 2016 gradle-verticles Bump version to 3.3.2 Jul 12, 2016 heroku-example Bump version to 3.3.2 Jul 10, 2016 ignite-examples Bump version to 3.3.2 Jul 10, 2016 jca-examples Bump version to 3.3.2 Jul 10, 2016 jdbc-examples Bump version to 3.3.2 Jul 10, 2016 jgroups-examples Bump version to 3.3.2 Jul 10, 2016 kotlin-example Bump version to 3.3.2 Jul 12, 2016 mail-examples Bump version to 3.3.2 Jul 12, 2016 maven-service-factory-examples update version of the verticle to deploy Jul 10, 2016 maven-simplest Bump version to 3.3.2 Jul 10, 2016 maven-verticles Bump version to 3.3.2 Jul 10, 2016 metrics-examples Bump version to 3.3.2 Jul 10, 2016 mongo-examples Bump version to 3.3.2 Jul 10, 2016 openshift2-example Bump version to 3.3.2 Jul 10, 2016 openshift3-examples Bump version to 3.3.2 Jul 12, 2016 osgi-examples Bump version to 3.3.2 Jul 10, 2016 redis-examples Bump version to 3.3.2 Jul 10, 2016 rx-examples Bump version to 3.3.2 Jul 10, 2016 service-proxy-examples Bump version to 3.3.2 Jul 12, 2016 shell-examples Bump version to 3.3.2 Jul 10, 2016 spring-examples Bump version to 3.3.2 Jul 10, 2016 sync-examples Bump version to 3.3.2 Jul 10, 2016 unit-examples Bump version to 3.3.2 Jul 10, 2016 web-examples Bump version to 3.3.2 Jul 12, 2016 .gitignore Remove binaries file Jun 6, 2016 LICENSE.txt Create LICENSE.txt Jun 21, 2016 Procfile Bump version Jun 23, 2016 README.adoc Fix #138 Jul 6, 2016 app.json Added app.json and Procfile to work from root Nov 13, 2015 pom.xml Bump version to 3.3.2 Jul 10, 2016 README.adoc Vert.x examples This repository contains a range of examples so you can get up and running easily with Vert.x. Maven users Start with the simplest Maven example to show you how setup a simple Vert.x project that uses Maven. You need to have Java 8 installed and set the JAVA_HOME environment variable to point to your Java 8 installation. Use that as a template for setting up a Maven project with Vert.x. Progress to the simple Maven verticle example to show you how to deploy your code as verticles in a Maven project. Also check Maven verticles examples to show how you can use Maven with Groovy, Ruby and JavaScript verticles. Want to start right away, open the first example in Eclipse Che: Gradle users Start with the simplest Gradle example to show you how setup a simple Vert.x project that uses Gradle. Use that as a template for setting up a Gradle project with Vert.x. Progress to the simple Gradle verticle example to show you how to deploy your code as verticles in a Gradle project Neither Maven nor Gradle users That’s fine too :) You can run most of the examples at the command line using vertx run if you have Vert.x installed (see below). Some examples need examples-utils You may find examples that need examples-utils. You need to build it from link:examples-utils: git clone https://github.com/vert-x3/vertx-examples.git cd vertx-examples cd examples-utils mvn clean install The examples The examples demonstrate how to use all different bits of Vert.x including Vert.x core, Vert.x-Web and various other services and features. Most of the examples have been written as verticles. For more information on verticles please consult the Vert.x core manual. Using verticles allows you to scale and run your code from the command line more easily, but if you prefer to embed, the exact same code in the verticles can also be run embedded if you prefer. It’s really up to you. Verticles are entirely optional in Vert.x. Different languages Most of the examples are available in all the languages that Vert.x supports. You’ll find the examples for the relevant language in a directory src/main/${lang} where lang is an identifier for the language, e.g. java, js, ruby etc. For example you’ll find the Vert.x core Java examples in core-examples/src/main/java and you’ll find the Vert.x core JavaScript examples in core-examples/src/main/js Note All the non-Java examples are automatically generated from the Java examples using our magic code translator. Running the examples Running in your IDE Most of the Java examples can be run directly in your IDE (if you have one!). We provide a main() method in most of the example classes which allows you to conveniently run them directly in your IDE. Just right click the main method or class in your IDE and run as…​ application (supported by all major IDEs). Running at the command line If vertx is installed you can also run any verticle directly on the command line. This way of working may be of interest to you if you don’t want to use Maven or Gradle or perhaps you don’t use an IDE. Or perhaps you’re just a command line kind of person. If you’re coming from using other platforms such as Node.js you might want to work this way. You can think of the vertx command as a bit like the node command in Node.js. Instructions for installing Vert.x are in the next section. Once Vert.x is installed, to run an example, just cd to the example directory and type vertx run followed by the filename of the verticle. For example cd core-examples/src/main/java/io/vertx/example/core vertx run EchoServer.java  cd core-examples/src/main/js/echo vertx run echo_server.js Yes! You can run Java source files directly using Vert.x (no compilation required). Installing Vert.x Note This is only necessary if you want to run Vert.x at the command line. If you’d prefer to work with Maven or Gradle projects then you don’t need to pre-install Vert.x - you just let Maven/Gradle pull in the Vert.x dependencies into your project as you would with any dependency. Pre-requisites: You will need to have Java 8 JDK installed. Download a Vert.x 3 distribution Unzip it somewhere on your disk (e.g. in your home directory) Set your PATH environment variable to include the installation directory Test the install by typing vertx -version. On the download page, you will find several distributions. Each distribution has a different set of components: Min: The min distribution contains only Vert.x Core, the support of the different languages, and the Hazelcast clustering. Full: The full distribution contains all the components of the Vert.x stack. It includes Vert.x web and the different data access technologies. Vert.x core examples The Vert.x core examples contains a wide range of examples using just Vert.x Core. Vert.x-Web examples Vert.x-Web is a toolkit for building web applications using Vert.x The Vert.x-Web examples contains a wide range of examples using Vert.x-Web Vertx unit examples Vertx-unit is a toolkit for writing asynchronous tests. We include some examples of how to use this tool to test your Vert.x (or other asynchronous) applications. The Vert.x Unit examples shows how to use Vert.x Unit. Mail examples The Vert.x Mail examples contains a few examples using Vert.x Mail The mail examples show different ways to create the mail message and send it via tls, ssl etc. The examples either use localhost:25 to send a mail or use host mail.example.com. To actually run the examples you will have to change the mail server and the user credentials in the MailLogin example. Maven Service Factory examples The Vert.x Maven service factory examples shows how to package a verticle that can be deployed using the Maven Service Factory. Such a deployment is also demonstrated using either the command line or the api. Service Proxy Examples The Vert.x Service Proxy examples contains an example of service proxy usage. It depicts how a service provider can be implemented and how the published service can be consumed. OSGi Examples The Vert.x OSGi examples contains a few examples using Vert.x in an OSGi context. Docker examples The Vert.x Docker examples shows how to deploy Vert.x application in Docker containers. It also contains an example of application that can be deployed on Fabric8. Openshift & Kubernetes examples The Vert.x OpenShift 2 example shows how to deploy Vert.x application to OpenShift 2 following the two proposed approaches: using the DIY cartridge or the vert.x 3 cartridge. The Vert.x OpenShift 3 examples shows how to deploy Vert.x applications on Openshift 3 and Kubernetes. It also demonstrates clustering and service discovery. Spring Examples The Vert.x Spring Examples shows how vert.x application can be integrated inside a Spring ecosystem. Redis example The Vert.x Redis Example shows how you can interact with Redis using the vert.x redis client. Mongo example The Vert.x Mongo Example shows how you can interact with MongoDB using the vert.x mongo client. JDBC example The Vert.x JDBC Examples shows how you can interact JDBC Databases using the vert.x JDBC client. JCA example The Vert.x JCA Example project provides a JEE compliant application that enables to you deploy the application into a [Wildfly](http://wildfly.org) application server. While simple in implementation, the JCA examples provides a good point of departure for your own development. AMQP Bridge examples The Vert.x AMQP Bridge Examples show how you can interact with AMQP 1.0 servers using the Vert.x AMQP Bridge. Camel Bridge examples The Vert.x Camel Bridge Examples show how you can use Apache Camel routes from the event bus. Vert.x fatjar examples The Vert.x fatjar Examples show how you can build fatjar with Maven or Gradle. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/vert-x3/vertx-examples"	"A polyglot reactive application platform for the JVM which aims to be an alternative to node.js. Its concurrency model resembles actors. It supports, Clojure, Java, Javascript, Ruby, Groovy and Python."	"true"
"Functional Reactive Programming"	"Scala"	"http://vertx.io/core_manual_scala.html"	"A polyglot reactive application platform for the JVM which aims to be an alternative to node.js. Its concurrency model resembles actors. It supports, Clojure, Java, Javascript, Ruby, Groovy and Python."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"A polyglot reactive application platform for the JVM which aims to be an alternative to node.js. Its concurrency model resembles actors. It supports, Clojure, Java, Javascript, Ruby, Groovy and Python."	"false"
"Modularization and Dependency Injection"	"Cableguy ★ 0 ⧗ 23"	"https://github.com/akozhemiakin/cableguy"	"Macro based compile time Dependency Injection library."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"0"	"1"	"1"	"GitHub - akozhemiakin/cableguy: Yet another Scala Dependency Injection Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 0 Fork 1 akozhemiakin/cableguy Code Issues 1 Pull requests 0 Pulse Graphs Yet another Scala Dependency Injection Library 16 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 1626b95 May 24, 2016 akozhemiakin Add work in progress stuff until critical issue is resolved Permalink Failed to load latest commit information. core/src Adjust generated services naming and add test for providers Apr 13, 2016 project Add publish info and update the README Apr 13, 2016 .gitignore Initial commit Apr 12, 2016 .travis.yml LICENSE Initial commit Apr 12, 2016 README.md Add work in progress stuff until critical issue is resolved May 24, 2016 build.sbt Add publish info and update the README Apr 12, 2016 cableguy-logo.jpeg Initial commit Apr 12, 2016 README.md EXPERIMENTAL STUFF! WORK IN PROGRESS! Cableguy is yet another Dependency Injection (DI) library for Scala. It is compile time, macros based and runtime-reflection free. Badges Quick facts It is a compile-time dependency injection implementation (I.e. no runtime-reflection involved) It is implemented with a help of Scala macros It allows you to perform deep Just-In-Time resolution It allows you to explicitly provide your dependencies using providers It allows you to use tagged dependencies It allows you to depend either on a singleton (every time the same instance) or a prototype (every time a new instance) It does not allow you (at least yet) to resolve not concrete types (traits, abstract classes) just-in-time. However you can do it with providers. Installation This project is published at Maven Central. However, there is no release version yet. To depend on the SNAPSHOT version use the following sbt snippet: resolvers += Resolver.sonatypeRepo(""snapshots"")  libraryDependencies ++= Seq(   ""ru.arkoit"" %% ""cableguy-core"" % ""0.1.0-SNAPSHOT"" ) For now this project is compatible only with scala 2.11.*. There is no final decision whether it's worth to support scala 2.10.*. Just-In-Time resolution import ru.arkoit.cableguy._  case class A(b: B) case class B()  val a = Resolver().resolve[A]  // eq: A(B()) Explicit providers To declare explicit provider you should extend Provider trait and use method definitions annotated with @provides annotation to provide some service. Then pass all of your providers to the Resolver constructor in a form of Shapeless HList (Do not worry if you do not know what is it, just import shapeless._ and use the following pattern: Resolver(FooProvider :: BarProvider :: HNil)). Obviously, explicit providers have higher priority than Just-In-Time resolution. Provider methods could have dependencies as well import ru.arkoit.cableguy._ import ru.arkoit.cableguy.annotations._ import shapeless._  case class A(b: B, d: D) case class B(label: String) case class C() case class D(num: Int, c: C)  object SomeProvider extends Provider {   @provides   def provideB = B(""Foo"")    @provides   def provideD(c: C) = D(10, c) }  val a = Resolver(SomeProvider :: HNil).resolve[A] // eq: A(B(""Foo""), D(10, C())) Tagged dependencies import ru.arkoit.cableguy._ import ru.arkoit.cableguy.annotations._ import shapeless._  case class A(label: String)  class myTag extends ServiceTag  case class B(@myTag a: A)  object AProvider extends Provider {   @provides   def provideANotTagged = A(""not tagged one"")    @provides @myTag   def provideATagged = A(""tagged one"") }  val result = Resolver(AProvider :: HNil).resolve[B] // eq: B(A(""tagged one"")) Scopes Cableguy supports two scopes of dependency resolution: singleton and prototype. If the dependency is annotated with a singletonScope annotation (or is not scope-annotated at all) the same instance of the dependency will be used across all depended services (which depend on the singleton version of this service). If the dependency is annotated with a prototypeScope annotation, new instance of the dependency will be provided to each dependent service. The following example clearly demonstrates this behavior: val r = scala.util.Random  class B {   val rn = r.nextInt }  class D {   val rn = r.nextInt }  class C(b: B, @prototypeScope d: D) class A(c: C, b: B, @prototypeScope d: D)  val a = Resolver().resolve[A]  a.b.rn == a.c.b.rn // returns true a.d.rn == a.c.d.rn // returns false Other considerations Probably, the best way to use this library is to resolve single, top-level, ""bootstrap"" object to build single dependency tree. That way you'll end up with a minimum amount of macro-generated code and fast compilation time. Known issue with Scala Repl In Scala Repl the Resolver could fail to resolve the requested service due to the issue with Scala Repl. To eliminate it try to perform the Resolver instantiation and the dependency resolution in two separate Scala Repl statement executions, like this: .... val resolver = Resolver(FooProvider :: HNil) hit ""Enter"" val bar = resolver.resolve[Bar] hit ""Enter"" Status Cableguy is a very young project and it is not tested in production yet (at least by its creator :)). Nevertheless it has good test coverage and, because it performs dependency resolution compile-time, most of the possible issues should appear during the compilation phase. Todos First of all, the existing code base (especially macros implementation) should be cleaned up and made less cryptic. Also, it would be useful to introduce some resolution tracing logic to inform the end user why something could not be resolved or why it is resolved that way and not another. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/akozhemiakin/cableguy"	"Macro based compile time Dependency Injection library."	"true"
"Modularization and Dependency Injection"	"Domino ★ 2 ⧗ 87"	"https://github.com/helgoboss/domino"	"Write elegant OSGi bundle activators in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2"	"2"	"14"	"GitHub - helgoboss/domino: OSGi dynamics made easy Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 2 Fork 14 helgoboss/domino forked from domino-osgi/domino Code Pull requests 0 Pulse Graphs OSGi dynamics made easy 80 commits 5 branches 3 releases Fetching contributors Scala 93.8% Shell 6.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bnd-maven-plugin filtercheck gh-pages master travis Nothing to show domino-gh-pages-1.0.0 domino-1.1.0 domino-1.0.0 Nothing to show New pull request Pull request Compare This branch is 21 commits behind domino-osgi:master. Latest commit 662a1bd Aug 5, 2015 lefou Added waffle.io badge Permalink Failed to load latest commit information. .mvn Updated toolchain Jun 29, 2015 src Implemented more service providing tests Aug 5, 2015 .gitignore gitignore May 28, 2015 .travis.yml Added Gitter notification to travis build Aug 5, 2015 FAQ.adoc Migrated and reworked documentation May 28, 2015 LICENSE Renamed to Dominoe Mar 8, 2013 README.adoc Added waffle.io badge Aug 5, 2015 UserGuide.adoc Correcting sample code for creating a new Capsule in User Guide. Jul 29, 2015 makerelease.sh Fixed release script May 28, 2015 mvnw Use maven-wrapper for build May 26, 2015 osgi.bnd Bump version of service_providing package Aug 5, 2015 pom.scala Further improved test coverage; Fixed another type- vs classname issue Aug 5, 2015 README.adoc Domino Domino is a small library for the programming language Scala designed to support developers in writing bundle activators for the Java module system OSGi. It strives to make writing complex and highly-dynamic bundle activators as easy as possible without sacrificing the power of the OSGi API. As such, Domino is a lightweight alternative to OSGi component models like iPOJO, Blueprint and Declarative Services. Especially for those who want to leverage the power of pure code instead of reverting to an XML- or annotation-based approach. Table of Contents Features Getting started Wait for service and register service Listen for configuration updates Download Documentation Development Contribute Building Domino Creating a Release Credits License Changelog Domino {master} Domino 1.1.0 - 2015-05-28 Domino 1.0.0 - 2013-03-31 Features Expressive Domino offers an expressive DSL which reflects the event-driven nature of OSGi and allows you to define very easily when your logic is made available and when it is revoked. Unrestrictive Some libraries just cover the most common use cases and let you fall deeply whenever you need to do something more special. Then you suddenly find yourself bypassing the library completely. Domino tries to prevent this by staying close to the low-level OSGi API. Type safe Most features in Domino benefit from static typing. That means the compiler and your IDE can help you write correct code. Additionally, there’s support for using generic type parameters in the service registry. Familiar Instead of inventing a completely foreign DSL, Domino tries to use familiar Scala data types such as Option, List etc. whenever possible so you can make use of those many nice methods like filter or map you probably fell in love with. Extensible If the Domino core DSL is not enough for you, simply extend it. Do you want to run a job as long as a certain service is available? Such things can be easily integrated. Comprehensive Many of OSGi’s core features are natively supported by the DSL (services, configuration, bundles, meta types). Getting started Some lines of code often say more than 1000 words. Wait for service and register service import domino.DominoActivator import org.osgi.service.http.HttpService  class MyService(httpService: HttpService)  class Activator extends DominoActivator {   whenBundleActive {     // Make service available as long as another     // service is present     whenServicePresent[HttpService] { httpService =>       val myService = new MyService(httpService)       myService.providesService[MyService]     }   } } Listen for configuration updates import domino.DominoActivator  class KeyService(key: String)  class Activator extends DominoActivator {   whenBundleActive {     // Reregister service whenever configuration changes     whenConfigurationActive(""my_service"") { conf =>       val key = conf.getOrElse(""key"", ""defaultKey"")       new KeyService(key).providesService[KeyService]     }   } } Read more in the complete Domino User Guide. Download The latest stable version is 1.1.0 and can be downloaded from Maven Central. Maven <dependency>   <groupId>com.github.domino-osgi</groupId>   <artifactId>domino_${scala.version}</artifactId>   <version>1.1.0</version> </dependency> SBT ""com.github.domino-osgi"" %% ""domino"" % ""1.1.0"" Gradle compile 'com.github.domino-osgi:domino_${scala.version}:1.1.0' Manual Scala 2.10 domino-1.1.0.jar, domino-1.1.0-sources.jar, domino-1.1.0-scaladoc.jar Scala 2.11 domino-1.1.0.jar, domino-1.1.0-sources.jar, domino-1.1.0-scaladoc.jar Documentation User Guide Scaladoc FAQ Gitter chat Development Contribute If you want to report a bug or suggest a feature, please do it in the GitHub issues section. If you want to provide a fix or improvement, please fork Domino and send us a pull request on GitHub. Thank you! If you want to give general feedback, please do it in the Gitter chat. If you want to show appreciation for the project, please ""star"" it on GitHub. That helps us setting our priorities. Building Domino Domino is build with Apache Maven and the Polygot Scala Extension. At least Maven 3.3.1 is required. To cleanly build domino, use: mvn clean package To build domino for another Scala version, e.g. 2.10.5 (under a Unix-like OS), use: SCALA_VERSION=2.10.5 mvn clean package Creating a Release Bump version in pom file Update Changelog Review documentation Create a git tag with the version Upload the release artifacts up to Maven Central Deploy to Maven Central / Sonatype Open Source Respository (OSSRH) Unfortunately, not all Maven plugins are ready yet for a Polyglot Maven setup, thus the current version 1.6.3 of the Maven Staging Plugin simply doen’t work. To deploy a release to , use the shell script makerelase.sh. Please review the variables in the script, namely the DOMINO_VERSION and the SCALA_VERSIONS. When executed the script will create a staging-settings.xml (to which you should add your login credentials) and wait. After pressing enter it will build all artifatcs and upload them to the OSSRH Nexus where you must log-in and manually release these artifacts. Credits Thanks to …​ helgoboss for creating Domino 1.0.0 ScalaModules for being an inspiration, in particular for the bundle and service watching functionality Nyenyec for creating the image from which the Domino logo is derived License Domino is licensed under the MIT License. Changelog Domino {master} Removed Logging trait from DominoActivator. You can restore the old behavior be mixing in the trait into your activator class. Improved test suite. Now we use PojoSR to test most OSGi dynamics without the need to run a separate container. Fixed naming issues for service provisioning and comsumption. Domino 1.1.0 - 2015-05-28 Switched Maintainer to Tobias Roeser Renamed base package from org.helgoboss.domino to domino Embedded former dependencies (org.helgoboss.capsule, org.helgoboss.scala-osgi-metatype, org.helgoboss.scala-logging) as sub packages Switched to Polyglot Scala extension for Maven 3.3 Cross-Release for Scala 2.10 and 2.11 Domino 1.0.0 - 2013-03-31 Initial release for Scala 2.10 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/helgoboss/domino"	"Write elegant OSGi bundle activators in Scala."	"true"
"Modularization and Dependency Injection"	"MacWire ★ 466 ⧗ 0"	"https://github.com/adamw/macwire"	"Scala Macro to generate wiring code for class instantiation. DI container replacement."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"515"	"32"	"35"	"GitHub - adamw/macwire: Lightweight and Nonintrusive Scala Dependency Injection Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 32 Star 515 Fork 35 adamw/macwire Code Issues 14 Pull requests 0 Pulse Graphs Lightweight and Nonintrusive Scala Dependency Injection Library http://www.softwaremill.com 333 commits 5 branches 26 releases 15 contributors Scala 97.1% Java 2.9% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bug-82-2 bug-82 master revert-96-zunder-patch-readme-discrepancy scala-2.10 Nothing to show release-2.2.3 release-2.2.2 release-2.2.1 release-2.2.0 release-2.1.0 release-2.0.0 release-1.0.7 release-1.0.6 release-1.0.5 release-1.0.4 release-1.0.3 release-1.0.2 release-1.0.1 release-1.0.0 release-0.8.0 release-0.7.3 release-0.7.2 release-0.7.1 release-0.7 release-0.6 release-0.5 release-0.4.1 release-0.4 release-0.3 release-0.2 release-0.1 Nothing to show New pull request Latest commit e9eeecc May 19, 2016 adamw Merge pull request #99 from agulowaty/master … fixup README formatting Permalink Failed to load latest commit information. examples Use Markdown for example README. Dec 31, 2015 macros/src Removing clippy.xml Jan 14, 2016 notes Moving large comment to notes Mar 28, 2013 project 2.2.3 release May 17, 2016 proxy/src Splitting macros module into ""macros"" and ""util"". Renaming ""runtime"" … Sep 2, 2015 test-util/src/main 2.2.0 release: moving tagging to a separate project Nov 25, 2015 tests/src/test Add support for aliased self-types Dec 9, 2015 tests2/src/test/scala/com/softwaremill/macwire Splitting macros module into ""macros"" and ""util"". Renaming ""runtime"" … Sep 2, 2015 util-tests/src/test 2.2.0 release: moving tagging to a separate project Nov 25, 2015 util/src/main/scala/com/softwaremill/macwire 2.2.0 release: moving tagging to a separate project Nov 25, 2015 .gitignore Initial commit Mar 17, 2013 .travis.yml 2.2.3 release May 17, 2016 LICENSE.txt Update LICENSE.txt May 27, 2014 README.md fixup README formatting May 19, 2016 README.md Table of Contents Introduction Guide to DI in Scala (external link) How wiring works Factories Factory methods lazy val vs. val Composing modules Scopes Accessing wired instances dynamically Interceptors Qualifiers Limitations Installation, using with SBT Debugging Scala.js Future development - vote! Activators Migrating from 1.x Play 2.4.x MacWire MacWire generates new instance creation code of given classes, using values in the enclosing type for constructor parameters, with the help of Scala Macros. For a general introduction to DI in Scala, take a look at the Guide to DI in Scala, which also features MacWire. MacWire helps to implement the Dependency Injection (DI) pattern, by removing the need to write the class-wiring code by hand. Instead, it is enough to declare which classes should be wired, and how the instances should be accessed (see Scopes). Classes to be wired should be organized in ""modules"", which can be Scala traits, classes or objects. Multiple modules can be combined using inheritance or composition; values from the inherited/nested modules are also used for wiring. MacWire can be in many cases a replacement for DI containers, offering greater control on when and how classes are instantiated, typesafety and using only language (Scala) mechanisms. Example usage: class DatabaseAccess() class SecurityFilter() class UserFinder(databaseAccess: DatabaseAccess, securityFilter: SecurityFilter) class UserStatusReader(userFinder: UserFinder)  trait UserModule {     import com.softwaremill.macwire._      lazy val theDatabaseAccess   = wire[DatabaseAccess]     lazy val theSecurityFilter   = wire[SecurityFilter]     lazy val theUserFinder       = wire[UserFinder]     lazy val theUserStatusReader = wire[UserStatusReader] } will generate: trait UserModule {     lazy val theDatabaseAccess   = new DatabaseAccess()     lazy val theSecurityFilter   = new SecurityFilter()     lazy val theUserFinder       = new UserFinder(theDatabaseAccess, theSecurityFilter)     lazy val theUserStatusReader = new UserStatusReader(theUserFinder) } For testing, just extend the base module and override any dependencies with mocks/stubs etc, e.g.: trait UserModuleForTests extends UserModule {     override lazy val theDatabaseAccess = mockDatabaseAccess     override lazy val theSecurityFilter = mockSecurityFilter } The core library has no dependencies. For more motivation behind the project see also these blogs: Dependency injection with Scala macros: auto-wiring MacWire 0.1: Framework-less Dependency Injection with Scala Macros MacWire 0.2: Scopes are simple! Implementing factories in Scala & MacWire 0.3 Dependency Injection in Play! with MacWire MacWire 0.5: Interceptors Using Scala traits as modules, or the ""Thin Cake"" Pattern You can also try MacWire through Typesafe Activator. A similar project for Java is Dagger. How wiring works For each constructor parameter of the given class, MacWire tries to find a value conforming to the parameter's type in the enclosing method and trait/class/object: first it tries to find a unique value declared as a value in the current block, argument of enclosing methods and anonymous functions. then it tries to find a unique value declared or imported in the enclosing type then it tries to find a unique value in parent types (traits/classes) if the parameter is marked as implicit, it is ignored by MacWire and handled by the normal implicit resolution mechanism Here value means either a val or a no-parameter def, as long as the return type matches. A compile-time error occurs if: there are multiple values of a given type declared in the enclosing block/method/function's arguments list, enclosing type or its parents. parameter is marked as implicit and implicit lookup fails to find a value there is no value of a given type The generated code is then once again type-checked by the Scala compiler. Factories A factory is simply a method. The constructor of the wired class can contain parameters both from the factory (method) parameters, and from the enclosing/super type(s). For example: class DatabaseAccess() class TaxDeductionLibrary(databaseAccess: DatabaseAccess) class TaxCalculator(taxBase: Double, taxDeductionLibrary: TaxDeductionLibrary)  trait TaxModule {     import com.softwaremill.macwire._      lazy val theDatabaseAccess      = wire[DatabaseAccess]     lazy val theTaxDeductionLibrary = wire[TaxDeductionLibrary]     def taxCalculator(taxBase: Double) = wire[TaxCalculator]     // or: lazy val taxCalculator = (taxBase: Double) => wire[TaxCalculator] } will generate: trait TaxModule {     lazy val theDatabaseAccess      = new DatabaseAccess()     lazy val theTaxDeductionLibrary = new TaxDeductionLibrary(theDatabaseAccess)     def taxCalculator(taxBase: Double) =        new TaxCalculator(taxBase, theTaxDeductionLibrary) } Factory methods You can also wire an object using a factory method, instead of a constructor. For that, use wireWith instead of wire. For example: class A()  class C(a: A, specialValue: Int) object C {   def create(a: A) = new C(a, 42) }  trait MyModule {   lazy val a = wire[A]   lazy val c = wireWith(C.create _) } lazy val vs. val It is safer to use lazy vals, as when using val, if a value is forward-referenced, it's value during initialization will be null. With lazy val the correct order of initialization is resolved by Scala. Composing modules Modules (traits or classes containing parts of the object graph) can be combined using inheritance or composition. The inheritance case is straightforward, as wire simply looks for values in parent traits/classes. With composition, you need to tell MacWire that it should look inside the nested modules. To do that, you can use imports: class FacebookAccess(userFind: UserFinder)   class UserModule {    lazy val userFinder = ... // as before  }   class SocialModule(userModule: UserModule) {   import userModule._    lazy val facebookAccess = wire[FacebookAccess]  } Or, if you are using that pattern a lot, you can annotate your modules using @Module, and they will be used when searching for values automatically: class FacebookAccess(userFind: UserFinder)   @Module class UserModule { ... } // as before  class SocialModule(userModule: UserModule) {   lazy val facebookAccess = wire[FacebookAccess]  } Warning: the @Module annotation is an experimental feature, if you have any feedback regarding its usage, let us know! Scopes There are two ""built-in"" scopes, depending on how the dependency is defined: singleton: lazy val / val dependent - separate instance for each dependency usage: def MacWire also supports user-defined scopes, which can be used to implement request or session scopes in web applications. The proxy subproject defines a Scope trait, which has two methods: apply, to create a scoped value get, to get or create the current value from the scope To define a dependency as scoped, we need a scope instance, e.g.: trait WebModule {    lazy val loggedInUser = session(new LoggedInUser)     def session: Scope } With abstract scopes as above, it is possible to use no-op scopes for testing (NoOpScope). There's an implementation of Scope targeted at classical synchronous frameworks, ThreadLocalScope. The apply method of this scope creates a proxy (using javassist); the get method stores the value in a thread local. The proxy should be defined as a val or lazy val. In a web application, the scopes have to be associated and disassociated with storages. This can be done for example in a servlet filter. To implement a: request scope, we need a new empty storage for every request. The associateWithEmptyStorage is useful here session scope, the storage (a Map) should be stored in the HttpSession. The associate(Map) method is useful here For example usage see the MacWire+Scalatra example sources. You can run the example with sbt examples-scalatra/run and going to http://localhost:8080. Note that the proxy subproject does not depend on MacWire core, and can be used stand-alone with manual wiring or any other frameworks. Accessing wired instances dynamically To integrate with some frameworks (e.g. Play 2) or to create instances of classes which names are only known at run-time (e.g. plugins) it is necessary to access the wired instances dynamically. MacWire contains a utility class in the util subproject, Wired, to support such functionality. An instance of Wired can be obtained using the wiredInModule macro, given an instance of a module containing the wired object graph. Any vals, lazy vals and parameter-less defs (factories) from the module which are references will be available in the Wired instance. The object graph in the module can be hand-wired, wired using wire, or a result of any computation. Wired has two basic functionalities: looking up an instance by its class (or trait it implements), and instantiating new objects using the available dependencies. You can also extend Wired with new instances/instance factories. For example: // 1. Defining the object graph and the module trait DatabaseConnector class MysqlDatabaseConnector extends DatabaseConnector  class MyApp {     def securityFilter = new SecurityFilter()     val databaseConnector = new MysqlDatabaseConnector() }  // 2. Creating a Wired instance import com.softwaremill.macwire._ val wired = wiredInModule(new MyApp)  // 3. Dynamic lookup of instances wired.lookup(classOf[SecurityFilter])  // Returns the mysql database connector, even though its type is MysqlDatabaseConnector, which is  // assignable to DatabaseConnector. wired.lookup(classOf[DatabaseConnector])  // 4. Instantiation using the available dependencies {     package com.softwaremill     class AuthenticationPlugin(databaseConnector: DatabaseConnector) }  // Creates a new instance of the given class using the dependencies available in MyApp wired.wireClassInstanceByName(""com.softwaremill.AuthenticationPlugin"") Interceptors MacWire contains an implementation of interceptors, which can be applied to class instances in the modules. Similarly to scopes, the proxy subproject defines an Interceptor trait, which has only one method: apply. When applied to an instance, it should return an instance of the same class, but with the interceptor applied. There are two implementations of the Interceptor trait provided: NoOpInterceptor: returns the given instance without changes ProxyingInterceptor: proxies the instance, and returns the proxy. A provided function is called with information on the invocation Interceptors can be abstract in modules. E.g.: trait BusinessLogicModule {    lazy val moneyTransferer = transactional(wire[MoneyTransferer])     def transactional: Interceptor } During tests, you can then use the NoOpInterceptor. In production code or integration tests, you can specify a real interceptor, either by extending the ProxyingInterceptor trait, or by passing a function to the ProxyingInterceptor object: object MyApplication extends BusinessLogicModule {     lazy val tm = wire[TransactionManager]      lazy val transactional = ProxyingInterceptor { ctx =>         try {             tm.begin()             val result = ctx.proceed()             tm.commit()              result         } catch {             case e: Exception => tm.rollback()         }     } } The ctx is an instance of an InvocationContext, and contains information on the parameters passed to the method, the method itself, and the target object. It also allows to proceed with the invocation with the same or changed parameters. For more general AOP, e.g. if you want to apply an interceptor to all methods matching a given pointcut expression, you should use AspectJ or an equivalent library. The interceptors that are implemented in MacWire correspond to annotation-based interceptors in Java. Qualifiers Sometimes you have multiple objects of the same type that you want to use during wiring. Macwire needs to have some way of telling the instances apart. As with other things, the answer is: types! Even when not using wire, it may be useful to give the instances distinct types, to get compile-time checking. For that purpose Macwire includes support for tagging via scala-common, which lets you attach tags to instances to qualify them. This is a compile-time only operation, and doesn't affect the runtime. The tags are derived from Miles Sabin's gist. To bring the tagging into scope, import com.softwaremill.tagging._. Using tagging has two sides. In the constructor, when declaring a dependency, you need to declare what tag it needs to have. You can do this with the _ @@ _ type constructor, or if you prefer another syntax Tagged[_, _]. The first type parameter is the type of the dependency, the second is a tag. The tag can be any type, but usually it is just an empty marker trait. When defining the available instances, you need to specify which instance has which tag. This can be done with the taggedWith[_] method, which returns a tagged instance (A.taggedWith[T]: A @@ T). Tagged instances can be used as regular ones, without any constraints. The wire macro does not contain any special support for tagging, everything is handled by subtyping. For example: class Berry() trait Black trait Blue  case class Basket(blueberry: Berry @@ Blue, blackberry: Berry @@ Black)  lazy val blueberry = wire[Berry].taggedWith[Blue] lazy val blackberry = wire[Berry].taggedWith[Black] lazy val basket = wire[Basket] Multiple tags can be combined using the andTaggedWith method. E.g. if we had a berry that is both blue and black: lazy val blackblueberry = wire[Berry].taggedWith[Black].andTaggedWith[Blue] The resulting value has type Berry @ (Black with Blue) and can be used both as a blackberry and as a blueberry. Limitations When: referencing wired values within the trait/class/object using multiple modules in the same compilation unit using multiple modules with scopes due to limitations of the current macros implementation in Scala (for more details see this discussion) to avoid compilation errors it is recommended to add type ascriptions to the dependencies. This is a way of helping the type-checker that is invoked by the macro to figure out the types of the values which can be wired. For example: class A() class B(a: A)  // note the explicit type. Without it wiring would fail with recursive type compile errors lazy val theA: A = wire[A] // reference to theA; if for some reason we need explicitly write the constructor call lazy val theB = new B(theA) This is an inconvenience, but hopefully will get resolved once post-typer macros are introduced to the language. Also, wiring will probably not work properly for traits and classes defined inside the containing trait/class, or in super traits/classes. Note that the type ascription may be a subtype of the wired type. This can be useful if you want to expose e.g. a trait that the wired class extends, instead of the full implementation. Installation, using with SBT The jars are deployed to Sonatype's OSS repository. To use MacWire in your project, add a dependency: libraryDependencies += ""com.softwaremill.macwire"" %% ""macros"" % ""2.2.3"" % ""provided""  libraryDependencies += ""com.softwaremill.macwire"" %% ""util"" % ""2.2.3""  libraryDependencies += ""com.softwaremill.macwire"" %% ""proxy"" % ""2.2.3"" The macros subproject contains only code which is used at compile-time, hence the provided scope. The util subproject contains tagging, Wired and the @Module annotation; if you don't use these features, you don't need to include this dependency. The proxy subproject contains interceptors and scopes, and has a dependency on javassist. To use the snapshot version: resolvers += ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots""  libraryDependencies += ""com.softwaremill.macwire"" %% ""macros"" % ""2.2.4-SNAPSHOT"" % ""provided""  libraryDependencies += ""com.softwaremill.macwire"" %% ""util"" % ""2.2.4-SNAPSHOT"" Currently 2.x supports only Scala 2.11. Older 1.x release for Scala 2.10 and 2.11: libraryDependencies += ""com.softwaremill.macwire"" %% ""macros"" % ""1.0.7""  libraryDependencies += ""com.softwaremill.macwire"" %% ""runtime"" % ""1.0.7"" Debugging To print debugging information on what MacWire does when looking for values, and what code is generated, set the macwire.debug system property. E.g. with SBT, just add a System.setProperty(""macwire.debug"", """") line to your build file. Scala.js Macwire also works with Scala.js. For an example, see here: Macwire+Scala.js example. Future development - vote! Take a look at the available issues. If you'd like to see one developed please vote on it. Or maybe you'll attempt to create a pull request? Activators There are two Typesafe Activators which can help you to get started with Scala, Dependency Injection and Macwire: No-framework Dependency Injection with MacWire and Akka Activator No-framework Dependency Injection with MacWire and Play Activator Migrating from 1.x changed how code is split across modules. You'll need to depend on util to get tagging & Wired, and proxy to get interceptors and scopes tagging moved to a separate package. If you use tagging, you'll need to import com.softwaremill.tagging._ removed wireImplicit implicit parameters aren't handled by wire at all (they used to be subject to the same lookup procedure as normal parameters + implicit lookup) Play 2.4.x In Play 2.4.x, you can no longer use getControllerInstance in GlobalSettings for injection. Play has a new pattern for injecting controllers. You must extend ApplicationLoader, from there you can mix in your modules. import controllers.{Application, Assets} import play.api.ApplicationLoader.Context import play.api._ import play.api.routing.Router import router.Routes import com.softwaremill.macwire._  class AppApplicationLoader extends ApplicationLoader {   def load(context: Context) = {      // make sure logging is configured     Logger.configure(context.environment)      (new BuiltInComponentsFromContext(context) with AppComponents).application   } }  trait AppComponents extends BuiltInComponents with AppModule {   lazy val assets: Assets = wire[Assets]   lazy val prefix: String = ""/""   lazy val router: Router = wire[Routes] }  trait AppModule {   // Define your dependencies and controllers   lazy val applicationController = wire[Application] } In application.conf, add the reference to the ApplicationLoader. play.application.loader = ""AppApplicationLoader""  For more information and to see the sample project, go to examples/play24 Reference Play docs for more information: ScalaCompileTimeDependencyInjection Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/adamw/macwire"	"Scala Macro to generate wiring code for class instantiation. DI container replacement."	"true"
"Modularization and Dependency Injection"	"Scala-Guice ★ 146 ⧗ 0"	"https://github.com/codingwell/scala-guice"	"Scala extensions for Google Guice"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"167"	"14"	"29"	"GitHub - codingwell/scala-guice: Scala extensions for Google Guice Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 167 Fork 29 codingwell/scala-guice forked from benlings/scala-guice Code Issues 11 Pull requests 0 Wiki Pulse Graphs Scala extensions for Google Guice 173 commits 3 branches 10 releases 8 contributors Scala 99.7% Java 0.3% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags develop master support/v3 Nothing to show v4.0.1 v4.0.0 v4.0.0-beta5 v4.0.0-beta4 v4.0.0-beta v3.0.3 v3.0.2 v3.0.1 legacy-v3 legacy-v2 Nothing to show New pull request Pull request Compare This branch is 142 commits ahead, 1 commit behind benlings:master. Latest commit 7ec1119 May 17, 2016 tsuckow Merge pull request #54 from mslinn/master … Update README.md Permalink Failed to load latest commit information. project Update sbt version to 0.13.6 Dec 1, 2014 src Manifest is not needed on the existingBinding(Key) method Nov 11, 2015 .gitignore Add 'idea' to gitignore May 22, 2014 .travis.yml Update travis configuration May 23, 2014 LICENSE Update Copyright Dates. May 7, 2014 NOTICE Update Copyright Dates. May 7, 2014 README.md Update README.md May 17, 2016 build.sbt Bump version for 4.0.1 release Nov 24, 2015 README.md Scala extensions for Google Guice 4.0 Master: Develop: Please submit pull requests against the develop branch. Note: Multibinding behaviour had changed in beta4, duplicates are now ignored instead of throwing an exception. Getting Started Add dependency We currently support Scala 2.10, 2.11 maven: <dependency>     <groupId>net.codingwell</groupId>     <artifactId>scala-guice_2.10</artifactId>     <version>4.0.1</version> </dependency> sbt: ""net.codingwell"" %% ""scala-guice"" % ""4.0.1"" gradle: 'net.codingwell:scala-guice_2.10:4.0.1' Mixin Mixin ScalaModule with your AbstractModule for rich scala magic (or ScalaPrivateModule with your PrivateModule): class MyModule extends AbstractModule with ScalaModule {   def configure(): Unit = {     bind[Service].to[ServiceImpl].in[Singleton]     bind[CreditCardPaymentService]     bind[Bar[Foo]].to[FooBarImpl]     bind[PaymentService].annotatedWith(Names.named(""paypal"")).to[CreditCardPaymentService]   } }  class MyPrivateModule extends PrivateModule with ScalaPrivateModule {   def configure(): Unit = {     bind[Foo].to[RealFoo]     expose[Foo]      install(new TransactionalBarModule())     expose[Bar].annotatedWith[Transactional]      bind[SomeImplementationDetail]     install(new MoreImplementationDetailsModule())   } } Inject Wrap the injector in a ScalaInjector for even more rich scala magic: object MyServer {   def main(args: Array[String]) {     val injector = Guice.createInjector(new MyModule(), new MyPrivateModule)      import net.codingwell.scalaguice.InjectorExtensions._     val service = injector.instance[Service]     val foo = injector.instance[Foo]      // Retrieve a Bar annotated with Transactional     val bar = injector.instance[Bar, Transactional]      // Retrieve a PaymentService annotated with a specific Annotation instance.     val paymentService = injector.instance[PaymentService](Names.named(""paypal""))     ...   } } Additional Features Module Traits class MyModule extends AbstractModule with ScalaModule class MyPrivateModule extends PrivateModule with ScalaPrivateModule This gives to access to scala style bindings: bind[A].to[B] bind[A].to(classOf[B]) bind[A].to(typeLiteral[B]) bind[A].toInstance(""A"") bind[A].annotatedWith[Ann].to[B] bind[A].annotatedWith( Names.named(""name"") ).to[B] bind[A].annotatedWithName(""name"").to[B] bind[A].toProvider[BProvider] bind[A].toProvider[TypeProvider[B]] bind[A[String]].to[B[String]] bind[A].to[B].in[Singleton]  bindInterceptor[AOPI](methodMatcher = annotatedWith[AOP]) Multibinding The ScalaMultibinder adds scala style multibindings: class MyModule extends AbstractModule with ScalaModule {   def configure(): Unit = {     val stringMulti = ScalaMultibinder.newSetBinder[String](binder)     stringMulti.addBinding.toInstance(""A"")      val annotatedMulti = ScalaMultibinder.newSetBinder[A, Annotation](binder)     annotatedMulti.addBinding.to[A]      val namedMulti = ScalaMultibinder.newSetBinder[ServiceConfiguration](binder, Names.named(""backend""))     namedMulti.addBinding.toInstance(config.getAdminServiceConfiguration)   } } And then they may be retrieved as immutable.Set[T]. (examples in order) class StringThing @Inject() (strings: immutable.Set[String]) { ... }  class AThing @Inject() (@Annotation configs: immutable.Set[A]) { ... }  class Service @Inject() (@Names.named(""backend"") configs: immutable.Set[ServiceConfiguration]) { ... } OptionBinding Newly available in Guice 4.0-beta5, we've got some support for OptionalBinder. class MyModule extends AbstractModule with ScalaModule {   def configure(): Unit = {     val optBinder = ScalaOptionBinder.newOptionBinder[String](binder)     optBinder.setDefault.toInstance(""A"")     // To override the default binding (likely in another module):     optBinder.setBinding.toInstance(""B"")      val annotatedOptBinder = ScalaOptionBinder.newOptionBinder[A, Annotation](binder)     annotatedOptBinder.setDefault.to[A]      val namedOptBinder = ScalaOptionBinder.newOptionBinder[ServiceConfiguration](binder, Names.named(""backend""))     namedOptBinder.setBinding.toInstance(config.getAdminServiceConfiguration)   } } And then they may be retrieved as Option[T], Option[Provider[T]], and Option[javax.inject.Provider[T]]. (examples in order) class StringThing @Inject() (name: Option[String]) { ... }  class AThing @Inject() (@Annotation aProvider: Option[Provider[T]]) { ... }  class Service @Inject() (@Names.named(""backend"") configProvider: Option[javax.inject.Provider[ServiceConfiguration]]) { ... } MapBinding The ScalaMapBinder adds scala style mapbindings: class MyModule extends AbstractModule with ScalaModule {   def configure(): Unit = {     val mBinder = ScalaMapBinder.newMapBinder[String, Int](binder)     mBinder.addBinding(""1"").toInstance(1)   } } And then may be retrieved as any of the following: immutable.Map[K, V] immutable.Map[K, Provider[V]] immutable.Map[K, javax.inject.Provider[V]] If you call mapBinder.permitDuplicates() on the binder then you may also inject: immutable.Map[K, immutable.Set[V]] immutable.Map[K, immutable.Set[Provider[V]]] Interceptor Binding bindInterceptor adds scala style interceptor binding bindInterceptor(Matchers.any(), Matchers.annotatedWith(classOf[Logging]), new LoggingInterceptor()) bindInterceptor[LoggingInterceptor](methodMatcher = annotatedWith[Logging]) Gotchas In Scala, the words override and with are reserved and must be escaped to be used. Modules.`override`(new BaseModule).`with`(new TestModule) And the stuff we forgot... If you find a feature we support but don't mention here, submit an issue and we will add it. If you find a feature we don't support but want, implement it and send us a pull request. Alternatively, you can file an issue and we may or may not get to it. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/codingwell/scala-guice"	"Scala extensions for Google Guice"	"true"
"Modularization and Dependency Injection"	"Scaldi ★ 206 ⧗ 23"	"https://github.com/scaldi/scaldi"	"Lightweight Scala Dependency Injection Library."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"216"	"14"	"15"	"GitHub - scaldi/scaldi: Lightweight Scala Dependency Injection Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 216 Fork 15 scaldi/scaldi Code Issues 12 Pull requests 1 Pulse Graphs Lightweight Scala Dependency Injection Library http://scaldi.org 222 commits 3 branches 17 releases 7 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags coverage gh-pages master Nothing to show v0.5.7 v0.5.6 v0.5.5 v0.5.4 v0.5.3 v0.5.2 v0.5.1 v0.5 v0.4 v0.3.2 v0.3.1 v0.3 v0.2 v0.1.2 v0.1.1 before-cleanup 0.1 Nothing to show New pull request Latest commit d93308b Feb 20, 2016 OlegIlyenko Merge pull request #65 from Mironor/injector-scaladoc … Scaladoc for Injector.scala file Permalink Failed to load latest commit information. project `bindings` is not really necessary anymore Aug 18, 2015 src Added a note on scoped binding, and fixed a minor typo Feb 19, 2016 .gitignore Initial commit May 17, 2010 .travis.yml Updated scala version for travis Apr 29, 2015 CHANGELOG.md Changelog Dec 11, 2015 LICENSE renamed license Nov 9, 2013 README.md Release v0.5.7 Dec 11, 2015 build.sbt next dev iteration Dec 11, 2015 README.md Scaldi provides simple and elegant way to do dependency injection in Scala. By using expressive power of the Scala language it defines intuitive and idiomatic DSL for binding and injecting dependencies. It is very extensible library, so you can easily extend or customise almost any aspect of it. Some of the more unique Scaldi features are advanced module composition and conditional bindings which definitely will help you build all kinds of applications - from small command-line tools to big non-trivial web applications. Not to mention Scaldi also nicely integrates with Akka and Play. You can find project's home page here: http://scaldi.org Or jump directly to the documentation: Scaldi documentation Adding Scaldi in Your Build SBT Configuration: libraryDependencies += ""org.scaldi"" %% ""scaldi"" % ""0.5.7""  License Scaldi is licensed under Apache License, Version 2.0. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scaldi/scaldi"	"Lightweight Scala Dependency Injection Library."	"true"
"Modularization and Dependency Injection"	"Sclasner ★ 9 ⧗ 135"	"https://github.com/xitrum-framework/sclasner"	"Scala classpath scanner."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"9"	"3"	"0"	"GitHub - xitrum-framework/sclasner: Scala classpath scanner Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 9 Fork 0 xitrum-framework/sclasner Code Issues 0 Pull requests 0 Pulse Graphs Scala classpath scanner 63 commits 2 branches 2 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 1.7.0 1.6 Nothing to show New pull request Latest commit 83896e8 Aug 14, 2015 ngocdaothanh Update sbteclipse-plugin from 2.5.0 to 4.0.0 Permalink Failed to load latest commit information. dev Replace URLs to point from ngocdaothanh to xitrum-framework May 15, 2014 project Update sbteclipse-plugin from 2.5.0 to 4.0.0 Aug 14, 2015 src/main/scala/sclasner Fix #2 Add Scanner#foldLeft with File argument May 30, 2015 .gitignore Fix dev Dec 4, 2012 MIT-LICENSE First commit Sep 10, 2011 README.rst Fix #2 Add Scanner#foldLeft with File argument May 30, 2015 build.sbt Improve build.sbt Aug 14, 2015 README.rst Sclasner is a classpath scanner written in Scala. It is intended as a replacement of Annovention and mainly used for standalone JVM applications. If you want a more complex solution, please see Reflections. With Sclasner, you can: Scan all .class files (including those inside .jar files in classpath), then use Javassist or ASM to extract annotations Load all .po files etc. Scan See Scaladoc. For example, if you want to load all .txt files: import java.io.File import sclasner.{FileEntry, Scanner}  // We define a callback to process each FileEntry: // - The 1st argument is an accumulator to gather process results for each entry. // - The 2nd argument is each entry. // - The result of this callback will be passed to as the accumulator (the //   1st argument) to the next call. // - When all entries have been visited, the accumulator will be returned. def entryProcessor(acc: Seq[(String, String)], entry: FileEntry): Seq[(String, String)] = {   if (entry.relPath.endsWith("".txt"")) {     val fileName = entry.relPath.split(File.pathSeparator).last     val body     = new String(entry.bytes)     acc :+ (fileName, body)   } else {     acc   } }  // We actually do the scan: // - The 1st argument is the initial value of the accumulator. // - The 2nd argument is the callback above. val acc = Scanner.foldLeft(Seq.empty, entryProcessor)  Things in FileEntry: container: File, may be a directory or a JAR file in classpath. You may call container.isDirectory or container.isFile. Inside each container, there may be multiple items, represented by the two below. relPath: String, path to the file you want to check, relative to the container above. bytes: Array[Byte], body of the file the relPath above points to. This is a lazy val, accessing the first time will actually read the file from disk. Because reading from disk is slow, you should avoid accessing bytes if you don't have to. Signature of Scanner.foldLeft: foldLeft[T](acc: T, entryProcessor: (T, FileEntry) => T): T  Cache One scan may take 10-15 seconds, depending things in your classpath and your computer spec etc. Fortunately, because things in classpath do not change frequently, you may cache the result to a file and load it later. You provide the cache file name to foldLeft: // You can use File instead of file name val acc = Scanner.foldLeft(""sclasner.cache"", Seq.empty, entryProcessor)  If sclasner.cache exists, entryProcessor will not be run. Otherwise, entryProcessor will be run and the result will be serialized to the file. If you want to force entryProcessor to run, just delete the cache file. If the cache file cannot be successfully deserialized (for example, serialized classes are older than the current version of the classes), it will be automatically deleted and updated (entryProcessor will be run). For the result of entryProcessor to be written to file, it must be serializable. Cache in development mode Suppose you are using SBT, Maven, or Gradle. While developing, you normally do not want to cache the result of processing the directory target (SBT, Maven) or build (Gradle) in the current working directory. Sclasner's behavior: If container is a subdirectory of target or build, the result of processing that container will not be cached. When loading the cache file, if a container is a subdirectory of target or build, entryProcessor will be run for that container. Use with SBT Supported Scala versions: 2.11.x, 2.10.x libraryDependencies += ""tv.cntt"" %% ""sclasner"" % ""1.7.0""  Sclasner is used in Xitrum. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/sclasner"	"Scala classpath scanner."	"true"
"Modularization and Dependency Injection"	"SubCut ★ 388 ⧗ 34"	"https://github.com/dickwall/subcut"	"Scala Uniquely Bound Classes Under Traits."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"394"	"29"	"31"	"GitHub - dickwall/subcut: Scala Uniquely Bound Classes Under Traits Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 29 Star 394 Fork 31 dickwall/subcut Code Issues 4 Pull requests 3 Wiki Pulse Graphs Scala Uniquely Bound Classes Under Traits 90 commits 3 branches 0 releases Fetching contributors Scala 99.6% HTML 0.4% Scala HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master sbt_0_10_1 Nothing to show Nothing to show New pull request Latest commit 5eff047 Jul 26, 2015 dickwall Organize imports and trim default comments. Permalink Failed to load latest commit information. .idea Added support for typed properties file parsing for config, plus tests. Feb 12, 2014 project Incorporate the sbt-ghpages plugin to publish scaladoc Jul 25, 2015 src Organize imports and trim default comments. Jul 26, 2015 .gitignore Organize imports and trim default comments. Jul 27, 2015 GettingStarted.md Updated docs to reflect 2.1 update. Apr 27, 2014 NewIn2.0.md Updated documentation for 2.0 final. Mar 31, 2013 PropertyFiles.md Attempting to get github to like the markdown. Feb 15, 2014 README.md Linked up the property file documentation from README. Feb 15, 2014 README.txt Added readme and getting started files, also updated to 0.9 snapshot. Jun 11, 2011 build.sbt Incorporate the sbt-ghpages plugin to publish scaladoc Jul 25, 2015 pom.xml updated pom.xml to match sbt version Sep 12, 2013 scalac-plugin.xml General sweep and update of the scaladoc and markdown. Sep 22, 2012 README.md SubCut README SubCut, or Scala Uniquely Bound Classes Under Traits, is a mix of service locator and dependency injection patterns designed to provide an idiomatic way of providing configured dependencies to scala applications. It is not a full inversion of control solution like Spring, but instead provides flexible and nearly invisible binding of traits to instances, classes or provider methods, along with a convenient binding DSL (Domain Specific Language) and an emphasis on convenience for developers, compile time performance, compile time type safety and immutability. It is also small (a few hundred lines of code) and has no dependencies other than the Scala runtime libraries (plus scalatest and junit if you want to build from source and run the tests). The SubCut library is available as open source under the Apache v2 license. Documentation Just Added: Property File Configuration The scaladoc in the source code is reasonably complete, and will continue to be improved. The unit tests provide further code examples of how subcut can be used, but does not attempt to demonstrate recommended uses or effective recipes. There is a Getting Started document that spells out the quickest way to get going, and (in the author's opinion), the best way to use subcut effectively. Scaladocs can be found on the GitHub home page for SubCut. Or, take a look at an overview of What's new in SubCut 2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/dickwall/subcut"	"Scala Uniquely Bound Classes Under Traits."	"true"
"Distributed Systems"	"Akka"	"http://akka.io/"	"A toolkit and runtime for building highly concurrent, distributed, and fault tolerant event-driven applications."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"5764"	"535"	"1817"	"GitHub - akka/akka: Akka Project Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 535 Star 5,764 Fork 1,817 akka/akka Code Issues 648 Pull requests 25 Pulse Graphs Akka Project http://akka.io 19,359 commits 69 branches 195 releases 341 contributors Scala 80.2% Java 17.7% HTML 1.0% CSS 0.4% Shell 0.3% JavaScript 0.2% Other 0.2% Scala Java HTML CSS Shell JavaScript Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags artery-dev doc-remove-tracking-2 doc-remove-tracking ktoso-patch-2 master release-1.1.3 release-1.1.4 release-1.2 release-1.3 release-1.3.1 release-2.0 release-2.1 release-2.2 release-2.3 release-2.4 stream-http-2.0 streams-on-their-way-to-master trace-spi wip-2996-make-things-compile-and-test-on-ppc-j9-ban wip-3751-send-serialized-instance-√ wip-15086-lazy-actor-materialization-ban wip-15086-lazy-actor-materialization-patriknw wip-15716-http-new-dsl-internally-ban wip-15762-patriknw wip-15802-ticksource-workaround-patriknw wip-16051-allow-balance-to-wait-for-all-downstreams-ban wip-16224-RestartNode2Spec-patriknw wip-16262-mark-as-pending-johanandren wip-16346-QuarantineLeakSpec-patriknw wip-16911-repeat-tests-patriknw wip-17239-japi-function-master-patriknw wip-17380-java8-debug-patriknw wip-18989-make-akka-with-docker-googlable-johanandren wip-19315-document-websocket-client-johanandren wip-19469-better-stream-error-logging-johanandren wip-19530-FlowConcatSpec-RK wip-19547-make-stages-overview-complete-johanandren wip-19616-remove-unused-udp-setting-johanandren wip-19623-early-response-errors-johanandren wip-19623-subsource-cannot-push-twice-johanandren wip-19664-incorrect-framing-docs-johanandren wip-19873-MetricsBasedResizerSpec-RK wip-19964-broken-link-WebSocket-example-RK wip-20591-scalariform-patriknw wip-20639-restarting-node-patriknw wip-20831-migration-guide-patriknw wip-20856-netty-3.10.6-patriknw wip-artery-bench-patriknw wip-artery-perf-patriknw wip-changes-for-java6-generics-errors-on-mac-ban wip-cover-java-http-interface-with-tests-johanandren wip-debug-15439-patriknw wip-harden-ClusterShardingSpec-debug-patriknw wip-junit-patriknw wip-managed-blocking-for-tpe-√ wip-merge-hookup-patriknw wip-minor-framing-touchups-√ wip-minor-touchups-3000-√ wip-misc-doc-fixes-RK wip-new-Typed-Impl-RK wip-nicer-JournalSpec-RK wip-optimize-fusing-RK wip-release-2.11-patriknw wip-remove-jdk7-class-ref wip-rp-docs wip-streams-improvements-√ wip-typed-Java8-∂π wip-unbreak-master-√ wip-zip-dsl-patriknw Nothing to show v2.4.8 v2.4.7 v2.4.6_2.12-M4 v2.4.6 v2.4.5_2.12-M4 v2.4.5 v2.4.4 v2.4.3 v2.4.2 v2.4.2_with_search v2.4.2-RC3 v2.4.2-RC2 v2.4.2-RC1 v2.4.1 v2.4.1_with_search v2.4.0 v2.4.0_with_search v2.4.0-rp16i01p01 v2.4.0-RC3 v2.4.0-RC2 v2.4.0-RC1 v2.4-M3 v2.4-M2 v2.4-M1_2.12.0-M1 v2.4-M1 v2.4-ARTERY-M3 v2.4-ARTERY-M2 v2.4-ARTERY-M1 v2.3.15 v2.3.14 v2.3.13 v2.3.12 v2.3.11 v2.3.11-rp15v01p05 v2.3.11-rp15v01p04 v2.3.10 v2.3.9 v2.3.8 v2.3.7 v2.3.6_2.11.2 v2.3.6 v2.3.5_2.11.2 v2.3.5 v2.3.4_2.11.1 v2.3.4 v2.3.3_2.11.1 v2.3.3 v2.3.2_2.11.0 v2.3.2_2.11.0_b v2.3.2 v2.3.2_issue_tracking v2.3.1 v2.3.0_2.11.0-RC4 v2.3.0_2.11.0-RC3 v2.3.0 v2.3.0-RC4_2.11.0-RC1 v2.3.0-RC4 v2.3.0-RC3 v2.3.0-RC2 v2.3.0-RC1 v2.3-M2 v2.3-M1 v2.2.5 v2.2.4 v2.2.4_sbt-0.13 v2.2.3_2.11.0-M8 v2.2.3_2.11.0-M7 v2.2.3 v2.2.3_sbt-0.13 v2.2.2 v2.2.1_2.11.0-M5 v2.2.1 v2.2.1_sbt-0.13 v2.2.0_2.11.0-M3 v2.2.0 v2.2.0-RC2_2.11.0-M3 v2.2.0-RC2 v2.2.0-RC1_2.11.0-M3 v2.2.0-RC1 v2.2-M5 v2.2-M4 v2.2-M3 v2.2-M2 v2.2-M1 v2.1.4 v2.1.4-scala2.11 v2.1.3 v2.1.2 v2.1.1 v2.1.0 v2.1.0-RC6 v2.1.0-RC5 v2.1.0-RC4 v2.1.0-RC3 v2.1.0-RC2 v2.1.0-RC1 v2.1-M2 v2.1-M1 v2.0.5 v2.0.4 Nothing to show New pull request Latest commit e986989 Jul 15, 2016 ktoso committed on GitHub =htp fix typo in latency spec using AB (#20965) Permalink Failed to load latest commit information. akka-actor-tests +act #20936 add CompletionStage API to CircuitBreaker (#20937) Jul 14, 2016 akka-actor additional debug logging for fsm #20952 Jul 15, 2016 akka-agent #18765 Update to latest MiMa which is now an AutoPlugin. Mar 10, 2016 akka-bench-jmh =act #20910 optimize ByteString#copyToBuffer (#20911) Jul 11, 2016 akka-camel Update to a working version of Scalariform Jun 2, 2016 akka-cluster-metrics Update to a working version of Scalariform Jun 2, 2016 akka-cluster-sharding Update to a working version of Scalariform Jun 2, 2016 akka-cluster-tools Merge pull request #20828 from choffmeister/choffmeister-20826-unregi… Jul 5, 2016 akka-cluster recovery timeout for persistent actors #20698 Jun 3, 2016 akka-contrib Update to a working version of Scalariform Jun 2, 2016 akka-distributed-data =htc, doc replace usages of deprecated methods of FileIO (#20928) Jul 10, 2016 akka-docs Nikdon 20535 check same origin (#20962) Jul 15, 2016 akka-http-core Nikdon 20535 check same origin (#20962) Jul 15, 2016 akka-http-marshallers-java/akka-http-jackson !htp #18919 #19519 New JavaDSL for Akka HTTP (#20518) May 16, 2016 akka-http-marshallers-scala !htp #18919 #19519 New JavaDSL for Akka HTTP (#20518) May 16, 2016 akka-http-testkit =htp #20915 fix too small timing issue in DirectivesSpec (#20916) Jul 8, 2016 akka-http-tests =htp fix typo in latency spec using AB (#20965) Jul 15, 2016 akka-http Nikdon 20535 check same origin (#20962) Jul 15, 2016 akka-kernel Improve error when Cluster MBean is gone #2033 Mar 22, 2016 akka-multi-node-testkit Update to a working version of Scalariform Jun 2, 2016 akka-osgi Publish akka-osgi but do not include it in dist #20548 (#20549) May 19, 2016 akka-parsing Update to a working version of Scalariform Jun 2, 2016 akka-persistence-query Update to a working version of Scalariform Jun 2, 2016 akka-persistence-shared #18765 Update to latest MiMa which is now an AutoPlugin. Mar 10, 2016 akka-persistence-tck remove JUnitRunner annotation, #16112 Apr 5, 2016 akka-persistence additional debug logging for fsm #20952 Jul 15, 2016 akka-protobuf Remove 'the' duplicates Mar 31, 2016 akka-remote-tests =htp multinode latency spec for HTTP (#20964) Jul 15, 2016 akka-remote +rem Support serialization of scala Some and None (#20801) Jun 19, 2016 akka-samples Update to a working version of Scalariform Jun 2, 2016 akka-slf4j Update to a working version of Scalariform Jun 2, 2016 akka-stream-testkit =str switch to java std lib ThreadLocalRandom. (#20877) Jul 5, 2016 akka-stream-tests-tck Add basic support for Java 7 NIO file systems (#20293) Apr 25, 2016 akka-stream-tests migrate Fold, Sliding, Grouped to GraphStage (#20914) Jul 8, 2016 akka-stream =htc, doc replace usages of deprecated methods of FileIO (#20928) Jul 10, 2016 akka-testkit Update to a working version of Scalariform Jun 2, 2016 akka-typed Update to a working version of Scalariform Jun 2, 2016 project =htp multinode latency spec for HTTP (#20964) Jul 15, 2016 scripts included script for finding fixed tickets #20475 May 10, 2016 src/main/ls Update ls.implicit.ly sbt settings Mar 10, 2012 .gitignore splitting up TLS APIs Feb 16, 2016 .java-version +act #19085 adds Java API ByteString#createBuilder Dec 4, 2015 CONTRIBUTING.md Improved CONTRIBUTING.md (#20895) Jul 6, 2016 COPYING.protobuf =all #13783 replace protobuf dependency by embedded version Sep 4, 2015 LICENSE Update copyrights and links to the new company name #19851 Feb 23, 2016 README.md Add news link Jul 11, 2016 build.sbt #18765 Update to latest MiMa which is now an AutoPlugin. Mar 10, 2016 README.md Akka We believe that writing correct concurrent & distributed, resilient and elastic applications is too hard. Most of the time it's because we are using the wrong tools and the wrong level of abstraction. Akka is here to change that. Using the Actor Model we raise the abstraction level and provide a better platform to build correct concurrent and scalable applications. This model is a perfect match for the principles laid out in the Reactive Manifesto. For resilience we adopt the ""Let it crash"" model which the telecom industry has used with great success to build applications that self-heal and systems that never stop. Actors also provide the abstraction for transparent distribution and the basis for truly scalable and fault-tolerant applications. Learn more at akka.io. Reference Documentation The reference documentation is available at doc.akka.io, for Scala and Java. Community You can join these groups and chats to discuss and ask Akka related questions: Mailing list: Chat room about using Akka: Issue tracker: In addition to that, you may enjoy following: The news section of the page, which is updated whenever a new version is released The Akka Team Blog @akkateam on Twitter Questions tagged #akka on StackOverflow Contributing Contributions are very welcome! If you see an issue that you'd like to see fixed, the best way to make it happen is to help out by submitting a PullRequest implementing it. Refer to the CONTRIBUTING.md file for more details about the workflow, and general hints how to prepare your pull request. You can also chat ask for clarifications or guidance in github issues directly, or in the akka/dev chat if a more real time communication would be of benefit. A chat room is available for all questions related to developing and contributing to Akka: License Akka is Open Source and available under the Apache 2 License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/akka/akka"	"A toolkit and runtime for building highly concurrent, distributed, and fault tolerant event-driven applications."	"true"
"Distributed Systems"	"Clump"	"http://getclump.io"	"A library for expressive and efficient service composition"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"185"	"14"	"8"	"GitHub - getclump/clump: A library for expressive and efficient service composition Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 185 Fork 8 getclump/clump Code Issues 0 Pull requests 2 Pulse Graphs A library for expressive and efficient service composition http://getclump.io 354 commits 3 branches 13 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master scalajs Nothing to show v0.0.15 v0.0.14 v0.0.13 v0.0.12 v0.0.11 v0.0.10 v0.0.9 v0.0.8 v0.0.7 v0.0.6 v0.0.5 v0.0.4 v0.0.3 Nothing to show New pull request Latest commit 90b9187 Oct 30, 2015 stevenheidel Update CHANGELOG for 1.1.0 Permalink Failed to load latest commit information. project use scala 2.11 by default Sep 10, 2015 src Fix up formatting Oct 30, 2015 .gitignore fix codacy warnings Jul 23, 2015 .travis.yml enable travis retry Sep 7, 2015 CHANGELOG.md Update CHANGELOG for 1.1.0 Oct 30, 2015 LICENSE.txt use the Apache License Apr 18, 2015 README.md Fix up formatting Oct 30, 2015 STITCH.md Update documentation Jul 17, 2015 clump.png Update logo Mar 3, 2015 version.sbt Setting version to 1.1.1-SNAPSHOT Oct 30, 2015 README.md A library for expressive and efficient service composition Introduction Getting started Usage Example Sources Constants Composition Execution Composition behavior Filtering Exception handling Internals Known limitations Acknowledgments Versioning License Introduction Summary Clump is a Scala library that addresses the problem of knitting together data from multiple sources in an elegant and efficient way. In a typical microservice-powered system, it is common to find awkward wrangling code to facilitate manually bulk-fetching dependent resources. Worse, this problem of batching is often accidentally overlooked, resulting in n calls to a micro-service instead of 1. Clump removes the need for the developer to even think about bulk-fetching, batching and retries, providing a powerful and composable interface for aggregating resources. An example of batching fetches using futures without Clump: // makes 1 call to tracksService and 1 call to usersService tracksService.get(trackIds).flatMap { tracks =>   val userIds = tracks.map(_.creator)   usersService.get(userIds).map { users =>     val userMap = userIds.zip(users).toMap     tracks.map { track =>       new EnrichedTrack(track, userMap(track.creator))     }   } } The same composition using Clump: // also makes just 1 call to tracksService and 1 call to usersService Clump.traverse(trackIds) { trackId =>   for {     track <- trackSource.get(trackId)     user <- userSource.get(track.creator)   } yield {     new EnrichedTrack(track, user)   } Users The following companies are running Clump in production: Problem The microservices architecture introduces many new challenges when dealing with complex systems. One of them is the high number of remote procedure calls and the cost associated to them. Among the techniques applied to amortize this cost, batching of requests has an important role. Instead of paying the price of one call for each interaction, many interactions are batched in only one call. While batching introduces performance enhancements, it also introduces complexity to the codebase. The common approach is to extract as much information as possible about what needs to be fetched, perform the batched fetch and extract the individual values to compose the final result. The steps need to be repeated many times depending on how complex is the final structure. An example of batching using futures: tracksService.get(trackIds).flatMap { tracks =>   val userIds = tracks.map(_.creator)   usersService.get(userIds).map { users =>     val userMap = userIds.zip(users).toMap     tracks.map { track =>       new EnrichedTrack(track, userMap(track.creator))     }   } } This example has only one level of nested resources. In a complex system, it is common to have several levels: • timeline   • track post     • track       • creator   • track repost     • track       • creator     • reposter   • playlist post     • playlist       • track ids       • creator   • playlist repost     • playlist       • track ids       • creator     • reposter   • comment     • user   • user follow     • follower     • followee  This structure can also be part of a bigger structure that includes the user's data for instance. Given this scenario, the code that is capable of batching requests in an optimal way is really complex and hard to maintain. Solution The complexity comes mainly from declaring together what needs to be fetched and how it should be fetched. Clump offers an embedded Domain-Specific Language (DSL) that allows declaration of what needs to be fetched and an execution model that determines how the resources should be fetched. The execution model applies three main optimizations: Batch requests when it is possible; Fetch from the multiple sources in parallel; Avoid fetching the same resource multiple times by using a cache. The DSL is based on a monadic interface similar to Future. It is a Free Monad, that produces a nested series of transformations without starting the actual execution. This is the characteristic that allows triggering of the execution separately from the definition of what needs to be fetched. The execution model leverages on Applicative Functors to express the independence of computations. It exposes only join to the user but makes use of other applicative operations internally. This means that even without the user specifying what is independent, the execution model can apply optimizations. Getting started To use clump, just add the dependency to the project's build configuration. There are two versions of the project: clump-scala, that uses Scala Futures and doesn't have external dependencies. clump-twitter, that uses Twitter Futures and has the dependency to twitter-util. Important: Change x.x.x with the latest version listed by the CHANGELOG.md file. SBT libraryDependencies ++= Seq(   ""io.getclump"" %% ""clump-scala"" % ""x.x.x"" ) libraryDependencies ++= Seq(   ""io.getclump"" %% ""clump-twitter"" % ""x.x.x"" ) Maven <dependency>     <groupId>io.getclump</groupId>     <artifactId>clump-scala</artifactId>     <version>x.x.x</version> </dependency> <dependency>     <groupId>io.getclump</groupId>     <artifactId>clump-twitter</artifactId>     <version>x.x.x</version> </dependency> Usage Example Example usage of Clump: import io.getclump.Clump  // Creates sources using the batched interfaces val tracksSource = Clump.source(tracksService.fetch _)(_.id) val usersSource = Clump.source(usersService.fetch _)(_.id)  def renderTrackPosts(userId: Long) = {    // Defines the clump   val clump: Clump[List[EnrichedTrack]] = enrichedTrackPosts(userId)    // Triggers execution   val future: Future[Option[List[EnrichedTrack]]] = clump.get    // Renders the response   future.map {     case Some(trackPosts) => render.json(trackPosts)     case None             => render.notFound   } }  // Composes a clump with the user's track posts def enrichedTrackPosts(userId: Long) =   for {     trackPosts <- Clump.future(timelineService.fetchTrackPosts(userId))     enrichedTracks <- Clump.traverse(trackPosts)(enrichedTrack(_))   } yield {     enrichedTracks   }  // Composes an enriched track clump def enrichedTrack(trackId: Long) =   for {     track <- tracksSource.get(trackId)     creator <- usersSource.get(track.creatorId)   } yield {     new EnrichedTrack(track, creator)   } The usage of renderTrackPosts produces only three remote procedure calls: Fetch the track posts list (from timelineService); Fetch the metadata for all the tracks (from tracksService); Fetch the user metadata for all the tracks' creators (from usersService). The final result can be notFound because the user can be found or not. Sources Sources represent the remote systems' batched interfaces. Clump offers some methods to create sources using different strategies. Map The Clump.source method accepts a function that returns a Map with the values for the found inputs. def fetch(ids: List[Int]): Future[Map[Int, User]] = ...  val usersSource = Clump.source(fetch _)  val userClump = usersSource.get(id) Collection with value-to-key function The Clump.source method also accepts a function that may return a collection with less elements or in a different order than requested. In these cases, a function may be provided to match the found results back to their input keys. def fetch(ids: Set[Int]): Future[Set[User]] = ...  val usersSource = Clump.source(fetch _)(_.id)  val userClump = usersSource.get(id) Map with Success and Failure Sometimes sources can mark individual entries as failed, even though the entire fetch function succeeded. For these sources, a function can be provided that maps from id to a Try object that can be marked as either Success or Failure. def fetch(ids: List[Int]): Future[Map[Int, Try[User]]] = ...  val usersSource = Clump.sourceTry(fetch _)  val userClump = usersClump.get(id) The userClump will be either contain a User if the value for that id was Success(User), otherwise it will contain the exception in the Failure object for that id. As usual, if the id is not found in the map then the Clump will be undefined. List with zip function The Clump.sourceZip methods accepts a function that produces a list of outputs for each provided input. The result must keep the same order as the inputs list. def fetch(ids: List[Int]): Future[List[User]] = ...  val usersSource = Clump.sourceZip(fetch _)  val userClump = usersSource.get(id) A single source The Clump.sourceSingle method accepts a functions that produces a single output for a single input. Useful for interfaces that don't expose batch endpoints. def fetch(id: Int): Future[User] = ...  val usersSource = Clump.sourceSingle(fetch _)  val userClump = usersSource.get(id) Additional parameters For the three sourcing options above, it is possible to create sources that have up to four additional parameters, with the resulting ClumpSource accepting each parameter and a singular input. There is a restriction that the inputs must be the last parameter of the fetch function. def fetch(session: UserSession, ids: List[Int]): Future[List[User]] = ...  val usersSource = Clump.source(fetch _)(_.id)  val userClump = usersSource.get(session, id) Additional configurations Some services have a limitation on how many resources can be fetched in a single request. It is possible to define this limit for each source instance: val usersSource = Clump.source(fetch _).maxBatchSize(100) The source instance can be also configured to automatically retry failed fetches by using the maxRetries method. It receives a partial function that defines the number of retries for each type of exception. The default number of retries is zero. val usersSource =      Clump.source(fetch _).maxRetries {       case e: SomeException => 10     } Constants It is possible to create Clump instances based on values. From a value: val clump: Clump[Int] = Clump.value(111) From a future: // This method is useful as a bridge between Clump and non-batched services. val clump: Clump[Int] = Clump.future(counterService.currentValueFor(111)) It is possible to create a failed Clump instance: val clump: Clump[Int] = Clump.exception(new NumberFormatException) There is a shortcut for a constant empty Clump: val clump: Clump[Int] = Clump.empty Composition Clump has a monadic interface similar to Future. It is possible to apply a simple transformation by using map: val intClump: Clump = Clump.value(1) val stringClump: Clump[String] = intClump.map(_.toString) If the transformation results on another Clump instance, it is possible to use flatMap: val clump: Clump[(Track, User)] =   tracksSource.get(trackId).flatMap { track =>     usersSource.get(track.id).map { user =>       (track, user)     }   } The join method produces a Clump that has a tuple with the values of two Clump instances: val clump: Clump[(User, List[Track])] =     usersSource.get(userId).join(userTracksSource.get(userId)) There are also methods to deal with collections. Use collect to transform a collection of Clump instances into a single Clump: val userClumps: List[Clump[User]] = userIds.map(usersSource.get(_)) val usersClump: Clump[List[User]] = Clump.collect(usersClump) Instead of map and then collect, it is possible to use the shortcut traverse: val usersClump: Clump[List[User]] = Clump.traverse(userIds)(usersSource.get(_)) It is possible to use for-comprehensions as syntactic sugar to avoid having to write the compositions: val trackClump: Clump[EnrichedTrack] =     for {       track <- tracksSource.get(trackId)       creator <- usersSource.get(track.creatorId)     } yield {       new EnrichedTrack(track, creator)     } Execution The creation of Clump instances doesn't trigger calls to the remote services. The only exception is when the code explicitly uses Clump.future to invoke a service. To trigger execution, it is possible to use get: val trackClump: Clump[EnrichedTrack] = ... val user: Future[Option[EnrichedTrack]] = trackClump.get Clump assumes that the remote services can return less elements than requested. That's why the result is an Option, since the input's result may be missing. It is possible to define a default value by using getOrElse: val trackClump: Clump[EnrichedTrack] = ... val user: Future[EnrichedTrack] = trackClump.getOrElse(unknownTrack) If it is guaranteed that the underlying service will always return results for all fetched inputs, it is possible to use apply, that throws a NotFoundException if the result is empty: val trackClump: Clump[EnrichedTrack] = ... val user1: Future[EnrichedTrack] = trackClump.apply() val user2: Future[EnrichedTrack] = trackClump() // syntactic sugar When a Clump instance has a collection, it is possible to use the list method. It returns an empty collection if the result is None: val usersClump: Clump[List[User]] = Clump.traverse(userIds)(usersSource.get(_)) val users: Future[List[User]] = usersClump.list Composition behavior The composition of Clump instances takes in consideration that the sources may not return results for all requested inputs. It has a behavior similar to the relational databases' joins, where not found joined elements make the tuple be filtered-out. val clump: Clump[(Track, User)]   for {     track <- tracksSource.get(111)     user <- usersSource.get(track.creator)   } yield (track, user) In this example, if the track's creator isn't found, the final result will be None. val future: Future[Option[(Track, User)]] = clump.get val result: Option[(Track, User)] = Await.result(future) require(result === None) If a nested clump is expected to be optional, it is possible to use the optional method to have a behavior similar to an outer join. val clump: Clump[(Track, Option[User])]   for {     track <- tracksSource.get(111)     user <- usersSource.get(track.creator).optional   } yield (track, user) Another alternative is to define a fallback by using orElse: val clump: Clump[(Track, Option[User])]   for {     track <- tracksSource.get(111)     user <- usersSource.get(track.creator).orElse(usersSource.get(track.uploader))   } yield (track, user) Filtering The behavior introduced by the optional fetch compositions allows defining of filtering conditions: val clump: Clump[(Track, User)]   for {     track <- tracksSource.get(111) if(track.owner == currentUser)     user <- usersSource.get(track.creator)   } yield (track, user) Exception handling Clump offers some mechanisms to deal with failed fetches. The handle method defines a fallback value given an exception: val clump: Clump[User] =     usersService.get(userId).handle {       case _: SomeException =>         defaultUser     } If the fallback value is another Clump instance, it is possible to use rescue: val clump: Clump[User] =     usersService.get(trackCreatorId).rescue {       case _: SomeException =>         usersService.get(trackUploaderId)     } Internals This section explains how Clump works under the hood. The codebase is relatively small. The only type explicitly exposed to the user is Clump, but internally there are four in total: Clump - Defines the public interface of Clump and represents the abstract syntactic tree (AST) for the compositions. ClumpSource - Represents the external systems' batched interfaces. ClumpFetcher - It has the logic to fetch from a ClumpSource, maintains the implicit cache and implements the logic to retry failed fetches. ClumpContext - It is the execution model engine created automatically for each execution. It keeps the state by using a collection of ClumpFetchers. Take some time to read the code of these classes. It will help to have a broader view and understand the explanation that follows. Lets see what happens when this example is executed: val usersSource = Clump.source(usersService.fetch _)(_.id) val tracksSource = Clump.source(tracksService.fetch _)(_.id)  val clump: Clump[List[EnrichedTrack]] =     Clump.traverse(trackIds) { trackId =>       for {         track <- tracksSource.get(trackId)         user <- usersSource.get(track.creatorId)       } yield {         new EnrichedTrack(track, user)       }     }  val tracks: Future[List[Track]] = clump.list Sources creation val usersSource = Clump.source(usersService.fetch _)(_.id) val tracksSource = Clump.source(tracksService.fetch _)(_.id) The ClumpSource instances are created using one of the shortcuts that the Clump object provides. They don't hold any state and allow to create Clump instances representing the fetch. Clump uses the source's identity to group requests and perform batched fetches, so it is not possible to have multiple instances of the same source within a clump composition and execution. Composition val clump: Clump[List[EnrichedTrack]] =     Clump.traverse(trackIds) { trackId =>       ...     } The traverse method is used as a shortcut for map and then collect, so this code could be rewritten as follows: val clump: Clump[List[EnrichedTrack]] =     Clump.collect(trackIds.map { trackId =>        ...     } For each trackId, a for-comprehension is used to compose a Clump that has the EnrichedTrack:       for {         track <- tracksSource.get(trackId)         user <- usersSource.get(track.creatorId)       } yield {         new EnrichedTrack(track, user)       } The for-comprehension is actually just syntactic sugar using map and flatMap, so this code is equivalent to:       tracksSource.get(trackId).flatMap { track =>         usersSource.get(track.creatorId).map { user =>           new EnrichedTrack(track, user)         }       } There are three methods being used in this composition: get creates a ClumpFetch instances that is the AST element representing the fetch. It doesn't trigger the actual fetch, only uses the ClumpFetcher instance to produce a Future that will be executed by the ClumpContext when the execution is triggered. flatMap creates a ClumpFlatMap instance representing the operation. It just composes a new future that is based on the result of the initial Clump and the result of the nested Clump. map creates a ClumpMap instance representing the map operation. It composes a new future by applying the specified transformation. Execution Now comes the most important part. Until now, the compositions only create Clump* instances to represent the operations and produce futures that will be fulfilled when the execution is triggered. You probably have noticed that the Clump instances define three things: result that has the Future result for the operation upstream that returns the upstream Clump instances that were used as the basis for the composition downstream that returns the downstream Clump instances created as a result of the operation Note that downstream returns a Future[List[Clump[_]]], while upstream returns a List[Clump[_]] directly. This happens because downstream produces Clump instances that are available only after the upstream execution. These methods are used by the ClumpContext to apply the execution model. It has a collection with all ClumpFetcher instances in the composition. This is the code that triggers the execution: val tracks: Future[List[Track]] = clump.list The list method is just a shortcut to ease getting the value of Clump instances that have a collection. The actual execution is triggered by the get method. It flushes the context and returns the Clump's result. The context flush is a mutually recursive function that uses the following steps: Retrieve a list of visible Clump instances. Recall that upstream returns a List[Clump[_]] directly. All the visible Clump instances are therefore retrieved by recursively getting a list of upstream Clumps, then all the upstream Clumps from each Clump in that list, and so on. Perform all the fetches among the visible Clump instances. This batches together calls to the same ClumpSource and also performs all batch flushes in parallel. Finally, flush the downstream instances The rule is that all upstream instances must be flushed before any of the downstream instances. The only Clump instances that fulfill this requirement at this point are the downstream instances of the Clumps at the deepest level of the visible traversal. Flush these first. As each level of downstream instances is flushed, move up a level and flush again since the pre-requisite must be fulfilled. (The upstream have already been flushed in the first step, and any downstream Clumps from the current Clump's upstream instances would have been at a deeper level of composition and therefore have been flushed already). You could consider this a upstream-first traversal of the Clump graph. In case you are wondering why we need this upstream mechanism since we have the Clump instance at hand and could start the execution from it: actually the instance used to trigger the execution isn't the ""root"" of the composition. For instance: val clump: Clump[EnrichedTrack] =     tracksSource.get(trackId).flatMap { track =>       usersSource.get(track.creatorId).map { user =>         new EnrichedTrack(track, user)       }     } The clump instance will be a ClumpFlatMap, not the ClumpFetch created by the tracksSource.get(trackId) call. This is the AST behind the clump instance:                                    +----------> Empty                                                        |Up                                                                +------+-----+                                                            | ClumpFetch |                                               +----------> |  (line 2)  |                                               |Up          +------+-----+                                               |                   |Down                                                 |                   +----------> Empty                            +-------+------+                                                          | ClumpFlatMap |                                                   get--> |   (line 2)   |                                   +-------> Empty        +-------+------+                                   |Up                            |                                +---------+--+                           |                   +----------> | ClumpFetch |                           |                   |Up          |  (line 3)  |                           |Down        +------+-----+      +---------+--+                           +----------> |  ClumpMap  |                |Down                                       |  (line 3)  |                +-------> Empty                             +------+-----+                                                                   |Down                                                                     +----------> Empty                      The steps to execute this composition happen as follows: The execution is triggered by the get method on the ClumpFlatMap instance The only visible Clumps are ClumpFlatMap and ClumpFetch The fetch is executed Downstream instances are flushed starting at the deepest level Downstream of ClumpFetch is empty and returns immediately Downstream of ClumpFlatMap can now be flushed because the entire upstream path was already flushed Visibility has increased, so now the visible Clumps are ClumpMap and ClumpFetch The fetch is executed Downstream instances are flushed starting at the deepest level Downstream of ClumpFetch is empty and returns immediately Downstream of ClumpMap is empty and returns immediately Note that this example has only one Clump instance per flush phase, but normally there are multiple instances. This is what allows Clump to batch requests that are in the same flush phase. Known limitations The execution model is capable of batching requests that are in the same level of the composition. For instance, this example produces only one fetch from usersSource: val clump: Clump[List[EnrichedTrack]] =     Clump.traverse(trackIds) { trackId =>       for {         track <- tracksSource.get(trackId)         user <- usersSource.get(track.creatorId)       } yield {         new EnrichedTrack(track, user)       }     } The next example has two fetches from usersSource: one for the playlists' creators and other for the tracks' creators. val clump: Clump[List[EnrichedPlaylist]] =     Clump.traverse(playlistIds) { playlistId =>       for {         playlist <- playlistsSource.get(playlistId)         creator <- usersSource.get(playlist.creator)         tracks <-            Clump.traverse(playlist.trackids) {             for {               track <- tracksSource.get(trackId)               user <- usersSource.get(track.creatorId)             } yield {               new EnrichedTrack(track, user)             }           }       } yield {         new EnrichedPlaylist(playlist, creator, tracks)       }     } Considering that they happen in different levels of the composition, the execution model will execute two batched fetches to usersSource, not one. This limitation is alleviated by the implicit caching if the playlist and tracks have the same creator. Acknowledgments Clump was inspired by the Twitter's Stitch project. The initial goal was to have a similar implementation, but the project evolved to provide an approach more adherent to some use-cases we have in mind. See STITCH.md for more information about the differences between Stich and Clump. Facebook's Haxl paper and the Futurice's blog post about Jobba also were important sources for the development phase. The project was initially built using SoundCloud's Hacker Time. Versioning Clump adheres to Semantic Versioning 2.0.0. If there is a violation of this scheme, report it as a bug. Specifically, if a patch or minor version is released and breaks backward compatibility, that version should be immediately yanked and/or a new version should be immediately released that restores compatibility. Any change that breaks the public API will only be introduced at a major-version release. License See the LICENSE file for details. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/getclump/clump"	"A library for expressive and efficient service composition"	"true"
"Distributed Systems"	"CurioDB ★ 408 ⧗ 1"	"https://github.com/stephenmcd/curiodb"	"Distributed & Persistent Redis Clone built with Scala & Akka."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"421"	"31"	"32"	"GitHub - stephenmcd/curiodb: Distributed and Persistent NoSQL Database Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 31 Star 421 Fork 32 stephenmcd/curiodb Code Issues 2 Pull requests 1 Pulse Graphs Distributed and Persistent NoSQL Database http://curiodb.jupo.org 134 commits 5 branches 0 releases Fetching contributors Scala 98.3% Python 1.7% Scala Python Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bugfix exists_branch gh-pages master publish_branch Nothing to show Nothing to show New pull request Latest commit f11f7fb Feb 27, 2016 stephenmcd README grammar fix Permalink Failed to load latest commit information. project Added sbt-revolver plugin. Sep 2, 2015 src/main Fix debug logging for responses without a client destination. Feb 23, 2016 .hgignore Change scores in sorted sets from ints to floats, and use consistent … Jul 12, 2015 AUTHORS Added Gan Wenchuang to AUTHORS. Jan 5, 2016 LICENSE Added LICENSE. Feb 20, 2015 README.md README grammar fix Feb 27, 2016 benchmark.py Tidy up benchmark script and fix some comment typos. Dec 10, 2015 README.md Created by Stephen McDonald CurioDB is a distributed and persistent Redis clone, built with Scala and Akka. Please note that despite the fancy logo, this is a toy project, hence the name ""Curio"", and any suitability as a drop-in replacement for Redis is purely incidental. :-) Installation I've been using sbt to build the project, which you can install on OS X, Linux or Windows. With that done, you just need to clone this repository and run it: $ git clone git://github.com/stephenmcd/curiodb.git $ cd curiodb $ sbt ""~re-start --config=path/to/config.file""  You can also build a binary (executable JAR file): $ sbt assembly $ ./target/scala-2.11/curiodb-0.0.1 --config=path/to/config.file  Note that the --config=path/to/config.file argument is optional, see the Configuration section below for more detail. Overview Why build a Redis clone? Well, I'd been learning Scala and Akka and wanted a nice project I could take them further with. I've used Redis heavily in the past, and Akka gave me some really cool ideas for implementing a clone, based on each key/value pair (or KV pair) in the system being implemented as an actor: Concurrency Since each KV pair in the system is an actor, CurioDB will happily use all your CPU cores, so you can run 1 server using 32 cores instead of 32 servers each using 1 core (or use all 1,024 cores of your 32 server cluster, why not). Each actor operates on its own value atomically, so the atomic nature of Redis commands is still present, it just occurs at the individual KV level instead of in the context of an entire running instance of Redis. Distributed by Default Since each KV pair in the system is an actor, the interaction between multiple KV pairs works the same way when they're located across the network as it does when they're located on different processes on a single machine. This negates the need for features of Redis like ""hash tagging"", and allows commands that deal with multiple keys (SUNION, SINTER, MGET, MSET, etc) to operate seamlessly across a cluster. Virtual Memory Since each KV pair in the system is an actor, the unit of disk storage is the individual KV pair, not a single instance's entire data set. This makes Redis' abandoned virtual memory feature a lot more feasible. With CurioDB, an actor can simply persist its value to disk after some criteria occurs, and shut itself down until requested again. Simple Implementation Scala is concise, you get a lot done with very little code, but that's just the start - CurioDB leverages Akka very heavily, taking care of clustering, concurrency, persistence, and a whole lot more. This means the bulk of CurioDB's code mostly deals with implementing all of the Redis commands, so far weighing in at only a paltry 1,000 lines of Scala! Currently, the majority of commands have been fully implemented, as well as the Redis wire protocol itself, so existing client libraries can be used. Some commands have been purposely omitted where they don't make sense, such as cluster management, and things specific to Redis' storage format. Pluggable Storage Since Akka Persistence is used for storage, many strange scenarios become available. Want to use PostgreSQL or Cassandra for storage, with CurioDB as the front-end interface for Redis commands? This should be possible! By default, CurioDB uses Akka's built-in LevelDB storage. Design Here's a bad diagram representing one server in the cluster, and the flow of a client sending a command: An outside client sends a command to the server actor (Server.scala). There's at most one per cluster node (which could be used to support load balancing), and at least one per cluster (not all nodes need to listen for outside clients). Upon receiving a new outside client connection, the server actor will create a Client Node actor (System.scala), it's responsible for the life-cycle of a single client connection, as well as parsing the incoming and writing the outgoing protocol, such as the Redis protocol for TCP clients, or JSON for HTTP clients (Server.scala). Key Node actors (System.scala) manage the key space for the entire system, which are distributed across the entire cluster using consistent hashing. A Client Node will forward the command to the matching Key Node for its key. A Key Node is then responsible for creating, removing, and communicating with each KV Node actor, which are the actual actors that store the underlying value for each key, such as a strings, hashes, and sorted sets, and perform the operations on them for each of their respective commands. (Data.scala). The KV Node then sends a response back to the originating Client Node, which returns it to the outside client. Not diagrammed, but in addition to the above: Some commands require coordination with multiple KV Nodes, in which case a temporary Aggregate actor (Aggregation.scala) is created by the Client Node, which coordinates the results for multiple commands via Key Nodes and KV Nodes in the same way a Client Node does. PubSub is implemented by adding behavior to Key Nodes and Client Nodes, which act as PubSub servers and clients respectively (PubSub.scala). Lua scripting is fully supported (Scripting.scala) thanks to LuaJ, and is implemented similarly to PubSub, where behavior is added to Key Nodes which store and run compiled Lua scripts (via EVALSHA), and Client Nodes which can run uncompiled scripts directly (via EVAL). Transactions Distributed transactions are fully supported, both by way of the MULTI and EXEC commands, and for Lua scripts with the EVAL and EVALSHA commands. Transactions are implemented using basic two-phase commit (2PC) with rollback support, multiversion concurrency control (MVCC), and configurable isolation levels. 2PC A Client Node acts as a transaction coordinator in 2PC parlance. It is responsible for coordinating initial agreement with each Node that will participate in the transaction, aggregating responses for all executed (but uncommitted) commands, and then finally coordinating the commit phase for each participating Node. Given the use of MVCC, performing rollback on errors during a transaction is fully supported, and is the default behavior. This differs however from the way Redis deals with errors, as it does not support transaction rollbacks, therefore in CurioDB the behavior can be configured by changing the curiodb.transactions.on-error setting from rollback to commit, if this level of compatibility with Redis is required. MVCC Each KV Node in the system stores multiple versions of its underlying value internally, using a map that contains each transaction's version, as well as the current committed version of the value, or ""main"" value. When a transaction begins, the main value is copied into the map, stored against its transaction ID, and from that point, all commands received within the transaction will read and write to the transaction version until the transaction is commited, at which point the transaction version is copied back to the main value. Isolation Three levels of transaction isolation are available, which can be configured by the curiodb.transactions.isolation setting, to control how a key's value is read during a command: repeatable (default): Inside a transaction, only the transaction's version will be read, otherwise when outside of a transaction, the current committed version will be read. committed: Inside or outside of a transaction, the current committed version will be read. uncommitted: Inside or outside of a transaction, the most recently written version will be read, even if uncommitted. Note there is no serializable isolation level typically found in SQL databases, since neither Redis nor CurioDB have a notion of range queries. Configuration Here are the few configuration settings and their default values that CurioDB implements, along with the large range of settings provided by Akka itself, which both use typesafe-config - consult those projects for detailed information on configuration implementation. curiodb {    // Addresses listening for clients.   listen = [     ""tcp://127.0.0.1:6379""    // TCP server using Redis protocol.     ""http://127.0.0.1:2600""   // HTTP server using JSON.     ""ws://127.0.0.1:6200""     // WebSocket server, also using JSON.   ]    // Duration settings (either time value, or ""off"").   persist-after = 1 second    // Like ""save"" in Redis.   sleep-after   = 10 seconds  // Virtual memory threshold.   expire-after  = off         // Automatic key expiry.    transactions {     timeout   = 3 seconds     // Max time a transaction may take to run.     isolation = repeatable    // ""repeatable"", ""committed"", or ""uncommitted"".     on-error  = rollback      // ""commit"" or ""rollback"".   }    commands {     timeout  = 1 second       // Max time a command may take to run.     disabled = [SHUTDOWN]     // List of disabled commands.     debug    = off            // Print debug info for every command run.   }    // Cluster nodes.   nodes = {     node1: ""tcp://127.0.0.1:9001""     // node2: ""tcp://127.0.0.1:9002""     // node3: ""tcp://127.0.0.1:9003""   }    // Current cluster node (from the ""nodes"" keys above).   node = node1  }  You can also optionally provide your own configuration file, using the --config=path/to/config.file command-line argument. Your configuration file need only define the values you wish to override. For example, suppose you wanted to only listen over TCP, and disable extra commands: curiodb.listen = [""tcp://127.0.0.1:3333""]  curiodb.commands.disabled = [SHUTDOWN, DEL, FLUSHDB, FLUSHALL]  The sleep-after and expire-after settings are worth some explanation. Each of these configure a time duration that starts each time a command is run against a key, and elapses if no further commands run against that key within the duration. Once the duration elapses, an action is performed. After the sleep-after duration elapses for a key, it will persist its value to disk, and shut down, essentially going to sleep - this is how virtual memory is implemented. expire-after is similar, but after its duration elapses, the key is deleted entirely, just as if the EXPIRE command was used. HTTP/WebSocket JSON API As alluded to in the configuration example above, CurioDB also supports a HTTP/WebSocket JSON API, as well as the same wire protocol that Redis implements over TCP. Commands are issued with HTTP POST requests, or WebSocket messages, containing a JSON Object with a single args key, containing an Array of arguments. Responses are returned as a JSON Object with a single result key. HTTP: $ curl -X POST -d '{""args"": [""SET"", ""foo"", ""bar""]}' http://127.0.0.1:2600 {""result"":""OK""}  $ curl -X POST -d '{""args"": [""MGET"", ""foo"", ""baz""]}' http://127.0.0.1:2600 {""result"":[""bar"",null]}  WebSocket: var socket = new WebSocket('ws://127.0.0.1:6200');  socket.onmessage = function(response) {   console.log(JSON.parse(response.data)); };  socket.send(JSON.stringify({args: [""DEL"", ""foo""]}));  SUBSCRIBE and PSUBSCRIBE commands work as expected over WebSockets, and are also fully supported by the HTTP API, by using chunked transfer encoding to allow a single HTTP connection to receive a stream of published messages over an extended period of time. In the case of errors such as invalid arguments to a command, WebSocket connections will transmit a JSON Object with a single error key containing the error message, while HTTP requests will return a response with a 400 status, contaning the error message in the response body. Disadvantages compared to Redis I haven't measured it, but it's safe to say memory consumption is much poorer due to the JVM. Somewhat alleviated by the virtual memory feature. It's slower, but not by as much as you'd expect. Without any optimization, it's roughly about half the speed of Redis. See the performance section below. PubSub pattern matching may perform poorly. PubSub channels are distributed throughout the cluster using consistent hashing, which makes pattern matching impossible. To work around this, patterns get stored on every node in the cluster, and the PSUBSCRIBE and PUNSUBSCRIBE commands get broadcast to all of them. This needs rethinking! Mainly though, Redis is an extremely mature and battle-tested project that's been developed by many over the years, while CurioDB is a one-man hack project worked on over a few months. As much as this document attempts to compare them, they're really not comparable in that light. That said, it's been tons of fun building it, and it has some cool ideas thanks to Akka. I hope others can get something out of it too. Performance These are the results of redis-benchmark -q on an early 2014 MacBook Air running OS X 10.9 (the numbers are requests per second): Benchmark Redis CurioDB % PING_INLINE 57870.37 46296.29 79% PING_BULK 55432.37 44326.24 79% SET 50916.50 33233.63 65% GET 53078.56 38580.25 72% INCR 57405.28 33875.34 59% LPUSH 45977.01 28082.00 61% LPOP 56369.79 23894.86 42% SADD 59101.65 25733.40 43% SPOP 50403.23 33886.82 67% LRANGE_100 22246.94 11228.38 50% LRANGE_300 9984.03 6144.77 61% LRANGE_500 6473.33 4442.67 68% LRANGE_600 5323.40 3511.11 65% MSET 34554.25 15547.26 44% Generated with the bundled benchmark.py script. Further Reading These are some articles I published on developing CurioDB: CurioDB: A Distributed and Persistent Redis Clone Embedding Lua in Scala using Java Distributed Transactions in Actor Systems License BSD. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/stephenmcd/curiodb"	"Distributed & Persistent Redis Clone built with Scala & Akka."	"true"
"Distributed Systems"	"Finagle"	"https://twitter.github.io/finagle/"	"An extensible, protocol-agnostic RPC system designed for high performance and concurrency."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Finagle Finagle Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols. Finagle is written in Scala, but provides both Scala and Java idiomatic APIs. Github project User’s guide API documentation Gitter channel Google groups Contributing We feel that a welcoming community is important and we ask that you follow Twitter’s Open Source Code of Conduct in all interactions with the community. Finagle is actively maintained by Twitter’s infrastructure team, but we have many external contributors as well. The master branch represents the most recent published release while active development happens on the develop branch. Before endeavoring on large changes, please discuss them with the Google groups to receive feedback and suggestions. For all patches, please review our contributing docs. Other resources Twitter engineering blog post motivating and introducting Finagle Twitter’s Scala School ends with an introduction to Finagle, and finally an example distributed system. A talk introducing Finagle, given by Marius at ScalaDays 2011 Slides from another talk explaining the role of Finagle in Twitter’s distributed systems A blog post explaining Twitter’s server stack, in which Finagle plays a central role Matt Ho’s NEScala talk about Finagle Companion Projects In no particular order... Finatra - fast, testable, Scala services built on TwitterServer and Finagle Finch - a pure functional wrapper around Finagle HTTP built with Shapeless and Cats Finagle Serial - a Mux-powered, Finagle protocol over Scala's case classes with a serialization library of your choice Finagle OAuth2 - an OAuth2 server-side provider for Finagle Unfinagled - a small library for using unfiltered as a Finagle frontend Finagle Postgres - Postgres protocol support for Finagle Finagle IRC - an implementation of the IRC protocol on Finagle WebSockets implementation for Finagle Fintop - a top-like utility for monitoring Finagle services Fintrospect - adds an intelligent HTTP routing layer to Finagle. It provides a simple way to implement contracts for both server and client-side HTTP services"	"null"	"null"	"An extensible, protocol-agnostic RPC system designed for high performance and concurrency."	"true"
"Distributed Systems"	"Glokka ★ 44 ⧗ 25"	"https://github.com/xitrum-framework/glokka"	"Library to register and lookup actors by names in an Akka cluster."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"46"	"8"	"2"	"GitHub - xitrum-framework/glokka: Library to register and lookup actors by names in an Akka cluster Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 8 Star 46 Fork 2 xitrum-framework/glokka Code Issues 3 Pull requests 0 Pulse Graphs Library to register and lookup actors by names in an Akka cluster 103 commits 2 branches 11 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 2.4 2.3 2.2 2.1 2.0 1.9 1.8 1.7 1.6 1.5 1.4 Nothing to show New pull request Latest commit 5f55b32 Jan 7, 2016 ngocdaothanh Update sbteclipse-plugin from 2.3.0 to 4.0.0 Permalink Failed to load latest commit information. config_example Use ""MyClusterSystem"" in application.conf as in README Mar 18, 2014 dev Fix #19 Add RegisterByRef Nov 20, 2014 project Update sbteclipse-plugin from 2.3.0 to 4.0.0 Jan 7, 2016 src Fix #23 Update Akka from 2.3.10 to 2.4.1 Jan 7, 2016 .gitignore Add .gitignore Jul 16, 2013 CHANGELOG Fix #23 Update Akka from 2.3.10 to 2.4.1 Jan 7, 2016 README.rst Fix #22 Add Tell(name, msg) and Tell(name, props, msg) May 2, 2015 build.sbt Fix #23 Update Akka from 2.3.10 to 2.4.1 Jan 7, 2016 README.rst Glokka = Global + Akka Glokka is a Scala library that allows you to register and lookup actors by names in an Akka cluster. See: Erlang's ""global"" module Akka's cluster feature Glokka is used in Xitrum to implement its distributed SockJS feature. See Glokka's Scaladoc. SBT libraryDependencies += ""com.typesafe.akka"" %% ""akka-actor""   % ""2.3.10"" libraryDependencies += ""com.typesafe.akka"" %% ""akka-cluster"" % ""2.3.10"" libraryDependencies += ""com.typesafe.akka"" %% ""akka-contrib"" % ""2.3.10""  libraryDependencies += ""tv.cntt"" %% ""glokka"" % ""2.3""  Create registry import akka.actor.ActorSystem import glokka.Registry  val system    = ActorSystem(""MyClusterSystem"") val proxyName = ""my proxy name"" val registry  = Registry.start(system, proxyName)  You can start multiple registry actors. They must have different proxyName. For convenience, proxyName can be any String, you don't have to URI-escape it. Register actor by props // For convenience, ``actorName`` can be any String, you don't have to URI-escape it. val actorName = ""my actor name""  // Props to create the actor you want to register. val props = ...  registry ! Registry.Register(actorName, props)  If the named actor exists, the registry will just return it. You will receive: Registry.Found(actorName, actorRef)  Otherwise props will be used to create the actor locally (when the actor dies, it will be unregistered automatically). You will receive: Registry.Created(actorName, actorRef)  If you don't need to differentiate Found and Created: registry ! Registry.Register(actorName, props) context.become {   case msg: Registry.FoundOrCreated =>     val actorName = msg.name     val actorRef  = msg.ref }  Register actor by ref registry ! Registry.Register(actorName, actorRefToRegister)  If the actor has not been registered, or has already been registered with the same name, you will receive: Registry.Registered(actorName, actorRef)  Otherwise if there's another actor that has been registered with the name, you will receive: Registry.Conflict(actorName, otherActorRef, actorRefToRegister)  In this case, you may need to stop actorRefToRegister, depending on your application logic. Lookup actor by name Send: registry ! Registry.Lookup(actorName)  You will receive: Registry.Found(actorName, actorRef)  Or: Registry.NotFound(actorName)  Tell If you don't want to lookup and keep the actor reference: registry ! Registry.Tell(actorName, msg)  registry ! Registry.Tell(actorName, props, msg)  If the named actor exists, msg will be sent to it. Otherwise, props will be used to create the named actor, and msg will be sent to it. Cluster Glokka can run in Akka non-cluster mode (local or remote). While developing, you can run Akka in local mode, then later config Akka to run in cluster mode. In cluster mode, Glokka uses Akka's Cluster Singleton Pattern to maintain an actor that stores the name -> actorRef lookup table. Akka config file for a node should look like this (note ""ClusterSystem"" in the source code example above and the config below): akka {   actor {     provider = ""akka.cluster.ClusterActorRefProvider""   }    # This node   remote {     log-remote-lifecycle-events = off     netty.tcp {       hostname = ""127.0.0.1""       port = 2551  # 0 means random port     }   }    cluster {     seed-nodes = [       ""akka.tcp://MyClusterSystem@127.0.0.1:2551"",       ""akka.tcp://MyClusterSystem@127.0.0.1:2552""]      auto-down-unreachable-after = 10s   } }  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xitrum-framework/glokka"	"Library to register and lookup actors by names in an Akka cluster."	"true"
"Distributed Systems"	"Lagom"	"https://www.lightbend.com/lagom"	"Framework for creating microservice-based systems."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"750"	"106"	"107"	"GitHub - lagom/lagom: Reactive Microservices for the JVM Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 106 Star 750 Fork 107 lagom/lagom Code Issues 34 Pull requests 3 Pulse Graphs Reactive Microservices for the JVM https://lightbend.com/lagom 142 commits 2 branches 4 releases 22 contributors Scala 56.8% Java 42.6% Groff 0.5% HTML 0.1% Protocol Buffer 0.0% CSS 0.0% Scala Java Groff HTML Protocol Buffer CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master swagger Nothing to show 1.0.0 1.0.0-RC1 1.0.0-M2 1.0.0-M1 Nothing to show New pull request Latest commit 4f78af4 Jun 28, 2016 dotta committed on GitHub Merge pull request #129 from markusjura/bump-sbt-conductr … Doc: Bump sbt-conductr to 2.1.4 Permalink Failed to load latest commit information. .github Make template lagom specific and add ""WorkingWithGit.md"" Mar 11, 2016 api-tools/src API improvements (#117) Jun 1, 2016 api/src API improvements (#117) Jun 1, 2016 client/src/main API improvements (#117) Jun 1, 2016 cluster/src/main Not too big, not too small. Lagom. Mar 10, 2016 core/src/main/java/com/lightbend/lagom/internal/guice Not too big, not too small. Lagom. Mar 10, 2016 dev Fix : Stopping runAll task takes too much time (#66) (#119) Jun 7, 2016 docs Doc: Bump sbt-conductr to 2.1.4 Jun 28, 2016 immutables/src/main/java/com/lightbend/lagom/javadsl/immutable Not too big, not too small. Lagom. Mar 10, 2016 integration-client/src/main/java/com/lightbend/lagom/javadsl/client/integration Added full details to exceptions when in dev mode May 27, 2016 jackson/src Added full details to exceptions when in dev mode May 27, 2016 logback/src/main Not too big, not too small. Lagom. Mar 10, 2016 persistence API improvements (#117) Jun 1, 2016 project Link to frames version of javadocs Apr 9, 2016 pubsub/src Not too big, not too small. Lagom. Mar 10, 2016 server/src/main API improvements (#117) Jun 1, 2016 service-integration-tests/src/test API improvements (#117) Jun 1, 2016 spi/src/main Not too big, not too small. Lagom. Mar 10, 2016 testkit/src Moved circuit breakers to service locators May 27, 2016 .gitignore harden PersistentEntityActorSpec Mar 10, 2016 .travis-jvmopts Updated travis.yml Mar 10, 2016 .travis.yml Updated travis.yml Mar 10, 2016 CONTRIBUTING.md Added GitHub contributing/templates/readme pages Mar 10, 2016 LICENSE Not too big, not too small. Lagom. Mar 10, 2016 README.md Merge pull request #127 from mjkrumlauf/patch-1 Jun 23, 2016 WorkingWithGit.md Make template lagom specific and add ""WorkingWithGit.md"" Mar 11, 2016 build.sbt Updated documentation for new website (#124) Jun 20, 2016 version.sbt Setting version to 1.0.1-SNAPSHOT Jun 20, 2016 README.md Lagom Framework - The Reactive Microservices Framework Lagom is a Swedish word meaning just right, sufficient. Microservices are about creating services that are just the right size, that is, they have just the right level of functionality and isolation to be able to adequately implement a scalable and resilient system. Lagom focuses on ensuring that your application realises the full potential of the Reactive Manifesto, while delivering a high productivity development environment, and seamless production deployment experience. Learn More Website Documentation Installation Community Chat Mailing list Get help Contributors Chat License This software is licensed under the Apache 2 license, quoted below. Copyright (C) 2009-2016 Lightbend Inc. (https://www.lightbend.com). Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lagom/lagom"	"Framework for creating microservice-based systems."	"true"
"Distributed Systems"	"Akka-tracing"	"https://github.com/levkhomich/akka-tracing"	"A distributed tracing extension for Akka. Provides integration with Play framework, Spray and Akka HTTP."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"204"	"22"	"16"	"GitHub - levkhomich/akka-tracing: A distributed tracing extension for Akka. Provides integration with Play framework, Spray and Akka HTTP. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 22 Star 204 Fork 16 levkhomich/akka-tracing Code Issues 12 Pull requests 1 Wiki Pulse Graphs A distributed tracing extension for Akka. Provides integration with Play framework, Spray and Akka HTTP. 318 commits 2 branches 4 releases Fetching contributors Scala 98.5% Other 1.5% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show v0.4 v0.3 v0.2 v0.1 Nothing to show New pull request Latest commit ac68179 May 22, 2016 levkhomich improve core documentation Permalink Failed to load latest commit information. akka-http/src flushing and raw span tests, remove internal call May 21, 2016 core improve core documentation May 22, 2016 notes update notes Jan 15, 2015 play/src support client side trace sampling May 21, 2016 project bump sbt to 0.13.11, enable travis-ci caching for it May 21, 2016 spray-client/src Switched back to use of Boolean sampled value, for consistency with F… Apr 8, 2016 spray/src support client side trace sampling May 21, 2016 .gitignore Update .gitignore Dec 6, 2014 .travis.yml bump sbt to 0.13.11, enable travis-ci caching for it May 21, 2016 LICENSE initial commit Mar 16, 2014 README.md add codacy badge Mar 17, 2016 README.md Akka Tracing A distributed tracing Akka extension based on Twitter's Zipkin, which can be used as performance diagnostics and debugging tool. It allows you to: trace call hierarchies inside an actor system; debug request processing pipelines (you can log to traces, annotate them with custom key-value pairs); see dependencies between derived requests and their contribution to resulting response time; find and analyse slowest requests in your system. Distributed tracing approach used by Zipkin synergise with akka-remote and akka-cluster. Extension provides integration with Play framework and Spray toolkit. See wiki for more information. Getting started The best way is to use project's activator templates: general features, Scala and Java API, Spray integration, Play integration. Also, you can read tracing overview page. Development Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/levkhomich/akka-tracing"	"A distributed tracing extension for Akka. Provides integration with Play framework, Spray and Akka HTTP."	"true"
"Extensions"	"Ammonite-Ops"	"http://lihaoyi.github.io/Ammonite/#Ammonite-Ops"	"Safe, easy, filesystem operations in Scala as convenient as in the Bash shell."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Ammonite Ammonite Ammonite enables shell-like scripting in the Scala programming language. It is made of a few subprojects: Ammonite-REPL: A Modernized Scala REPL, with tons of bug fixes and features Ammonite-Ops: A Scala Library for convenient, rock-solid Filesystem Operations Ammonite-Shell: A modern replacement for the Bash system shell Depending on why you are here, click on the above links to jump straight to the documentation that interests you. For an overview of the project and it's motivation, check out this talk: Ammonite-REPL A Modernized Scala REPL Ammonite is an improved Scala REPL, re-implemented from first principles. It is much more featureful than the default REPL and comes with a lot of ergonomic improvements and configurability that may be familiar to people coming from IDEs or other REPLs such as IPython or Zsh. It can be combined with Ammonite-Ops to replace Bash as your systems shell, but also can be used alone as a superior version of the default Scala REPL, or as a debugging tool, or for many other fun and interesting things! Getting Started If you want to use Ammonite as a plain Scala shell, download the standalone executable: $ curl -L -o amm https://git.io/vo4w5 && chmod +x amm && ./amm  This will give you access to Ammonite for Scala: With Pretty Printing, Syntax Highlighting for input and output, Artifact Loading in-REPL, and all the other nice Features! If you want to use Ammonite as a filesystem shell, take a look at Ammonite-Shell. If you're not sure what to do with Ammonite, check out the REPL Cookbook for some fun ideas! If you want some initialization code available to the REPL, you can add it to your ~/.ammonite/predef.scala. If you have any questions, come hang out on the mailing list or gitter channel and get help! Ammonite-REPL in SBT You can also try out Ammonite in an existing SBT project, add the following to your build.sbt libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" % ""test"" cross CrossVersion.full  initialCommands in (Test, console) := """"""ammonite.repl.Main().run()""""""  After that, simple hit sbt projectName/test:console To activate the Ammonite REPL You can also pass a string to the run call containing any commands or imports you want executed at the start of every run. If you want Ammonite to be available in all projects, simply add the above snippet to a new file ~/.sbt/0.13/global.sbt. Note: Ammonite-REPL does not support Windows, even though Ammonite-Ops does. See #119 if you are interested in details or want to try your hand at making it work. Features Ammonite-REPL supports many more features than the default REPL, including: Artifact Loading @ import scalatags.Text.all._ error: not found: value scalatags  @ load.ivy(""com.lihaoyi"" %% ""scalatags"" % ""0.4.5"")  @ import scalatags.Text.all._ import scalatags.Text.all._  @ a(""omg"", href:=""www.google.com"").render res2: String = $tq <a href=""www.google.com"">omg</a> $tq Ammonite allows you to load artifacts directly from maven central by copy & pasting their SBT ivy-coordinate-snippet. In addition, you can also load in jars as java.io.Files to be included in the session or simple Strings to be executed using the load command. This makes Ammonite ideal for trying out new libraries or tools. You can pull down projects like Scalaz or Shapeless and immediately start working with them in the REPL: @ load.ivy(""com.chuusai"" %% ""shapeless"" % ""2.2.5"")  @ import shapeless._  @ (1 :: ""lol"" :: List(1, 2, 3) :: HNil) res2: Int :: String :: List[Int] :: HNil = 1 :: lol :: List(1, 2, 3) :: HNil  @ res2(1) res3: String = ""lol""  @ import shapeless.syntax.singleton._  @ 2.narrow res5: 2 = 2 Even non-trivial web frameworks like Finagle or Akka-HTTP can be simply pulled down and run in the REPL! @ load.ivy(""com.twitter"" %% ""finagle-httpx"" % ""6.26.0"")  @ import com.twitter.finagle._; import com.twitter.util._  @ var serverCount = 0  @ var clientResponse = 0  @ val service = new Service[httpx.Request, httpx.Response] { @   def apply(req: httpx.Request): Future[httpx.Response] = { @     serverCount += 1 @     Future.value( @       httpx.Response(req.version, httpx.Status.Ok) @     ) @   } @ }  @ val server = Httpx.serve("":8080"", service)  @ val client: Service[httpx.Request, httpx.Response] = Httpx.newService("":8080"")  @ val request = httpx.Request(httpx.Method.Get, ""/"")  @ request.host = ""www.scala-lang.org""  @ val response: Future[httpx.Response] = client(request)  @ response.onSuccess { resp: httpx.Response => @   clientResponse = resp.getStatusCode @ }  @ Await.ready(response)  @ serverCount res12: Int = 1  @ clientResponse res13: Int = 200  @ server.close() Ammonite-REPL is configured with a set of default resolvers but you can add your own @ load.ivy(""com.ambiata"" %% ""mundane"" % ""1.2.1-20141230225616-50fc792"") error: IvyResolutionException  @ import ammonite.repl._, Resolvers._  @ val oss = Resolver.Http( @   ""ambiata-oss"", @   ""https://ambiata-oss.s3-ap-southeast-2.amazonaws.com"", @   IvyPattern, @   false @ )  @ resolvers() = resolvers() :+ oss  @ load.ivy(""com.ambiata"" %% ""mundane"" % ""1.2.1-20141230225616-50fc792"")  @ import com.ambiata.mundane._ Pretty-printed output @ Seq.fill(10)(Seq.fill(3)(""Foo"")) res0: Seq[Seq[String]] = List(   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo"") )  @ case class Foo(i: Int, s0: String, s1: Seq[String]) defined class Foo  @ Foo(1, """", Nil) res2: ${sessionPrefix}Foo = Foo(1, """", List())  @ Foo( @   1234567, @   ""I am a cow, hear me moo"", @   Seq(""I weigh twice as much as you"", ""and I look good on the barbecue"") @ ) res3: ${sessionPrefix}Foo = Foo(   1234567,   ""I am a cow, hear me moo"",   List(""I weigh twice as much as you"", ""and I look good on the barbecue"") ) Ammonite-REPL uses PPrint to display its output by default. That means that everything is nicely formatted to fit within the width of the terminal, and is copy-paste-able! By default, Ammonite truncates the pretty-printed output to avoid flooding your terminal. If you want to disable truncation, call show(...) on your expression to pretty-print it's full output. You can also pass in an optional height = ... parameter to control how much you want to show before truncation. Configurable Truncation @ Seq.fill(20)(100) res0: Seq[Int] = List(   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100, ...  @ show(Seq.fill(20)(100)) res1: ammonite.pprint.Show[Seq[Int]] = List(   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100 )  @ show(Seq.fill(20)(100), height = 3) res2: ammonite.pprint.Show[Seq[Int]] = List(   100,   100, ...  @ pprintConfig() = pprintConfig().copy(height = 5 )  @ Seq.fill(20)(100) res4: Seq[Int] = List(   100,   100,   100,   100, ... Ammonite-REPL intelligently truncates your output when it's beyond a certain size. You can request for the full output to be printed on-demand, print a certain number of lines, or even change the implicit pprintConfig so subsequent lines all use your new configuration. Save/Load Session Ammonite allows you to save your work half way through, letting you discard and future changes and returning to the state of the world you saved. Defined some memory-hogging variable you didn't need? Loaded the wrong version of some third-party library? Reluctant to reload the REPL because reloading is slow? Fear not! With Ammonite, you can save your important work, do whatever you want later, and simply discard all the jars you loaded, variables you defined @ val veryImportant = 1 veryImportant: Int = 1  @ sess.save()  @ val oopsDontWantThis = 2 oopsDontWantThis: Int = 2  @ // Let's try this new cool new library  @ load.ivy(""com.lihaoyi"" %% ""scalatags"" % ""0.5.3"")  @ veryImportant res4: Int = 1  @ oopsDontWantThis res5: Int = 2  @ import scalatags.Text.all._  @ div(""Hello"").render res7: String = ""<div>Hello</div>""  @ // Oh no, maybe we don't want scalatags!  @ sess.load()  @ veryImportant res9: Int = 1  @ oopsDontWantThis error: not found: value oopsDontWantThis  @ import scalatags.Text.all._ error: not found: value scalatags """""") Apart from plain saves and loads, which simply discard everything after the most recent save, you can also provide a name to these functions. That lets you stop working on a branch, go do something else for a while, and be able to come back later to continue where you left off: @ val (x, y) = (1, 2) x: Int = 1 y: Int = 2  @ sess.save(""xy initialized"")  @ val z = x + y z: Int = 3  @ sess.save(""first z"")  @ sess.load(""xy initialized"")  @ val z = x - y z: Int = -1  @ sess.save(""second z"")  @ z res7: Int = -1  @ sess.load(""first z"")  @ z res9: Int = 3  @ sess.load(""second z"")  @ z res11: Int = -1             """""") Lastly, you have the sess.pop() function. Without any arguments, it behaves the same as sess.load(), reseting you to your last savepoint. However, you can pass in a number of session frames which you'd like to pop, allow you to reset your session to even earlier save points. sess.pop(2) would put you two save-points ago, sess.pop(3) would put you three save-points ago, letting you reach earlier save-points even if you did not give them names. Passing in a large number like sess.pop(999) would reset your session all the way until the start. Ammonite's save and load functionality is implemented via Java class-loaders. Superior Autocomplete The original Scala REPL provides no autocomplete except for the most basic scenarios of value.<complete>. In the Ammonite-REPL, you get the same autocomplete-anywhere support that you get in a modern IDE. @ Seq(1, 2, 3).map(x => x.) getClass            ##                  asInstanceOf        isInstanceOf toString            hashCode            equals              != ==                  %                   /                   * -                   +                   ^                   & |                   >=                  >                   <= <                   >>                  >>>                 << unary_-             unary_+             unary_~             toDouble toFloat             toLong              toInt               toChar toShort             toByte              compareTo           doubleValue ...  @ Futu scala.collection.parallel.FutureThreadPoolTasks scala.collection.parallel.FutureTasks scala.concurrent.impl.Future$PromiseCompletingRunnable scala.concurrent.impl.Future scala.concurrent.Future scala.concurrent.FutureTaskRunner scala.concurrent.Future$InternalCallbackExecutor scala.concurrent.Future$class java.util.concurrent.Future java.util.concurrent.FutureTask$WaitNode java.util.concurrent.FutureTask com.sun.corba.se.impl.orbutil.closure.Future  Neither of these examples work in the standard Scala REPL. Interrupting run-away execution with Ctrl-C @ while(true) () ... hangs ... ^Ctrl-C Interrupted!  @  The traditional Scala REPL doesn't handle runaway code, and gives you no option but to kill the process, losing all your work. Ammonite-REPL lets you interrupt the thread, stop the runaway-command and keep going. Compiler-crash Robustness @ val x = 1 x: Int = 1  @ /* trigger compiler crash */ trait Bar { super[Object].hashCode } error: java.lang.AssertionError: assertion failed  @ 1 + x res1: Int = 2 The default Scala REPL throws away all your work if the compiler crashes. This doesn't make any sense, because all the compiler is is a dumb String => Array[Byte] pipe. In the Ammonite, we simply swap out the broken compiler for a new one and let you continue your work. Other Fixes Apart from the above features, the Ammonite REPL fixes a large number of bugs in the default Scala REPL, including but not limited to: SI-6302 SI-8971 SI-9249 SI-4438 SI-8603 SI-6660 SI-7953 SI-6659 SI-8456 SI-1067 SI-8307 SI-9335 Editing Ammonite by default ships with a custom implementation of readline, which provides... Syntax Highlighting Ammonite syntax highlights both the code you're entering as well as any output being echoed in response. This should make it much easier to work with larger snippets of input. All colors are configurable, and you can easily turn off colors entirely via the Configuration. Stack traces are similarly highlighted, for easier reading: Multi-line editing You can use the Up and Down arrows to navigate between lines within your snippet. Enter only executes the code when you're on the last line of a multi-line snippet, meaning you can take your time, space out your code nicely, and fix any mistakes anywhere in your snippet. History is multi-line too, meaning re-running a multi-line snippet is trivial, even with tweaks. Long gone are the days where you're desperately trying to cram everything in a single line, or curse quietly when you notice a mistake in an earlier line you are no longer able to fix. No more painstakingly crafting a multi-line snippet, and then having to painstakingly fish it line by individual line out of the history so you can run it again! Desktop key-bindings You can use Alt-Left/Right to move forward/backwards by one word at a time or hold down Shift to select text to delete. These compose as you'd be used to: e.g. Shift-Up selects all the text between your current cursor and the same column one row up. Tab and Shift-Tab now work to block-indent and -dedent sections of code, as you'd expect in any desktop editor like Sublime Text or IntelliJ. This further enhances the multi-line editing experience, letting your nicely lay-out your more-complex REPL commands the same way you'd format code in any other editor. Console key-bindings All the readline-style navigation hotkeys like Ctrl-W to delete a word or Esc-Left/Right to navigate one word left/right still work. If you're comfortable with consoles like Bash, Python, IPython or even the default Scala console, you should have no trouble as all the exact same hotkeys work in Ammonite History Search Apart from browsing your command-line history with UP, you can also perform a history search by entering some search term and then pressing UP. That will pull up the most recent history line with that term in it, underlined. You can continue to press UP or DOWN to cycle through the matches, or Backspace or continue typing characters to refine your search to what you want. You can press TAB, or any other command character (LEFT, RIGHT, ...) to end the search and let you continue working with the currently-displayed command. Pressing ENTER will end the search and immediately submit the command to be run. You can also kick off a history search using Ctrl-R, and use Ctrl-R to cycle through the matches. Block Input To enter block input (many independent lines all at once) into the Ammonite-REPL, simply wrap the multiple lines in curly braces { ... }, and Ammonite will wait until you close it before evaluating the contents: @ { @   val x = 1 @   val y = 2 @   x + y @ } x: Int = 1 y: Int = 2 res0_2: Int = 3 As you can see, the contents of the { ... } block are unwrapped and evaluated as top-level statements. You can use this to e.g. declare mutually recursive functions or classes & companion-objects without being forced to squeeze everything onto a single line. If you don't want this un-wrapping behavior, simply add another set of curlies and the block will be evaluated as a normal block, to a single expression: @ {{ @   val x = 1 @   val y = 2 @   x + y @ }} res0: Int = 3 Undo & Redo The Ammonite command-line editor allows you to undo and re-do portions of your edits: Ctrl -: Undo last change Alt/Esc -: Redo last change Each block of typing, deletes, or navigation counts as one undo. This should make it much more convenient to recover from botched copy-pastes or bulk-deletions. Builtins The Ammonite REPL contains a bunch of built-in imports and definitions by default. This includes: Repl API: the way you can interact with the REPL programmatically and access things like it's history, modify it's prompt, etc. repl: the object representing the Repl API, aliased as repl rather than it's full name ammonite.repl.frontend.ReplBridge.repl. Although you can call them API methods directly (e.g. history) you can also call them via the repl object (e.g. repl.history) and you can use autocomplete or typeOf on the repl object to see what is available. Utilities: tools such as time, grep or browse that are independent from the REPL, but are extremely useful to have in it. Artifact Loading implicits to provide the SBT-like syntax All of these are imported by default into any Ammonite REPL, in order to provide a rich and consistent REPL experience. If you want to disable these imports and run the REPL with a clean namespace (with only the core implicits needed for result pretty-printing/type-printing to work) pass in defaultPredef = false to the REPL's Main API or --no-default-predef to the REPL from the command-line. Repl API Ammonite contains a range of useful built-ins implemented as normal functions. Everything inside the ReplAPI trait is imported by default and can be accessed directly by default to control the console. trait ReplAPI {   /**    * Exit the Ammonite REPL. You can also use Ctrl-D to exit    */   def exit = throw ReplExit(())   /**    * Exit the Ammonite REPL. You can also use Ctrl-D to exit    */   def exit(value: Any) = throw ReplExit(value)     /**    * Read/writable prompt for the shell. Use this to change the    * REPL prompt at any time!    */   val prompt: Ref[String]   /**    * The front-end REPL used to take user input. Modifiable!    */   val frontEnd: Ref[FrontEnd]    /**    * Display help text if you don't know how to use the REPL    */   def help: String    /**     * The last exception that was thrown in the REPL; `null` if nothing has     * yet been thrown. Useful if you want additional information from the     * thrown exception than the printed stack trace (e.g. many exceptions have     * additional metadata attached) or if you want to show the stack trace     * on an exception that doesn't normally print it (e.g. seeing the stack     * when a Ctrl-C interrupt happened) via `lastException.printStackTrace`.     */   def lastException: Throwable   /**    * History of commands that have been entered into the shell, including    * previous sessions    */   def fullHistory: History    /**    * History of commands that have been entered into the shell during the    * current session    */   def history: History    /**    * Get the `Type` object of [[T]]. Useful for finding    * what its methods are and what you can do with it    */   def typeOf[T: WeakTypeTag]: Type    /**    * Get the `Type` object representing the type of `t`. Useful    * for finding what its methods are and what you can do with it    *    */   def typeOf[T: WeakTypeTag](t: => T): Type      /**    * Tools related to loading external scripts and code into the REPL    */   def load: Load    /**    * resolvers to use when loading jars     */   def resolvers: Ref[List[Resolver]]    /**    * The colors that will be used to render the Ammonite REPL in the terminal    */   val colors: Ref[Colors]    /**    * Throw away the current scala.tools.nsc.Global and get a new one    */   def newCompiler(): Unit    /**    * Access the compiler to do crazy things if you really want to!    */   def compiler: scala.tools.nsc.Global    /**    * Show all the imports that are used to execute commands going forward    */   def imports: String   /**    * Controls how things are pretty-printed in the REPL. Feel free    * to shadow this with your own definition to change how things look    */   implicit val pprintConfig: Ref[pprint.Config]    implicit def derefPPrint(implicit t: Ref[pprint.Config]): pprint.Config = t()    implicit def tprintColors: pprint.TPrintColors    implicit def codeColors: CodeColors   /**    * Current width of the terminal    */   def width: Int   /**    * Current height of the terminal    */   def height: Int    def replArgs: Vector[ammonite.repl.Bind[_]]    /**    * Lets you configure the pretty-printing of a value. By default, it simply    * disables truncation and prints the entire thing, but you can set other    * parameters as well if you want.    */   def show[T: PPrint](implicit cfg: Config): T => Unit   def show[T: PPrint](t: T,                       width: Integer = 0,                       height: Integer = null,                       indent: Integer = null,                       colors: pprint.Colors = null)                      (implicit cfg: Config = Config.Defaults.PPrintConfig): Unit   /**     * Functions that can be used to manipulate the current REPL session:     * check-pointing progress, reverting to earlier checkpoints, or deleting     * checkpoints by name.     *     * Frames get pushed on a stack; by default, a saved frame is     * accessible simply by calling `load`. If you provide a name     * when `save`ing a checkpoint, it can later be `load`ed directly     * by providing the same name to `load`     *     * Un-named checkpoints are garbage collected, together with their     * classloader and associated data, when they are no longer accessible     * due to `restore`. Named checkpoints are kept forever; call `delete`     * on them if you really want them to go away.     */   def sess: Session } trait Session{   /**     * The current stack of frames     */   def frames: List[Frame]   /**     * Checkpoints your current work, placing all future work into its own     * frames. If a name is provided, it can be used to quickly recover     * that checkpoint later.     */   def save(name: String = """"): Unit    /**     * Discards the last frames, effectively reverting your session to     * the last `save`-ed checkpoint. If a name is provided, it instead reverts     * your session to the checkpoint with that name.     */   def load(name: String = """"): SessionChanged    /**     * Resets you to the last save point. If you pass in `num`, it resets     * you to that many savepoints since the last one.     */   def pop(num: Int = 1): SessionChanged   /**     * Deletes a named checkpoint, allowing it to be garbage collected if it     * is no longer accessible.     */   def delete(name: String): Unit }  trait LoadJar {    /**    * Load a `.jar` file or directory into your JVM classpath    */   def cp(jar: Path): Unit   /**    * Load a library from its maven/ivy coordinates    */   def ivy(coordinates: (String, String, String), verbose: Boolean = true): Unit } trait Load extends (String => Unit) with LoadJar{   /**    * Loads a command into the REPL and    * evaluates them one after another    */   def apply(line: String): Unit    /**    * Loads and executes the scriptfile on the specified path.    * Compilation units separated by `@\n` are evaluated sequentially.    * If an error happens it prints an error message to the console.    */    def exec(path: Path): Unit    def module(path: Path): Unit    def plugin: LoadJar  } All of these are also available as part of the repl object which is imported in scope by default. Utilities Apart from the core Builtins of the REPL, the Ammonite REPL also includes many helpers that are not strictly necessarily but are very useful in almost all REPL sessions. Here are a few of them time grep browse desugar The REPL also imports the pipe-operators from Ammonite-Ops by default to make it easy for you to use tools like grep interactively, and imports all the Builtins from the repl. These tools are useful but not strictly necessary; time bash$ time ls -a . .. .git .gitignore .idea .travis.yml LICENSE appveyor.yml build.sbt integration internals-docs ops project readme readme.md repl shell sshd target terminal  real	0m0.012s user	0m0.003s sys	0m0.005s haoyi-Ammonite@ time{ls!}  res0: (LsSeq, concurrent.duration.FiniteDuration) = (    "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd ,   25968654 nanoseconds ) Just as bash provides a time command that you can use to see how long a command took to run, Ammonite-Shell provides a time function which serves the same purpose. While the bash version spits out the time in an ad-hoc table format, stuck together with the output of the command, Ammonite-Shell's time instead returns a tuple containing the expression that was evaluated, and the time taken to evaluate it. grep bash$ ls -a . | grep re .gitignore readme readme.md repl haoyi-Ammonite@ ls! wd || grep! ""re""  res0: Seq[GrepResult] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) bash$ ls -a . | grep re .gitignore readme readme.md repl haoyi-Ammonite@ ls! wd |? grep! ""re""  res0: Seq[Path] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) Ammonite provides its own grep command, which lets you easily perform ad-hoc searches within a list. As shown above, Ammonite's grep can be used via || (flatMap) or |? (filter). In the case of ||, it displays the matches found, highlighted, with some additional context before and after the match. When used with |?, it simply returns the relevant items. In general, || is useful for manual exploration, while |? is useful in scripts where you want to deal with the list of matched results later. By default, Ammonite's grep matches a string as a literal. If you want to match via a regex, append a .r to the string literal to turn it into a regex: bash$ ls -a . | grep -G ""re[a-z]\+"" readme readme.md repl haoyi-Ammonite@ ls! wd || grep! ""re[a-z]+"".r  res0: Seq[GrepResult] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) Ammonite's grep isn't limited to ""filesystem""-y things; any collection of objects can be piped through grep! For example, here's grep being used to quickly search through the JVM system properties: haoyi-Ammonite@ // I'm only interested in OS-related properties, show them to me!  haoyi-Ammonite@ sys.props || grep! ""os|OS"".r  res0: collection.mutable.Iterable[GrepResult] = ArrayBuffer(   (""sun.os.patch.level"", ""unknown""),   (""os.arch"", ""x86_64""),   (""os.name"", ""Mac OS X""),   (""os.version"", ""10.11.5""),   (""http.nonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16""),   (""java.awt.printerjob"", ""sun.lwawt.macosx.CPrinterJob""),   (""awt.toolkit"", ""sun.lwawt.macosx.LWCToolkit""),   (""socksNonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16""),   (""ftp.nonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16"") ) You can even use Ammonite's grep to dig through the methods of an object, even large messy objects with far-too-many methods to go over by hand hunting for what you want: haoyi-Ammonite@ typeOf(compiler).members.size // Too many methods to dig through!  res0: Int = 1567 haoyi-Ammonite@ // I forgot what I want but I think it has Raw in the name  haoyi-Ammonite@ typeOf(compiler).members || grep! ""Raw""  res1: Iterable[GrepResult] = List(   class RawTreePrinter,   method newRawTreePrinter,   method isRawParameter,   method isRaw,   method isRawType,   method isRawIfWithoutArgs,   method showRaw$default$7,   method showRaw$default$6,   method showRaw$default$5,   method showRaw$default$4,   method showRaw$default$3,   method showRaw$default$2,   method showRaw,   method showRaw,   method showRaw,   method showRaw ) In general, Ammonite's grep serves the same purpose of grep in the Bash shell: a quick and dirty way to explore large amounts of semi-structured data. You probably don't want to build your enterprise business logic on top of grep's string matching. While you're working, though, grep can be a quick way to find items of interest in collections of things (anything!) too large to sift through by hand, when you're not yet sure exactly what you want. browse browse is a utility that lets you open up far-too-large data structures in the less pager, letting you page through large quantities of text, navigating around it and searching through it, without needing to spam your terminal output with its contents and losing your earlier work to the output-spam. Simple call browse on whatever value you want, e.g. this 50 thousand line ls.rec result show above. If you're dealing with large blobs of data that you want to dig through manually, you might normally format it nicely, write it to a file, and open it in vim or less or an editor such as Sublime Text. browse makes that process quick and convenient. You can customize the browse call like you would a show call or pprint.pprintln call, e.g. setting an optional width, colors or indent. You can also choose a viewer program in case you don't want to use less: e.g. here's a command that would open it up in vim: haoyi-Ammonite@ browse(res0, viewer=""vim"", colors = pprint.Colors.BlackWhite)  Apart from using viewer=""vim"", we also set the colors to black and white because Vim by default doesn't display ANSI colors nicely. You can also pass in a Seq of strings to viewer if you want to pass additional flags to your editor, and of course use any other editor you would like such as ""emacs"" or ""nano"" or ""subl"" desugar desugar allows you to easily see what the compiler is doing with your code before it gets run. For example, in the above calls to desugar, you can see: List(...) being converted to List.apply(...) true -> false being converted to Predef.ArrayAssoc(true).$minus$greater(false) default.write$default, default.SeqishW, etc. being injected as implicits for comprehensions with if filters being converted into the relevant withFilter and map calls In general, if you are having trouble understanding the combination of implicit parameters, implicit conversions, macros, and other odd Scala features, desugar could you see what is left after all the magic happens. desugar only works in Scala 2.11.x and above, not in 2.10.x Script Files Ammonite defines a format that allows you to load external scripts into the REPL; this can be used to save common functionality so it can be used at a later date. In the simplest case, a script file is simply a sequence of Scala statements, e.g. // script.scala // print banner println(""Welcome to the XYZ custom REPL!!"")  // common imports import sys.process._ import collection.mutable  // common initialization code val x = 123 ...  Which you can then load into the REPL as desired: @ mutable.Seq(x) // doesn't work Compilation Failed Main.scala:122: not found: value mutable mutable.Seq(x) // doesn't work ^ Main.scala:122: not found: value x mutable.Seq(x) // doesn't work             ^ @ import ammonite.ops._ @ load.module(cwd / ""script.scala"") Welcome to the XYZ custom REPL!!  @ mutable.Seq(x) // works res1: mutable.Seq[Int] = ArrayBuffer(123)  By default, everything in a script is compiled and executed as a single block. That means that if you want to perform classpath-modifying operations, such as load.cp or load.ivy, its results will not be available within the same script if you want to use methods, values or packages defined in the loaded code. To make this work, break the script up into multiple compilation units with an @ sign, e.g. // print banner println(""Welcome to the XYZ custom REPL!!"")  load.ivy(""org.scalaz"" %% ""scalaz-core"" % ""7.1.1"")  @  // common imports import scalaz._ import Scalaz._  // common initialization code ... Ammonite provides two ways to load scripts, load.exec and load.module. With load.exec the script is executed like it was pasted in the REPL. Exec scripts can access all values previously defined in the REPL, and all side-effects are guaranteed to be applied. This is useful for one-off sets of commands. With load.module, the script is loaded like a Scala module. That means it can't access values previously defined in the REPL, but it is guaranteed to only execute once even if loaded many times by different scripts. If you want to execute the script code multiple times, put it in a function and call it after you load the script. Any scripts you load can themselves load scripts. You can also run scripts using the Ammonite executable from an external shell (e.g. bash): bash$ ./amm path/to/script.scala  All types, values and imports defined in scripts are available to commands entered in REPL after loading the script. You can also make an Ammonite script self-executable by using a shebang #!. This is an example script named hello. There is no need to add the .scala extension. The amm command needs to be in the PATH: #!/usr/bin/env amm  println(""hello world"") make it executable and run it from an external shell (e.g. bash): $ chmod +x /path/to/script $ /path/to/script  Script Arguments Often when calling a script from the external command- line (e.g. Bash), you need to pass arguments to configure its behavior. With Ammonite, this is done by defining a main method, e.g. // Args.scala val x = 1 import ammonite.ops._ def main(i: Int, s: String, path: Path = cwd) = {   println(s""Hello! ${s * i} ${path.relativeTo(cwd)}."") } When the script is run from the command line: ~/amm Args.scala 3 Moo The top-level definitions execute first (e.g. setting x), and then the main method is called with the arguments you passed in. Default arguments behave as you would expect (i.e. they allow you to omit it when calling) and arguments are parsed using the scopt.Read typeclass, which provides parsers for primitives like Int, Double, String, as well as basic data-structures like Seqs (taken as a comma-separated list) and common types like Paths. If you pass in the wrong number of arguments, or if an argument fails to deserialize, the script will fail with an exception. The main method does not get automatically called when you load.module or load.exec a script from within the Ammonite REPL. It gets imported into scope like any other method or value defined in the script, and you can just call it normally. Configuration Ammonite is configured via Scala code, that can live in the ~/.ammonite/predef.scala file, passed in through SBT's initialCommands, or passed to the command-line executable as --predef='...'. Anything that you put in predef.scala will be executed when you load the Ammonite REPL. This is a handy place to put common imports, setup code, or even call load.ivy to load third-party jars. The compilation of the predef is cached, so after the first run it should not noticeably slow down the initialization of your REPL. Some examples of things you can configure: @ // Set the shell prompt to be something else  @ repl.prompt() = "">""  @ // Change the terminal front end; the default is  @ // Ammonite on Linux/OSX and JLineWindows on Windows  @ repl.frontEnd() = ammonite.repl.frontend.FrontEnd.JLineUnix  @ repl.frontEnd() = ammonite.repl.frontend.FrontEnd.JLineWindows  @ repl.frontEnd() = ammonite.repl.frontend.AmmoniteFrontEnd()  @ // Changing the colors used by Ammonite; all at once:  @ repl.colors() = ammonite.repl.Colors.BlackWhite  @ repl.colors() = ammonite.repl.Colors.Default  @ // or one at a time:  @ repl.colors().prompt() = fansi.Color.Red  @ repl.colors().ident() = fansi.Color.Green  @ repl.colors().`type`() = fansi.Color.Yellow  @ repl.colors().literal() = fansi.Color.Magenta  @ repl.colors().prefix() = fansi.Color.Cyan  @ repl.colors().comment() = fansi.Color.Red  @ repl.colors().keyword() = fansi.Bold.On  @ repl.colors().selected() = fansi.Underlined.On  @ repl.colors().error() = fansi.Color.Yellow Refs By default, all the values you're seeing here with the () after them are Refs, defined as trait StableRef[T]{   /**    * Get the current value of the this [[StableRef]] at this instant in time    */   def apply(): T    /**    * Set the value of this [[StableRef]] to always be the value `t`    */   def update(t: T): Unit }  trait Ref[T] extends StableRef[T]{   /**    * Return a function that can be used to get the value of this [[Ref]]    * at any point in time    */   def live(): () => T    /**    * Set the value of this [[Ref]] to always be the value of the by-name    * argument `t`, at any point in time    */   def bind(t: => T): Unit } As you can see from the signature, you can basically interact with the Refs in two ways: either getting or setting their values as values, or binding their values to expressions that will be evaluated every time the Ref's value is needed. As an example of the latter, you can use bind to set your prompt to always include your current working directory repl.prompt.bind(wd.toString + ""@ "")  As is common practice in other shells. Further modifications to make it include e.g. your current branch in Git (which you can call through Ammonite's subprocess API or the current timestamp/user are similarly possible. Compiler Flags Apart from configuration of the rest of the shell through Refs, configuration of the Scala compiler takes place separately through the compiler's own configuration mechanism. You have access to the compiler as compiler, and can modify its settings as you see fit. Here's an example of this in action: @ // Disabling default Scala imports  @ List(1, 2, 3) + ""lol"" res0: String = ""List(1, 2, 3)lol""  @ compiler.settings.noimports.value = true  @ List(1, 2, 3) + ""lol"" // predef imports disappear error: not found: value List  @ compiler.settings.noimports.value = false  @ List(1, 2, 3) + ""lol"" res3: String = ""List(1, 2, 3)lol""  @ // Disabling Scala language-import enforcement  @ object X extends Dynamic error: extension of type scala.Dynamic needs to be enabled  @ compiler.settings.language.tryToSet(List(""dynamics""))  @ object X extends Dynamic defined object X  @ 1 + 1 // other things still work  @ // Enabling warnings (which are disabled by default)  @ List(1) match { case _: List[Double] => 2 } res7: Int = 2  @ compiler.settings.nowarnings.value = false  @ List(1) match { case _: List[Double] => 2 } warning: $fruitlessTypeTestWarningMessageBlahBlahBlah If you want these changes to always be present, place them in your ~/.ammonite/predef.scala. Embedding Ammonite The Ammonite REPL is just a plain-old-Scala-object, just like any other Scala object, and can be easily used within an existing Scala program. This is useful for things like interactive Debugging or hosting a Remote REPL to interact with a long-lived Scala process, or Instantiating Ammonite inside an existing program to serve as a powerful interactive console. Instantiating Ammonite To use Ammonite inside an existing Scala program, you need to first add it to your dependencies: libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" cross CrossVersion.full  Then instantate it with this code anywhere within your program: package ammonite.repl   object TestMain{   def main(args: Array[String]): Unit = {     Main.main(args ++ Array(""--home"", ""target/tempAmmoniteHome""))   } } You can configure the instantiated REPL by passing in arguments to the Main() call, e.g. to redirect the input/output streams or to run a predef to configure it further. Debugging Ammonite can be used as a tool to debug any other Scala program, by conveniently opening a REPL at any point within your program with which you can interact with live program data, similar to pdb/ipdb in Python. To do so, first add Ammonite to your classpath, e.g. through this SBT snippet: libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" cross CrossVersion.full  Note that unlike the snippet given above, we leave out the % ""test"" because we may want ammonite to be available within the ""main"" project, and not just in the unit tests. Then, anywhere within your program, you can place a breakpoint via: package ammonite.integration object TestMain {   def main(args: Array[String]): Unit = {     val hello = ""Hello""     // Break into debug REPL with     ammonite.repl.Main(       predef = ""println(\""Starting Debugging!\"")""     ).run(       ""hello"" -> hello,       ""fooValue"" -> foo()     )   }   def foo() = 1 } And when your program reaches that point, it will pause and open up an Ammonite REPL with the values you provided it bound to the names you gave it. From there, you can interact with those values as normal Scala values within the REPL. Use Ctrl-D or exit to exit the REPL and continue normal program execution. Note that the names given must be plain Scala identifiers. Here's an example of it being used to debug changes to the WootJS webserver: In this case, we added the debug statement within the websocket frame handler, so we can inspect the values that are taking part in the client-server data exchange. You can also put the debug statement inside a conditional, to make it break only when certain interesting situations (e.g. bugs) occur. As you can see, you can bind the values you're interested in to names inside the debug REPL, and once in the REPL are free to explore them interactively. The debug() call returns : Any; by default, this is (): Unit, but you can also return custom values by passing in an argument to exit(...) when you exit the REPL. This value will then be returned from debug(), and can be used in the rest of your Scala application. Remote REPL Ammonite can also be used to remotely connect to your running application and interact with it in real-time, similar to Erlang's erl -remsh command. This is useful if e.g. you have multiple Scala/Java processes running but aren't sure when/if you'd want to inspect them for debugging, and if so which ones. With Ammonite, you can leave a ssh server running in each process. You can then and connect-to/disconnect-from each one at your leisure, working with the in-process Scala/Java objects and methods and classes interactively, without having to change code and restart the process to add breakpoints or instrumentation. To do this, add ammonite-sshd to your classpath, for example with SBT: libraryDependencies += ""com.lihaoyi"" % ""ammonite-sshd"" % ""0.6.2"" cross CrossVersion.full  Now add repl server to your application: import ammonite.sshd._ val replServer = new SshdRepl(   SshServerConfig(     address = ""localhost"", // or ""0.0.0.0"" for public-facing shells     port = 22222, // Any available port     username = ""repl"", // Arbitrary     password = ""your_secure_password"" // or """"   ) ) replServer.start()  And start your application. You will be able to connect to it using ssh like this: ssh repl@localhost -p22222 and interact with your running app. Invoke stop() method whenever you want to shutdown ammonite sshd server. Here for example sshd repl server is embedded in the Akka HTTP microservice example: Here we can interact with code live, inspecting values or calling methods on the running system. We can try different things, see which works and which not, and then put our final bits in application code. In this example app is located on local machine, but you are free to connect to any remote node running your code. Security notes: It is probably unsafe to run this server publicly (on host ""0.0.0.0"") in a production, public-facing application. Currently it doesn't supports key-based auth, and password-based auth is notoriously weak. Despite this, it is perfectly possible to run these on production infrastructure: simply leave the host set to ""localhost"", and rely on the machine's own SSH access to keep out unwanted users: you would first ssh onto the machine itself, and then ssh into the Ammonite REPL running on localhost. Typically most organizations already have bastions, firewalls, and other necessary infrastructure to allow trusted parties SSH access to the relevant machines. Running on localhost lets you leverage that and gain all the same security properties without having to re-implement them in Scala. REPL Cookbook The Ammonite Scala REPL is meant to be extended: you can load in arbitary Java/Scala modules from the internet via load.ivy. Using this third-party code, you extend the REPL to do anything you wish to do, and tools like Ammonite-Shell are simply modules like any other. Simple install Java, download Ammonite onto any Linux/OSX machine, and try out one of these fun snippets! HTTP Requests Scraping HTML GUI Applications Office Automation Image Processing Machine Learning HTTP Requests Ammonite does not come with a built-in way to make HTTP requests, but there are Java /Scala modules that do this quite well! Here's an example: Welcome to the Ammonite Repl @ load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.0"") :: loading settings :: :: resolving dependencies :: ... [SUCCESSFUL ] org.scalaj#scalaj-http_2.11;2.2.0!scalaj-http_2.11.jar (63ms)  @ import ammonite.ops._, scalaj.http._ import ammonite.ops._, scalaj.http._  @ val resp = Http(""https://api.github.com/repos/scala/scala"").asString resp: HttpResponse[String] = HttpResponse( {""id"":2888818,""name"":""scala"",""full_name"":""scala/scala"",""owner"": {""login"":""scala"",""id"":57059,""avatar_url"": ""https://avatars.githubusercontent.com/u/57059?v=3"",""gravatar_id"":"""", ""url"":""https://api.github.com/users/scala"",""html_url"":""https://github.com/scala"", ""followers_url"":""https://api.github.com/users/scala/followers"", ""following_url"":""https://api.github.com/users/scala/following{/other_user}"", ""gists_url"":""https://api.github.com/users/scala/gists{/gist_id}"", ""starred_url"":""https://api.github.com/users/scala/starred{/owner}{/repo}"", ""subscriptions_url"":""https://api.github.com/users/scala/subscriptions"", ...  @ val parsed = upickle.json.read(resp.body).asInstanceOf[upickle.Js.Obj] parsed: upickle.Js.Obj = Obj(   ArrayBuffer(     (""id"", Num(2888818.0)),     (""name"", Str(""scala"")),     (""full_name"", Str(""scala/scala"")),     (       ""owner"",       Obj(         ArrayBuffer(           (""login"", Str(""scala"")), ...  @ for((k, v) <- parsed.value) write(cwd/'target/'temp/k, upickle.json.write(v))  @ ls! cwd/'target/'temp res6: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/archive_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/assignees_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/blobs_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/branches_url,, ...  In this example, we use the Scalaj HTTP library to download a URL, and we use uPickle and Ammonite-Ops to parse the JSON and write it into files. uPickle and Ammonite-Ops are bundled with the Ammonite REPL and are used internally, and while Scalaj HTTP isn't, we can simply load it from the public repositories via load.ivy. This is a small example, but it illustrates the potential: if you find yourself needing to scrape some website or bulk-download large quantities of data from some website's HTTP/JSON API, you can start doing so within a matter of seconds using Ammonite. The results are given to you in nicely structured data, and you can deal with them using any Java or Scala libraries or tools you are used to rather than being forced to munge around in Bash. Sometimes, you may find that you need to get data from somewhere without a nice JSON API, which means you'd need to fall back to Scraping HTML... Scraping HTML Not every website has an API, and not every website is meant to be accessed programmatically. That doesn't mean you can't do it! Using libraries like JSoup, you can quickly and easily get the computer to extract useful information from HTML that was meant to humans. Using the Ammonite REPL, you can do it interactively and without needing to set up annoying project scaffolding. @ load.ivy(""org.jsoup"" % ""jsoup"" % ""1.7.2"")  @ import org.jsoup._ // import Jsoup  @ import collection.JavaConversions._ // make Java collections easier to use  @ val doc = Jsoup.connect(""http://en.wikipedia.org/"").get()  @ doc.select(""h1"") res54: select.Elements = <h1 id=""firstHeading"" class=""firstHeading"" lang=""en"">Main Page</h1> @ doc.select(""h2"") // this is huge and messy res55: select.Elements = <h2 id=""mp-tfa-h2"" style=""margin:3px; background:#cef2e0; font-family:inherit; font-size:120%; font-weight:bold; border:1px solid #a3bfb1; text-align:left; color:#000; padding:0.2em 0.4em;""><span class=""mw-headline"" id=""From_today.27s_featured_article"">From today's featured article</span></h2> <h2 id=""mp-dyk-h2"" style=""margin:3px; background:#cef2e0; font-family:inherit; font-size:120%; font-weight:bold; border:1px solid #a3bfb1; text-align:left; color:#000; padding:0.2em 0.4em;""><span class=""mw-headline"" id=""Did_you_know..."">Did you know...</span></h2> ...  @ doc.select(""h2"").map(_.text) // but you can easily pull out the bits you want res56: collection.mutable.Buffer[String] = ArrayBuffer(   ""From today's featured article"",   ""Did you know..."",   ""In the news"",   ""On this day..."",   ""From today's featured list"",   ""Today's featured picture"",   ""Other areas of Wikipedia"",   ""Wikipedia's sister projects"",   ""Wikipedia languages"",   ""Navigation menu"" ) If you wanted to scrape headlines off some news-site or scrape video game reviews off of some gaming site, you don't need to worry about setting up a project and installing libraries and all that stuff. You can simply load libraries like Jsoup right into the Ammonite REPL, copy some example from their website, and start scraping useful information in less than a minute. GUI Applications The Ammonite REPL runs on the Java virtual machine, which means you can use it to create Desktop GUI applications like anyone else who uses Java! Here's an example of how to create a hello-world interactive desktop app using Swing @ {   import javax.swing._, java.awt.event._   val frame = new JFrame(""Hello World Window"")    val button = new JButton(""Click Me"")   button.addActionListener(new ActionListener{     def actionPerformed(e: ActionEvent) = button.setText(""You clicked the button!"")   })   button.setPreferredSize(new java.awt.Dimension(200, 100))   frame.getContentPane.add(button)   frame.pack()   frame.setVisible(true)    } This can be run inside the Ammonite REPL without installing anything, and will show the following window with a single button: When clicked, it changes text: Although this is just a small demo, you can use Ammonite yourself to experiment with GUI programming without needing to go through the hassle of setting up an environment and project and all that rigmarole. Just run the code right in the console! You can even interact with the GUI live in the console, e.g. running this snippet of code to add another action listener to keep count of how many times you clicked the button @ {   var count = 0   button.addActionListener(new ActionListener{     def actionPerformed(e: ActionEvent) = {       count += 1       frame.setTitle(""Clicks: "" + count)     }   })   }  Which immediately becomes visible in the title of the window: Even while you're clicking on the button, you can still access count in the console: @ count res12: Int = 6 This is a level of live interactivity which is traditionally hard to come by in the world of desktop GUI applications, but with the Ammonite REPL, it's totally seamless Office Automation Apart from writing code, you very often find yourself dealing with documents and spreadsheets of various sorts. This is often rather tedious. Wouldn't it be cool if you could deal with these things programmatically? It turns out that there are open-soure Java libraries such as Apache POI that let you do this, and with the Ammonite-REPL you can quickly and easily load these libraries and get to work on your favorite documents. Here's an example extracting some data from my old Resume, in .docx format: @ load.ivy(""org.apache.poi"" % ""poi-ooxml"" % ""3.13"")  @ import ammonite.ops._                  // Prepare to deal with some files @ import org.apache.poi.xwpf.usermodel._ // Bring Ms-Word APIs into scope @ import collection.JavaConversions._    // Make use of Java collections easier  @ val path = cwd/'repl/'src/'test/'resources/'testdata/""Resume.docx""  @ val docx = new XWPFDocument(new java.io.ByteArrayInputStream(read.bytes(path)))  @ docx.get<tab> getAllEmbedds                            getParagraphArray getAllPackagePictures                    getParagraphPos getAllPictures                           getParagraphs getBodyElements                          getParagraphsIterator getBodyElementsIterator                  getParent getClass                                 getPart getCommentByID                           getPartById getComments                              getPartType getDocument                              getPictureDataByID ...  @ docx.getParagraphs.map(_.getText) res28: collection.mutable.Buffer[String] = ArrayBuffer(   """""" Haoyi Li   """""",   """""" Education	Massachusetts Institute of Technology		Cambridge, MA   """""",   """""" Bachelor of Science degree in Computer Science, GPA 4.8/5.0	 Sep 2010 – Jun 2013   """""",   """""" Work	Dropbox		San Francisco, CA   """""" ...  @ docx.getHyperlinks.map(_.getURL) res27: Array[String] = Array(   ""http://vimeo.com/87845442"",   ""http://www.github.com/lihaoyi/scalatags"",   ""http://www.github.com/lihaoyi/scala.rx"",   ""http://www.github.com/lihaoyi/scala-js-fiddle"",   ""http://www.github.com/lihaoyi/metascala"",   ""https://www.github.com/lihaoyi/macropy"",   ""http://www.github.com/lihaoyi"",   ""http://www.github.com/lihaoyi"" ) As you can see, loading the Apache POI library is just a single command, reading in my resume file is just one or two more, and then you can immediate start exploring the document's data model to see what inside interests you. You even get tab-completion on the methods of the document, making it really easy for you to interactively explore all the things that a word document has to offer! This is just a small example, but you can easily do more things in the same vein: Apache POI lets you create/modify/save .docx files in addition to reading from them, meaning you can automatically perform batch operations on large numbers of documents. The library also provides mechanisms to load in Excel spreadsheets and Powerpoint slide decks, meaning you have easy, programmable access to the great bulk of any Microsoft-Office files you find yourself dealing with. Image Processing You can perform lots of image operations in Java. You can use BufferedImagee if you want to access the low-level details or read/write individual pixels, and using Java2D you can draw shapes, perform transforms, or do anything you could possibly want to do with the images. There are also simple libraries like Thumbnailator if you're doing basic things like renaming/resizing/rotating and don't need pixel-per-pixel access. This is an example of using Thumbnailator to resize a folder of images and put them somewhere else: @ load.ivy(""net.coobird"" % ""thumbnailator"" % ""0.4.8"")  @ import net.coobird.thumbnailator._ import net.coobird.thumbnailator._  @ val images = ls! cwd/'repl/'src/'test/'resources/'testdata/'images images: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/GettingStarted.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/Highlighting.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/SystemShell.png ) @ val dest = cwd/'target/'thumbnails dest: Path = /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails  @ mkdir! dest  @ for(i <- images) {     Thumbnails.of(i.toString).size(200, 200).toFile(dest/i.last toString)   }  @ val thumbnails = ls! dest thumbnails: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/GettingStarted.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/Highlighting.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/SystemShell.png )  @ images.map(_.size) // Original image files are pretty big res44: Seq[Long] = List(180913L, 208328L, 227570L)  @ thumbnails.map(_.size) // Thumbnailed image files are much smaller res45: Seq[Long] = List(11129L, 11790L, 11893L) Machine Learning The word ""Machine Learning"" sounds big and intimidating, like something you'd need to spend 6 years getting a PhD before you understand. What if you could ""do some machine-learning"" (whatever that means) in your spare time, in a minute or two? It turns out there are many Java libraries that can help you with basics, and with the Ammonite REPL getting started is easy. Here's one example of how you can get started using the OpenNLP project to do some natural-language processing in just a few minutes. The example was found online, and shows how to extract english names from a raw String using NLP: @ load.ivy(""org.apache.opennlp"" % ""opennlp-tools"" % ""1.6.0"") // load OpenNLP  @ // Turns out you need training data, use Scalaj-HTTP to download it  @ load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.0"")  @ val tokenDataUrl = ""http://opennlp.sourceforge.net/models-1.5/en-token.bin""  @ val tokenData = scalaj.http.Http(tokenDataUrl).asBytes tokenData: HttpResponse[Array[Byte]] = HttpResponse(   Array(     80,     75,     3,     4,     20, ...  @ import opennlp.tools.tokenize._ // let's get started with OpenNLP!  @ val str = ""Hi. How are you? This is Mike. Did you see book about Peter Smith?""  @ import java.io.ByteArrayInputStream  @ val tokenModel = new TokenizerModel(new ByteArrayInputStream(tokenData.body))  @ val tokenizer = new TokenizerME(tokenModel)  @ val tokens = tokenizer.tokenize(str) tplems: Array[String] = Array(   ""Hi"",   ""."",   ""How"",   ""are"",   ""you"" ...  @ import opennlp.tools.namefind._ // Turns out we need more test data...  @ val nameDataUrl = ""http://opennlp.sourceforge.net/models-1.5/en-ner-person.bin""  @ val nameData = Http(nameDataUrl).asBytes nameData: HttpResponse[Array[Byte]] = HttpResponse(   Array(     80,     75,     3,     4,     20, ...  @ val nameModel = new TokenNameFinderModel(new ByteArrayInputStream(nameData.body))  @ val nameFinder = new NameFinderME(nameModel) nameFinder: NameFinderME = opennlp.tools.namefind.NameFinderME@491eb5ef  @ val names = nameFinder.find(tokens) names: Array[opennlp.tools.util.Span] = Array([8..9) person, [15..17) person)  @ opennlp.tools.util.Span.spansToStrings(names, tokens) // Woohoo, names! res96: Array[String] = Array(""Mike"", ""Peter Smith"")  This took a while, but only in comparison to the earlier cookbook recipes: this one is still less than 20 steps, which is not bad for something that installs multiple third-party modules, pulls down training data off the internet, and then does natural language processing to extract the english names from a text blob! Obviously we did not go very deep into the field. If you did, it would definitely be a lot more reading and understanding than just blindly following tutorials like I did above, and you probably would find it worth the time to set up a proper project. Nevertheless, this quick 5-minute run through of how to perform the basics of NLP is a fun way to get started whether or not you decide to take it further, and is only possible because of the Ammonite REPL! Play Framework Server /**   * Single-file play framework application!   */ load.ivy(""com.typesafe.play"" %% ""play"" % ""2.5.0"") load.ivy(""com.typesafe.play"" %% ""play-netty-server"" % ""2.5.0"") load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.1"")  @  import play.core.server._, play.api.routing.sird._, play.api.mvc._ import scalaj.http._ val server = NettyServer.fromRouter(new ServerConfig(   rootDir = new java.io.File("".""),   port = Some(19000), sslPort = None,   address = ""0.0.0.0"", mode = play.api.Mode.Dev,   properties = System.getProperties,   configuration = play.api.Configuration(     ""play.server.netty"" -> Map(       ""maxInitialLineLength"" -> 4096,       ""maxHeaderSize"" -> 8192,       ""maxChunkSize"" -> 8192,       ""log.wire"" -> false,       ""eventLoopThreads"" -> 0,       ""transport"" -> ""jdk"",       ""option.child"" -> Map()     )   ) )) {   case GET(p""/hello/$to"") => Action { Results.Ok(s""Hello $to"") } } try {   println(Http(""http://localhost:19000/hello/bar"").asString.body) }finally{   server.stop() } Ammonite's script-running capabilities can also be used as a way to set up lightweight Scala projects without needing SBT or an IDE to get started. For example, here is a single-file Play Framework test that Spins up a HTTP server and Makes a single HTTP request against it and prints the response Shuts down the server. And can be run via ./amm PlayFramework.scala Although this is just a hello world example, you can easily keep the server running (instead of exiting after a test request) and extend it with more functionality, possibly splitting it into multiple Script Files. Ammonite-Ops Rock-solid Filesystem Operations Ammonite-Ops is a library to make common filesystem operations in Scala as concise and easy-to-use as from the Bash shell, while being robust enough to use in large applications without getting messy. It lives in the same repo as the Ammonite REPL, but can easily be used stand-alone in a normal SBT/maven project. To get started with Ammonite-Ops, add this to your build.sbt: libraryDependencies += ""com.lihaoyi"" %% ""ammonite-ops"" % ""0.6.2"" And you're all set! Here's an example of some common operations you can do with Ammonite-Ops import ammonite.ops._  // Pick the directory you want to work with, // relative to the process working dir val wd = cwd/'ops/'target/""scala-2.11""/""test-classes""/'example2  // Delete a file or folder, if it exists rm! wd  // Make a folder named ""folder"" mkdir! wd/'folder  // Copy a file or folder to a particular path cp(wd/'folder, wd/'folder1) // Copy a file or folder *into* another folder at a particular path // There's also `cp.over` which copies it to a path and stomps over // anything that was there before. cp.into(wd/'folder, wd/'folder1)   // List the current directory val listed = ls! wd  // Write to a file without pain! Necessary // enclosing directories are created automatically write(wd/'dir2/""file1.scala"", ""package example\nclass Foo{}\n"") write(wd/'dir2/""file2.scala"", ""package example\nclass Bar{}\n"")  // Rename all .scala files inside the folder d into .java files ls! wd/'dir2 | mv{case r""$x.scala"" => s""$x.java""}  // List files in a folder val renamed = ls! wd/'dir2  // Line-count of all .java files recursively in wd val lineCount = ls.rec! wd |? (_.ext == ""java"") | read.lines | (_.size) sum  // Find and concatenate all .java files directly in the working directory ls! wd/'dir2 |? (_.ext == ""java"") | read |> write! wd/'target/""bundled.java"" These examples make heavy use of Ammonite-Ops' Paths, Operations and Extensions to achieve their minimal, concise syntax As you can see, Ammonite-Ops replaces the common mess of boilerplate: def removeAll(path: String) = {   def getRecursively(f: java.io.File): Seq[java.io.File] = {     f.listFiles.filter(_.isDirectory).flatMap(getRecursively) ++ f.listFiles   }   getRecursively(new java.io.File(path)).foreach{f =>     println(f)     if (!f.delete())       throw new RuntimeException(""Failed to delete "" + f.getAbsolutePath)   }   new java.io.File(path).delete } removeAll(""target/folder/thing"") With a single, sleek expression: rm! cwd/'target/'folder/'thing That handles the common case for you: recursively deleting folders, not-failing if the file doesn't exist, etc. Note: Ammonite-Ops supports Windows experimentally, even if Ammonite-REPL does not. That means you can use these convenient filesystem operations and commands in your Scala programs that run on Windows. Try it out and let me know if there are problems. Paths Ammonite uses strongly-typed data-structures to represent filesystem paths. The two basic versions are: Path: an absolute path, starting from the root RelPath: a relative path, not rooted anywhere Generally, almost all commands take absolute Paths. These are basically defined as: case class Path private[ops] (root: java.nio.file.Path, segments: Vector[String]) With a number of useful operations that can be performed on them. Absolute paths can be created in a few ways: // Get the process' Current Working Directory. As a convention // the directory that ""this"" code cares about (which may differ // from the cwd) is called `wd` val wd = cwd  // A path nested inside `wd` wd/'folder/'file  // A path starting from the root root/'folder/'file  // A path with spaces or other special characters wd/""My Folder""/""My File.txt""  // Up one level from the wd wd/up  // Up two levels from the wd wd/up/up Note that there are no in-built operations to change the `cwd`. In general you should not need to: simply defining a new path, e.g. val target = cwd/'target Should be sufficient for most needs. Above, we made use of the cwd built-in path. There are a number of Paths built into Ammonite: cwd: The current working directory of the process. This can't be changed in Java, so if you need another path to work with the convention is to define a wd variable. root: The root of the filesystem. home: The home directory of the current user. tmp()/tmp.dir(): Creates a temporary file/folder and returns the path. RelPaths RelPaths represent relative paths. These are basically defined as: case class RelPath private[ops] (segments: Vector[String], ups: Int) The same data structure as Paths, except that they can represent a number of ups before the relative path is applied. They can be created in the following ways: // The path ""folder/file"" val rel1 = 'folder/'file val rel2 = 'folder/'file  // The path ""file""; will get converted to a RelPath by an implicit val rel3 = 'file  // The relative difference between two paths val target = cwd/'target/'file assert((target relativeTo cwd) == 'target/'file)  // `up`s get resolved automatically val minus = cwd relativeTo target val ups = up/up assert(minus == ups) In general, very few APIs take relative paths. Their main purpose is to be combined with absolute paths in order to create new absolute paths. e.g. val target = cwd/'target/'file val rel = target relativeTo cwd val newBase = root/'code/'server assert(newBase/rel == root/'code/'server/'target/'file) up is a relative path that comes in-built: val target = root/'target/'file assert(target/up == root/'target) Note that all paths, both relative and absolute, are always expressed in a canonical manner: assert((root/'folder/'file/up).toString == ""/folder"") // not ""/folder/file/..""  assert(('folder/'file/up).toString == ""folder"") // not ""folder/file/.."" So you don't need to worry about canonicalizing your paths before comparing them for equality or otherwise manipulating them. Path Operations Ammonite's paths are transparent data-structures, and you can always access the segments and ups directly. Nevertheless, Ammonite defines a number of useful operations that handle the common cases of dealing with these paths: sealed trait BasePath{   type ThisType <: BasePath   /**     * The individual path segments of this path.     */   def segments: Seq[String]    /**     * Combines this path with the given relative path, returning     * a path of the same type as this one (e.g. `Path` returns `Path`,     * `RelPath` returns `RelPath`     */   def /(subpath: RelPath): ThisType    /**     * Relativizes this path with the given `base` path, finding a     * relative path `p` such that base/p == this.     *     * Note that you can only relativize paths of the same type, e.g.     * `Path` & `Path` or `RelPath` & `RelPath`. In the case of `RelPath`,     * this can throw a [[PathError.NoRelativePath]] if there is no     * relative path that satisfies the above requirement in the general     * case.     */   def relativeTo(target: ThisType): RelPath    /**     * This path starts with the target path, including if it's identical     */   def startsWith(target: ThisType): Boolean    /**     * The last segment in this path. Very commonly used, e.g. it     * represents the name of the file/folder in filesystem paths     */   def last: String    /**     * Gives you the file extension of this path, or the empty     * string if there is no extension     */   def ext: String }  object BasePath {    def invalidChars = Set('/')   def checkSegment(s: String) = {     def fail(msg: String) = throw PathError.InvalidSegment(s, msg)     def considerStr =       ""use the Path(...) or RelPath(...) constructor calls to convert them. ""      s.find(BasePath.invalidChars) match{       case Some(c) => fail(         s""[$c] is not a valid character to appear in a path segment. "" +           ""If you want to parse an absolute or relative path that may have "" +           ""multiple segments, e.g. path-strings coming from external sources"" +           considerStr       )       case None =>     }     def externalStr = ""If you are dealing with path-strings coming from external sources, ""     s match{       case """" =>         fail(           ""Ammonite-Ops does not allow empty path segments "" +             externalStr + considerStr         )       case ""."" =>         fail(           ""Ammonite-Ops does not allow [.] as a path segment "" +             externalStr + considerStr         )       case "".."" =>         fail(           ""Ammonite-Ops does not allow [..] as a path segment "" +             externalStr +             considerStr +             ""If you want to use the `..` segment manually to represent going up "" +             ""one level in the path, use the `up` segment from `ammonite.ops.up` "" +             ""e.g. an external path foo/bar/../baz translates into 'foo/'bar/up/'baz.""         )       case _ =>     }   }   def chunkify(s: java.nio.file.Path) = {     import collection.JavaConversions._     s.iterator().map(_.toString).filter(_ != ""."").toVector   } }   /**   * Represents a value that is either an absolute [[Path]] or a   * relative [[ResourcePath]], and can be constructed from   */ In this definition, ThisType represents the same type as the current path; e.g. a Path's / returns a Path while a RelPath's / returns a RelPath. Similarly, you can only compare or subtract paths of the same type. Apart from RelPaths themselves, a number of other data structures are convertible into RelPaths when spliced into a path using /: Strings Symbolss Array[T]s where T is convertible into a RelPath Seq[T]s where T is convertible into a RelPath Constructing Paths Apart from built-ins like cwd or root or home, you can also construct Ammonite's Paths from Strings, java.io.Files or java.nio.file.Paths: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world""  assert(   RelPath(relStr) == 'hello/'cow,   Path(absStr) == root/'hello/'world  )  // You can also pass in java.io.File and java.nio.file.Path // objects instead of Strings when constructing paths val relIoFile = new java.io.File(relStr) val absNioFile = java.nio.file.Paths.get(absStr)  assert(   RelPath(relIoFile) == 'hello/'cow,   Path(absNioFile) == root/'hello/'world,   Path(relIoFile, root/'base) == root/'base/'hello/'cow ) Trying to construct invalid paths fails with exceptions: val relStr = ""hello/.."" intercept[java.lang.IllegalArgumentException]{   Path(relStr) }  val absStr = ""/hello"" intercept[java.lang.IllegalArgumentException]{   RelPath(absStr) }  val tooManyUpsStr = ""/hello/../.."" intercept[PathError.AbsolutePathOutsideRoot.type]{   Path(tooManyUpsStr) } As you can see, attempting to parse a relative path with Path or an absolute path with RelPath throws an exception. If you're uncertain about what kind of path you are getting, you could use BasePath to parse it: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world"" assert(   FilePath(relStr) == 'hello/'cow,   FilePath(absStr) == root/'hello/'world ) This converts it into a BasePath, which is either a Path or RelPath. It's then up to you to pattern-match on the types and decide what you want to do in each case. You can also pass in a second argument to Path(..., base). If the path being parsed is a relative path, this base will be used to coerce it into an absolute path: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world"" val basePath: FilePath = FilePath(relStr) assert(   Path(relStr, root/'base) == root/'base/'hello/'cow,   Path(absStr, root/'base) == root/'hello/'world,   Path(basePath, root/'base) == root/'base/'hello/'cow ) For example, if you wanted the common behavior of converting relative paths to absolute based on your current working directory, you can pass in cwd as the second argument to Path(...). Apart from passing in Strings or java.io.Files or java.nio.file.Paths, you can also pass in BasePaths you parsed early as a convenient way of converting it to a absolute path, if it isn't already one. In general, Ammonite is very picky about the distinction between relative and absolute paths, and doesn't allow ""automatic"" conversion between them based on current-working-directory the same way many other filesystem APIs (Bash, Java, Python, ...) do. Even in cases where it's uncertain, e.g. you're taking user input as a String, you have to either handle both possibilities with BasePath or explicitly choose to convert relative paths to absolute using some base. While this adds some boilerplate, it should overall result in more robust filesystem code that doesn't contain bugs like this one. Operations Paths not aren't interesting on their own, but serve as a base to use to perform filesystem operations in a concise and easy to use way. Here is a quick tour of the core capabilities that Ammonite-Ops provides: import ammonite.ops._  // Let's pick our working directory val wd: Path = cwd/'ops/'target/""scala-2.11""/""test-classes""/'example3  // And make sure it's empty rm! wd mkdir! wd  // Reading and writing to files is done through the read! and write! // You can write `Strings`, `Traversable[String]`s or `Array[Byte]`s write(wd/""file1.txt"", ""I am cow"") write(wd/""file2.txt"", Seq(""I am cow\n"", ""hear me moo"")) write(wd/'file3, ""I weigh twice as much as you"".getBytes)  // When reading, you can either `read!` a `String`, `read.lines!` to // get a `Vector[String]` or `read.bytes` to get an `Array[Byte]` read! wd/""file1.txt""        ==> ""I am cow"" read! wd/""file2.txt""        ==> ""I am cow\nhear me moo"" read.lines! wd/""file2.txt""  ==> Vector(""I am cow"", ""hear me moo"") read.bytes! wd/""file3""      ==> ""I weigh twice as much as you"".getBytes  // These operations are mirrored in `read.resource`, // `read.resource.lines` and `read.resource.bytes` to conveniently read // files from your classpath: val resourcePath = resource/'test/'ammonite/'ops/'folder/""file.txt"" read(resourcePath).length        ==> 18 read.bytes(resourcePath).length  ==> 18 read.lines(resourcePath).length  ==> 1  // You can read resources relative to any particular class, including // the ""current"" class by passing in `getClass` val relResourcePath = resource(getClass)/'folder/""file.txt"" read(relResourcePath).length        ==> 18 read.bytes(relResourcePath).length  ==> 18 read.lines(relResourcePath).length  ==> 1  // You can also read `InputStream`s val inputStream = new java.io.ByteArrayInputStream(   Array[Byte](104, 101, 108, 108, 111) ) read(inputStream)           ==> ""hello""  // By default, `write` fails if there is already a file in place. Use // `write.append` or `write.over` if you want to append-to/overwrite // any existing files write.append(wd/""file1.txt"", ""\nI eat grass"") write.over(wd/""file2.txt"", ""I am cow\nHere I stand"")  read! wd/""file1.txt""        ==> ""I am cow\nI eat grass"" read! wd/""file2.txt""        ==> ""I am cow\nHere I stand""  // You can create folders through `mkdir!`. This behaves the same as // `mkdir -p` in Bash, and creates and parents necessary val deep = wd/'this/'is/'very/'deep mkdir! deep // Writing to a file also creates neccessary parents write(deep/'deeeep/""file.txt"", ""I am cow"")  // `ls` provides a listing of every direct child of the given folder. // Both files and folders are included ls! wd    ==> Seq(wd/""file1.txt"", wd/""file2.txt"", wd/'file3, wd/'this)  // `ls.rec` does the same thing recursively ls.rec! deep ==> Seq(deep/'deeeep, deep/'deeeep/""file.txt"")  // You can move files or folders with `mv` and remove them with `rm!` ls! deep  ==> Seq(deep/'deeeep) mv(deep/'deeeep, deep/'renamed_deeeep) ls! deep  ==> Seq(deep/'renamed_deeeep)  // `mv.into` lets you move a file into a // particular folder, rather than to particular path mv.into(deep/'renamed_deeeep/""file.txt"", deep) ls! deep/'renamed_deeeep ==> Seq() ls! deep  ==> Seq(deep/""file.txt"", deep/'renamed_deeeep)  // `mv.over` lets you move a file to a particular path, but // if something was there before it stomps over it mv.over(deep/""file.txt"", deep/'renamed_deeeep) ls! deep  ==> Seq(deep/'renamed_deeeep) read! deep/'renamed_deeeep ==> ""I am cow"" // contents from file.txt  // `rm!` behaves the same as `rm -rf` in Bash, and deletes anything: // file, folder, even a folder filled with contents rm! deep/'renamed_deeeep rm! deep/""file.txt"" ls! deep  ==> Seq()  // You can stat paths to find out information about any file or // folder that exists there val info = stat! wd/""file1.txt"" info.isDir  ==> false info.isFile ==> true info.size   ==> 20 info.name   ==> ""file1.txt""  // Ammonite provides an implicit conversion from `Path` to // `stat`, so you can use these attributes directly (wd/""file1.txt"").size ==> 20  // You can also use `stat.full` which provides more information val fullInfo = stat.full(wd/""file1.txt"") fullInfo.ctime: FileTime fullInfo.atime: FileTime fullInfo.group: GroupPrincipal In these definitions, Op1 and Op2 are isomorphic to Function1 and Function2. The main difference is that ops can be called in two ways: rm(filepath) rm! filepath  The latter syntax allows you to use it more easily from the command line, where remembering to close all your parenthesis is a hassle. Indentation signifies nesting, e.g. in addition to write! you also have write.append! and write.over! Operator Reference All of these operations are pre-defined and strongly typed, so feel free to jump to their implementation to look at what they do or what else is available. Here's a shortlist of the one that may interest you: ls! path [doc] returning Vector[Path], and ls.iter! path returning a Iterator[Path] ls.rec! path [doc] and ls.rec.iter! read! path [doc] returning a String, and read.lines! path and read.bytes! path returning Seq[String] and Array[Byte]. You can also use the various read! commands for Reading Resources or reading java.io.InputStreams write(path, contents), [doc], which lets you write Strings, Array[Byte]s, and Seqs of those rm! path, [doc], roughly Bash's rm -rf mv(src, dest), [doc] cp(src, dest), [doc], roughly Bash's cp -r exists! path, [doc] stat! path, [doc] stat.full! path, [doc] ln(src, dest), [doc] kill(9)! processId, [doc] In general, each operator has sensible/safe defaults: rm and cp are recursive rm ignores the file if it doesn't exist all operations that create a file or folder (mkdir, write, mv) automatically create any necessary parent directories write also does not stomp over existing files by default. You need to use write.over In general, this should make these operations much easier to use; the defaults should cover the 99% use case without needing any special flags or fiddling. Extensions Ammonite-Ops contains a set of extension methods on common types, which serve no purpose other than to make things more concise. These turn Scala from a ""relatively-concise"" language into one as tight as Bash scripts, while still maintaining the high level of type-safety and maintainability that comes with Scala code. Traversable These extensions apply to any Traversable: Seqs, Lists, Arrays, and others. things | f is an alias for things map f things || f is an alias for things flatMap f things |? f is an alias for things filter f things |& f is an alias for things reduce f things |! f is an alias for things foreach f These should behave exactly the same as their implementations; their sole purpose is to make things more concise at the command-line. Pipeable thing |> f is an alias for f(thing) This lets you flip around the function and argument, and fits nicely into the Ammonite's | pipelines. Callable f! thing is an alias for f(thing) This is another syntax-saving extension, that makes it easy to call functions without having to constantly be opening and closing brackets. It does nothing else. Chaining The real value of Ammonite is the fact that you can pipe things together as easily as you could in Bash. No longer do you need to write reams of boilerplate. to accomplish simple tasks. Some of these chains are listed at the top of this readme, here are a few more fun examples: // Move all files inside the ""py"" folder out of it ls! wd/""py"" | mv.all*{case d/""py""/x => d/x }  // Find all dot-files in the current folder val dots = ls! wd |? (_.last(0) == '.')  // Find the names of the 10 largest files in the current working directory ls.rec! wd | (x => x.size -> x) sortBy (-_._1) take 10  // Sorted list of the most common words in your .scala source files def txt = ls.rec! wd |? (_.ext == ""scala"") | read def freq(s: Seq[String]) = s groupBy (x => x) mapValues (_.length) toSeq val map = txt || (_.split(""[^a-zA-Z0-9_]"")) |> freq sortBy (-_._2) As you can see, you can often compose elaborate operations entirely naturally using the available pipes, without needing to remember any special flags or techniques. Here's another example: // Ensure that we don't have any Scala files in the current working directory // which have lines more than 100 characters long, excluding generated sources // in `src_managed` folders.  def longLines(p: Path) =   (p, read.lines(p).zipWithIndex |? (_._1.length > 100) | (_._2))  val filesWithTooLongLines = (   ls.rec! cwd |? (_.ext == ""scala"")               | longLines               |? (_._2.length > 0)               |? (!_._1.segments.contains(""src_managed"")) )  assert(filesWithTooLongLines.length == 0) Reading Resources In addition to manipulating paths on the filesystem, you can also manipulate resource paths in order to read resources off of the Java classpath. By default, the path used to load resources is absolute, using the Thread.currentThread().getContextClassLoader. You can also pass in a classloader explicitly to the resource call: val contents = read(resource/'test/'ammonite/'ops/'folder/""file.txt"") assert(contents.contains(""file contents lols""))  val cl = getClass.getClassLoader val contents2 = read(resource(cl)/'test/'ammonite/'ops/'folder/""file.txt"") assert(contents2.contains(""file contents lols"")) If you want to load resources relative to a particular class, pass in a class for the resource to be relative, or getClass to get something relative to the current class. val cls = classOf[test.ammonite.ops.Testing] val contents = read! resource(cls)/'folder/""file.txt"" assert(contents.contains(""file contents lols""))  val contents2 = read! resource(getClass)/'folder/""file.txt"" assert(contents2.contains(""file contents lols"")) In both cases, reading resources is performed as if you did not pass a leading slash into the getResource(""foo/bar"") call. In the case of ClassLoader#getResource, passing in a leading slash is never valid, and in the case of Class#getResource, passing in a leading slash is equivalent to calling getResource on the ClassLoader. Ammonite-Ops ensures you only use the two valid cases in the API, without a leading slash, and not the two cases with a leading slash which are redundant (in the case of Class#getResource, which can be replaced by ClassLoader#getResource) or invalid (a leading slash with ClassLoader#getResource) Note that you can only read! from paths; you can't write to them or perform any other filesystem operations on them, since they're not really files. Note also that resources belong to classloaders, and you may have multiple classloaders in your application e.g. if you are running in a servlet or REPL. Make sure you use the correct classloader (or a class belonging to the correct classloader) to load the resources you want, or else it might not find them. Spawning Subprocesses Ammonite-Ops provides easy syntax for anyone who wants to spawn sub-processes, e.g. commands like ls or git commit -am ""wip"". This is provided through the % and %% operators, which are used as follows: @ import ammonite.ops._ @ import ammonite.ops.ImplicitWd._ @ %ls build.sbt	log		ops		readme		repl		terminal echo		modules		project		readme.md	target		shell res2: Int = 0 @ %%('ls) res3: CommandResult = build.sbt echo log modules ops project readme readme.md repl target terminal ...  In short, % lets you run a command as you would in bash, and dumps the output to standard-out in a similar way, returning the return-code. This lets you run git commands, edit files via vim, open ssh sessions or even start SBT or Python shells right from your Scala REPL! %% on the other hand is intended for programmatic usage: rather than printing to stdout, it returns a CommandResult, which contains the standard output .out and standard error .err of the subprocess. These provide helper methods to retrieve the stdout or stderr as a list of lines val res = %%('ls, ""ops/src/test/resources/testdata"") assert(res.out.lines == Seq(""File.txt"", ""folder1"", ""folder2"")) Or as a single string: val res = %%('ls, ""ops/src/test/resources/testdata"") assert(res.out.string == ""File.txt\nfolder1\nfolder2\n"") Or as an array of bytes: if(Unix()){   val res = %%('echo, ""abc"")   val listed = res.out.bytes   //        assert(listed == ""File.txt\nfolder\nfolder2\nFile.txt"".getBytes)   listed.toSeq } %% throws an ShelloutException containig the CommandResult if the return-code is non-zero. val ex = intercept[ShelloutException]{ %%('ls, ""does-not-exist"") } val res: CommandResult = ex.result assert(   res.exitCode != 0,   res.err.string.contains(""No such file or directory"") ) In both cases, you end up with a CommandResult can then be used however you like. You can also use backticks to execute commands which aren't valid Scala identifiers, e.g. @ %`ssh-add` Enter passphrase for /Users/haoyi/.ssh/id_rsa:  Lastly, you can also pass arguments into these subprocess calls, as Strings, Symbols or Seqs of Strings: @ %git 'branch   gh-pages   history * master   speedip res4: Int = 0  @ %%('git, 'branch) res5: CommandResult =   gh-pages   history * master   speedip  @ %%('git, checkout, ""master"") Already on 'master' res6: CommandResult = M	readme/Index.scalatex Your branch is up-to-date with 'origin/master'.  @ %git(""checkout"", 'master) M	readme/Index.scalatex Already on 'master' Your branch is up-to-date with 'origin/master'. res8: Int = 0  @ val stuff = List(""readme.md"", ""build.sbt"") stuff: List[String] = List(""readme.md"", ""build.sbt"") @ %('ls, '"".gitignore"", stuff) .gitignore	build.sbt	readme.md Ammonite-Ops currently does not provide many convenient ways of piping together multiple processes, but support may come in future if someone finds it useful enough to implement. % calls subprocesses in a way that is compatible with a normal terminal. That means you can easily call things like %vim to open a text editor, %python to open up a Python terminal, or %sbt to open up the SBT prompt! @ %python Python 2.7.6 (default, Sep  9 2014, 15:04:36) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> print ""Hello %s%s"" % (""World"", ""!""*3) Hello World!!! >>> ^D res3: Int = 0  @ %sbt [info] Loading global plugins from /Users/haoyi/.sbt/0.13/plugins [info] Updating {file:/Users/haoyi/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to haoyi (in build file:/Users/haoyi/) >  %% does not do this. Environment Variables Ammonite lets you pass in environment variables to subprocess calls; just pass them in as named arguments when you invoke the subprocess ia % or %%: val res0 = %%('bash, ""-c"", ""echo \""Hello$ENV_ARG\"""", ENV_ARG=12) assert(res0.out.lines == Seq(""Hello12""))  val res1 = %%('bash, ""-c"", ""echo \""Hello$ENV_ARG\"""", ENV_ARG=12) assert(res1.out.lines == Seq(""Hello12""))  val res2 = %%('bash, ""-c"", ""echo 'Hello$ENV_ARG'"", ENV_ARG=12) assert(res2.out.lines == Seq(""Hello$ENV_ARG""))  val res3 = %%('bash, ""-c"", ""echo 'Hello'$ENV_ARG"", ENV_ARG=123) assert(res3.out.lines == Seq(""Hello123"")) Invoking Files You can invoke files on disk using % and %% the same way you can invoke shell commands: val res: CommandResult =   %%(root/'bin/'bash, ""-c"", ""echo 'Hello'$ENV_ARG"", ENV_ARG=123)  assert(res.out.string.trim == ""Hello123"") Current Working Directory In Ammonite the current working directory is not a side-effect unlike in bash. Instead it is an argument to the command you are invoking. It can be passed in explicitly or implicitly. val res1 = %.ls()(cwd) // explicitly // or implicitly import ammonite.ops.ImplicitWd._ val res2 = %ls Note how passing it inexplicitly, you need to use a . before the command-name in order for it to parse properly. That's a limitation of the Scala syntax that isn't likely to change. Another limitation is that when invoking a file, you need to call .apply explicitly rather than relying on the plain-function-call syntax: if(Unix()){   val output = %%.apply(scriptFolder/'echo_with_wd, 'HELLO)(root/'usr)   assert(output.out.lines == Seq(""HELLO /usr"")) } Ammonite-Shell Replacing Bash for the 21st Century Ammonite-Shell is a rock-solid system shell that can replace Bash as the interface to your operating system, using Scala as the primary command and scripting language, running on the JVM. Apart from system operations, Ammonite-Shell provides the full-range of Java APIs for usage at the command-line, including loading libraries from Maven Central. Why would you want to use Ammonite-Shell instead of Bash? Possible reasons include: You can never remember the syntax to write an if-statement in Bash You are sick of googling the same set of inconsistent, ad-hoc commands over and over: ""obviously you need the flag -nrk 7 to sort by file size!"" You've seen Bash's dynamic/sloppy nature fail hard, and don't want your future work to fall victim to the same bugs You think that technology has improved in the last 38 years and a modern systems shell should be better than the shells of our forefathers If none of these apply to you, then likely you won't be interested. If any of these bullet points strikes a chord, then read on to get started. For more discussion about why this project exists, take a look at the presentation slides for Beyond Bash: shell scripting in a typed, OO language, presented at Scala by the Bay 2015, or check out the section on Design Decisions & Tradeoffs. Getting Ammonite-Shell To begin using Ammonite-Shell, simply download the default predef.scala to configure your REPL to be a usable systems shell before downloading the Ammonite-REPL executable (below): $ mkdir ~/.ammonite && curl -L -o ~/.ammonite/predef.scala https://git.io/vo4wx $ curl -L -o amm https://git.io/vo4w5 && chmod +x amm && ./amm  You can then start using Ammonite as a replacement for Bash: Shell Basics Ammonite-Shell isn't backwards compatible with Bash. It isn't even the same language, giving you access to all of Scala instead of the quirky Bash scripting language. Nevertheless, lots of things you'd expect in Bash turn up in Ammonite-Shell: Working Directory bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite haoyi-Ammonite@ wd  res0: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite Bash's pwd is instead called wd. Instead of being a subprocess that prints to stdout, wd is simply a variable holding the working directory. As you can see, the path syntax is also different: as an absolute path, wd must start from root and the path segments must be quoted as Scala ""string""s or 'symbols. Apart from that, however, it is basically the same. The documentation about Paths goes over the syntax and semantics of Paths in more detail. You can navigate around the filesystem using cd!, instead of Bash's cd: bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite bash$ cd target bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target bash$ cd .. bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite haoyi-Ammonite@ wd  res0: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite haoyi-Ammonite@ cd! 'target  res1: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'target haoyi-target@ wd  res2: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'target haoyi-target@ cd! up  res3: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite haoyi-Ammonite@ wd  res4: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite Listing Files bash$ ls LICENSE appveyor.yml build.sbt integration internals-docs ops project readme readme.md repl shell sshd target terminal haoyi-Ammonite@ ls!  res0: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  Bash's ls syntax is tweaked slightly to become ls!. Apart from that, it basically does the same thing. Listing files in other folders behaves similarly: bash$ ls project Constants.scala build.properties build.sbt project target haoyi-Ammonite@ ls! 'project  res0: LsSeq =  ""Constants.scala""   ""build.properties""  ""build.sbt""         'project            'target  bash$ ls project/target config-classes resolution-cache scala-2.10 streams haoyi-Ammonite@ ls! 'project/'target  res0: LsSeq =  ""config-classes""    ""resolution-cache""  ""scala-2.10""        'streams  Again, we have to use the quoted 'symbol/""string"" syntax when defining Paths, but otherwise it behaves identically. You can press <tab> at any point after a / or halfway through a file-name to auto-complete it, just like in Bash. Listing recursively is done via ls.rec, instead of find: bash$ find ops/src/main ops/src/main ops/src/main/scala ops/src/main/scala/ammonite ops/src/main/scala/ammonite/ops ops/src/main/scala/ammonite/ops/Extensions.scala ops/src/main/scala/ammonite/ops/FileOps.scala ops/src/main/scala/ammonite/ops/Model.scala ops/src/main/scala/ammonite/ops/package.scala ops/src/main/scala/ammonite/ops/Path.scala ops/src/main/scala/ammonite/ops/PathUtils.scala ops/src/main/scala/ammonite/ops/Shellout.scala haoyi-Ammonite@ ls.rec! 'ops/'src/'main  res0: LsSeq =  'scala                                            'scala/'ammonite/'ops/""Model.scala"" 'scala/'ammonite                                  'scala/'ammonite/'ops/""Path.scala"" 'scala/'ammonite/'ops                             'scala/'ammonite/'ops/""PathUtils.scala"" 'scala/'ammonite/'ops/""Extensions.scala""          'scala/'ammonite/'ops/""Shellout.scala"" 'scala/'ammonite/'ops/""FileOps.scala""             'scala/'ammonite/'ops/""package.scala""  ls, ls.rec and other commands are all functions defined by Ammonite-Ops. Filesystem Operations Ammonite-Shell uses Ammonite-Ops to provide a nice API to use filesystem operations. The default setup will import ammonite.ops._ into your Ammonite-REPL, gives the nice path-completion shown above, and also provides some additional command-line-friendly functionality on top of the default Ammonite-Ops commands: bash$ mkdir target/test bash$ echo ""hello"" > target/test/hello.txt bash$ cat target/test/hello.txt hello bash$ ls target/test hello.txt bash$ cp target/test/hello.txt target/test/hello2.txt bash$ ls target/test hello.txt hello2.txt bash$ mv target/test/hello.txt target/test/hello3.txt bash$ ls target/test hello2.txt hello3.txt bash$ rm -rf target/test haoyi-Ammonite@ mkdir! 'target/'test   haoyi-Ammonite@ write('target/'test/""hello.txt"", ""hello"")   haoyi-Ammonite@ ls! 'target/'test  res2: LsSeq =  ""hello.txt""  haoyi-Ammonite@ cp('target/'test/""hello.txt"", 'target/'test/""hello2.txt"")   haoyi-Ammonite@ ls! 'target/'test  res4: LsSeq =  ""hello.txt""   ""hello2.txt""  haoyi-Ammonite@ mv('target/'test/""hello.txt"", 'target/'test/""hello3.txt"")   haoyi-Ammonite@ ls! 'target/'test  res6: LsSeq =  ""hello2.txt""  ""hello3.txt""  haoyi-Ammonite@ rm! 'target/'test   Piping Ammonite allows piping similar to how Bash does it. Unlike Bash, Ammonite has a variety of pipes you can use that do different things: things | f is an alias for things map f things || f is an alias for things flatMap f things |? f is an alias for things filter f things |& f is an alias for things reduce f things |! f is an alias for things foreach f For example, this is how you can get the dot-files in the current directory: bash$ ls -a | grep ""^\."" . .. .git .gitignore .idea .travis.yml haoyi-Ammonite@ ls! cwd |? (_.last(0) == '.')  res0: Seq[Path] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".git"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".idea"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".travis.yml"" ) Here, we're using the |? pipe, which basically performs a filter on the paths coming in on the left. In this case, we're checking that for each path, the first character of the last segment of that path is the character '.'. This is slightly more verbose than Bash the bash equivalent shown above, but not by too much. Here is how to find the largest 3 files in a given directory tree: bash$ find ./repl/src -ls | sort -nrk 7 | head -3 169433236        8 -rw-r--r--    1 haoyi            DROPBOX\Domain Users     3977 Jun 15 12:32 ./repl/src/main/scala/ammonite/repl/Repl.scala 169430434       24 -rw-r--r--    1 haoyi            DROPBOX\Domain Users     8492 Jun 15 12:28 ./repl/src/test/scala/ammonite/repl/session/ImportTests.scala 169430433       24 -rw-r--r--    1 haoyi            DROPBOX\Domain Users    10727 Jun 15 12:28 ./repl/src/main/scala/ammonite/repl/interp/Preprocessor.scala haoyi-Ammonite@ ls.rec! wd/'repl/'src | (x => x.size -> x.last) sortBy (-_._1) take 3  res0: Seq[(Long, String)] = List((340324L, ""Resume.docx""), (227570L, ""SystemShell.png""), (208328L, ""Highlighting.png"")) And lastly, here is how to performa recursive line count of all the Scala files in your current directory tree: bash$ find ./ops/src/main -name '*.scala' | xargs wc -l      143 ./ops/src/main/scala/ammonite/ops/Extensions.scala      418 ./ops/src/main/scala/ammonite/ops/FileOps.scala      137 ./ops/src/main/scala/ammonite/ops/Model.scala      119 ./ops/src/main/scala/ammonite/ops/package.scala      369 ./ops/src/main/scala/ammonite/ops/Path.scala      101 ./ops/src/main/scala/ammonite/ops/PathUtils.scala      194 ./ops/src/main/scala/ammonite/ops/Shellout.scala     1481 total haoyi-Ammonite@ ls.rec! wd/'ops/'src/'main |? (_.ext == ""scala"") | read.lines | (_.size) sum  res0: Int = 1483 For more examples of how to use Ammonite's pipes, check out the section on Extensions and Chaining Subprocesses Ammonite provides a convenient way to spawn subprocesses using the % and %% commands: %cmd(arg1, arg2): Spawn a subprocess with the command cmd and command-line arguments arg1, arg2. print out any stdout or stderr, take any input from the current console, and return the exit code when all is done. %%cmd(arg1, arg2): Spawn a subprocess similar to using %, but return the stdout of the subprocess as a String, and throw an exception if the exit code is non-zero. For example, this is how you use the bash command to run a standalone bash script in Bash and Ammonite: bash$ bash ops/src/test/resources/scripts/echo HELLO HELLO haoyi-Ammonite@ %bash('ops/'src/'test/'resources/'scripts/'echo, ""HELLO"")  HELLO  Note that apart from quoting each path segment as a 'symbol, we also need to quote ""HELLO"" as a string. That makes things slightly more verbose than a traditional shell, but also makes it much clearer when arguments are literals v.s. variables. If you are only passing a single argument, or no arguments, Scala allows you to leave off parentheses, as shown: bash$ git branch   389   gh-pages   import-hooks * master   subprocess haoyi-Ammonite@ %git 'branch    389   gh-pages   import-hooks * master   subprocess  bash$ date Wed Jun 15 13:08:19 SGT 2016 haoyi-Ammonite@ %date  Wed Jun 15 13:08:26 SGT 2016  You can use Ammonite-Ops' support for Spawning Subprocesses to call any external programs, even interactive ones like Python or SBT! @ %python Python 2.7.6 (default, Sep  9 2014, 15:04:36) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> print ""Hello %s%s"" % (""World"", ""!""*3) Hello World!!! >>> ^D res3: Int = 0  @ %sbt [info] Loading global plugins from /Users/haoyi/.sbt/0.13/plugins [info] Updating {file:/Users/haoyi/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to haoyi (in build file:/Users/haoyi/) >  Scripting Ammonite-Shell uses Scala as its command and scripting language. Although the commands seem short and concise, you have the full power of the language available at any time. This lets you do things that are difficult or unfeasible to do when using a traditional shell like Bash. Scala Scripting Since Ammonite-Shell runs Scala code, you can perform math: haoyi-Ammonite@ (1 + 2) * 3  res0: Int = 9 haoyi-Ammonite@ math.pow(4, 4)  res1: Double = 256.0 Assign things to values (vals): haoyi-Ammonite@ val x = (1 + 2) * 3  x: Int = 9 haoyi-Ammonite@ x + x  res1: Int = 18 Define re-usable functions: haoyi-Ammonite@ def addMul(x: Int) = (x + 2) * 3  defined function addMul haoyi-Ammonite@ addMul(5)  res1: Int = 21 haoyi-Ammonite@ addMul(5) + 1  res2: Int = 22 haoyi-Ammonite@ addMul(5 + 1)  res3: Int = 24 Or make use of mutable vars, conditionals or loops: haoyi-Ammonite@ var total = 0  total: Int = 0 haoyi-Ammonite@ for(i <- 0 until 100){ if (i % 2 == 0) total += 1 }   haoyi-Ammonite@ total  res2: Int = 50 Typed Values In Ammonite-Shell, everything is a typed value and not just a stream of bytes as is the case in Bash. That means you can assign them to variables and call methods on them just like you can in any programming language: haoyi-Ammonite@ val files = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ val count = files.length  count: Int = 18 As is the case in Scala, you can annotate types. haoyi-Ammonite@ val files: LsSeq = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ val count: Int = files.length  count: Int = 18 This is often not required (e.g. in the earlier example), since Scala has type inference, but it may make your code clearer. Furthermore, if you make a mistake, having types annotated will help the compiler give a more specific error message. The fact that variables are typed means if you try to perform the wrong operation on a variable, you get an error even before the code runs: haoyi-Ammonite@ val files = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ ls + 123  cmd1.scala:1: type mismatch;  found   : Int(123)  required: String val res1 = ls + 123                 ^ Compilation Failed The fact that Ammonite-Shell uses typed, structured values instead of byte streams makes a lot of things easier. For example, all the common data structures like Arrays and Maps are present: haoyi-Ammonite@ val numbers = Array(1, 3, 6, 10)  numbers: Array[Int] = Array(1, 3, 6, 10) haoyi-Ammonite@ numbers(0)  res1: Int = 1 haoyi-Ammonite@ numbers(3)  res2: Int = 10 haoyi-Ammonite@ numbers.sum  res3: Int = 20 haoyi-Ammonite@ numbers(3) = 100   haoyi-Ammonite@ numbers.sum  res5: Int = 110 haoyi-Ammonite@ val scores = Map(""txt"" -> 5, ""scala"" -> 0)  scores: Map[String, Int] = Map(""txt"" -> 5, ""scala"" -> 0) haoyi-Ammonite@ scores(""txt"")  res7: Int = 5 Naturally, these data structures are typed too! Trying to put the wrong sort of value inside of them results in compilation errors before the code gets a chance to run: haoyi-Ammonite@ val numbers = Array(1, 3, 6, 10)  numbers: Array[Int] = Array(1, 3, 6, 10) haoyi-Ammonite@ val myValue = ""3""  myValue: String = ""3"" haoyi-Ammonite@ numbers(myValue) // Doesn't work  cmd2.scala:1: type mismatch;  found   : String  required: Int val res2 = numbers(myValue) // Doesn't work                    ^ Compilation Failed haoyi-Ammonite@ numbers(1) = myValue // Also doesn't work  cmd2.scala:1: type mismatch;  found   : String  required: Int val res2 = numbers(1) = myValue // Also doesn't work                         ^ Compilation Failed haoyi-Ammonite@  // Need to convert the string to an Int  haoyi-Ammonite@ numbers(myValue.toInt)  res2: Int = 10 haoyi-Ammonite@ numbers(1) = myValue.toInt   haoyi-Ammonite@ numbers(1) = ""2"".toInt   In general, apart from the filesystem-specific commands, you should be able to do anything you would expect to be able to do in a Scala shell or Java project. This documentation isn't intended to be a full tutorial on the Scala language, check out the Scala Documentation if you want to learn more! Scala/Java APIs Apart from the pipe operators described in the earlier section on Piping, Ammonite-Shell allows you to call any valid Scala method on any value; it's just Scala after all! Here's an example using normal Scala collection operations to deal with a list of files, counting how many files exist for each extension: haoyi-Ammonite@ val allFiles = ls.rec! 'ops/'src/'test/'resources  allFiles: LsSeq =  'scripts 'test 'testdata 'scripts/'echo 'scripts/'echo_with_wd 'test/'ammonite 'testdata/""File.txt"" 'testdata/'folder1 'testdata/'folder2 'test/'ammonite/'ops 'testdata/'folder1/""Yoghurt Curds Cream Cheese.txt"" 'testdata/'folder2/'folder2a 'testdata/'folder2/'folder2b 'test/'ammonite/'ops/'folder 'testdata/'folder2/'folder2a/""I am.txt"" 'testdata/'folder2/'folder2b/""b.txt"" 'test/'ammonite/'ops/'folder/""file.txt""  haoyi-Ammonite@ val extensionCounts = allFiles.groupBy(_.ext).mapValues(_.length)  extensionCounts: Map[String, Int] = Map(""txt"" -> 5, """" -> 12) Any Java APIs are likewise available: haoyi-Ammonite@ System.out.println(""Hello from Java!"")  Hello from Java!  haoyi-Ammonite@ import java.util._  import java.util._ haoyi-Ammonite@ val date = new Date()  date: Date = Wed Jun 15 13:10:57 SGT 2016 haoyi-Ammonite@ date.getDay()  res3: Int = 3 In fact, Ammonite-Shell allows you to ask for any published third-party Java/Scala library for usage in the shell, and have them downloaded, automatically cached, and made available for use. e.g. we can load popular libraries like Google Guava and using it in the shell: haoyi-Ammonite@ import com.google.common.collect.ImmutableBiMap // Doesn't work  cmd0.scala:1: object google is not a member of package com import com.google.common.collect.ImmutableBiMap // Doesn't work            ^ Compilation Failed haoyi-Ammonite@ load.ivy(""com.google.guava"" % ""guava"" % ""18.0"") // Load from Maven Central  :: loading settings :: url = jar:file:/Users/haoyi/Dropbox%20(Personal)/Workspace/Ammonite/repl/target/scala-2.11/ammonite-repl-0.6.2-2.11.8!/org/apache/ivy/core/settings/ivysettings.xml :: resolving dependencies :: com.google.guava#guava-caller;working 	confs: [default] 	found com.google.guava#guava;18.0 in central 	found com.google.code.findbugs#jsr305;1.3.9 in chain-resolver  haoyi-Ammonite@ import com.google.common.collect.ImmutableBiMap // Works now  import com.google.common.collect.ImmutableBiMap // Works now haoyi-Ammonite@ val bimap = ImmutableBiMap.of(1, ""one"", 2, ""two"", 3, ""three"")  bimap: ImmutableBiMap[Int, String] = {1=one, 2=two, 3=three} haoyi-Ammonite@ bimap.get(1)  res3: String = ""one"" haoyi-Ammonite@ bimap.inverse.get(""two"")  res4: Int = 2 Or Joda Time: haoyi-Ammonite@ load.ivy(""joda-time"" % ""joda-time"" % ""2.8.2"")   :: loading settings :: url = jar:file:/Users/haoyi/Dropbox%20(Personal)/Workspace/Ammonite/repl/target/scala-2.11/ammonite-repl-0.6.2-2.11.8!/org/apache/ivy/core/settings/ivysettings.xml :: resolving dependencies :: joda-time#joda-time-caller;working 	confs: [default] 	found joda-time#joda-time;2.8.2 in central 	found org.joda#joda-convert;1.2 in central  haoyi-Ammonite@ import org.joda.time.{DateTime, Period, Duration}  import org.joda.time.{DateTime, Period, Duration} haoyi-Ammonite@ val dt = new DateTime(2005, 3, 26, 12, 0, 0, 0)  dt: DateTime = 2005-03-26T12:00:00.000+08:00 haoyi-Ammonite@ val plusPeriod = dt.plus(Period.days(1))  plusPeriod: DateTime = 2005-03-27T12:00:00.000+08:00 haoyi-Ammonite@ dt.plus(new Duration(24L*60L*60L*1000L))  res4: DateTime = 2005-03-27T12:00:00.000+08:00 See the section on Artifact Loading to learn more. Writing/Loading Scripts You can write scripts in the same way you write commants, and load them using the load.script(...) and load.module(...) methods. To read more about this, check out the documentation on Script Files. Design Decisions & Tradeoffs Ammonite-Shell takes a fundamentally different architecture from traditional shells, or even more-modern shell-alternatives. Significant differences include: The command & scripting language is a standard, well-known application language (Scala) rather than one specially-designed for the shell The shell runs on the JVM, and can execute or integrate-with arbitrary Java/JVM code or libraries. In this section we'll examine each of these decisions and their consequences in turn. As the incumbents in this space, we'll be looking at traditional system shells like Bash, Zsh or Fish, as well as popular non-system REPLs like the Python/IPython REPL. Scala as the Language The use of Scala as the command & scripting language is unusual among shells, for many reasons. Firstly, most shells implement their own, purpose built language: Bash, Zsh, Fish, and even more obscure ones like Xonsh each implement their own language. Secondly, all of these languages are extremely dynamic, and apart from those most popular languages with REPLs (Python, Ruby, Javascript, ...) tend to be dynamical, interpreted languages. Scala falls at the opposite end of the spectrum: statically typed and compiled. Scala brings many changes over using traditional dynamic, interpreted REPL languages: The code being entered in the shell takes time to compile. The first command easily takes 3-4 to compile, and even when the compiler is ""warm"" there is a 0.2-0.3 second delay before any command begins executing. Once compiled, the code runs extremely fast: compute-intensive code easily runs 50-100x faster in Scala vs Python or Bash, once you've paid the cost of compiling it. Many mistakes get caught even before a command begins executing. This is less valuable for small commands that execute quickly, but for slower commands processing more data, it is nice to get an error after 0.2s of compilation rather than 10s into execution. Apart from the differences between Scala and dynamic languages (Python, Ruby, etc.) for REPL usage, Scala is even further away from the sort of ad-hoc, ultra-dynamic languages most often associated with traditional shells (Bash, sh, zsh, etc.). In particular: Scala provides a set of proper data structures for you to work with. Rather than just byte-streams, you have unicode Strings, proper numbers like Int or Double, absolute Paths and relative RelPaths, Arrays, Maps, Iterators and all sorts of other handy data-structures. Many commands return objects which have fields: this sounds simple until you realize that none of bash/zsh/fish behave this way. Scala is a general-purpose language: you can do math, you can work with Strings, you can write non-trivial algorithms quickly and easily. While this is not surprising coming from a Python REPL, these simple tasks are difficult-to-impossible in traditional system shells like Bash. Scala runs most code in the same process. While you can shell-out to subprocesses in Ammonite using the % syntax, most commands like ls! and rm! are simple functions living in-process rather than in separate processes. This reduces the overhead as compared to spawning new processes each time, but does cause some additional risk: if a command causes the process to crash hard, the entire shell fails. In bash, only that command would fail. The latter set of tradeoffs would be also present in many of the shell-replacements written in dynamic languages, like Xonsh which is written in Python. The earlier set, on the other hand, are pretty unique to Ammonite using Scala. There are both positive and negative points in this list. Running on the JVM Running Ammonite directly on the JVM again is very different from how most shells work: most have their own scripting language, and their own interpreter. Most are implemented in C. What is it like running your code directly as bytecode on the JVM? Here are some of the negatives: You get JVM boot time; although some of the initial several-second delay is due to the Scala compiler's slowness, some of it is also due to the cost of JVM classloading and initialization. While a hello-world JVM project loads instantly, one which uses a large number of class-files takes longer. In contrast, shells written in C load basically instantly. You get the JVM bloat: Ammonite, implemented in only a few thousand lines of code, wraps up to become a 30mb .jar file. That's already larger than most other shells out there, and gets >100mb larger if you bundle the JVM along with it! In general, the JVM class-file format is bloated and inefficient, and there is no way to exclude to numerous un-needed parts of the JVM during the initial download. Project Jigsaw will help with this when it finally lands in Java 9. Ammonite uses hundreds of megabytes (~500mb at last count) of memory, again orders-of-magnitude more than an interpreter written in C or Python. The JVM has traditionally been a very pointer-heavy, memory-intensive platform for running code, and it shows. Project Valhalla would help with this, also scheduled to land in Java 9. In general, the JVM has traditionally been used as a server-side platform for long-runing services, and its slow-startup and bloated disk/memory footprints are a symptom of that. Running on the JVM also has some upsides, though: Ammonite code runs ridiculously fast, once you've paid 0.2-0.3s for its compilation. 50x faster than Python or Bash! This is not a trivial multiplier, and really makes a different if you're dealing with non-trivial data sets: a computation that takes a minute in Ammonite might take an hour if done at the Python REPL! This means that a moderately-large computation which may require special tools, libraries or optimizations to perform in Python might be trivial to implement naively in Ammonite while still enjoying reasonable speed. Ammonite can make use of any JVM APIs, and there are a lot of them! The JVM is a general-purpose platform for a general purpose language (Java), and thus has APIs for doing all sorts of things: dealing with dates and times, math, networks, threads, and many other things. While you may not always need all of these capabilities, it is nice to have them at your disposal where necessary. Ammonite can make use of any Java/JVM libraries, and the excelent infrastructure used by Java developers to download and manage them! Any library is just a load.ivy away. Need to parse Python source into an AST? Load Jython and just do it. Need a high-performance web server? Load Akka-HTTP. Need some data someone stored in a YAML file? Load SnakeYAML and parse it. You don't even need to download and manage these libraries yourself: just load.ivy them from Ammonite, and Java's excellent dependency-management infrastructure will download them (along with any transitive dependencies!), cache them locally, and make them available to your code. There are both pros and cons with running Ammonite on the JVM: we gain its heavy startup/memory overhead, but also get access to its high-performance JIT, massive ecosystem of available packages. Goals of Ammonite-Shell Overall, Ammonite-Shell blurs the line between a ""programming language REPL"" like IPython or Ruby's IRB and a ""system shell"" like Bash or Zsh. Like system shells, Ammonite-Shell provides concise filesystem operations, path-completion, and easy spawning of subprocesses. Like programming language REPLs, it provides a full-fledged, general-purpose language for you to use, rather than a crippled cut-down command-language that is available in most system shells. The goal is to provide something general enough to use as both a system shell and a general-purpose programming language. Traditionally, there has always been some tension when deciding between these: Should I write this script in Bash? I'm already using Bash as my shell It's getting complicated, I can't follow the logic in Bash. Should I re-write it in Python? If I write it in Python, I'll need to deal with argument-parsing and forwarding between Bash and Python and Bash, which is annoying Since my scripts are in Python, should I use Python/IPython as my shell instead of Bash when dealing with these things? But if I use Python/IPython as my shell, basic filesystem operations becomes impossible Traditionally, there really has been no good answer to this dilemma: whether you use Bash or Python to write your scripts, whether you use Bash or Python as your shell, there is always something frustrating about the set-up. With Ammonite-Shell, there is no dilemma. You can use the same concise, general-purpose language for your shell as you would for your scripts, large or small. In Ammonite-Shell, you can concisely deal with files at the command-line with the same language you use to write maintainable scripts, large or small, and the same language that you use to write rock-solid application code. Reference Community Ammonite is primarily maintained by Li Haoyi, with a lot of help from Laszlo Mero over the summer through Google Summer of Code, and help from many other contributors. We have an active Gitter channel and a mailing list: Gitter Channel Mailing List Talks I've also given a number of talks about Ammonite at conferences: Rock-solid Shell Scripting in Scala, at Scala Exchange 2015 Shell scripting in a typed, OO language, at New Object Oriented Languages 2015. Beyond Bash: shell scripting in a typed, OO language, at Scala by the Bay 2015. Slides Scaladoc Here's the Scaladoc for the various projects: Ammonite-Ops Ammonite-Terminal Ammonite-Repl Ammonite-Shell Ammonite-Sshd Although it's best to read the documentation on this page to learn how to use these projects, the Scaladoc is still useful as a reference. Changelog 0.6.2 Fix #403: Errors are silently swallowed in 0.6.1 0.6.1 Fixed #400: load.exec in predef.scala causes NPE in 0.5.9 or 0.6.0 Fixed #398: by default, using Ammonite inside a SBT project uses the old `~/.ammonite/` storage folder, not the `InMemory` storage system. This restores the pre-0.5.9 behavior Fix regression causing BACKSPACE to not work while performing a history-search More internal refactoring 0.6.0 Made browse use the process current working directory if there's no implicit path in scope, since most times it doesn't matter Make the welcome banner ""Welcome to the Ammonite Repl..."" customizable Fixed bug where triggering autocomplete resulted in a broken REPL session 0.5.9 Introduced the desugar helper to Ammonite-REPL, letting you easily see what the compiler is transforming your code into before it gets run. Prefix _root_ to imports from packages, which should reduce the chance of naming collisions Improved source locations for error messages: now failures in scripts have a filename matching the name of the script (instead of Main.scala), and line numbers matching the original line numbers (instead of the line numbers in the synthetic code) thanks to Abhishek Kumar Failures in scripts run using Ammonite from the command line or via load.module should show only the meaningful error and not irrelevant internal stacktraces Wrapper names are now greatly simplified; now the names of wrapper objects for scripts match the name of the script (e.g. MyScript) rather than based on the code hash (e.g. cache5a8890cd7c2ab07eea3fe8d605c7e188) Placed most synthetic code into packages; loaded scripts go into ammonite.scripts and code entered at the REPL goes in ammonite.session. Wrote some basic Internals Documentation in case people want to read about the internal workings of Ammonite in a way that's easier than digging through tons of code. Changed the interface for Embedding Ammonite to make configuring the Ammonite REPL before invoking it programmatically much more consistent. Moved tools such as grep, time, browse from Ammonite-Shell into the base Ammonite-REPL, and made them imported by default, so everyone can enjoy them by default. Imported the various pipe operations from Ammonite-Ops: aliasing .map as |, .filter as |?, etc. to make it more convenient to use tools like grep from the base REPL Massive internal refactors to try and clean up the Ammonite codebase and get it ready for future work; if you find any bugs please report them! Fixed #393: REPL requires two carriage returns to move to a new line Compiler settings set in the predef now get preserved when the session starts, and when replacing compilers, thanks to Rob Norris Fixed #395: Fixed resolver pattern for local ivy, thanks to Aish Fenton Fixed the kill command in Ammonite-Ops, thanks to 杨博 Unknown Ansi escape codes now have their '\u001b' escape character removed, rather than messing up the REPL rendering 0.5.8 write has been generalized to work on any combination of Array, Traversable and Iterator. e.g. write(foo: Iterator[Iterator[Array[String]]]) write no longer inserts newlines between items by default. Introduced the browse helper to Ammonite-Shell, letting you easily open up large data structures in external editors like Vim or Emacs to browse them without spamming the console Improved the error messages for invalid Path segments to make them more specific and suggest alternatives to what a user is trying to do. Broke out the FilePath sub-trait from the BasePath trait, to differentiate those BasePaths are filesystem paths and can be constructed from java.nio.file.Path or java.io.Files (RelPath and Path)from those which can't (ResourcePath) Path.makeTemp has been renamed tmp() and tmp.dir() Arrow-keys now work properly in the previously odd case where they were creating \u033O{A,B,C,D}"" codes instead of \u033[{A,B,C,D} codes Converted all string-encoding methods to take a scala.io.Codec instead of a String or Charset, letting you pass in either of those types and having it be implicitly converted. 0.5.7 Improved performance of various read! commands to be competitive with java.nio (#362) read! and read.lines! now take an optional charset, passed via read(file, charSet: String) or read.lines(file, charSet: String) which defaults to ""utf-8"" Make read! resource read from the Thread.currentThread.getContextClassLoader by default, fixing #348 Re-organize Reading Resources in Ammonite-Ops to allow proper handling of absolute and relative resources by passing in Classs or ClassLoaders Make read! work on InputStreams Renamed InputPath to Readable, a more appropiate name now that it works on two different non-path entities (resources and InputStreams) Bump uPickle and PPrint to 0.3.9 Now published for Scala 2.11.8, thanks to Clark Kampfe 0.5.6 Fixed #341: stack overflow when lsing large directories Fixed regression preventing you from running scripts via ./amm using relative paths #353 Ammonite should be more robust when interacting with other compiler plugins Fixed #352: imports now don't get improperly collapsed, and defining a value called repl no longer borks your session. Improved readline-emulation of AmmoniteFrontEnd: Ctrl-T and Alt-T now properly transpose characters and words, and the kill-ring now properly aggregates multiple consecutive kills. Added asserts to rm cp and mv to prevent you from removing the root folder, or copying/moving folders into themselves. Command-line-undo via Ctrl - and redo via Esc/Alt - are now supported. Page-up and Page-down (fn-up and fn-down on Macs) scrolls through history when used at the start/end of input, allowing you to use page-up/page-down to quickly scroll through history with lots of multi-line blocks. 0.5.5 Experimental support for Ammonite-Ops in Windows! I haven't tested it but basic CI passes here, so try it out and let me know if there are problems (#120) Changes around Ammonite-Ops's definition of Paths: they now wrap a java.nio.file.Path (#346), and thus can be used on Windows, on multiple drives, or with virtual filesystems such as JimFS. Construction of Paths from various types (Strings, java.nio.file.Path, java.io.File) is much more well behaved & consistent now. See Constructing Paths for details. read.resource! root/'foo is now read! resource/'foo Parser improvements which fix bugs when trying to write some multi-line snippets #343 cp and mv now have .into and .over modes #206 Wrapping content is automatically shifted onto a new line, to avoid problems when copying and pasting #205 Thrown exceptions are now made available to you in the REPL under the lastException variable #289, in case you need more metadata from it or you want the stack-trace of a non-printed-by-default exception (e.g. ThreadDeath due to Ctrl-C). Thanks to coderabhishek! Fixed #280: Ammonite REPL confused by singleton types 0.5.4 Improve the pretty-printing for Range.Inclusive #337, thanks to Saheb More fixes to shadowing behavior of types and vals #199 Pressing BACKSPACE now drops you out of history-browsing properly, preserving your edits if you then press ENTER without entering any other characters #338 0.5.3 Added support for multi-line prompts, thanks to thirstycrow Fix #312 lsing empty directory gives error, thanks to coderabhishek Implemented History Search, also known as reverse-i-search/Ctrl-R Fixed #325: error due to function types with by-name parameters Fixed #258: java.util.NoSuchElementException: head of empty list in Ammonite-REPL Fixed #198: NoSuchElementException thrown in REPL when using a type alias to refer to a shapeless coproduct Warnings can now be enabled with the flag compiler.settings.nowarn.value = false Stopped Ivy from spitting out countless useless unknown resolver null warnings when resolving dependencies (#144) Fixed edge cases around import shadowing and sequencing (#199, #248) Started the REPL Cookbook, examples of using the Ammonite REPL to do useful work. 0.5.2 Fixed #80: Support artifact resolvers to load libraries not published to maven central, by Eric Torreborre Fixed #310: java.util.NoSuchElementException: None.get on Shift+Tab without selection, by senia-psm 0.5.1 Fix performance regression causing slowness when C&Ping large snippets #274 Added the ability to pass Script Arguments to Ammonite scripts from an external command line (e.g. bash) #277 Iterator-returning commands like ls!! ls.rec!! and read.lines!! have been renamed ls.iter! ls.rec.iter! and read.lines.iter!, to help cut down on cryptic operators Added the time command to Ammonite-Shell, roughly equivalent to the bash time command, allowing easy timing of simple commands ls.rec now exposes basic configurability of the recursion, allowing you to skip directories or controlling the pre/post-order of results Paths and RelPaths no longer permit ""."" or "".."" as a path segment #215 Paths and RelPaths now us Vector[String] instead of Seq[String] as the segments data-structure, promising more consistent semantics Pretty-printing of the results of ls! now properly gets truncated when too large #209 Cross-build for 2.10.6 #282 Refactor of the CommandResult type being returned from the %% operator, to now properly capture the raw byte output, stdout, stderr, exit code. See Spawning Subprocesses for details. #207 Added a new grep command. Added support for word navigation with Ctrl+Arrow in Linux (#217), thanks to Ian McIntosh Moved the initialization calls from ammonite.repl.Repl.run and ammonite.repl.Repl.debug into ammonite.repl.Main.run and ammonite.repl.Main.debug 0.5.0 Fixed def<tab> auto-complete crasher #257, thanks to Matthew Edwards! Fixed input-height bug around multi-line selection that would case the prompt to fly up the console Multi-line-select tab-indent (and shift-tab-dedent) now works! Pressing [Enter] now only submits the input if your cursor is at the bottom Added a powerful Save/Load Session API, letting you save your work at any point in time and return to it later. Compiled-code-caches are now properly invalidated when you change project-code while using Ammonite as a REPL for an existing SBT project. Simplify the way shelling to to run files as subprocesses works, and align with documentation (#234) 0.4.9 Update to fastparse/scalaparse 0.3.1 Fix for perennial classpath problems, thanks to Johannes Rudolph! Fix wildcard-imports from Java libraries like Joda or Guava #213 Added an MIT license Slightly more robust tab completion #252, thanks to Sanjiv Sahayam! Properly handly EOF in standard input #242, thanks to Patrick Premont! 0.4.8 Swapped to G1 garbage collector to reduce unnecessary memory footprint Allow splicing Seq[String]s into subprocess arguments Fix source packaging which was causing problems with ensime Allow shebang line to make Ammonite scripts more conveniently executable Robustify line-breaking-logic 0.4.7 Path-completion now works when using Ammonite as a filesystem shell Ammonite's filesystem functionality (cd!, wd, path-completion) has been pulled out of Ammonite-REPL, and is now available separately as Ammonite-Shell. Improve the pretty-printing of the ls and ls.rec commands Ammonite can now be used as a Remote REPL into an already-running Scala process, letting you SSH in to poke around at any time while it's running, thanks to Viacheslav Blinov Fix execution of files via symbols in the current working directory. Load.ivy now properly attempts to load artifacts from the local ~/.ivy/cache, ~/.ivy/local and ~/.m2 folders, before fetching from maven central Wrote up a good amount of documentation for Ammonite-Shell: using Ammonite as a Bash replacement 0.4.6 Provide a way of Invoking Files and passing Environment Variables Documented existing approach for setting Compiler Flags Fixed a bug in the readline re-implementation causing barely-full lines in the terminal to mess up cursor positioning and line re-drawing Remove cache1234567890abcdef1234567890abcdef objects from the autocomplete list, because they're not helpful Trim all the useless members of Any from the default import lists. Fix a file-handle-leak for most usages of read.lines and ls/ls.rec Fix bugs #186, #152, #149, #180 0.4.5 Fix for running Ammonite using OpenJDK, thanks to Johannes Rudolph Support for HOME and END keys, thanks to Johannes Rudolph Fix for incorrect syntax highlighting (#159) Support for loading compiler plugins, thanks to Alexandre Archambault You can now use Ammonite as a Debugging tool like Python's pdb, placing an interactive breakpoint anywhere within a normal Scala application 0.4.4 Lots and lots of terminal improvements, courtesy of Erik Osheim Only the last @-delimited block in a script loaded via load.module gets its names dumped into the REPL's environment now, letting you create some semblance of hygiene, thanks to Laszlo Mero 0.4.3 Remove embarassing debug println left behind in autocomplete code Fix pathSeparator so Ammonite-REPL is at least basically-runnable on windows, although buggy Update to more robust version of pprint to fix #140 0.4.2 Fix #139: Can't fix typos? Fix bad wrapping of long lines in ammonite-repl 0.4.1 Fix crasher running the REPL on new machines 0.4.0 Re-added support for 2.10.x, minus features that don't work in it (e.g. scope-aware type-printing) Added a standalone distributable that comes bundled with Scala 2.10.4 or 2.11.7, letting you quickly load and experiment with libraries without SBT User input now has Syntax Highlighting by default! Exception stack traces are now highlighted as well, to make them easier to read Pretty-printing has been extracted into a separate project, and aside from that is greatly improved. Many more common cases (e.g. sealed trait hierarchies) are now pretty-printed rather than falling back to toString Exposed the show function by default, letting you pretty-print any value with custom configuration (wrapping-width, truncation-height, colors, ...) Fixed cases where PPrint/TPrint was causing compilation errors Persistent data is now stored in a ~/.ammonite folder. This includes ~/.ammonite/history, ~/.ammonite/predef.scala, and various cache, thanks to Laszlo Mero You can now define a ~/.ammonite/predef.scala Configuration file which will be executed the first thing when the Ammonite REPL is loaded. This is useful for common imports, load.ivying libraries, or other configuration for your REPL Added the ability to load arbitrary Script Files via load.exec and load.module, thanks to Laszlo Mero Configuration that was previously passed into the REPLs constructor is now done in-REPL, Multi-line editing and other features via a custom terminal interface that should behave just like readline, but with added conveniences. Removed the ability to reload classes; using load.ivy no longer causes all existing values to be lazily recomputed. Added the cd! and wd built-ins to make working with filesystem operations via Ammonite-Ops more pleasant Evaluated values of type Unit are no longer echo-ed to the user Performance improvements to the startup time of the REPL, with more to come Third-party library resolution via load.ivy is now cached after the first call, letting you e.g. load libraries in your ~/.ammonite/predef.scala without waiting for the slow ivy-resolution every startup Standardized the use of Refs for configuration, including the ability to bind them ""live"" to the value of an expression. Allows you to trivially spawn subprocesses, letting you run git commands, edit files via vim, open ssh sessions or even start SBT or Python shells right from your Scala REPL 0.3.2 Fix pretty-printing of higher-kinded types. Drop support for 2.10.x; ammonite is 2.11.x-only now 0.3.1 Many of the collection PPrints are much lazier and will avoid stringifying the whole collection if its going to get truncated anyway. Types now get printed semi-qualified (depending on what's in scope), with simple highlighting. You can define custom TPrint[T]s to provide custom printing for any type. Operator-named two-param generic types are now printed infix by default. 0.3.0 allow predef parameter to be passed into Main.run() call, letting you configure initialization commands or imports Compilation errors in expressions no longer show synthetic code in the message Ivy module loading now lets you configure verbosity level Defining macros in the REPL and using them in subsequent lines now works Output lines are now truncated past a certain length, which is configurable, thanks to Laszlo Mero 0.2.9 Lots of improvements to Ctrl-C and Ctrl-D handling, to make it behave more like other REPLs 0.2.8 Fix #47: PPrint derivation fails with nested case class Fix #14: Exception when trying to use Ammonite REPL #15 by cross building against Scala 2.10.{3,4,5} and 2.11.{3,4,5,6} Autocomplete results are sorted alphabetically (Fixed #42) Fix #39: nothing echoed on multiple import Importing things from Java packages now works properly Capture Exceptions and expose them to repl as repl.lastException including exceptions causing Failures"	"null"	"null"	"Safe, easy, filesystem operations in Scala as convenient as in the Bash shell."	"true"
"Extensions"	"better-files ★ 445 ⧗ 1"	"https://github.com/pathikrit/better-files"	"Simple, safe and intuitive Scala I/O. better-files is a dependency-free pragmatic thin Scala wrapper around Java NIO."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"570"	"35"	"30"	"GitHub - pathikrit/better-files: Simple, safe and intuitive Scala I/O Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 35 Star 570 Fork 30 pathikrit/better-files Code Issues 19 Pull requests 0 Pulse Graphs Simple, safe and intuitive Scala I/O https://github.com/scala/slip/issues/19 381 commits 2 branches 4 releases Fetching contributors Scala 97.6% Java 2.0% HTML 0.4% Scala Java HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show v2.16.0 v2.15.0 v2.14.0 v2.13.0 Nothing to show New pull request Latest commit 75a1425 Jun 17, 2016 pathikrit committed on GitHub Merge pull request #79 from olafurpg/patch-1 … Backtick right-associative example Permalink Failed to load latest commit information. akka Define default value for implicit parameter of byte array methods May 18, 2016 benchmarks Format comments May 11, 2016 core/src Rename write(Array[Byte]) to writeByteArray. Same for append bytes. May 20, 2016 project Fix #61: Add updateImpact Mar 27, 2016 shapeless/src Shapeless Scanner May 11, 2016 site move site folder to top-level Oct 14, 2015 .gitignore setup project Sep 11, 2015 CHANGES.md Rename write(Array[Byte]) to writeByteArray. Same for append bytes. May 21, 2016 LICENSE Release v2.14.1 Jan 30, 2016 README.md Backtick right-associative example Jun 17, 2016 build.sbt Prevent gh-pages build to trigger on CI May 16, 2016 circle.yml Prevent gh-pages build to trigger on CI May 17, 2016 version.sbt Setting version to 2.16.1-SNAPSHOT May 12, 2016 README.md better-files better-files is a dependency-free pragmatic thin Scala wrapper around Java NIO. Tutorial Instantiation Simple I/O Streams and Codecs Java compatibility Pattern matching Globbing File system operations UNIX DSL File attributes File comparison Zip/Unzip Automatic Resource Management Scanner File Monitoring Reactive File Watcher sbt In your build.sbt, add this: libraryDependencies += ""com.github.pathikrit"" %% ""better-files"" % version To use the Akka based file monitor, also add this: libraryDependencies ++= Seq(     ""com.github.pathikrit""  %% ""better-files-akka""  % version,   ""com.typesafe.akka""     %% ""akka-actor""         % ""2.3.15"" ) Latest version: Although this library is compatible with both Scala 2.10 and 2.11, it needs minimum JDK 8. Tests FileSpec FileWatcherSpec Benchmarks Instantiation The following are all equivalent: import better.files._ import java.io.{File => JFile}  val f = File(""/User/johndoe/Documents"")                      // using constructor val f1: File = file""/User/johndoe/Documents""                 // using string interpolator val f2: File = ""/User/johndoe/Documents"".toFile              // convert a string path to a file val f3: File = new JFile(""/User/johndoe/Documents"").toScala  // convert a Java file to Scala val f4: File = root/""User""/""johndoe""/""Documents""             // using root helper to start from root val f5: File = `~` / ""Documents""                             // also equivalent to `home / ""Documents""` val f6: File = ""/User""/""johndoe""/""Documents""                 // using file separator DSL val f7: File = home/""Documents""/""presentations""/`..`         // Use `..` to navigate up to parent Resources in the classpath can be accessed using resource interpolator e.g. resource""production.config"" Note: Rename the import if you think the usage of the class File may confuse your teammates: import better.files.{File => ScalaFile, _} import java.io.File I personally prefer renaming the Java crap instead: import better.files._ import java.io.{File => JFile} File Read/Write Dead simple I/O: val file = root/""tmp""/""test.txt"" file.overwrite(""hello"") file.appendLine().append(""world"") assert(file.contentAsString == ""hello\nworld"") If you are someone who likes symbols, then the above code can also be written as: file < ""hello""     // same as file.overwrite(""hello"") file << ""world""    // same as file.appendLines(""world"") assert(file! == ""hello\nworld"") Or even, right-associatively: ""hello"" `>:` file ""world"" >>: file val bytes: Array[Byte] = file.loadBytes Fluent Interface:  (root/""tmp""/""diary.txt"")   .createIfNotExists()     .appendLine()   .appendLines(""My name is"", ""Inigo Montoya"")   .moveTo(home/""Documents"")   .renameTo(""princess_diary.txt"")   .changeExtensionTo("".md"")   .lines Streams and Codecs Various ways to slurp a file without loading the contents into memory: val bytes  : Iterator[Byte]            = file.bytes val chars  : Iterator[Char]            = file.chars val lines  : Iterator[String]          = file.lines val source : scala.io.BufferedSource   = file.newBufferedSource // needs to be closed, unlike the above APIs which auto closes when iterator ends Note: The above APIs can be traversed atmost once e.g. file.bytes is a Iterator[Byte] which only allows TraversableOnce. To traverse it multiple times without creating a new iterator instance, convert it into some other collection e.g. file.bytes.toStream You can write an Iterator[Byte] or an Iterator[String] back to a file: file.writeBytes(bytes) file.printLines(lines) You can supply your own codec too for anything that does a read/write (it assumes scala.io.Codec.default if you don't provide one): val content: String = file.contentAsString  // default codec // custom codec: import scala.io.Codec file.contentAsString(Codec.ISO8859) //or import scala.io.Codec.string2codec file.write(""hello world"")(codec = ""US-ASCII"") Java interoperability You can always access the Java I/O classes: val file: File = tmp / ""hello.txt"" val javaFile     : java.io.File                 = file.toJava val uri          : java.net.uri                 = file.uri val reader       : java.io.BufferedReader       = file.newBufferedReader  val outputstream : java.io.OutputStream         = file.newOutputStream  val writer       : java.io.BufferedWriter       = file.newBufferedWriter  val inputstream  : java.io.InputStream          = file.newInputStream val path         : java.nio.file.Path           = file.path val fs           : java.nio.file.FileSystem     = file.fileSystem val channel      : java.nio.channel.FileChannel = file.newFileChannel val ram          : java.io.RandomAccessFile     = file.newRandomAccess val fr           : java.io.FileReader           = file.newFileReader val fw           : java.io.FileWriter           = file.newFileWriter(append = true) val printer      : java.io.PrintWriter          = file.newPrintWriter The library also adds some useful implicits to above classes e.g.: file1.reader > file2.writer       // pipes a reader to a writer System.in > file2.out             // pipes an inputstream to an outputstream src.pipeTo(sink)                  // if you don't like symbols  val bytes   : Iterator[Byte]        = inputstream.bytes val bis     : BufferedInputStream   = inputstream.buffered   val bos     : BufferedOutputStream  = outputstream.buffered    val reader  : InputStreamReader     = inputstream.reader val writer  : OutputStreamWriter    = outputstream.writer val printer : PrintWriter           = outputstream.printWriter val br      : BufferedReader        = reader.buffered val bw      : BufferedWriter        = writer.buffered val mm      : MappedByteBuffer      = fileChannel.toMappedByteBuffer Pattern matching Instead of if-else, more idiomatic powerful Scala pattern matching: /**  * @return true if file is a directory with no children or a file with no contents  */ def isEmpty(file: File): Boolean = file match {   case File.Type.SymbolicLink(to) => isEmpty(to)  // this must be first case statement if you want to handle symlinks specially; else will follow link   case File.Type.Directory(files) => files.isEmpty   case File.Type.RegularFile(content) => content.isEmpty   case _ => file.notExists    // a file may not be one of the above e.g. UNIX pipes, sockets, devices etc } // or as extractors on LHS: val File.Type.Directory(researchDocs) = home/""Downloads""/""research"" Globbing No need to port this to Scala: val dir = ""src""/""test"" val matches: Iterator[File] = dir.glob(""**/*.{java,scala}"") // above code is equivalent to: dir.listRecursively.filter(f => f.extension == Some("".java"") || f.extension == Some("".scala"")) You can even use more advanced regex syntax instead of glob syntax: val matches = dir.glob(""^\\w*$"")(syntax = File.PathMatcherSyntax.regex) For custom cases: dir.collectChildren(_.isSymbolicLink) // collect all symlinks in a directory For simpler cases, you can always use dir.list or dir.walk(maxDepth: Int) File system operations Utilities to ls, cp, rm, mv, ln, md5, diff, touch, cat etc: file.touch() file.delete()     // unlike the Java API, also works on directories as expected (deletes children recursively) file.clear()      // If directory, deletes all children; if file clears contents file.renameTo(newName: String) file.moveTo(destination) file.copyTo(destination)       // unlike the default API, also works on directories (copies recursively) file.linkTo(destination)                     // ln file destination file.symbolicLinkTo(destination)             // ln -s file destination file.{checksum, md5, sha1, sha256, sha512, digest}   // also works for directories file.setOwner(user: String)      // chown user file file.setGroup(group: String)     // chgrp group file Seq(file1, file2) `>:` file3     // same as cat file1 file2 > file3 Seq(file1, file2) >>: file3      // same as cat file1 file2 >> file3 file.isReadLocked / file.isWriteLocked / file.isLocked File.newTemporaryDirectory() / File.newTemporaryFile() // create temp dir/file UNIX DSL All the above can also be expressed using methods reminiscent of the command line: import better.files_, Cmds._   // must import Cmds._ to bring in these utils pwd / cwd     // current dir cp(file1, file2) mv(file1, file2) rm(file) /*or*/ del(file) ls(file) /*or*/ dir(file) ln(file1, file2)     // hard link ln_s(file1, file2)   // soft link cat(file1) cat(file1) >>: file touch(file) mkdir(file) mkdirs(file)         // mkdir -p chown(owner, file) chgrp(owner, file) chmod_+(permission, files)  // add permission chmod_-(permission, files)  // remove permission md5(file) / sha1(file) / sha256(file) / sha512(file) unzip(zipFile)(targetDir) zip(file*)(zipFile) File attributes Query various file attributes e.g.: file.name       // simpler than java.io.File#getName file.extension file.contentType file.lastModifiedTime     // returns JSR-310 time file.owner / file.group file.isDirectory / file.isSymbolicLink / file.isRegularFile file.isHidden file.hide() / file.unhide() file.isOwnerExecutable / file.isGroupReadable // etc. see file.permissions file.size                 // for a directory, computes the directory size file.posixAttributes / file.dosAttributes  // see file.attributes file.isEmpty      // true if file has no content (or no children if directory) or does not exist file.isParentOf / file.isChildOf / file.isSiblingOf / file.siblings All the above APIs let's you specify the LinkOption either directly: file.isDirectory(LinkOption.NOFOLLOW_LINKS) Or using the File.Links helper: file.isDirectory(File.Links.noFollow) chmod: import java.nio.file.attribute.PosixFilePermission file.addPermission(PosixFilePermission.OWNER_EXECUTE)      // chmod +X file file.removePermission(PosixFilePermission.OWNER_WRITE)     // chmod -w file assert(file.permissionsAsString == ""rw-r--r--"")  // The following are all equivalent: assert(file.permissions contains PosixFilePermission.OWNER_EXECUTE) assert(file(PosixFilePermission.OWNER_EXECUTE)) assert(file.isOwnerExecutable) File comparison Use == to check for path-based equality and === for content-based equality: file1 == file2    // equivalent to `file1.isSamePathAs(file2)` file1 === file2   // equivalent to `file1.isSameContentAs(file2)` (works for regular-files and directories) file1 != file2    // equivalent to `!file1.isSamePathAs(file2)` file1 =!= file2   // equivalent to `!file1.isSameContentAs(file2)` There are also various Ordering[File] included e.g.: val files = myDir.list.toSeq files.sorted(File.Order.byName)  files.max(File.Order.bySize)  files.min(File.Order.byDepth)  files.max(File.Order.byModificationTime)  files.sorted(File.Order.byDirectoriesFirst) Zip APIs You don't have to lookup on StackOverflow ""How to zip/unzip in Java/Scala?"": // Unzipping: val zipFile: File = file""path/to/research.zip"" val research: File = zipFile.unzipTo(destination = home/""Documents""/""research"")   // Zipping: val zipFile: File = directory.zipTo(destination = home/""Desktop""/""toEmail.zip"")  // Zipping/Unzipping to temporary files/directories: val someTempZipFile: File = directory.zip() val someTempDir: File = zipFile.unzip() assert(directory === someTempDir)  // Gzip handling: File(""countries.gz"").newInputStream.gzipped.lines.take(10).foreach(println) Lightweight ARM Auto-close Java closeables: for {   in <- file1.newInputStream.autoClosed   out <- file2.newOutputStream.autoClosed } in.pipeTo(out) // The input and output streams are auto-closed once out of scope better-files provides convenient managed versions of all the Java closeables e.g. instead of writing: for {  reader <- file.newBufferedReader.autoClosed } foo(reader) You can write: for {  reader <- file.bufferedReader    // returns ManagedResource[BufferedReader] } foo(reader)  // or simply: file.bufferedReader.map(foo) Or use a utility to convert any closeable to an iterator: val eof = -1 val bytes: Iterator[Byte] = inputStream.autoClosedIterator(_.read())(_ != eof).map(_.toByte) Note: The autoClosedIterator only closes the resource when hasNext i.e. (_ != eof) returns false. If you only partially use the iterator e.g. .take(5), it may leave the resource open. In those cases, use the managed autoClosed version instead. Scanner Although java.util.Scanner has a feature-rich API, it only allows parsing primitives. It is also notoriously slow since it uses regexes and does un-Scala things like returns nulls and throws exceptions. better-files provides a faster, richer, safer, more idiomatic and compossible Scala replacement that does not use regexes, allows peeking, accessing line numbers, returns Options whenever possible and lets the user mixin custom parsers: val data = t1 << s""""""   | Hello World   | 1 true 2 3 """""".stripMargin val scanner: Scanner = data.newScanner() assert(scanner.next[String] == ""Hello"") assert(scanner.lineNumber == 1) assert(scanner.next[String] == ""World"") assert(scanner.next[(Int, Boolean)] == (1, true)) assert(scanner.tillEndOfLine() == "" 2 3"") assert(!scanner.hasNext) If you are simply interested in tokens, you can use file.tokens() Writing your own custom scanners: sealed trait Animal case class Dog(name: String) extends Animal case class Cat(name: String) extends Animal  implicit val animalParser: Scannable[Animal] = Scannable {scanner =>   val name = scanner.next[String]   if (name == ""Garfield"") Cat(name) else Dog(name) }  val scanner = file.newScanner() println(scanner.next[Animal]) The shapeless-scanner module let's you scan HLists e.g.: val in = Scanner(""""""   12 Bob True   13 Mary False   26 Rick True """""")  import shapeless._  type Row = Int :: String :: Boolean :: HNil  val out = Seq.fill(3)(in.next[Row]) assert(out == Seq(   12 :: ""Bob"" :: true :: HNil,   13 :: ""Mary"" :: false :: HNil,   26 :: ""Rick"" :: true :: HNil )) File Monitoring Vanilla Java watchers: import java.nio.file.{StandardWatchEventKinds => EventType} val service: java.nio.file.WatchService = myDir.newWatchService myDir.register(service, events = Seq(EventType.ENTRY_CREATE, EventType.ENTRY_DELETE)) The above APIs are cumbersome to use (involves a lot of type-casting and null-checking), are based on a blocking polling-based model, does not easily allow recursive watching of directories and nor does it easily allow watching regular files without writing a lot of Java boilerplate. better-files abstracts all the above ugliness behind a simple interface: val watcher = new ThreadBackedFileMonitor(myDir, recursive = true) {   override def onCreate(file: File) = println(s""$file got created"")   override def onModify(file: File) = println(s""$file got modified"")   override def onDelete(file: File) = println(s""$file got deleted"") } watcher.start() Sometimes, instead of overwriting each of the 3 methods above, it is more convenient to override the dispatcher itself: import java.nio.file.{Path, StandardWatchEventKinds => EventType, WatchEvent}  val watcher = new ThreadBackedFileMonitor(myDir, recursive = true) {   override def dispatch(eventType: WatchEvent.Kind[Path], file: File) = eventType match {     case EventType.ENTRY_CREATE => println(s""$file got created"")     case EventType.ENTRY_MODIFY => println(s""$file got modified"")     case EventType.ENTRY_DELETE => println(s""$file got deleted"")   } } Akka File Watcher better-files also provides a powerful yet concise reactive file watcher based on Akka actors that supports dynamic dispatches: import akka.actor.{ActorRef, ActorSystem} import better.files._, FileWatcher._  implicit val system = ActorSystem(""mySystem"")  val watcher: ActorRef = (home/""Downloads"").newWatcher(recursive = true)  // register partial function for an event watcher ! on(EventType.ENTRY_DELETE) {       case file if file.isDirectory => println(s""$file got deleted"")  }  // watch for multiple events watcher ! when(events = EventType.ENTRY_CREATE, EventType.ENTRY_MODIFY) {      case (EventType.ENTRY_CREATE, file) => println(s""$file got created"")   case (EventType.ENTRY_MODIFY, file) => println(s""$file got modified"") } Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/pathikrit/better-files"	"Simple, safe and intuitive Scala I/O. better-files is a dependency-free pragmatic thin Scala wrapper around Java NIO."	"true"
"Extensions"	"Cassovary ★ 808 ⧗ 3"	"https://github.com/twitter/cassovary"	"A Scala library that is designed from the ground up for space efficiency, handling graphs with billions of nodes and edges."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"835"	"191"	"111"	"GitHub - twitter/cassovary: Cassovary is a simple big graph processing library for the JVM Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 191 Star 835 Fork 111 twitter/cassovary Code Issues 28 Pull requests 7 Pulse Graphs Cassovary is a simple big graph processing library for the JVM http://twitter.com/cassovary 480 commits 4 branches 8 releases Fetching contributors Scala 98.3% Java 1.1% Shell 0.6% Scala Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags issue188 master seq2array sparse_array Nothing to show 5.1.2 5.1.1 5.1.0 5.0.0 4.0.0 3.4.0 3.2.0 2.0.4 Nothing to show New pull request Latest commit 10afd5f Jun 29, 2016 pankajgupta Upgraded sonatype plugin version. Permalink Failed to load latest commit information. cassovary-benchmarks Minor. Oct 27, 2015 cassovary-core/src Merge pull request #208 from szymonm/feature/specialization-test Dec 3, 2015 cassovary-examples Minor changes to load-graph-examples script and to README. Nov 6, 2015 cassovary-server update to the finagle 6.35.0 era dependencies Jun 23, 2016 project Upgraded sonatype plugin version. Jun 29, 2016 .gitignore review fixes Apr 9, 2014 .travis.yml Only doing CI on cassovary-core now. Apr 21, 2015 README.md Minor changes to load-graph-examples script and to README. Nov 6, 2015 sbt Serializing a small computation to trade-off space for time. Nov 4, 2015 README.md Cassovary Cassovary is a simple ""big graph"" processing library for the JVM. Most JVM-hosted graph libraries are flexible but not space efficient. Cassovary is designed from the ground up to first be able to efficiently handle graphs with billions of nodes and edges. A typical example usage is to do large scale graph mining and analysis of a big network. Cassovary is written in Scala and can be used with any JVM-hosted language. It comes with some common data structures and algorithms. Please follow the cassovary project on twitter at @cassovary for updates. Quick Start and Examples After cloning the repository, type ./sbt which will download the sbt launch jar and launch the sbt console. Then type the following in the console: project cassovary-examples runMain HelloLoadGraph There is a subproject included called cassovary-examples containing simple java and scala examples of using the library. See this README to get started with these examples. Some other subprojects to check are cassovary-benchmarks for helping benchmark some graph algorithms and cassovary-server that exposes Cassovary on a web server. Building Cassovary is built using sbt and was tested last using sbt version 0.13.9 ./sbt update (might take a couple of minutes) ./sbt test ./sbt package Alternative for using for local projects ./sbt publishLocal cd ../<dependent project> ./sbt update Using maven published version of library Cassovary is published to maven central with crosspath scala versions 2.10.4 and 2.11.7. Please see the latest version number (such as 6.3.0) released alongside the maven-central image at the top of the README. To use with sbt, substitute the latest version number and use: libraryDependencies += ""com.twitter"" %% ""cassovary-core"" % ""6.3.0"" and resolvers += ""twitter"" at ""http://maven.twttr.com"" The last Cassovary version to support scala 2.9 is 3.4.0, and support for scala version 2.9.x has been discontinued since. Also, Cassovary requires Java 7+ and the last Cassovary version to support Java 6 was 3.4.0. The only dependency that Cassovary uses which is not bundled with it (because of its size) is it.unimi.dsi.fastutil. You can add that dependency in your sbt project as follows: libraryDependencies += ""it.unimi.dsi"" % ""fastutil"" % ""7.0.7"" Comparison to Other Graph Libraries There are many excellent graph mining libraries already in existence. Most of them have one or more of the following characteristics: Written in C/C++. Examples include SNAP from Stanford and GraphLab from CMU. The typical way to use these from JVM is to use JNI bridges. Sacrifice storage efficiency for flexibility. Examples include JUNG which is written in Java but stores nodes and edges as big objects. Are meant to do much more, typically a full graph database. Examples include Neo4J. On the other hand, Cassovary is intended to be easy to use in a JVM-hosted environment and yet be efficient enough to scale to billions of edges. It is deliberately not designed to provide any persistence or database functionality. Also, it currently skips any concerns of partitioning the graph and hence is not directly comparable to distributed graph processing systems like Apache Giraph. This allows complex algorithms to be run on the graph efficiently, an otherwise recurring issue with distributed graph processing systems because of the known difficulty of achieving good graph partitions. On the flip side, the size of the graph it works with is bounded by the memory available in a machine, though the use of space efficient data structures does not seem to make this a limitation for most practical graphs. For example, a SharedArrayBasedDirectedGraph instance of a unidirectional graph with 10M nodes and 1B edges consumes less than 6GB of memory, and scales linearly beyond that. Some other data points for memory usage can be checked out using the script bash cassovary-examples/src/main/bash/load-graph-examples.sh. As the script shows, a randomly generated unidirectional directed graph with 0.5M nodes and 10M edges can be built with 60MB of memory, and one with 5M nodes and 100M edges can be built with 500MB of memory. Loading both directions of those graphs takes respectively 120MB and 1.1GB of memory. Mailing list http://groups.google.com/group/twitter-cassovary Please follow the cassovary project on twitter at @cassovary for updates. Bugs Please report any bugs to: https://github.com/twitter/cassovary/issues Acknowledgments Thanks to all the contributors of Cassovary. We use the Yourkit Java Profiler for profiling and tuning Cassovary. License Copyright 2015 Twitter, Inc. Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/cassovary"	"A Scala library that is designed from the ground up for space efficiency, handling graphs with billions of nodes and edges."	"true"
"Extensions"	"cats ★ 971 ⧗ 0"	"https://github.com/non/cats"	"Lightweight, modular, and extensible library for functional programming."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"3"	"1"	"260"	"GitHub - non/cats: Lightweight, modular, and extensible library for functional programming. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 3 Fork 260 non/cats forked from typelevel/cats Code Pull requests 0 Pulse Graphs Lightweight, modular, and extensible library for functional programming. http://typelevel.org/cats/ 1,893 commits 31 branches 8 releases 78 contributors Scala 99.7% Shell 0.3% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags bug/fix-memoize bug/fix-streaming-thunk feature/task gh-pages master prochoice topic/add-kailuowang topic/applicative-syntax topic/bimonad-laws topic/continuations topic/doc-overhaul-wip topic/du topic/eval-call-bench topic/examples-project topic/fix-kernel-methods topic/foldable-check topic/free-additions-cleanup topic/free-tostring topic/js-jvm-projs topic/kernel topic/kernel2 topic/more-bimonad-laws topic/nmonad topic/no-lazy-val-eval topic/publish-to-typelevel topic/set-version-to-snapshot topic/streaming-tut topic/syntax-docs topic/tags topic/update-versions-0.6.0 traverse_fold Nothing to show v0.4.1 v0.4.0 v0.3.0 v0.2.0 v0.1.2 v0.1.1 v0.1.0 v0.0.0 Nothing to show New pull request Pull request Compare This branch is 423 commits behind typelevel:master. Latest commit ed194a7 Apr 18, 2016 ceedubs Merge pull request #952 from ceedubs/scalacheck-params … Reduce maxSize and minSuccessful in Scalacheck params Permalink Failed to load latest commit information. bench/src/main/scala/cats/bench Onward to scala 2.11.8. Mar 18, 2016 core/src/main/scala/cats Merge pull request #911 from adelbertc/monad-writer Apr 17, 2016 docs/src Fix outdated import for `cats.syntax.apply._` Apr 13, 2016 js/src Onward to scala 2.11.8. Mar 18, 2016 jvm/src Improving test coverage Mar 28, 2016 laws/src/main/scala/cats/laws Merge pull request #952 from ceedubs/scalacheck-params Apr 18, 2016 macros/src/main/scala/cats/macros Use scala-bricks to extract platform-specific code. Sep 12, 2015 project Upgrade scala.js from 0.6.7 -> 0.6.8 Mar 28, 2016 scripts Onward to scala 2.11.8. Mar 18, 2016 tests/src/test/scala/cats/tests Merge pull request #952 from ceedubs/scalacheck-params Apr 18, 2016 .gitignore Add .ensime_cache to gitignore Mar 26, 2016 .jvmopts Fix #18: Scala.JS support Aug 3, 2015 .travis.yml Enable `batchMode` on scala.js Optimizer Mar 28, 2016 AUTHORS.md Version 0.4.1 release notes and prep Feb 4, 2016 CHANGES.md added changes for 0.5.0 to CHANGES.md Apr 16, 2016 CONTRIBUTING.md Update README and other root docs for 0.4.0 Feb 1, 2016 COPYING add basic license Jan 28, 2015 PROCESS.md Fix typos and clarify. Sep 1, 2015 README.md Actions trump intentions. Apr 2, 2016 build.sbt Disable scaladoc during packaging for scala 2.10 Mar 28, 2016 scalastyle-config.xml Add return types to all public methods in core May 5, 2015 version.sbt Setting version to 0.5.0-SNAPSHOT Feb 4, 2016 README.md Cats Overview Cats is a library which provides abstractions for functional programming in Scala. The name is a playful shortening of the word category. Getting Started Cats is currently available for Scala 2.10 and 2.11. To get started with SBT, simply add the following to your build.sbt file: libraryDependencies += ""org.typelevel"" %% ""cats"" % ""0.4.1"" This will pull in all of Cats' modules. If you only require some functionality, you can pick-and-choose from amongst these modules (used in place of ""cats""): cats-macros: Macros used by Cats syntax (required). cats-core: Core type classes and functionality (required). cats-laws: Laws for testing type class instances. Release notes for Cats are available in CHANGES.md. Cats 0.4.1 is a pre-release: there are not currently source- or binary-compatibility guarantees. Documentation Among the goals of Cats is to provide approachable and useful documentation. Documentation is available in the form of tutorials on the Cats website, as well as through Scaladoc (also reachable through the website). Building Cats To build Cats you should have sbt and Node.js installed. Run sbt, and then use any of the following commands: compile: compile the code console: launch a REPL test: run the tests unidoc: generate the documentation scalastyle: run the style-checker on the code validate: run tests, style-checker, and doc generation Scala and Scala-js Cats cross-compiles to both JVM and Javascript(JS). If you are not used to working with cross-compiling builds, the first things that you will notice is that builds: Will take longer: To build JVM only, just use the catsJVM, or catsJS for JS only. And if you want the default project to be catsJVM, just copy the file scripts/sbtrc-JVM to .sbtrc in the root directory. May run out of memory: We suggest you use Paul Philips's sbt script that will use the settings from Cats. Design The goal is to provide a lightweight, modular, and extensible library that is approachable and powerful. We will also provide useful documentation and examples which are type-checked by the compiler to ensure correctness. Cats will be designed to use modern best practices: simulacrum for minimizing type class boilerplate machinist for optimizing implicit operators scalacheck for property-based testing discipline for encoding and testing laws kind-projector for type lambda syntax algebra for shared algebraic structures ...and of course a pure functional subset of the Scala language. (We also plan to support Miniboxing in a branch.) Currently Cats is experimenting with providing laziness via a type constructor (Eval[_]), rather than via ad-hoc by-name parameters.This design may change if it ends up being impractical. The goal is to make Cats as efficient as possible for both strict and lazy evaluation. There are also issues around by-name parameters that mean they are not well-suited to all situations where laziness is desirable. Modules Cats will be split into modules, both to keep the size of the artifacts down and also to avoid unnecessarily tight coupling between type classes and data types. Initially Cats will support the following modules: macros: Macro definitions needed for core and other projects. core: Definitions for widely-used type classes and data types. laws: The encoded laws for type classes, exported to assist third-party testing. tests: Verifies the laws, and runs any other tests. Not published. As the type class families grow, it's possible that additional modules will be added as well. Modules which depend on other libraries (e.g. Shapeless-based type class derivation) may be added as well. How can I contribute to Cats? There are many ways to support Cats' development: Fix bugs: Despite using static types, law-checking, and property-based testing bugs can happen. Reporting problems you encounter (with the documentation, code, or anything else) helps us to improve. Look for issues labelled ""ready"" as good targets, but please add a comment to the issue if you start working on one. We want to avoid any duplicated effort. Write ScalaDoc comments: One of our goals is to have ScalaDoc comments for all types in Cats. The documentation should describe the type and give a basic usage (it may also link to relevant papers). Write tutorials and examples: In addition to inline ScalaDoc comments, we hope to provide Markdown-based tutorials which can demonstrate how to use all the provided types. These should be literate programs i.e. narrative text interspersed with code. Improve the laws and tests: Cats' type classes rely on laws (and law-checking) to make type classes useful and reliable. If you notice laws or tests which are missing (or could be improved) you can open an issue (or send a pull request). Help with code review: Most of our design decisions are made through conversations on issues and pull requests. You can participate in these conversations to help guide the future of Cats. We will be using the meta label for large design decisions, and your input on these is especially appreciated. Contribute new code: Cats is growing! If there are type classes (or concrete data types) which you need, feel free to contribute! You can open an issue to discuss your idea, or start hacking and submit a pull request. One advantage of opening an issue is that it may save you time to get other opinions on your approach. Ask questions: we are hoping to make Cats (and functional programming in Scala) accessible to the largest number of people. If you have questions it is likely many other people do as well, and as a community this is how we can grow and improve. Maintainers The current maintainers (people who can merge pull requests) are: ceedubs Cody Allen rossabaker Ross Baker travisbrown Travis Brown adelbertc Adelbert Chang tpolecat Rob Norris stew Mike O'Connor non Erik Osheim mpilquist Michael Pilquist milessabin Miles Sabin fthomas Frank Thomas julien-truffaut Julien Truffaut We are currently following a practice of requiring at least two sign-offs to merge PRs (and for large or contentious issues we may wait for more). For typos or other small fixes to documentation we relax this to a single sign-off. Contributing Discussion around Cats is currently happening in the Gitter channel as well as on Github issue and PR pages. You can get an overview of who is working on what via Waffle.io. Feel free to open an issue if you notice a bug, have an idea for a feature, or have a question about the code. Pull requests are also gladly accepted. For more information, check out the contributor guide. You can also see a list of past contributors in AUTHORS.md. People are expected to follow the Typelevel Code of Conduct when discussing Cats on the Github page, Gitter channel, or other venues. We hope that our community will be respectful, helpful, and kind. If you find yourself embroiled in a situation that becomes heated, or that fails to live up to our expectations, you should disengage and contact one of the project maintainers in private. We hope to avoid letting minor aggressions and misunderstandings escalate into larger problems. If you are being harassed, please contact one of us immediately so that we can support you. Related Projects Cats is closely-related to Structures; both projects are descended from Scalaz. There are many related Haskell libraries, for example: semigroupoids profunctors contravariant ...and so on. Copyright and License All code is available to you under the MIT license, available at http://opensource.org/licenses/mit-license.php and also in the COPYING file. The design is informed by many other projects, in particular Scalaz. Copyright the maintainers, 2015. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/non/cats"	"Lightweight, modular, and extensible library for functional programming."	"true"
"Extensions"	"Each ★ 72 ⧗ 7"	"https://github.com/ThoughtWorksInc/each"	"A macro library that converts native imperative syntax to 's monadic expressions."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"93"	"21"	"19"	"GitHub - ThoughtWorksInc/sde: A macro library that converts native imperative syntax to scalaz's monadic expressions Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 21 Star 93 Fork 19 ThoughtWorksInc/sde Code Issues 1 Pull requests 1 Wiki Pulse Graphs A macro library that converts native imperative syntax to scalaz's monadic expressions 393 commits 5 branches 17 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.0.1-rebuild MonadicTransformerTest-wip gen master scalaz-7.1.x Nothing to show v2.0.0 v1.0.1 v1.0.0 v1.0.0-alpha1 v0.6.0 v0.5.1 v0.5.0 v0.4.3 v0.4.2 v0.4.1 v0.4.0 v0.3.0 v0.2.3 v0.2.2 v0.2.1 v0.2.0 v0.1.0 Nothing to show New pull request Latest commit 8231951 Jul 15, 2016 Atry Use PhantomJS instead of Rhino to test Scala.js Permalink Failed to load latest commit information. comprehension-monad Test comprehension-monad on Scala.js as well Jun 13, 2016 core Add @gen annotation Jul 14, 2016 each Disable some tests for Scala 2.10 Jul 11, 2016 future Reuse future.apply in generated code Jul 14, 2016 gen Add @gen annotation Jul 14, 2016 project Upgrade sbt-sonatype Jul 11, 2016 source Move source to sde package Jul 14, 2016 .gitignore Merge branch 'master' of https://github.com/ThoughtWorksInc/scala-pro… Jun 12, 2016 .travis.yml Test cross version on Travis CI Jun 13, 2016 LICENSE Add license information (fix #4) Jun 6, 2015 NOTICE Add license information (fix #4) Jun 6, 2015 README.md Update README.md Jan 14, 2016 build.sbt Use PhantomJS instead of Rhino to test Scala.js Jul 15, 2016 deploy.sbt.disabled Setting version to 2.0.0 Jul 15, 2016 sonatypeResolver.sbt Add sonatype repository for plugins as well Jun 12, 2016 version.sbt Setting version to 2.0.1-SNAPSHOT Jul 15, 2016 README.md Each Each is a macro library that converts native imperative syntax to scalaz's monadic expression. Motivation There is a macro library Stateless Future that provides await for asynchronous programming. await is a mechanism that transform synchronous-like code into asynchronous expressions. C# 5.0, ECMAScript 7 and Python 3.5 also support the mechanism. The await mechanism in Stateless Future is implemented by an algorithm called CPS transform. When learning scalaz, we found that the same algorithm could be applied for any monadic expression, including Option monad, IO monad, and Future monad. So we started this project, Each. Each is a superset of await syntax. Each supports multiple types of monads, while await only works with Future. When we perform a CPS transform for monadic expression with the Future monad, the use case looks almost the same as the await syntax in Stateless Future. Each is like F#'s Computation Expressions, except Each reuses the normal Scala syntax instead of reinventing new syntax. For example: import com.thoughtworks.each.Monadic._ import scalaz.std.scalaFuture._  // Returns a Future of the sum of the length of each string in each parameter Future, // without blocking any thread. def concat(future1: Future[String], future2: Future[String]): Future[Int] = monadic[Future] {   future1.each.length + future2.each.length } The similar code works for monads other than Future: import com.thoughtworks.each.Monadic._ import scalaz.std.option._  def plusOne(intOption: Option[Int]) = monadic[Option] {   intOption.each + 1 } assertEquals(None, plusOne(None)) assertEquals(Some(16), plusOne(Some(15))) import com.thoughtworks.each.Monadic._ import scalaz.std.list._  def plusOne(intSeq: List[Int]) = monadic[List] {   intSeq.each + 1 } assertEquals(Nil, plusOne(Nil)) assertEquals(List(16), plusOne(List(15))) assertEquals(List(16, -1, 10), plusOne(List(15, -2, 9))) Usage Step 1: Add the following line in your build.sbt libraryDependencies += ""com.thoughtworks.each"" %% ""each"" % ""0.5.1"" or %%% for Scala.js projects: libraryDependencies += ""com.thoughtworks.each"" %%% ""each"" % ""0.5.1"" Note that Each version 0.5.x requires Scalaz 7.2.x . If you have to use Scalaz 7.1.x, please specify version of Each 0.4.x . See https://repo1.maven.org/maven2/com/thoughtworks/each/ for a list of available versions. Step 2: In your source file, import monadic and each method import com.thoughtworks.each.Monadic._ Step 3: Import implicit Monad instances Scalaz has provided Option monad, so you just import it. import com.thoughtworks.each.Monadic._ import scalaz.std.option._ Please import other monad instances if you need other monads. Step 4: Use monadic[F] to create a monadic expression import com.thoughtworks.each.Monadic._ import scalaz.std.option._ val result: Option[String] = monadic[Option] {   ""Hello, Each!"" } Step 5: In the monadic block, use .each postfix to extract each element in a F import com.thoughtworks.each.Monadic._ import scalaz.std.option._ val name = Option(""Each"") val result: Option[String] = monadic[Option] {   ""Hello, "" + name.each + ""!"" } Exception handling monadic blocks do not support try, catch and finally. If you want these expressions, use throwableMonadic or catchIoMonadic instead, for example: var count = 0 val io = catchIoMonadic[IO] {   count += 1                // Evaluates immediately   val _ = IO(()).each       // Pauses until io.unsafePerformIO()   try {     count += 1     (null: Array[Int])(0)   // Throws a NullPointerException   } catch {     case e: NullPointerException => {       count += 1       100     }   } finally {     count += 1   } } assertEquals(1, count) assertEquals(100, io.unsafePerformIO()) assertEquals(4, count) Note that catchIoMonadic requires an implicit parameter scalaz.effect.MonadCatchIO[F] instead of Monad[F]. scalaz.effect.MonadCatchIO[F] is only provided for scalaz.effect.IO by default. for loop Each supports .each magic in a for loop on MonadicLoop. You can create a MonadicLoop instance via .monadicLoop method from any instances that support Foldable type class. For example, you could import scalaz.std.list._ to enable the Foldable type class for List. import com.thoughtworks.each.Monadic._ import scalaz.std.list._ import scalaz.std.option._ val n = Some(10) val result = monadic[Option] {   var count = 1   for (i <- List(300, 20).monadicLoop) {     count += i * n.each   }   count } Assert.assertEquals(Some(3201), result) for comprehension Each also supports .each magic in a for comprehension on MonadicLoop. You can create a MonadicLoop instance via .monadicLoop method from any instances that support Traverse and MonadPlus type class. import com.thoughtworks.each.Monadic._ import scalaz.std.list._ val n = Some(4000) val result = monadic[Option] {   (for {     i <- List(300, 20).monadicLoop     (j, k) <- List(50000 -> ""1111"", 600000 -> ""yyy"").monadicLoop     if i > n.each - 3900     a = i + j   } yield {     a + n.each * k.length   }).underlying } Assert.assertEquals(Some(List(66300, 612300)), result) Limitation If a call-by-name parameter of a method call is a monadic expression, Each will transform the monadic expression before the method call. The behavior was discussed at #37. def innerFailureFuture = Future.failed(new Exception(""foo"")) val someValue = Some(""value"") val result = monadic[Future] {   someValue.getOrElse(innerFailureFuture.each) } result will be a future of failure because the above example equals to def innerFailureFuture = Future.failed(new Exception(""foo"")) val someValue = Some(""value"") val result = innerFailureFuture.map(someValue.getOrElse) innerFailureFuture.each is evaluated before being passed to getOrElse method call, even if getOrElse accepts a call-by-name parameter. Links The API Documentation Utilities ComprehensionMonad Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ThoughtWorksInc/each"	"A macro library that converts native imperative syntax to 's monadic expressions."	"true"
"Extensions"	"Scalaz ★ 2469 ⧗ 0"	"https://github.com/scalaz/scalaz"	"An extension to the core Scala library for functional programming."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2628"	"241"	"429"	"GitHub - scalaz/scalaz: An extension to the core Scala library for functional programming. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 241 Star 2,628 Fork 429 scalaz/scalaz Code Issues 64 Pull requests 23 Wiki Pulse Graphs An extension to the core Scala library for functional programming. http://scalaz.github.io/scalaz/#scaladoc 5,704 commits 16 branches 74 releases 144 contributors Scala 99.0% Shell 1.0% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: series/7.3.x Switch branches/tags Branches Tags gh-pages reboot rebooteroo release/6.0.2 release/6.0.3 release/6.0.4 release/7.0.5 release/7.0.6 scalaz-seven series/6.0.x series/7.0.x series/7.1.x series/7.2.x series/7.3.x series/8.0.x topic/8-optics Nothing to show v7.3.0-M4 v7.3.0-M3 v7.3.0-M2 v7.3.0-M1 v7.2.4 v7.2.3 v7.2.2 v7.2.1 v7.2.0 v7.2.0-RC1 v7.2.0-M5 v7.2.0-M4 v7.2.0-M3 v7.2.0-M2 v7.2.0-M2-2.12.0-M2 v7.2.0-M1 v7.1.9 v7.1.8 v7.1.7 v7.1.6 v7.1.5 v7.1.4 v7.1.4-2.12.0-M3 v7.1.3 v7.1.3-2.12.0-M2 v7.1.2 v7.1.2-2.12.0-M1 v7.1.1 v7.1.0 v7.1.0-RC2 v7.1.0-RC1 v7.1.0-M7 v7.1.0-M6 v7.1.0-M5 v7.1.0-M4 v7.1.0-M3 v7.1.0-M2 v7.1.0-M1 v7.0.9 v7.0.8 v7.0.7 v7.0.7-2.12.0-M1 v7.0.6 v7.0.5 v7.0.4 v7.0.3 v7.0.2 v7.0.1 v7.0.0 v7.0.0-RC2 v7.0.0-RC1 v7.0.0-M9 v7.0.0-M8 v7.0.0-M7 v7.0.0-M6 v7.0.0-M5 v7.0.0-M4 v7.0.0-M3 v7.0.0-M2 v7.0.0-M1 v6.0.4 v6.0.4-2.10.0-M4 v6.0.3-2.10-M4 scalaz-5.0 6.0.3 6.0.2 6.0.1-for-2.9.x 6.0.1-for-2.8.x 6.0.RC2-for-2.9 6.0.RC2-for-2.8 6.0.RC1-for-2.9 6.0.RC1-for-2.8 6.0-for-2.9.x 6.0-for-2.8.x Nothing to show New pull request Latest commit d4ffe95 Jul 12, 2016 xuwei-k remove unused imports Permalink Failed to load latest commit information. concurrent/src/main/scala/scalaz/concurrent use JavaConverters instead of JavaConversions Jun 30, 2016 core Add monad transformer laws. Jul 11, 2016 effect/src/main/scala/scalaz remove tailing whitespaces Jun 5, 2016 etc nuke deprecated MetricSpace and BKTree Jul 26, 2014 example/src/main/scala/scalaz/example remove unused imports Jul 12, 2016 iteratee/src/main/scala/scalaz/iteratee remove tailing whitespaces Jun 5, 2016 project scalacheck 1.13.2 Jul 11, 2016 scalacheck-binding Add monad transformer laws. Jul 11, 2016 tests remove unused imports Jul 12, 2016 .gitignore remove tailing whitespaces Jun 5, 2016 .travis.yml Scala 2.12.0-M5 Jul 1, 2016 CONTRIBUTING.md remove tailing whitespaces Jun 5, 2016 README.md latest stable version is 7.2.4 Jun 15, 2016 TODO.txt remove ReaderWriterStateT form TODO.txt Jun 19, 2013 sbt update latest version sbt-extras Jul 28, 2015 version.sbt Setting version to 7.3.0-SNAPSHOT Jul 11, 2016 README.md Scalaz Scalaz is a Scala library for functional programming. It provides purely functional data structures to complement those from the Scala standard library. It defines a set of foundational type classes (e.g. Functor, Monad) and corresponding instances for a large number of data structures. Getting Scalaz The current stable version is 7.2.4, which is cross-built against Scala 2.10.x and 2.11.x. If you're using SBT, add the following line to your build file: libraryDependencies += ""org.scalaz"" %% ""scalaz-core"" % ""7.2.4"" For Maven and other build tools, you can visit search.maven.org. (This search will also list all available modules of scalaz.) To get sample configurations, click on the version of the module you are interested in. You can also find direct download links at the bottom of that page. Choose the file ending in 7.2.4.jar. Quick Start import scalaz._ import std.option._, std.list._ // functions and type class instances for Option and List  scala> Apply[Option].apply2(some(1), some(2))((a, b) => a + b) res0: Option[Int] = Some(3)  scala> Traverse[List].traverse(List(1, 2, 3))(i => some(i)) res1: Option[List[Int]] = Some(List(1, 2, 3)) Use of the Ops classes, defined under scalaz.syntax. import scalaz._ import std.list._ // type class instances for List import syntax.bind._ // syntax for the Bind type class (and its parents)  scala> List(List(1)).join res0: List[Int] = List(1)  scala> List(true, false).ifM(List(0, 1), List(2, 3)) res1: List[Int] = List(0, 1, 2, 3) We've gone to great lengths to give you an a-la-carte importing experience, but if you prefer an all-you-can-eat buffet, you're in luck: import scalaz._ import Scalaz._  scala> NonEmptyList(1, 2, 3).cojoin res0: scalaz.NonEmptyList[scalaz.NonEmptyList[Int]] = NonEmptyList(NonEmptyList(1, 2, 3), NonEmptyList(2, 3), NonEmptyList(3))  scala> 1.node(2.leaf, 3.node(4.leaf)) res1: scalaz.Tree[Int] = <tree>  scala> List(some(1), none).suml res2: Option[Int] = Some(1) Resources Let the types speak for themselves via the Scalaz Scaladocs! The examples module contains some snippets of Scalaz usage. The wiki contains release and migration information. The typelevel blog has some great posts such as Towards Scalaz by Adelbert Chang. Learning Scalaz is a great series of blog posts by Eugene Yokota. Thanks, Eugene! Changes in Version 7 Scalaz 7 represents a major reorganization of the library. We have taken a fresh look at the challenges of encoding type classes in Scala, in particular at when and how to employ the implicit scope. At a glance scalaz.{concurrent, effect, iteratee} split to separate sub-projects; scalaz.{http, geo} dropped. Refined and expanded the type class hierarchy. Type class instances are no longer defined in the companion objects of the type class. Instances for standard library types are defined under scalaz.std, and instances for Scalaz data types are defined in the companion object for those types. An instance definition can provide multiple type classes in a single place, which was not always possible in Scalaz 6. Type class instances have been organized to avoid ambiguity, a problem that arises when instances are dependent on other instances (for example, Monoid[(A, B)]) Use of implicit views to provide access to Scalaz functionality as extension methods has been been segregated to scalaz.syntax, and can be imported selectively, and need not be used at all. Related functions are defined in the type class trait, to support standalone usage of the type class. In Scalaz 6, these were defined in Identity, MA, or MAB. New data structures have been added, and existing ones generalized. A number of monad transformers have been provided, in some cases generalizing old data structures. Modularity Scalaz has been been modularised. scalaz-core: Type class hierarchy, data structures, type class instances for the Scala and Java standard libraries, implicit conversions / syntax to access these. scalaz-effect: Data structures to represent and compose IO effects in the type system. scalaz-concurrent: Actor and Future implementation scalaz-iteratee: Experimental new Iteratee implementation Type Class Hierarchy Type classes form an inheritance hierarchy, as in Scalaz 6. This is convenient both at the call site and at the type class instance definition. At the call site, it ensures that you can call a method requiring a more general type class with an instance of a more specific type class: def bar[M[_]: Functor] = ()  def foo[M[_]: Monad] = bar[M] // Monad[M] is a subtype of Functor[M] The hierarchy itself is largely the same as in Scalaz 6. However, there have been a few adjustments, some method signatures have been adjusted to support better standalone usage, so code depending on these will need to be re-worked. Type Class Instance Definition Constructive implicits, which create a type class instance automatically based on instances of all parent type classes, are removed. These led to subtle errors with ambiguous implicits, such as this problem with FunctorBindApply Type class instances are no longer declared in fragments in the companion objects of the type class. Instead, they are defined in the package scalaz.std, and must be imported. These instances are defined in traits which will be mixed together into an object for importing en-masse, if desired. A single implicit can define a number of type class instances for a type. A type class definition can override methods (including derived methods) for efficiency. Here is an instance definition for Option. Notice that the method map has been overriden.   implicit val option = new Traverse[Option] with MonadPlus[Option] {     def point[A](a: => A) = Some(a)     def bind[A, B](fa: Option[A])(f: A => Option[B]): Option[B] = fa flatMap f     override def map[A, B](fa: Option[A])(f: A => B): Option[B] = fa map f     def traverseImpl[F[_], A, B](fa: Option[A])(f: A => F[B])(implicit F: Applicative[F]) =       fa map (a => F.map(f(a))(Some(_): Option[B])) getOrElse F.point(None)     def empty[A]: Option[A] = None     def plus[A](a: Option[A], b: => Option[A]) = a orElse b     def foldR[A, B](fa: Option[A], z: B)(f: (A) => (=> B) => B): B = fa match {       case Some(a) => f(a)(z)       case None => z     }   } To use this, one would: import scalaz.std.option.optionInstance // or, importing all instances en-masse // import scalaz.Scalaz._  val M = Monad[Option] val oi: Option[Int] = M.point(0) Syntax We co-opt the term syntax to refer to the way we allow the functionality of Scalaz to be called in the object.method(args) form, which can be easier to read, and, given that type inference in Scala flows from left-to-right, can require fewer type annotations. No more Identity, MA, or MAB from Scalaz 6. Syntax is segregated from rest of the library, in a sub-package scalaz.syntax. All Scalaz functionality is available without using the provided syntax, by directly calling methods on the type class or its companion object. Syntax is available a-la-carte. You can import the syntax for working with particular type classes where you need it. This avoids flooding the autocompletion in your IDE with every possible extension method. This should also help compiler performance, by reducing the implicit search space. Syntax is layered in the same way as type classes. Importing the syntax for, say, Applicative will also provide the syntax for Apply and Functor. Syntax can be imported in two ways. Firstly, the syntax specialized for a particular instance of a type class can be imported directly from the instance itself. // import the type class instance import scalaz.std.option.optionInstance  // import the implicit conversions to `MonadOps[Option, A]`, `BindOps[Option, A]`, ... import optionInstance.monadSyntax._  val oi: Option[Option[Int]] = Some(Some(1))  // Expands to: `ToBindOps(io).join` oi.join Alternatively, the syntax can be imported for a particular type class. // import the type class instance import scalaz.std.option.optionInstance  // import the implicit conversions to `MonadOps[F, A]`, `BindOps[F, A]`, ... import scalaz.syntax.monad._  val oi: Option[Option[Int]] = Some(Some(1))  // Expands to: ToBindOps(io).join oi.join For some degree of backwards compatibility with Scalaz 6, the über-import of import scalaz.Scalaz._ will import all implicit conversions that provide syntax (as well as type class instances and other functions). However, we recommend to review usage of this and replace with more focussed imports. Standalone Type Class Usage Type classes should be directly usable, without first needing to trigger implicit conversions. This might be desirable to reduce the runtime or cognitive overhead of the pimped types, or to define your own pimped types with a syntax of your choosing. The methods in type classes have been curried to maximize type inference. Derived methods, based on the abstract methods in a type class, are defined in the type class itself. Each type class companion object is fitted with a convenient apply method to obtain an instance of the type class.     // Equivalent to `implicitly[Monad[Option]]`     val O = Monad[Option]      // `bind` is defined with two parameter sections, so that the type of `x` is inferred as `Int`.     O.bind(Some(1))(x => Some(x * 2))      def plus(a: Int, b: Int) = a + b      // `Apply#lift2` is a function derived from `Apply#ap`.     val plusOpt = O.lift2(plus) Type Class Instance Dependencies Type class instances may depend on other instances. In simple cases, this is as straightforward as adding an implicit parameter (or, equivalently, a context bound), to the implicit method.   implicit def optionMonoid[A: Semigroup]: Monoid[Option[A]] = new Monoid[Option[A]] {     def append(f1: Option[A], f2: => Option[A]): Option[A] = (f1, f2) match {       case (Some(a1), Some(a2)) => Some(Semigroup[A].append(a1, a2))       case (Some(a1), None) => f1       case (None, Some(a2)) => f2       case (None, None) => None     }      def zero: Option[A] = None   } Type class instances for 'transformers', such as OptionT, present a more subtle challenge. OptionT[F, A] is a wrapper for a value of type F[Option[A]]. It allows us to write: val ot = OptionT(List(Some(1), None)) ot.map((a: Int) => a * 2) // OptionT(List(Some(2), None)) The method OptionT#map requires an implicit parameter of type Functor[F], whereas OptionT#flatMap requires one of type Monad[F]. The capabilities of OptionT increase with those of F. We need to encode this into the type class instances for [a]OptionT[F[A]]. This is done with a hierarchy of type class implementation traits and a corresponding set of prioritized implicit methods. In case of ambiguous implicits, Scala will favour one defined in a sub-class of the other. This is to avoid ambiguity when in cases like the following: type OptionTList[A] = OptionT[List[A]] implicitly[Functor[OptionTList]]  // Candidates: // 1. OptionT.OptionTFunctor[List](implicitly[Functor[List]]) // 2. OptionT.OptionTMonad[List](implicitly[Functor[List]]) // #2 is defined in a subclass of the enclosing class of #1, so #2 is preferred. Transformers and Identity A stronger emphasis has been placed on transformer data structures (aka Monad Transformers). For example State is now a type alias for StateT[Id, A, B]. Id is defined in the scalaz package object as: type Id[A] = A Contributing Documentation for contributors Credits Support for Scalaz development is provided by Jetbrains. Thanks to Mark Harrah and the sbt contributors for providing our build tool. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalaz/scalaz"	"An extension to the core Scala library for functional programming."	"true"
"Extensions"	"enableIf.scala ★ 24 ⧗ 6"	"https://github.com/ThoughtWorksInc/enableIf.scala"	"A library that switches Scala code at compile-time, like in C/C++."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"26"	"10"	"2"	"GitHub - ThoughtWorksInc/enableIf.scala: A library that toggles Scala code at compile-time, like #if in C/C++ Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 10 Star 26 Fork 2 ThoughtWorksInc/enableIf.scala forked from Atry/Preprocessor.scala Code Pull requests 0 Pulse Graphs A library that toggles Scala code at compile-time, like #if in C/C++ 40 commits 1 branch 3 releases Fetching contributors Scala 93.4% Shell 6.6% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v1.1.0 v1.0.1 v1.0.0 Nothing to show New pull request Pull request Compare This branch is 38 commits ahead of Atry:master. Latest commit 10c610c Jul 8, 2016 travis@localhost Setting version to 1.1.1-SNAPSHOT Permalink Failed to load latest commit information. project Add necessary plugins Apr 14, 2016 src Add `enableMembersIf` annotation Jul 8, 2016 .gitignore Merge branch 'ci' of https://github.com/Atry/ci-template-for-sbt-libr… Apr 14, 2016 .travis.yml Update .travis.yml Apr 16, 2016 LICENSE Initial commit Apr 13, 2016 README.md Update README.md Apr 17, 2016 build.sbt Update build.sbt Apr 16, 2016 ci.sbt Setup CI for deployment Feb 1, 2016 deploy.sh.disabled Setting version to 1.1.0 Jul 8, 2016 pubring.asc Setup CI for deployment Jan 31, 2016 version.sbt Setting version to 1.1.1-SNAPSHOT Jul 8, 2016 README.md enableIf.scala enableIf.scala is a library that switches Scala code at compile-time, like #if in C/C++. Motivation Suppose you want to create a library for both Scala 2.10 and Scala 2.11. When you implement the library, you may want to call the flatMap method on TailRec. However, the method does not exist on Scala 2.10. With the help of this library, You can create my own implementation of flatMap for Scala 2.10 target, and the Scala 2.11 target should still use the flatMap method implemented by Scala standard library. Usage Step 1: Add the library dependency in your build.sbt addCompilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.1.0"" cross CrossVersion.full)  libraryDependencies += ""com.thoughtworks.enableIf"" %% ""enableif"" % ""latest.release""  Step 2: Create an implicit class for Scala 2.10 import com.thoughtworks.enableIf  @enableIf(scala.util.Properties.versionNumberString.startsWith(""2.10."")) implicit class FlatMapForTailRec[A](underlying: TailRec[A]) {   final def flatMap[B](f: A => TailRec[B]): TailRec[B] = {     tailcall(f(underlying.result))   } } The @enableIf annotation accepts a Boolean expression that indicates if the FlatMapForTailRec definition should be compiled. The Boolean expression is evaluated at compile-time instead of run-time. Step 3: Call the flatMap method on your TailRec import scala.util.control.TailCalls._ def ten = done(10) def tenPlusOne = ten.flatMap(i => done(i + 1)) assert(tenPlusOne.result == 11) For Scala 2.10, the expression scala.util.Properties.versionNumberString.startsWith(""2.10."") is evaluated to true, hence the FlatMapForTailRec definition will be enabled. As a result, ten.flatMap will call to flatMap of the implicit class FlatMapForTailRec. For Scala 2.11, the expression scala.util.Properties.versionNumberString.startsWith(""2.10."") is evaluated to false, hence the FlatMapForTailRec definition will be disabled. As a result, ten.flatMap will call the native TailRec.flatmap. Limitation The enableIf annotation does not work for top level traits, classes and objects. Enable different code for Scala.js and JVM targets Suppose you want to create a Buffer-like collection, you may want create an ArrayBuffer for JVM target, and the native js.Array for Scala.js target. private object Jvm {    /**    * Enable if no Scala.js plugin is found (i.e. Normal JVM target)    */   @enableIf(c => !c.compilerSettings.exists(_.matches(""""""^-Xplugin:.*scalajs-compiler_[0-9\.\-]*\.jar$"""""")))   def newBuffer[A] = collection.mutable.ArrayBuffer.empty[A]  }  private object Js {    /**    * Enable if a Scala.js plugin is found (i.e. Scala.js target)    */   @enableIf(c => c.compilerSettings.exists(_.matches(""""""^-Xplugin:.*scalajs-compiler_[0-9\.\-]*\.jar$"""""")))   @inline def newBuffer[A] = new scalajs.js.Array[A]    /**    * Enable if a Scala.js plugin is found (i.e. Scala.js target)    */   @enableIf(c => c.compilerSettings.exists(_.matches(""""""^-Xplugin:.*scalajs-compiler_[0-9\.\-]*\.jar$"""""")))   @inline implicit final class ReduceToSizeOps[A] @inline()(array: scalajs.js.Array[A]) {     @inline def reduceToSize(newSize: Int) = array.length = newSize   }  }  import Js._ import Jvm._  val optimizedBuffer = newBuffer[Int]  optimizedBuffer += 1 optimizedBuffer += 2 optimizedBuffer += 3  // resolved to native ArrayBuffer.reduceToSize for JVM, implicitly converted to ReduceToSizeOps for Scala.js optimizedBuffer.reduceToSize(1) You can define a c parameter because the enableIf annotation accepts either a Boolean expression or a scala.reflect.macros.Context => Boolean function. You can extract information from the macro context c. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ThoughtWorksInc/enableIf.scala"	"A library that switches Scala code at compile-time, like in C/C++."	"true"
"Extensions"	"Enumeratum ★ 21 ⧗ 9"	"https://github.com/lloydmeta/enumeratum"	"A macro to replace Scala enumerations with a sealed family of case objects. This allows additional checks for the compiler, e.g. for missing cases in a match statement. Has additinal support for Json libraries and the Play framework."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"155"	"15"	"24"	"GitHub - lloydmeta/enumeratum: A type-safe, reflection-free, powerful enumeration implementation for Scala with exhaustive pattern match warnings Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 155 Fork 24 lloydmeta/enumeratum Code Issues 1 Pull requests 1 Pulse Graphs A type-safe, reflection-free, powerful enumeration implementation for Scala with exhaustive pattern match warnings 162 commits 3 branches 24 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/more-slick-docs gh-pages master Nothing to show v1.4.5 v1.4.4 v1.4.3 v1.4.2 v1.4.1 v1.4.0 v1.3.7 v1.3.6 v1.3.5 v1.3.4 v1.3.3 v1.3.2 v1.0.0 v0.0.5 v0.0.4 1.3.6 1.3.1 1.3.0 1.2.3 1.2.2 1.2.1 1.2.0 1.1.0 1.0.1 Nothing to show New pull request Latest commit e059cb1 Jul 13, 2016 lloydmeta Add benchmark results Permalink Failed to load latest commit information. benchmarking Feature/jmh benchmarks (#45) Jul 13, 2016 enumeratum-circe Feature/jmh benchmarks (#45) Jul 13, 2016 enumeratum-core-jvm-tests/src/test/scala/enumeratum Feature/value enums (#29) Apr 16, 2016 enumeratum-core Add .in support to ValueEnums as well (#44) Jul 13, 2016 enumeratum-play-json 1.4.4 release May 7, 2016 enumeratum-play Feature/value enums (#29) Apr 15, 2016 enumeratum-reactivemongo-bson Refactor ReactiveMongo code (#34) May 4, 2016 enumeratum-upickle Feature/value enums (#29) Apr 15, 2016 macros Feature/value enums (#29) Apr 15, 2016 project Feature/jmh benchmarks (#45) Jul 13, 2016 .gitignore Initial commit Dec 5, 2014 .travis.yml ReactiveMongo (#32) May 3, 2016 README.md Add benchmark results Jul 13, 2016 README.md Enumeratum Enumeratum is a type-safe and powerful enumeration implementation for Scala that offers exhaustive pattern match warnings, integrations with popular Scala libraries, and idiomatic usage that won't break your IDE. It aims to be similar enough to Scala's built in Enumeration to be easy-to-use and understand while offering more flexibility, type-safety (see this blog post describing erasure on Scala's Enumeration), and richer enum values without having to maintain your own collection of values. Enumeratum has the following niceties: Zero dependencies Allows your Enum members to be full-fledged normal objects with methods, values, inheritance, etc. Idiomatic: you're very clearly still writing Scala, and no funny colours in your IDE means less cognitive overhead for your team Simplicity; most of the complexity in this lib is in its macro, and the macro is fairly simple conceptually No usage of reflection at run time. This may also help with performance but it means Enumeratum is compatible with ScalaJS and other environments where reflection is a best effort (such as Android) No usage of synchronized, which may help with performance and deadlocks prevention All magic happens at compile-time so you know right away when things go awry Compatible with Scala 2.11+ and 2.10 as well as ScalaJS. Integrations are available for: Play: JVM only Play JSON: JVM only (included in Play integration but also available separately) Circe: JVM and ScalaJS UPickle: JVM and ScalaJS ReactiveMongo BSON: JVM only Table of Contents Quick start SBT Usage More examples Enum Mixins ValueEnum ScalaJS Play integration Play JSON integration Circe integration UPickle integration ReactiveMongo BSON integration Slick integration Benchmarking Known issues Licence Quick start SBT In build.sbt, set the Enumeratum version in a variable (for the latest version, set val enumeratumVersion = the version you in the Maven badge above). libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion ) Enumeratum has different integrations that can be added to your build a la cart. For more info, see the respective secions in the Table of Contents Usage Using Enumeratum is simple. Just declare your own sealed trait or class A that extends EnumEntry and implement it as case objects inside an object that extends from Enum[A] as shown below. import enumeratum._  sealed trait Greeting extends EnumEntry  object Greeting extends Enum[Greeting] {    /*    `findValues` is a protected method that invokes a macro to find all `Greeting` object declarations inside an `Enum`     You use it to implement the `val values` member   */   val values = findValues    case object Hello extends Greeting   case object GoodBye extends Greeting   case object Hi extends Greeting   case object Bye extends Greeting  }  // Object Greeting has a `withName(name: String)` method Greeting.withName(""Hello"")  // => res0: Greeting = Hello  Greeting.withName(""Haro"") // => java.lang.IllegalArgumentException: Haro is not a member of Enum (Hello, GoodBye, Hi, Bye)  Note that by default, findValues will return a Seq with the enum members listed in written-order (relevant if you want to use the indexOf method). More examples Enum Continuing from the enum declared in the quick-start section: import Greeting._  def tryMatching(v: Greeting): Unit = v match {   case Hello => println(""Hello"")   case GoodBye => println(""GoodBye"")   case Hi => println(""Hi"") }  /** Pattern match warning ...  <console>:24: warning: match may not be exhaustive. It would fail on the following input: Bye        def tryMatching(v: Greeting): Unit = v match {  */  Greeting.indexOf(Bye) // => res2: Int = 3  The name is taken from the toString method of the particular EnumEntry. This behavior can be changed in two ways. The first is to manually override the def entryName: String method. import enumeratum._  sealed abstract class State(override val entryName: String) extends EnumEntry  object State extends Enum[State] {     val values = findValues     case object Alabama extends State(""AL"")    case object Alaska extends State(""AK"")    // and so on and so forth. }  import State._  State.withName(""AL"")  Mixins The second is to mixin the stackable traits provided for common string conversions, Snakecase, Uppercase, and Lowercase. import enumeratum._ import enumeratum.EnumEntry._  sealed trait Greeting extends EnumEntry with Snakecase  object Greeting extends Enum[Greeting] {    val values = findValues    case object Hello extends Greeting   case object GoodBye extends Greeting   case object ShoutGoodBye extends Greeting with Uppercase  }  Greeting.withName(""hello"") Greeting.withName(""good_bye"") Greeting.withName(""SHOUT_GOOD_BYE"")  ValueEnum Asides from enumerations that resolve members from String names, Enumeratum also supports ValueEnums, enums that resolve members from various primitive types like Int, Long andShort. In order to ensure at compile-time that multiple members do not share the same value, these enums are powered by a separate macro and exposed through a different set of traits. import enumeratum.values._  sealed abstract class LibraryItem(val value: Int, val name: String) extends IntEnumEntry  case object LibraryItem extends IntEnum[LibraryItem] {    case object Book extends LibraryItem(value = 1, name = ""book"")   case object Movie extends LibraryItem(name = ""movie"", value = 2)   case object Magazine extends LibraryItem(3, ""magazine"")   case object CD extends LibraryItem(4, name = ""cd"")   // case object Newspaper extends LibraryItem(4, name = ""newspaper"") <-- will fail to compile because the value 4 is shared    /*   val five = 5   case object Article extends LibraryItem(five, name = ""article"") <-- will fail to compile because the value is not a literal   */    val values = findValues  }  assert(LibraryItem.withValue(1) == LibraryItem.Book)  LibraryItem.withValue(10) // => java.util.NoSuchElementException: Restrictions ValueEnums must have their value members implemented as literal values. ValueEnums are not available in Scala 2.10.x and does not work in the REPL because constructor argument calls are not yet typed during macro expansion (fun.tpe returns null). ScalaJS In a ScalaJS project, add the following to build.sbt: libraryDependencies ++= Seq(     ""com.beachape"" %%% ""enumeratum"" % enumeratumVersion ) As expected, usage is exactly the same as normal Scala. Play Integration The enumeratum-play project is published separately and gives you access to various tools to help you avoid boilerplate in your Play project. SBT For enumeratum with full Play support: libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %% ""enumeratum-play"" % enumeratumVersion ) Note that as of version 1.4.0, enumeratum-play for Scala 2.11 is compatible with Play 2.5+ while 2.10 is compatible with Play 2.4.x. Versions prior to 1.4.0 are compatible with 2.4.x. Usage PlayEnum The included PlayEnum trait is probably going to be the most interesting as it includes a bunch of built-in implicits like Json formats, Path bindables, Query string bindables, and Form field support. For example: package enums._  import enumeratum._  sealed trait Greeting extends EnumEntry  object Greeting extends PlayEnum[Greeting] {    val values = findValues    case object Hello extends Greeting   case object GoodBye extends Greeting   case object Hi extends Greeting   case object Bye extends Greeting  }  /*   Then make sure to import your PlayEnums into your routes in your Build.scala   or build.sbt so that you can use them in your routes file.    `routesImport += ""enums._""` */   // You can also use the String Interpolating Routing DSL:  import play.api.routing.sird._ import play.api.routing._ import play.api.mvc._ Router.from {     case GET(p""/hello/${Greeting.fromPath(greeting)}"") => Action {       Results.Ok(s""$greeting"")     } }  PlayValueEnums There are IntPlayEnum, LongPlayEnum, and ShortPlayEnum traits for use with IntEnumEntry, LongEnumEntry, and ShortEnumEntry respectively that provide Play-specific implicits as with normal PlayEnum. For example: import enumeratum.values._  sealed abstract class PlayLibraryItem(val value: Int, val name: String) extends IntEnumEntry  case object PlayLibraryItem extends IntPlayEnum[PlayLibraryItem] {    // A good mix of named, unnamed, named + unordered args   case object Book extends PlayLibraryItem(value = 1, name = ""book"")   case object Movie extends PlayLibraryItem(name = ""movie"", value = 2)   case object Magazine extends PlayLibraryItem(3, ""magazine"")   case object CD extends PlayLibraryItem(4, name = ""cd"")    val values = findValues  }  import play.api.libs.json.{ JsNumber, JsString, Json => PlayJson } PlayLibraryItem.values.foreach { item =>     assert(PlayJson.toJson(item) == JsNumber(item.value)) } PlayFormFieldEnum PlayEnum extends the trait PlayFormFieldEnum wich offers formField for mapping within a play.api.data.Form object. import play.api.data.Form import play.api.data.Forms._  object GreetingForm {   val form = Form(     mapping(       ""name"" -> nonEmptyText,       ""greeting"" -> Greeting.formField     )(Data.apply)(Data.unapply)   )    case class Data(     name: String,     greeting: Greeting) } Play JSON The enumeratum-play-json project is published separately and gives you access to Play's auto-generated boilerplate for JSON serialization in your Enum's. SBT libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %% ""enumeratum-play-json"" % enumeratumVersion ) Note that as of version 1.4.0, enumeratum-play for Scala 2.11 is compatible with Play 2.5+ while 2.10 is compatible with Play 2.4.x. Versions prior to 1.4.0 are compatible with 2.4.x. Usage PlayJsonEnum For example: import enumeratum.{ PlayJsonEnum, Enum, EnumEntry }  sealed trait Greeting extends EnumEntry  object Greeting extends Enum[Greeting] with PlayJsonEnum[Greeting] {    val values = findValues    case object Hello extends Greeting   case object GoodBye extends Greeting   case object Hi extends Greeting   case object Bye extends Greeting  }  PlayJsonValueEnum There are IntPlayJsonEnum, LongPlayJsonEnum, and ShortPlayJsonEnum traits for use with IntEnumEntry, LongEnumEntry, and ShortEnumEntry respectively. For example: import enumeratum.values._  sealed abstract class JsonDrinks(val value: Short, name: String) extends ShortEnumEntry  case object JsonDrinks extends ShortEnum[JsonDrinks] with ShortPlayJsonValueEnum[JsonDrinks] {    case object OrangeJuice extends JsonDrinks(value = 1, name = ""oj"")   case object AppleJuice extends JsonDrinks(value = 2, name = ""aj"")   case object Cola extends JsonDrinks(value = 3, name = ""cola"")   case object Beer extends JsonDrinks(value = 4, name = ""beer"")    val values = findValues  }  import play.api.libs.json.{ JsNumber, JsString, Json => PlayJson, JsSuccess }  // Use to deserialise numbers to enum members directly JsonDrinks.values.foreach { drink =>     assert(PlayJson.toJson(drink) == JsNumber(drink.value)) } assert(PlayJson.fromJson[JsonDrinks](JsNumber(3)) == JsSuccess(JsonDrinks.Cola)) assert(PlayJson.fromJson[JsonDrinks](JsNumber(19)).isError) Circe SBT To use enumeratum with Circe: libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %% ""enumeratum-circe"" % enumeratumVersion ) To use with ScalaJS: libraryDependencies ++= Seq(     ""com.beachape"" %%% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %%% ""enumeratum-circe"" % enumeratumVersion ) Usage Enum import enumeratum._  sealed trait ShirtSize extends EnumEntry  case object ShirtSize extends CirceEnum[ShirtSize] with Enum[ShirtSize] {    case object Small extends ShirtSize   case object Medium extends ShirtSize   case object Large extends ShirtSize    val values = findValues  }  import io.circe.Json import io.circe.syntax._  ShirtSize.values.foreach { size =>     assert(size.asJson == Json.fromString(size.entryName)) }  ValueEnum import enumeratum.values._  sealed abstract class CirceLibraryItem(val value: Int, val name: String) extends IntEnumEntry  case object CirceLibraryItem extends IntEnum[CirceLibraryItem] with IntCirceEnum[CirceLibraryItem] {    // A good mix of named, unnamed, named + unordered args   case object Book extends CirceLibraryItem(value = 1, name = ""book"")   case object Movie extends CirceLibraryItem(name = ""movie"", value = 2)   case object Magazine extends CirceLibraryItem(3, ""magazine"")   case object CD extends CirceLibraryItem(4, name = ""cd"")    val values = findValues  }  import io.circe.Json import io.circe.syntax._  CirceLibraryItem.values.foreach { item =>     assert(item.asJson == Json.fromInt(item.value)) } UPickle SBT To use enumeratum with uPickle: libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %% ""enumeratum-upickle"" % enumeratumVersion ) To use with ScalaJS: libraryDependencies ++= Seq(     ""com.beachape"" %%% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %%% ""enumeratum-upickle"" % enumeratumVersion ) Usage CirceEnum works pretty much the same as CirceEnum and PlayJsonEnum variants, so we'll skip straight to the ValueEnum integration. import enumeratum.values._  sealed abstract class ContentType(val value: Long, name: String) extends LongEnumEntry  case object ContentType     extends LongEnum[ContentType]     with LongUPickleEnum[ContentType] {    val values = findValues    case object Text extends ContentType(value = 1L, name = ""text"")   case object Image extends ContentType(value = 2L, name = ""image"")   case object Video extends ContentType(value = 3L, name = ""video"")   case object Audio extends ContentType(value = 4L, name = ""audio"")  }  import upickle.default.{ readJs, writeJs, Reader, Writer } enum.values.foreach { entry =>   val written = writeJs(entry)   assert(readJs(written) == entry) }  ReactiveMongo BSON The enumeratum-reactivemongo-bson project is published separately and gives you access to ReactiveMongo's auto-generated boilerplate for BSON serialization in your Enum's. SBT libraryDependencies ++= Seq(     ""com.beachape"" %% ""enumeratum"" % enumeratumVersion,     ""com.beachape"" %% ""enumeratum-reactivemongo-bson"" % enumeratumVersion ) Usage ReactiveMongoBsonEnum For example: import enumeratum.{ ReactiveMongoBsonEnum, Enum, EnumEntry }  sealed trait Greeting extends EnumEntry  object Greeting extends Enum[Greeting] with ReactiveMongoBsonEnum[Greeting] {    val values = findValues    case object Hello extends Greeting   case object GoodBye extends Greeting   case object Hi extends Greeting   case object Bye extends Greeting  }  ReactiveMongoBsonValueEnum There are IntReactiveMongoBsonValueEnum, LongReactiveMongoBsonValueEnum, and ShortReactiveMongoBsonValueEnum traits for use with IntEnumEntry, LongEnumEntry, and ShortEnumEntry respectively. For example: import enumeratum.values._  sealed abstract class BsonDrinks(val value: Short, name: String) extends ShortEnumEntry  case object BsonDrinks extends ShortEnum[BsonDrinks] with ShortReactiveMongoBsonValueEnum[BsonDrinks] {    case object OrangeJuice extends BsonDrinks(value = 1, name = ""oj"")   case object AppleJuice extends BsonDrinks(value = 2, name = ""aj"")   case object Cola extends BsonDrinks(value = 3, name = ""cola"")   case object Beer extends BsonDrinks(value = 4, name = ""beer"")    val values = findValues  }  import reactivemongo.bson._  // Use to deserialise numbers to enum members directly BsonDrinks.values.foreach { drink =>   val writer = implicitly[BSONWriter[BsonDrinks, BSONValue]]    assert(writer.write(drink) == BSONInteger(drink.value)) }  val reader = implicitly[BSONReader[BSONValue, BsonDrinks]]  assert(reader.read(BSONInteger(3)) == BsonDrinks.Cola) Slick integration Slick doesn't have a separate integration at the moment. You just have to provide a MappedColumnType for each database column that should be represented as an enum on the Scala side. For example when you want the Enum[Greeting] defined in the introduction as a database column, you can use the following code   implicit lazy val greetingMapper = MappedColumnType.base[Greeting, String](     greeting => greeting.entryName,     string => Greeting.withName(string)   ) You can then define the following line in your Table[...] class   // This maps a column of type VARCHAR/TEXT to enums of type [[Greeting]]   def greeting = column[Greeting](""GREETING"") If you want to represent your enum in the database with numeric IDs, just provide a different mapping. This example uses the enum of type LibraryItem defined in the introduction:   implicit lazy val libraryItemMapper = MappedColumnType.base[LibraryItem, Int](     item => item.value,     id => LibraryItem.withValue(id)   ) Again you can now simply use LibraryItem in your Table class:   // This maps a column of type NUMBER to enums of type [[LibaryItem]]   def item = column[LibraryItem](""LIBRARY_ITEM"") Note that because your enum values are singleton objects, you may get errors when you try to use them in Slick queries like the following: .filter(_.productType === ProductType.Foo)` This is because ProductType.Foo in the above example is inferred to be of its unique type (ProductType.Foo) rather than ProductType, thus causing a failure to find your mapping. In order to fix this, simply assist the compiler by ascribing the type to be ProductType: .filter(_.productType === (ProductType.Foo: ProductType))` Benchmarking Benchmarking is in the unpublished benchmarking project. It uses JMH and you can run them in the sbt console by issuing the following command from your command line: sbt +benchmarking/'jmh:run -i 10 -wi 10 -f3 -t 1' The above command will run JMH benchmarks against different versions of Scala. Leave off + to run against the main/latest supported version of Scala. On my late 2013 MBP using Java8 on OSX El Capitan: [info] Benchmark                                            Mode  Cnt     Score    Error  Units [info] EnumBenchmarks.indexOf                               avgt   30    11.731 ±  0.179  ns/op [info] EnumBenchmarks.withNameDoesNotExist                  avgt   30  1706.310 ± 30.260  ns/op [info] EnumBenchmarks.withNameExists                        avgt   30    13.377 ±  0.319  ns/op [info] EnumBenchmarks.withNameOptionDoesNotExist            avgt   30     5.583 ±  0.047  ns/op [info] EnumBenchmarks.withNameOptionExists                  avgt   30     8.924 ±  0.094  ns/op [info] values.ValueEnumBenchmarks.withValueDoesNotExist     avgt   30  1711.705 ± 55.101  ns/op [info] values.ValueEnumBenchmarks.withValueExists           avgt   30     4.040 ±  0.036  ns/op [info] values.ValueEnumBenchmarks.withValueOptDoesNotExist  avgt   30     5.622 ±  0.029  ns/op [info] values.ValueEnumBenchmarks.withValueOptExists        avgt   30     6.472 ±  0.052  ns/op  Other than the methods that throw NoSuchElementExceptions, performance is in the 10ns range (taking into account JMH overhead of roughly 2-3ns), which is acceptable for almost all use-cases. PRs that promise to increase performance are expected to be demonstrably faster. Known issues ValueEnums is not available in Scala 2.10.x and does not work in the REPL because constructor function calls are not yet typed during macro expansion (fun.tpe returns null). Licence The MIT License (MIT) Copyright (c) 2016 by Lloyd Chan Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lloydmeta/enumeratum"	"A macro to replace Scala enumerations with a sealed family of case objects. This allows additional checks for the compiler, e.g. for missing cases in a match statement. Has additinal support for Json libraries and the Play framework."	"true"
"Extensions"	"Hamsters ★ 87 ⧗ 4"	"https://github.com/scala-hamsters/hamsters"	"A mini Scala utility library. Compatible with functional programming beginners. Featuring validation, monad transformers, HLists, Union types."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"97"	"14"	"6"	"GitHub - scala-hamsters/hamsters: A mini Scala utility library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 97 Fork 6 scala-hamsters/hamsters Code Issues 1 Pull requests 0 Pulse Graphs A mini Scala utility library 116 commits 3 branches 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags buildFromDependency gh-pages master Nothing to show Nothing to show New pull request Latest commit 9be2b54 Jun 5, 2016 loicdescotte Update README.md Permalink Failed to load latest commit information. src Union comment Jun 4, 2016 .gitignore git ignore Apr 27, 2016 .travis.yml Create .travis.yml Apr 24, 2016 LICENCE.md Create LICENCE.md Apr 24, 2016 README.md Update README.md Jun 5, 2016 build.sbt scaladocOpts Jun 5, 2016 README.md Hamsters A mini Scala utility library. Compatible with functional programming beginners. Currently, Hamsters supports : Validation OK/KO Monads Monad transformers HLists Union types Install as dependency libraryDependencies ++= Seq(   ""io.github.scala-hamsters"" %% ""hamsters"" % ""1.0.4"" )  resolvers += Resolver.url(""github repo for hamsters"", url(""http://scala-hamsters.github.io/hamsters/releases/""))(Resolver.ivyStylePatterns) Usage Validation and monadic OK/KO Statements can be OK or KO. Then you can get all successes and failures. import io.github.hamsters.Validation import Validation._  val e1 = OK(1) val e2 = KO(""error 1"") val e3 = KO(""error 2"")  val validation = Validation(e1,e2, e3) validation.hasFailures //true val failures = validation.failures //List[String] : List(""error 1"", ""error 2"") You can also use OK/KO in a monadic way if you want to stop processing at the first encountered error. val e1: Either[String, Int] = OK(1) val e2: Either[String, Int] = KO(""nan"") val e3: Either[String, Int] = KO(""nan2"")  // Stop at first error for {   v1 <- e1   v2 <- e2   v3 <- e3 } yield(s""$v1-$v2-$v3"") //KO(""nan"") Note : Validation relies on standard Either, Left and Right types. KO is used on the left side, OK on the right side. Monad transformers Example : combine Future and Option types then make it work in a for comprehension. More information on why it's useful here. FutureOption def foa: Future[Option[String]] = Future(Some(""a"")) def fob(a: String): Future[Option[String]] = Future(Some(a+""b""))  val composedAB: Future[Option[String]] = for {   a <- FutureOption(foa)   ab <- FutureOption(fob(a)) } yield ab FutureEither import io.github.hamsters.Validation._ import io.github.hamsters.{FutureEither, FutureOption} import io.github.hamsters.MonadTransformers._  def fea: Future[Either[String, Int]] = Future(OK(1)) def feb(a: Int): Future[Either[String, Int]] = Future(OK(a+2))  val composedAB: Future[Either[String, Int]] = for {   a <- FutureEither(fea)   ab <- FutureEither(feb(a)) } yield ab HList HLists can contain heterogeneous data types but are strongly typed. It's like tuples on steroids! When you're manipulating data using tuples, it's common to add or subtrack some elements, but you have to make each element explicit to build a new tuple. HList simplifies this kind of task. :: is used to append elements at the beggining of an HList + is used add element at the end of a HList ++ is used to concatenate 2 Hlists other operations : filter, map, foldLeft, foreach import io.github.hamsters.{HList, HCons, HNil} import HList._  val hlist = 2.0 :: ""hi"" :: HNil  val hlist1 = 2.0 :: ""hi"" :: HNil val hlist2 = 1 :: HNil  val sum = hlist1 + 1 val sum2 = hlist1 ++ hlist2 //(2.0 :: (hi :: (1 : HNil)))  (2.0 :: ""hi"" :: HNil).foldLeft("""")(_+_) // ""2.0hi""  (2.0 :: ""hi"" :: HNil).map(_.toString) // ""2.0"" :: ""hi"" :: HNil  (2.0 :: ""hi"" :: HNil).filter{   case s: String if s.startsWith(""h"") => true   case _ => false } //""hi"" :: HNil  Union types You can define functions or methods that are able to return several types, depending on the context. import io.github.hamsters.{Union3, Union3Type}  //json element can contain a String, a Int or a Double val jsonUnion = new Union3Type[String, Int, Double] import jsonUnion._  def jsonElement(x: Int): Union3[String, Int, Double] = {   if(x == 0) ""0""   else if (x % 2 == 0) 1   else 2.0 } Scaladoc You can find the API documentation here. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-hamsters/hamsters"	"A mini Scala utility library. Compatible with functional programming beginners. Featuring validation, monad transformers, HLists, Union types."	"true"
"Extensions"	"Lamma ★ 51 ⧗ 3"	"https://github.com/maxcellent/lamma"	"A Scala date library for date and schedule generation."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"55"	"8"	"8"	"GitHub - maxcellent/lamma: Lamma schedule generator for Scala is a professional schedule generation library for periodic schedules like fixed income coupon payment, equity deravitive fixing date generation etc. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 8 Star 55 Fork 8 maxcellent/lamma Code Issues 1 Pull requests 0 Pulse Graphs Lamma schedule generator for Scala is a professional schedule generation library for periodic schedules like fixed income coupon payment, equity deravitive fixing date generation etc. 226 commits 3 branches 12 releases Fetching contributors Scala 68.2% Java 31.8% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.2 2.3 master Nothing to show 2.3.0 2.2.3 2.2.2 2.2.0 2.1.0 2.0.1 2.0.0 1.1.3 1.1.2 1.1.1 1.1 1.0 Nothing to show New pull request Latest commit 66b9ffc Jul 16, 2015 maxcellent remove openjdk8 Permalink Failed to load latest commit information. project add publishing details to sonatype Jun 2, 2014 src add Quarter support Jul 15, 2015 .gitignore initial project setup + Frequency and Date definition May 28, 2014 .travis.yml remove openjdk8 Jul 16, 2015 LICENSE update license to 'DO WHAT YOU WANT TO PUBLIC LICENSE' Jun 8, 2014 README.md add build status Jun 12, 2014 build.sbt make javac target version = 1.5 Jul 12, 2015 README.md Lamma Lamma schedule generator is a professional schedule generation library for financial instruments schedules like mortgage repayment schedule, fixed income coupon payment, equity derivative fixing date generation etc. All documentations are now moved to http://lamma.io. Only sources and issues are maintained with github. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/maxcellent/lamma"	"A Scala date library for date and schedule generation."	"true"
"Extensions"	"Log4s"	"http://www.log4s.org/"	"Fast, Scala-friendly logging bindings on top of. Uses macros for extreme performance."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	""	"null"	"null"	"Fast, Scala-friendly logging bindings on top of. Uses macros for extreme performance."	"true"
"Extensions"	"SLF4J"	"http://slf4j.org/"	"Fast, Scala-friendly logging bindings on top of. Uses macros for extreme performance."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"SLF4J Please turn on Javascript to view this menu Simple Logging Facade for Java (SLF4J) The Simple Logging Facade for Java (SLF4J) serves as a simple facade or abstraction for various logging frameworks (e.g. java.util.logging, logback, log4j) allowing the end user to plug in the desired logging framework at deployment time. Before you start using SLF4J, we highly recommend that you read the two-page SLF4J user manual. Note that SLF4J-enabling your library implies the addition of only a single mandatory dependency, namely slf4j-api.jar. If no binding is found on the class path, then SLF4J will default to a no-operation implementation. In case you wish to migrate your Java source files to SLF4J, consider our migrator tool which can migrate your project to use the SLF4J API in just a few minutes. In case an externally-maintained component you depend on uses a logging API other than SLF4J, such as commons logging, log4j or java.util.logging, have a look at SLF4J's binary-support for legacy APIs. Projects depending on SLF4J Here is a non-exhaustive list of projects known to depend on SLF4J, in alphabetical order: Apache ActiveMQ Apache Archiva Apache Camel Apache Directory Apache FTPServer Apache Geronimo Apache Graffito Apache Jackrabbit Apache Mina Apache Qpid Apache ServiceMix Apache Sling Apache Solr Apache Tapestry Apache Wicket Aperture Apogee Artifactory AsyncWeb DbUnit Display tag Ehcache GMaven Gradle GreenMail GumTree H2 Database HA-JDBC Hibernate Igenko Jabsorb Jetty v6 jLynx JMesa JODConverter JTrac JWebUnit 2.x JQuantLib LIFERAY Lift log4jdbc Magnolia MRCP4J Mindquarry Mugshot Mule Nexus Novocode NetCDF OpenMeetings OpenRDF Penrose PSI Probe PZFileReader Quartz Scheduler QuickFIX/J Sonar SMSJ Spring-OSGi SpringSource dm Server™ StreamBase TimeFinder WTFIGO YASL Xooctory XWiki"	"null"	"null"	"Fast, Scala-friendly logging bindings on top of. Uses macros for extreme performance."	"true"
"Extensions"	"Monocle ★ 547 ⧗ 0"	"https://github.com/julien-truffaut/Monocle"	"An Optics/Lens library for purely functional manipulation of immutable objects."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"589"	"50"	"87"	"GitHub - julien-truffaut/Monocle: Optics library for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 50 Star 589 Fork 87 julien-truffaut/Monocle Code Issues 19 Pull requests 1 Wiki Pulse Graphs Optics library for Scala http://julien-truffaut.github.io/Monocle/ 1,070 commits 18 branches 17 releases 44 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.x adopters arbitrary-function auto-publish-doc cats compose-modify date fix-351 gh-pages java8-scalac-opt master note release-1.2.1 release-1.2.2 release-note-1.2.2 revert-348-patch-1 sans-rename scalaz-7.3.0-M3 Nothing to show v1.2.2 v1.2.1 v1.2.0 v1.2.0-RC1 v1.2.0-M2 v1.2.0-M1 v1.1.1 v1.1.0 v1.0.1 v1.0.0 v1.0.0-M1 v0.5.1 v0.5.0 v0.4.0 v0.3.0 v0.2.0 v0.1 Nothing to show New pull request Latest commit 5217bab Jul 11, 2016 aoiroaoino committed on GitHub Merge pull request #378 from julien-truffaut/auto-publish-doc … fix #372 auto publish site, add kind projector to all modules, upgrade tut Permalink Failed to load latest commit information. bench/src/main/scala/monocle/bench Remove modify benchmarks from ProductIsoBench Nov 21, 2015 core/shared/src/main/scala/monocle Update ScalaDoc for Wrapped Jun 16, 2016 docs/src add release notes for 1.0 and 1.1 to the website Jun 21, 2016 example/src Remove ala and au Jun 16, 2016 generic/shared/src/main/scala/monocle/generic Introduce generic Each derivation Apr 11, 2016 image new diagram Dec 21, 2014 law/shared/src/main/scala/monocle/law/discipline re add set idempotent laws May 31, 2016 macro/shared/src/main/scala/monocle/macros Apply Scala.JS directory structure Apr 1, 2016 project fix #372 auto publish site, add kind projector to all modules, upgrad… Jul 9, 2016 refined/shared/src/main/scala/monocle/refined Fix #315 #342 Apr 9, 2016 state/shared/src/main/scala/monocle/state Apply Scala.JS directory structure Mar 31, 2016 test/shared/src/test/scala Upgrade Scala.JS to 0.6.10 Jul 9, 2016 .gitignore Add first version of website Jul 2, 2015 .travis.yml fix #372 auto publish site, add kind projector to all modules, upgrad… Jul 9, 2016 LICENSE Initial commit Jan 17, 2014 PROCESS.md add release note related doc Sep 24, 2015 README.md post release update for 1.2.2 Jun 9, 2016 build.sbt fix #372 auto publish site, add kind projector to all modules, upgrad… Jul 9, 2016 version.sbt Setting version to 1.3.0-SNAPSHOT Dec 9, 2015 README.md Build import sbt._ resolvers += Resolver.sonatypeRepo(""releases"") resolvers += Resolver.sonatypeRepo(""snapshots"")  val scalaVersion   = ""2.11.8""    // or ""2.10.6"" val libraryVersion = ""1.2.2""     // or ""1.3.0-SNAPSHOT""  libraryDependencies ++= Seq(   ""com.github.julien-truffaut""  %%  ""monocle-core""    % libraryVersion,   ""com.github.julien-truffaut""  %%  ""monocle-generic"" % libraryVersion,   ""com.github.julien-truffaut""  %%  ""monocle-macro""   % libraryVersion,           ""com.github.julien-truffaut""  %%  ""monocle-state""   % libraryVersion,        ""com.github.julien-truffaut""  %%  ""monocle-refined"" % libraryVersion,   ""com.github.julien-truffaut""  %%  ""monocle-law""     % libraryVersion % ""test""  )  // for @Lenses macro support addCompilerPlugin(""org.scalamacros"" %% ""paradise"" % ""2.1.0"" cross CrossVersion.full) Table of contents Motivation What does it mean? Why do I need this? More abstractions Lens Creation Generic Optics and Instance Location Policy Optics Hierarchy Modules Maintainers and Contributors Contact Motivation Monocle is a Lens library, or more generally an Optics library where Optics gather the concepts of Lens, Traversal, Optional, Prism and Iso. Monocle is strongly inspired by Haskell Lens. What does it mean? Optics are a set of purely functional abstractions to manipulate (get, set, modify) immutable objects. Optics compose between each other and particularly shine with nested objects. Why do I need this? Scala already provides getters and setters for case classes but modifying nested object is verbose which makes code difficult to understand and reason about. Let's have a look at some examples: case class Street(name: String, ...)     // ... means it contains other fields case class Address(street: Street, ...) case class Company(address: Address, ...) case class Employee(company: Company, ...) Let's say we have an employee and we need to set the first character of his company street name address in upper case. Here is how we could write it in vanilla Scala: val employee: Employee = ...  employee.copy(   company = employee.company.copy(     address = employee.company.address.copy(       street = employee.company.address.street.copy(         name = employee.company.address.street.name.capitalize // luckily capitalize exists       }     )   ) ) As you can see copy is not convenient to update nested objects as we need to repeat at each level the full path to reach it. Let's see what could we do with Monocle: val _name   : Lens[Street  , String]  = ...  // we'll see later how to build Lens val _street : Lens[Address , Street]  = ... val _address: Lens[Company , Address] = ... val _company: Lens[Employee, Company] = ...  (_company composeLens _address composeLens _street composeLens _name).modify(_.capitalize)(employee)  // you can achieve the same result with less characters using symbolic syntax  (_company ^|-> _address ^|-> _street ^|-> _name).modify(_.capitalize)(employee) ComposeLens takes two Lens, one from A to B and another from B to C and creates a third Lens from A to C. Therefore, after composing _company, _address, _street and _name, we obtain a Lens from Employee to String (the street name). More abstractions In the above example, we used capitalize to upper case the first letter of a String. It works but it would be clearer if we could use Lens to zoom into the first character of a String. However, we cannot write such a Lens because a Lens defines how to focus from an object S into a mandatory object A and in our case, the first character of a String is optional as a String might be empty. For this we need a sort of partial Lens, in Monocle it is called Optional. import monocle.function.headOption._ // to use headOption (a generic optic) import monocle.std.string._          // to get String instance for HeadOption   (_company composeLens _address           composeLens _street           composeLens _name           composeOptional headOption).modify(toUpper)(employee) Similarly to composeLens, composeOptional takes two Optional, one from A to B and another from B to C and creates a third Optional from A to C. All Lens can be seen as Optional where the optional element to zoom to is always present, hence composing an Optional and a Lens always produces an Optional (see class diagram for full inheritance relation between Optics). For more examples, see the example module. Lens Creation There are 3 ways to create Lens, each with their pro and cons: The manual method where we construct a Lens by passing get and set functions:  val _company = Lens[Employee, Company](_.company)( c => e => e.copy(company = c))  // or with some type inference  val _company = Lens((_: Employee).company)( c => e => e.copy(company = c)) The semi-automatic method using the GenLens blackbox macro:  val _company = GenLens[Employee](_.company)  val _name    = GenLens[Employee](_.name)   // or  val genLens = GenLens[Employee]  val (_company, _name) = (genLens(_.company) , genLens(_.name)) Finally, the fully automatic method using the @Lenses macro annotation. @Lenses generates Lens for every accessor of a case class in its companion object (even if there is no companion object defined). This solution is the most boiler plate free but it has several disadvantages: users need to add the macro paradise plugin to their project. poor IDE supports, at the moment only IntelliJ recognises the generated Lens. requires access to the case classes since you need to annotate them. @Lenses case class Employee(company: Company, name: String, ...)  // generates Employee.company: Lens[Employee, Company] // and       Employee.name   : Lens[Employee, String]  // you can add a prefix to Lenses constructor  @Lenses(""_"") case class Employee(company: Company, name: String, ...)  // generates Employee._company: Lens[Employee, Company] Note: GenLens and @Lenses are both limited to case classes Optics in the REPL and tut Iso, Prism, Lens, Optional, Traversal and Setter are all type aliases for more general polymorphic optics, for example here is the definition of Lens: type Lens[S, A] = PLens[S, S, A, A]  object Lens {   def apply[S, A](get: S => A)(set: A => S => S): Lens[S, A] =      PLens(get)(set) } This is a completely fine Scala definition and it will work perfectly in your code. However, if you try to create optics in the REPL you will probably encounter a similar error: scala> import monocle.Lens import monocle.Lens  scala> case class Example(s: String, i: Int) defined class Example  scala> val s = Lens[Example, String](_.s)(s => _.copy(s = s)) s: monocle.Lens[Example,String] = monocle.PLens$$anon$7@46aa4219  scala> val i = Lens[Example, Int](_.i)(i => _.copy(i = i)) <console>:13: error: object Lens does not take type parameters.        val i = Lens[Example, Int](_.i)(i => _.copy(i = i))  We managed to create the first Lens but the second call to apply failed. This is a known bug in the REPL which is tracked by SI-7139. You will also face this error if you use tut to create documentation. Generic Optics and Instance Location Policy A generic optic is an optic that is applicable to different types. For example, headOption is an Optional from some type S to its optional first element of type A. In order to use headOption (or any generic optics), you need to: import the generic optic in your scope via import monocle.function.headOption._ or import monocle.function._ have the required instance of the type class monocle.HeadOption in your scope, e.g. if you want to use headOption from a List[Int], you need an instance of HeadOption[List[Int], Int]. This instance can be either provided by you or by Monocle. Monocle defines generic optic instances in the following packages: monocle.std for standard Scala library and Scalaz classes, e.g. List, Vector, Map, IList, OneAnd monocle.generic for Shapeless classes, e.g. HList, CoProduct An example shows how to use Monocle imports. Optics Hierarchy Modules Core defines the main library concepts: optics, typeclass, syntax. Core only depends on scalaz for type classes. Law defines properties for optics using discipline and scalacheck. Macro defines a set of macros to generate optics automatically. Generic is an experiment to provide highly generalised Optics using shapeless. Maintainers and Contributors The current maintainers (people who can merge pull requests) are: Julien Truffaut - @julien-truffaut Ilan Godik - @NightRa Naoki Aoyama - @aoiroaoino and the contributors (people who committed to Monocle). Contact If you have any question, we have a gitter channel and a mailing group Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/julien-truffaut/Monocle"	"An Optics/Lens library for purely functional manipulation of immutable objects."	"true"
"Extensions"	"Persist-Logging ★ 19 ⧗ 6"	"https://github.com/nestorpersist/logging"	"Comprehensive logging library for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"20"	"2"	"2"	"GitHub - nestorpersist/logging: Scala Logging Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 20 Fork 2 nestorpersist/logging Code Issues 0 Pull requests 0 Pulse Graphs Scala Logging Library 11 commits 1 branch 0 releases 1 contributor Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit e2ece4b Mar 19, 2016 nestorpersist Update README.md Permalink Failed to load latest commit information. demo 1.2.1 Mar 13, 2016 kafka-appender Update README.md Mar 19, 2016 logger 1.2.4 Mar 19, 2016 .gitignore first version Jan 16, 2016 CHANGES 1.2.1 Mar 13, 2016 License.txt first version Jan 16, 2016 README.md Update README.md Mar 19, 2016 README.md Scala Logging This library provides an advanced logging facility for Scala applications. It has the following features. All log messages are routed to a single Akka Actor. It captures logging messages from its own advanced Scala API, Java Slf4j and Akka loggers and sends them to that actor. All messages are logged by default as Json, but other formats can be defined. In the Scala API messages can be maps rather then just strings. The Scala API, supports logging exceptions and their stack trace as Json. Logs can be sent to stdout, local log files or a user defined appender. Stdout messages are logged with abbreviated info in a pretty format. Other logs log one message per line. Log messages have a timestamp set at the time of the log call. Logging within a Scala class logs the class name and line number. Logging inside an Akka actor also logs the actor path. The logging level can be set dynamically. The logging level can be overriden on a per id basis. There is an optional logger that will log garbage collection events. There is an optional time logger that can be used to track subtimes of a complex operation. This can be used across futures and actor message sends. There is an ""alternative"" logger that can be used to create custom logs. A user defined filter can be used to control which messages are logged. This filter can be modified at run-time. The Scala API supports request ids that can be used to track a single request across multiple services. New log appenders can be defined and can be used in place of the built-in stdout and file appender. These appenders can write in any format (including non Json ones) and can send logs to other services (such as Kafka or Flume). Documentation There are several kinds of documentation. This readme contains an overview of all features. Scaladoc is available for the logging API. One way to view this is to use the sbt view plugin https://github.com/nestorpersist/sbt-view The demo subdirectory contains a project with sample uses of the logging api. The logger/src/main/resources/logging.conf file documents all the configuration options and specifies their default values. Quick Start (see demo Simple) The Scala logger contains lots of advanced features, but this section introduces the simple core features that will get you up and going fast. First you must include the logging jar ""com.persist"" % ""logging_2.11"" % ""1.2.4""  Next add the following line to your application.conf file include ""logging.conf""  In your Scala files that use logging add the following line include com.persist.logging._  At the beginning of your program, initialize the logging system as follows val system = ActorSystem(""test"") val loggingSystem = LoggingSystem(system,""myproject"", ""1.0.2"",""myhost.com"")  See the Simple demo code for a nice way to initialize these parameters using the sbt buildinfo plugin. At the end of your program, shut down the logging and actor systems Await.result(loggingSystem.stop, 30 seconds) Await.result(system.terminate(), 30 seconds)  Inherit trait ClassLogging in classes to define logging methods. Inside those classes you can then call log.error(""Foo failed"")  or you can use a map for the msg log.warn(Map(""@msg""->""fail"",""value""->23))  By default, log messages will be written in a pretty Json form to stdout and in a compact (1 message per line) Json form in log files. Configuration The default configuration is in the file src/main/resources/logging.conf  You should include this in your application.conf file. include ""logging.conf""  This can then be followed by any of your overrides of the default configuration setting. The demo directory contains a sample application.conf file. Basic Logging (see demo Exceptions) Logging is enabled by inheriting the trait ClassLogging (see demo Simple) in classes and the trait ActorLogging in actors (see demo ActorDemo). A typical log call takes the form log.warn(msg,ex=ex,id=id)  msg. The msg can be either a string or a map. Use the map form rather than string interpolation. When using the map form by convention use the field name @msg for the main error message. This ensures in sorted output it will apperar first. log.warn(s""Size $size is too big"") log.warn(Map(""@msg""->""Size is too big"",""size""->size)) ex. Optional exception. A RichException allows its message to be either a string or a map. The logger can expand that map into the logged Json. Ordinary Scala/Java exceptions will, of course, also work just fine. try { ... throw RichException(Map(""@msg""->""Size is too big"", ""size""->size)) ... } catch { case ex => log.error(""Body failed"", ex) } id. Optional id. See the section below on Request Ids. The full set of logging levels are trace, debug, info, warn, error and fatal. Alternative Logging (see demo Alternative) You can create your own log files. log.alternative(""extra"", Map(""a""->""foo"", ""b""-> 5.6))  This will log a message to the extra log. If no extra log previously exists it will be created. Alternative messages are by default not logged to stdout. Rich Messages The rich message in logging calls and in RichExceptions can be either a string or a map from strings to values. But rich messages are even more general. A rich message can be String Boolean Long Double BigDecimal null Seq[RichMsg] Map[String,RichMsg] Standard Fields Logs can contain the following fields. These are listed in alphabetical order (this is the order displayed by the Json pretty printer). @category. The kind of log file: common, gc, time, or alterative log name. @host. The local host name. @service. The name and version of the application being run. @severity. The message level: trace, debug, info, warn, error or fatal. @timestamp. The time the message was logged. @traceId. The request id, if specified. @version. The logging system version. Currently 1. actor. The path for an actor when using ActorLogging. class. The class for ClassLogging or ActorLogging. file. The file containing the log call. kind. Either slf4j or akka. Not present for logger API. line. The line where the message was logged. msg. The rich message. trace. Information for any exception specfied in the logging call. The fields @host, @service, and @version are the same for all messages from a single application and are only present in logs when the fullHeaders option is specified. This option is off by default for stdout and on by default for files. The @category option is also not present in stdout (because it contains only common messages). Appenders (see demo Appender) Appenders output log messages. There are tow build-in appenders: stdout and file plus an optional Kafka appenders. Each of these has its own configuration options (see logging.conf for details). You can also define your own appenders. The set of appenders to use is specified via the optional appenderBuilders parameter to the LoggingSystem constructor. stdout. Common log messages are written in Pretty Json form without standard headers. This appender is useful during development. It is usually disabled in production file. Log messages are written as Json, one per line. Ordinary log messages are written to the common log files. Alterative, garbage collection, and timing messages have their own log files. Each log file includes a day as part of its name. Each day a new file is created. Kafka. The kafka-appender directory contains a Kafka appender. See the README there for details on how to use. custom. Custom appenders implement the LogAppender and LogAppenderBuilder traits. Custom loggers could be build for Flume. Also note that custom loggers need not output Json but can instead have their own custom format. Json Utilities Log files with one Json message per line can be hard to read. To make this easier, you can use the Json Pretty utility which you can get at <https://github.com/nestorpersist/json-tools/commits?author=nestorpersist> To view a log. cat common.2016-01-10.log | Pretty | less   To convert a log file to a Pretty version of that file. Pretty < common.2016-01-10.log > common.2016-01-10.pretty  Akka Logging (see demo OtherApis) Akka includes its own logger which can be added to Actors with the trait akka.actor.ActorLogging. These messages will be send to the common log with kind set to ""akka"". Akka logging has a minumum logging level with default warning. No messages with a level less than that mimimum level will ever be logged. You can change that minimum level by changing the value of the configuration value of akka.loglevel. See the section below on logging levels on how to more dynamically set the akka logging level higher than the minimum. Java Logging (see demo OtherApis) If you include Java, code in your application, you will often find it uses Slf4J to do its logging. Slf4J is routed to logback that in turn sends Slf4j messages to the common log with kind set to ""slf4j"". Slf4J logging has a minimum logging level with the default WARN. No messages with a level less than that minimum level will ever be logged. You can change that minimum level by setting the following Java property when launching the JVM. -DSLF4J_LEVEL=INFO  If you are using other loggers such as log4j, you can route those to slf4j using a bridge. See http://www.slf4j.org/legacy.html. See the section below on logging levels on how to more dynamically set the Slf4J logging level higher than the minimum. Log Levels There are separate log levels for the logging API, Akka logging, and Slf4J. The initial value of these is set in the configuration file. Default values appear in the logging.conf file and can be overriden in the application.conf file. These values can be changed dynamically by calling methods on the LoggingSystem class. The log level also be changed on individual requests (see the section below on request ids). Log Filters (see demo Filter) Logging filters provide a very powerful way of filtering common log messages. When a filter is enabled, a user specified function can see all the fields of the message and decide if that message is to be logged or ignored. A logging filter is enabled and disabled by call the setFilter method of LoggingSytem. A logging filter by itself can only reduce the number of messages logged, but it can also be used as part of a pattern to increase the number of messages logged. Suppose that the current logging level is info but a you want to also log all debug messages for class Foo. Add a logging filter (by calling logSystem.setFilter) that permits all messages that are either in class Foo with level at least debug or have a logging level of at least info. Lower the logging level from info to debug by calling logSystem.setLevel. Note that logging filters are powerful, but do introduce additional processing overhead. Garbage Collection Logging Garbage collection events are logged to the gc log when enabled by the gc configuration option. Request Ids (see demo RequestIds) Request Ids are based on the ideas from Google Dapper http://research.google.com/pubs/pub36356.html and Twitter Zipkin https://twitter.github.io/zipkin/. The basic idea is that individual requests into a distributed service-orriented architecture can be tracked within and across services. Information contained in the log files of the set of services is combined to yeild a complete picture of each individual request. A request Id contains three parts: trackingId. This value should be the same for all log calls across all services for a specific request. A guid is often used for this purpose. spanId. Suppose that for a given request to service A, it calls service B multiple times. The spanId should be different on each call to B. level. An optional field that can be used to change the log level of an individual request. A message is logged if its level is greater or equal to either the common log level or the level specified in the request id. This permits more detailed logging of single request without having to increase the log level for all requests. To implement full distributed tracing, in addition to the this logger, two other things are needed. Two log files (these can be created with log.alternative) A server log that logs the request ids of all incoming requests to a service. a client log that logs the request ids of outgoing requests from a service to other services. An analysis program that can join information across logs and display them in a usable form. Timing Logging (see demo Timing) Time logging provides a simple way to get fine grain timing of dynamically nested blocks of code. Timing is only enabled when the configuation value com.persist.logging.time is set to true. Results are written to time log where times are reported in microseconds. To add timing calls to you code, you must include trait Timing. Surround each block of code with a call to Time. For cases, such as futures, where nesting is not possible instead call time.start and time.end. Persist Json Persist Json is a light-weight high-performance Scala Json library. It is used in the implementation of the logger. The public logger APIs do not use Persist Json. So you are not required to use Persist Json in your code to use the logger. Note however that Persist Json parse trees are rich messages. In particular, Persist Json parse trees consist only of build-in Scala types and do not define any new classes. Persist Json provides a extensive set of features beyond what the logger provides for working with rich messages. Acknowledgements An earlier version of this logger was produced by Whitepages (http://www.whitepages.com). This earlier code can be found at https://github.com/whitepages/WP_Scala_Framework/tree/master/logging. Work improving this logger and making it suitable for general use was supported by 47 Degrees http://www.47deg.com. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nestorpersist/logging"	"Comprehensive logging library for Scala."	"true"
"Extensions"	"Quicklens ★ 153 ⧗ 16"	"https://github.com/adamw/quicklens"	"modify deeply nested case class fields with an elegant API"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"191"	"18"	"8"	"GitHub - adamw/quicklens: Modify deeply nested case class fields Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 18 Star 191 Fork 8 adamw/quicklens Code Issues 6 Pull requests 0 Pulse Graphs Modify deeply nested case class fields http://www.softwaremill.com 90 commits 2 branches 11 releases 6 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.10 master Nothing to show release-1.4.7 release-.1.4.6 release-1.4.4 release-1.4.3 release-1.4.1 release-1.4.0 release-1.3.1 release-1.3 release-1.2 release-1.1 release-1.0 Nothing to show New pull request Latest commit da22905 May 14, 2016 adamw Update scalatest Permalink Failed to load latest commit information. project Update scalatest May 14, 2016 quicklens/src/main/scala/com/softwaremill/quicklens Add `when`, which allows to modify only a given subtype May 14, 2016 tests/src/test/scala/com/softwaremill/quicklens Add `when`, which allows to modify only a given subtype May 14, 2016 .gitignore Initial Feb 27, 2015 README.md README update May 14, 2016 README.md Quicklens Modify deeply nested fields in case classes: import com.softwaremill.quicklens._  case class Street(name: String) case class Address(street: Street) case class Person(address: Address, age: Int)  val person = Person(Address(Street(""1 Functional Rd."")), 35)  val p2 = person.modify(_.address.street.name).using(_.toUpperCase) val p3 = person.modify(_.address.street.name).setTo(""3 OO Ln."")  // or  val p4 = modify(person)(_.address.street.name).using(_.toUpperCase) val p5 = modify(person)(_.address.street.name).setTo(""3 OO Ln."") Chain modifications: person   .modify(_.address.street.name).using(_.toUpperCase)   .modify(_.age).using(_ - 1) Modify several fields in one go: import com.softwaremill.quicklens._  case class Person(firstName: String, middleName: Option[String], lastName: String)  val person = Person(""john"", Some(""steve""), ""smith"")  person.modifyAll(_.firstName, _.middleName.each, _.lastName).using(_.capitalize) Traverse options/lists/maps using .each: import com.softwaremill.quicklens._  case class Street(name: String) case class Address(street: Option[Street]) case class Person(addresses: List[Address])  val person = Person(List(   Address(Some(Street(""1 Functional Rd.""))),   Address(Some(Street(""2 Imperative Dr.""))) ))  val p2 = person.modify(_.addresses.each.street.each.name).using(_.toUpperCase) .each can only be used inside a modify and ""unwraps"" the container (currently supports Lists, Options and Mapss - only values are unwrapped for maps). You can add support for your own containers by providing an implicit QuicklensFunctor[C] with the appropriate C type parameter. Traverse selected elements using .eachWhere: Similarly to .each, you can use .eachWhere(p) where p is a predicate to modify only the elements which satisfy the condition. All other elements remain unchanged. def filterAddress: Address => Boolean = ??? person   .modify(_.addresses.eachWhere(filterAddress)            .street.eachWhere(_.name.startsWith(""1"")).name)   .using(_.toUpperCase) Modify specific sequence elements using .at: person.modify(_.addresses.at(2).street.each.name).using(_.toUpperCase) Similarly to .each, .at modifies only the element at the given index. If there's no element at that index, an IndexOutOfBoundsException is thrown. Modify specific map elements using .at: case class Property(value: String)  case class Person(name: String, props: Map[String, Property])  val person = Person(   ""Joe"",   Map(""Role"" -> Property(""Programmmer""), ""Age"" -> Property(""45"")) )  person.modify(_.props.at(""Age"").value).setTo(""45"") Similarly to .each, .at modifies only the element with the given key. If there's no such element, an NoSuchElementException is thrown. Modify fields when they are of a certain subtype: trait Animal case class Dog(age: Int) extends Animal case class Cat(ages: List[Int]) extends Animal  case class Zoo(animals: List[Animal])  val zoo = Zoo(List(Dog(4), Cat(List(3, 12, 13))))  val olderZoo = zoo.modifyAll(   _.animals.each.when[Dog].age,   _.animals.each.when[Cat].ages.at(0) ).using(_ + 1) This is also known as a prism, see e.g. here. Re-usable modifications (lenses): import com.softwaremill.quicklens._  val modifyStreetName = modify(_: Person)(_.address.street.name)  val p3 = modifyStreetName(person).using(_.toUpperCase) val p4 = modifyStreetName(anotherPerson).using(_.toLowerCase)  //  val upperCaseStreetName = modify(_: Person)(_.address.street.name).using(_.toUpperCase)  val p5 = upperCaseStreetName(person) Composing lenses: import com.softwaremill.quicklens._  val modifyAddress = modify(_: Person)(_.address) val modifyStreetName = modify(_: Address)(_.street.name)  val p6 = (modifyAddress andThenModify modifyStreetName)(person).using(_.toUpperCase) Modify nested sealed hierarchies: Note: this feature is experimental and might not work due to compilation order issues. See https://issues.scala-lang.org/browse/SI-7046 for more details. import com.softwaremill.quicklens._  sealed trait Pet { def name: String } case class Fish(name: String) extends Pet sealed trait LeggedPet extends Pet case class Cat(name: String) extends LeggedPet case class Dog(name: String) extends LeggedPet  val pets = List[Pet](   Fish(""Finn""), Cat(""Catia""), Dog(""Douglas"") )  val juniorPets = pets.modify(_.each.name).using(_ + "", Jr."") Similar to lenses (1, 2), but without the actual lens creation. Read the blog for more info. Available in Maven Central: val quicklens = ""com.softwaremill.quicklens"" %% ""quicklens"" % ""1.4.7"" Also available for Scala.js! Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/adamw/quicklens"	"modify deeply nested case class fields with an elegant API"	"true"
"Extensions"	"Rapture"	"http://rapture.io/"	"() - a collection of libraries for common, everyday programming tasks (I/O, JSON, i18n, etc.)"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"109"	"10"	"23"	"GitHub - propensive/rapture: Rapture Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 10 Star 109 Fork 23 propensive/rapture Code Issues 45 Pull requests 2 Wiki Pulse Graphs Rapture 355 commits 77 branches 7 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: dev Switch branches/tags Branches Tags better-result-strings cli-log-import-clash code-formatting-scalafmt conditional-play-version css-stylesheets data-type-aliases dev dom-improvements fix-broken-file-reads fix-dependencies-build fix-headers fix-missing-interpolator fix-tests fix_applyDynamic_regression forms generalized-exec-types google-translate http-fixes http-query-fixes http-responses http-tweaks io-develelopment issue-24 issue-41 issue-42 issue-49 issue-52 issue-59 issue-61 issue-71 issue-73 issue-82 issue-98 issue-100 issue-103 issue-105 issue-112 issue-121 issue-125 issue-134 issue-149 issue-153 issue-157 issue-173 issue-197 jfc-json-backend json-http json-modification json-post-type json-strings linkable-enrichment linkable-typeclass linkable-typeclasses m4-release-notes mail minimum-edit-distance mixed-backends-improvements more-latex-email-changes more-sh-params net-http-fixes new-dom new-headers non-string-json-map-keys rapture-css reduce-dependencies reinstate-query remove-more-core-reflection remove-typetags rename-rapture-web-to-http renamed-add-to-copy revert-62-issue-61 scala-fmt tagged-types time-redevelopment timezones unused-imports url-query-param-encoding Nothing to show v2.0.0-M6 v2.0.0-M5 v2.0.0-M4 v2.0.0-M3 v2.0.0-M2 v2.0.0-M1 v0.0.0 Nothing to show New pull request Latest commit d1f9a63 Jul 8, 2016 mielientiev committed with propensive Reformatted code (#212) Permalink Failed to load latest commit information. base/shared/src/main Reformatted code (#212) Jul 8, 2016 cli Reformatted code (#212) Jul 7, 2016 codec Reformatted code (#212) Jul 7, 2016 core-scalaz Reformatted code (#212) Jul 7, 2016 core-test New headers (#179) Apr 27, 2016 core Reformatted code (#212) Jul 7, 2016 crypto Reformatted code (#212) Jul 7, 2016 css Reformatted code (#212) Jul 7, 2016 csv Reformatted code (#212) Jul 7, 2016 currency Reformatted code (#212) Jul 7, 2016 data Reformatted code (#212) Jul 7, 2016 doc [DOCS] add syntax needs an equals, not a comma. (#177) Apr 21, 2016 dom Reformatted code (#212) Jul 7, 2016 etc New headers (#179) Apr 27, 2016 fs Reformatted code (#212) Jul 7, 2016 google-translate Http tweaks (#191) May 22, 2016 html Reformatted code (#212) Jul 7, 2016 http-jetty Reformatted code (#212) Jul 7, 2016 http-json Reformatted code (#212) Jul 7, 2016 http Reformatted code (#212) Jul 7, 2016 i18n Reformatted code (#212) Jul 7, 2016 io Reformatted code (#212) Jul 7, 2016 js Reformatted code (#212) Jul 7, 2016 json-argonaut Reformatted code (#212) Jul 7, 2016 json-circe Reformatted code (#212) Jul 7, 2016 json-jackson Reformatted code (#212) Jul 7, 2016 json-jawn Reformatted code (#212) Jul 7, 2016 json-json4s Reformatted code (#212) Jul 7, 2016 json-lift Reformatted code (#212) Jul 7, 2016 json-play Reformatted code (#212) Jul 7, 2016 json-spray Reformatted code (#212) Jul 7, 2016 json-test Minor clean up (#203) Jul 4, 2016 json Reformatted code (#212) Jul 7, 2016 latex Reformatted code (#212) Jul 7, 2016 log Reformatted code (#212) Jul 7, 2016 mail Reformatted code (#212) Jul 7, 2016 mime Reformatted code (#212) Jul 7, 2016 net Reformatted code (#212) Jul 7, 2016 project Reformatted code (#212) Jul 7, 2016 test Reformatted code (#212) Jul 7, 2016 text Reformatted code (#212) Jul 7, 2016 time Reformatted code (#212) Jul 7, 2016 uri Reformatted code (#212) Jul 7, 2016 xml-stdlib Reformatted code (#212) Jul 7, 2016 xml-test Reformatted code (#212) Jul 7, 2016 xml Reformatted code (#212) Jul 7, 2016 .gitignore Add Scala.js support Jul 14, 2015 .jvmopts Build fixes Aug 8, 2015 .scalafmt Reformatted code (#212) Jul 7, 2016 .travis.yml Run tests on Travis Jul 7, 2016 README.md Readme mention Dec 25, 2015 build.sbt Reformatted code (#212) Jul 7, 2016 version.sbt Setting version to 2.0.0-M7 May 25, 2016 README.md Rapture Rapture is an evolving collection of useful libraries for solving common, everyday programming tasks, using advanced features of Scala to offer better type-safety through powerful APIs that all Scala developers, beginners and advanced users, should find intuitive. Rapture consists of a number of modules, the most notable of which are: Core (core) — a library of common utilities for other projects, notably modes and the Result type JSON (json) — comprehensive support for working with JSON data XML (xml) — comprehensive, but experimental, support for working with XML data I/O (io) — I/O (network, filesystem) functionality and infrastructure I18n (i18n) — simple, typesafe representation of internationalized strings CLI (cli) — support for working with command-line applications and shell interaction Themes in Rapture The Rapture modules share a common philosophy that has evolved over time and experience. Here are a few of the philosophical themes crosscutting all of the Rapture modules. A primary goal of intuitive, readable APIs and minimal code repetition Extreme type-safety, with a goal to reduce the surface area of code exposed to runtime exceptions Thoroughly typeclass-driven design, for extensibility Fearless exploitation of all Scala features, where (but only where) it is appropriate Agnostic support for multiple, alternative implementations of many operations with pluggable backends Extensive, but principled, usage of implicits to configure and constrain operations Support for modes in most APIs; the ability to change how failures are handled through return types Availability Snapshots of Rapture are available for Scala 2.10 and 2.11 under the Apache 2.0 License in the Sonatype Snapshots repository, with group ID com.propensive and artifact ID rapture-[module], where module is the name of the module, as taken from the list above. Development work to get most Rapture modules working on Scala.JS is ongoing. You can build and run Rapture locally by cloning this repository and running sbt publishLocal. Contributing Rapture openly welcomes contributions! We would love to receive pull requests of bugfixes and enhancements from other developers. To avoid potential wasted effort, bugs should first be reported on the Github issue tracker, and it's normally a good idea to talk about enhancements on the Gitter channel before embarking on any development. Alternatively, just send Jon Pretty (@propensive) a tweet to start a conversation. Current contributors include: Jon Pretty Raúl Raja Martínez Alistair Johnson Documentation Rapture's documentation is currently sparse, though we are working to improve this. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/propensive/rapture"	"() - a collection of libraries for common, everyday programming tasks (I/O, JSON, i18n, etc.)"	"true"
"Extensions"	"repo"	"https://github.com/propensive/rapture"	"() - a collection of libraries for common, everyday programming tasks (I/O, JSON, i18n, etc.)"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"109"	"10"	"23"	"GitHub - propensive/rapture: Rapture Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 10 Star 109 Fork 23 propensive/rapture Code Issues 45 Pull requests 2 Wiki Pulse Graphs Rapture 355 commits 77 branches 7 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: dev Switch branches/tags Branches Tags better-result-strings cli-log-import-clash code-formatting-scalafmt conditional-play-version css-stylesheets data-type-aliases dev dom-improvements fix-broken-file-reads fix-dependencies-build fix-headers fix-missing-interpolator fix-tests fix_applyDynamic_regression forms generalized-exec-types google-translate http-fixes http-query-fixes http-responses http-tweaks io-develelopment issue-24 issue-41 issue-42 issue-49 issue-52 issue-59 issue-61 issue-71 issue-73 issue-82 issue-98 issue-100 issue-103 issue-105 issue-112 issue-121 issue-125 issue-134 issue-149 issue-153 issue-157 issue-173 issue-197 jfc-json-backend json-http json-modification json-post-type json-strings linkable-enrichment linkable-typeclass linkable-typeclasses m4-release-notes mail minimum-edit-distance mixed-backends-improvements more-latex-email-changes more-sh-params net-http-fixes new-dom new-headers non-string-json-map-keys rapture-css reduce-dependencies reinstate-query remove-more-core-reflection remove-typetags rename-rapture-web-to-http renamed-add-to-copy revert-62-issue-61 scala-fmt tagged-types time-redevelopment timezones unused-imports url-query-param-encoding Nothing to show v2.0.0-M6 v2.0.0-M5 v2.0.0-M4 v2.0.0-M3 v2.0.0-M2 v2.0.0-M1 v0.0.0 Nothing to show New pull request Latest commit d1f9a63 Jul 8, 2016 mielientiev committed with propensive Reformatted code (#212) Permalink Failed to load latest commit information. base/shared/src/main Reformatted code (#212) Jul 8, 2016 cli Reformatted code (#212) Jul 7, 2016 codec Reformatted code (#212) Jul 7, 2016 core-scalaz Reformatted code (#212) Jul 7, 2016 core-test New headers (#179) Apr 27, 2016 core Reformatted code (#212) Jul 7, 2016 crypto Reformatted code (#212) Jul 7, 2016 css Reformatted code (#212) Jul 7, 2016 csv Reformatted code (#212) Jul 7, 2016 currency Reformatted code (#212) Jul 7, 2016 data Reformatted code (#212) Jul 7, 2016 doc [DOCS] add syntax needs an equals, not a comma. (#177) Apr 21, 2016 dom Reformatted code (#212) Jul 7, 2016 etc New headers (#179) Apr 27, 2016 fs Reformatted code (#212) Jul 7, 2016 google-translate Http tweaks (#191) May 22, 2016 html Reformatted code (#212) Jul 7, 2016 http-jetty Reformatted code (#212) Jul 7, 2016 http-json Reformatted code (#212) Jul 7, 2016 http Reformatted code (#212) Jul 7, 2016 i18n Reformatted code (#212) Jul 7, 2016 io Reformatted code (#212) Jul 7, 2016 js Reformatted code (#212) Jul 7, 2016 json-argonaut Reformatted code (#212) Jul 7, 2016 json-circe Reformatted code (#212) Jul 7, 2016 json-jackson Reformatted code (#212) Jul 7, 2016 json-jawn Reformatted code (#212) Jul 7, 2016 json-json4s Reformatted code (#212) Jul 7, 2016 json-lift Reformatted code (#212) Jul 7, 2016 json-play Reformatted code (#212) Jul 7, 2016 json-spray Reformatted code (#212) Jul 7, 2016 json-test Minor clean up (#203) Jul 4, 2016 json Reformatted code (#212) Jul 7, 2016 latex Reformatted code (#212) Jul 7, 2016 log Reformatted code (#212) Jul 7, 2016 mail Reformatted code (#212) Jul 7, 2016 mime Reformatted code (#212) Jul 7, 2016 net Reformatted code (#212) Jul 7, 2016 project Reformatted code (#212) Jul 7, 2016 test Reformatted code (#212) Jul 7, 2016 text Reformatted code (#212) Jul 7, 2016 time Reformatted code (#212) Jul 7, 2016 uri Reformatted code (#212) Jul 7, 2016 xml-stdlib Reformatted code (#212) Jul 7, 2016 xml-test Reformatted code (#212) Jul 7, 2016 xml Reformatted code (#212) Jul 7, 2016 .gitignore Add Scala.js support Jul 14, 2015 .jvmopts Build fixes Aug 8, 2015 .scalafmt Reformatted code (#212) Jul 7, 2016 .travis.yml Run tests on Travis Jul 7, 2016 README.md Readme mention Dec 25, 2015 build.sbt Reformatted code (#212) Jul 7, 2016 version.sbt Setting version to 2.0.0-M7 May 25, 2016 README.md Rapture Rapture is an evolving collection of useful libraries for solving common, everyday programming tasks, using advanced features of Scala to offer better type-safety through powerful APIs that all Scala developers, beginners and advanced users, should find intuitive. Rapture consists of a number of modules, the most notable of which are: Core (core) — a library of common utilities for other projects, notably modes and the Result type JSON (json) — comprehensive support for working with JSON data XML (xml) — comprehensive, but experimental, support for working with XML data I/O (io) — I/O (network, filesystem) functionality and infrastructure I18n (i18n) — simple, typesafe representation of internationalized strings CLI (cli) — support for working with command-line applications and shell interaction Themes in Rapture The Rapture modules share a common philosophy that has evolved over time and experience. Here are a few of the philosophical themes crosscutting all of the Rapture modules. A primary goal of intuitive, readable APIs and minimal code repetition Extreme type-safety, with a goal to reduce the surface area of code exposed to runtime exceptions Thoroughly typeclass-driven design, for extensibility Fearless exploitation of all Scala features, where (but only where) it is appropriate Agnostic support for multiple, alternative implementations of many operations with pluggable backends Extensive, but principled, usage of implicits to configure and constrain operations Support for modes in most APIs; the ability to change how failures are handled through return types Availability Snapshots of Rapture are available for Scala 2.10 and 2.11 under the Apache 2.0 License in the Sonatype Snapshots repository, with group ID com.propensive and artifact ID rapture-[module], where module is the name of the module, as taken from the list above. Development work to get most Rapture modules working on Scala.JS is ongoing. You can build and run Rapture locally by cloning this repository and running sbt publishLocal. Contributing Rapture openly welcomes contributions! We would love to receive pull requests of bugfixes and enhancements from other developers. To avoid potential wasted effort, bugs should first be reported on the Github issue tracker, and it's normally a good idea to talk about enhancements on the Gitter channel before embarking on any development. Alternatively, just send Jon Pretty (@propensive) a tweet to start a conversation. Current contributors include: Jon Pretty Raúl Raja Martínez Alistair Johnson Documentation Rapture's documentation is currently sparse, though we are working to improve this. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/propensive/rapture"	"() - a collection of libraries for common, everyday programming tasks (I/O, JSON, i18n, etc.)"	"true"
"Extensions"	"refined ★ 235 ⧗ 0"	"https://github.com/fthomas/refined"	"Simple refinement types with compile- and runtime checking"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"291"	"22"	"18"	"GitHub - fthomas/refined: Simple refinement types for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 22 Star 291 Fork 18 fthomas/refined Code Issues 21 Pull requests 2 Wiki Pulse Graphs Simple refinement types for Scala http://refined.timepit.eu 884 commits 22 branches 22 releases 7 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master topic/concat topic/constant-collections topic/doc-bug topic/doc-groups topic/equal-show topic/excludedMiddle topic/isabelle topic/monocle topic/refine-ops topic/refined-smt topic/refineops2 topic/scalacheck-1.13.0 topic/scaladocSettings topic/scalafmt2 topic/slick topic/smt topic/unsafeFrom2 topic/0.5.0-2.12.0-M5 topic/2.12.0 wip/ConformsTo Nothing to show v0.5.0 v0.4.0 v0.3.7 v0.3.6 v0.3.5 v0.3.4 v0.3.3 v0.3.2 v0.3.1 v0.3.0 v0.2.3 v0.2.2 v0.2.1 v0.2.0 v0.1.3 v0.1.2 v0.1.1 v0.1.0 v0.0.3 v0.0.2 0.0.1 0.0.0 Nothing to show New pull request Latest commit dc3a877 Jul 14, 2016 fthomas Remove code duplication Permalink Failed to load latest commit information. contrib Document Equal and Show instances Jun 30, 2016 core Remove code duplication Jul 14, 2016 docs Use a variable in the refineV examples Jul 12, 2016 notes Update to scodec 1.10.2 Jul 6, 2016 project Update to WartRemover 1.0.0 Jul 1, 2016 .gitignore First draft May 10, 2015 .travis.yml Update codecov integration with Travis May 18, 2016 LICENSE Update copyright Apr 23, 2016 README.md Use a variable in the refineV examples Jul 12, 2016 build.sbt Update to scodec 1.10.2 Jul 6, 2016 scalastyle-config.xml Let scalastyle check the order of imports Dec 23, 2015 version.sbt Setting version to 0.5.1-SNAPSHOT Jun 17, 2016 README.md refined: simple refinement types for Scala refined is a Scala library for refining types with type-level predicates which constrain the set of values described by the refined type. It started as a port of the refined Haskell library (which also provides an excellent motivation why this kind of library is useful). A quick example: import eu.timepit.refined._ import eu.timepit.refined.api.Refined import eu.timepit.refined.auto._ import eu.timepit.refined.numeric._  // This refines Int with the Positive predicate and checks via an // implicit macro that the assigned value satisfies it: scala> val i1: Int Refined Positive = 5 i1: Int Refined Positive = 5  // If the value does not satisfy the predicate, we get a meaningful // compile error: scala> val i2: Int Refined Positive = -5 <console>:22: error: Predicate failed: (-5 > 0).        val i2: Int Refined Positive = -5                                        ^  // There is also the explicit refineMV macro that can infer the base // type from its parameter: scala> refineMV[Positive](5) res0: Int Refined Positive = 5  // Macros can only validate literals because their values are known at // compile-time. To validate arbitrary (runtime) values we can use the // refineV function:  scala> val x = 42 // suppose the value of x is not known at compile-time  scala> refineV[Positive](x) res1: Either[String, Int Refined Positive] = Right(42)  scala> refineV[Positive](-x) res2: Either[String, Int Refined Positive] = Left(Predicate failed: (-42 > 0).) refined also contains inference rules for converting between different refined types. For example, Int Refined Greater[W.`10`.T] can be safely converted to Int Refined Positive because all integers greater than ten are also positive. The type conversion of refined types is a compile-time operation that is provided by the library: scala> val a: Int Refined Greater[W.`5`.T] = 10 a: Int Refined Greater[Int(5)] = 10  // Since every value greater than 5 is also greater than 4, `a` can be // ascribed the type Int Refined Greater[W.`4`.T]: scala> val b: Int Refined Greater[W.`4`.T] = a b: Int Refined Greater[Int(4)] = 10  // An unsound ascription leads to a compile error: scala> val c: Int Refined Greater[W.`6`.T] = a <console>:23: error: type mismatch (invalid inference):  Greater[Int(5)] does not imply  Greater[Int(6)]        val c: Int Refined Greater[W.`6`.T] = a                                              ^ This mechanism allows to pass values of more specific types (e.g. Int Refined Greater[W.`10`.T]) to functions that take a more general type (e.g. Int Refined Positive) without manual intervention. Note that W is a shortcut for shapeless.Witness which provides syntax for literal-based singleton types. Table of contents More examples Using refined Documentation Provided predicates Contributors and participation Projects using refined Performance concerns Related projects License More examples import eu.timepit.refined.boolean._ import eu.timepit.refined.char._ import eu.timepit.refined.collection._ import eu.timepit.refined.generic._ import eu.timepit.refined.string._ import shapeless.{ ::, HNil }  scala> refineMV[NonEmpty](""Hello"") res2: String Refined NonEmpty = Hello  scala> refineMV[NonEmpty]("""") <console>:39: error: Predicate isEmpty() did not fail.             refineMV[NonEmpty]("""")                               ^  scala> type ZeroToOne = Not[Less[W.`0.0`.T]] And Not[Greater[W.`1.0`.T]] defined type alias ZeroToOne  scala> refineMV[ZeroToOne](1.8) <console>:40: error: Right predicate of (!(1.8 < 0.0) && !(1.8 > 1.0)) failed: Predicate (1.8 > 1.0) did not fail.        refineMV[ZeroToOne](1.8)                           ^  scala> refineMV[AnyOf[Digit :: Letter :: Whitespace :: HNil]]('F') res3: Char Refined AnyOf[Digit :: Letter :: Whitespace :: HNil] = F  scala> refineMV[MatchesRegex[W.`""[0-9]+""`.T]](""123."") <console>:39: error: Predicate failed: ""123."".matches(""[0-9]+"").               refineMV[MatchesRegex[W.`""[0-9]+""`.T]](""123."")                                                     ^  scala> val d1: Char Refined Equal[W.`'3'`.T] = '3' d1: Char Refined Equal[Char('3')] = 3  scala> val d2: Char Refined Digit = d1 d2: Char Refined Digit = 3  scala> val d3: Char Refined Letter = d1 <console>:39: error: type mismatch (invalid inference):  Equal[Char('3')] does not imply  Letter        val d3: Char Refined Letter = d1                                      ^  scala> val r1: String Refined Regex = ""(a|b)"" r1: String Refined Regex = (a|b)  scala> val r2: String Refined Regex = ""(a|b"" <console>:38: error: Regex predicate failed: Unclosed group near index 4 (a|b     ^        val r2: String Refined Regex = ""(a|b""                                       ^  scala> val u1: String Refined Url = ""htp://example.com"" <console>:38: error: Url predicate failed: unknown protocol: htp        val u1: String Refined Url = ""htp://example.com""                                     ^ Using refined The latest version of the library is 0.5.0, which is available for Scala and Scala.js version 2.10, 2.11, and 2.12.0-M5. If you're using sbt, add the following to your build: libraryDependencies ++= Seq(   ""eu.timepit"" %% ""refined""            % ""0.5.0"",   ""eu.timepit"" %% ""refined-scalaz""     % ""0.5.0"",         // optional   ""eu.timepit"" %% ""refined-scodec""     % ""0.5.0"",         // optional   ""eu.timepit"" %% ""refined-scalacheck"" % ""0.5.0"" % ""test"" // optional )  For Scala.js just replace %% with %%% above. Instructions for Maven and other build tools are available at search.maven.org. Release notes for the latest version are available in 0.5.0.markdown. The optional dependencies are add-on libraries that provide support for other tag types or integration of refined types in other libraries: refined-scalaz for support of Scalaz' tag type (scalaz.@@) refined-scodec for integration with scodec refined-scalacheck for ScalaCheck type class instances of refined types See also the list of projects that use refined for libraries that directly provide support for refined. Documentation API documentation of the latest release is available at: http://fthomas.github.io/refined/latest/api/ There are further (type-checked) examples in the docs directory including ones for defining custom predicates and working with type aliases. It also contains a description of refined's design and internals. Talks and other external resources are listed on the Resources page in the wiki. Provided predicates The library comes with these predefined predicates: boolean True: constant predicate that is always true False: constant predicate that is always false Not[P]: negation of the predicate P And[A, B]: conjunction of the predicates A and B Or[A, B]: disjunction of the predicates A and B Xor[A, B]: exclusive disjunction of the predicates A and B Nand[A, B]: negated conjunction of the predicates A and B Nor[A, B]: negated disjunction of the predicates A and B AllOf[PS]: conjunction of all predicates in PS AnyOf[PS]: disjunction of all predicates in PS OneOf[PS]: exclusive disjunction of all predicates in PS char Digit: checks if a Char is a digit Letter: checks if a Char is a letter LetterOrDigit: checks if a Char is a letter or digit LowerCase: checks if a Char is a lower case character UpperCase: checks if a Char is an upper case character Whitespace: checks if a Char is white space collection Contains[U]: checks if a Traversable contains a value equal to U Count[PA, PC]: counts the number of elements in a Traversable which satisfy the predicate PA and passes the result to the predicate PC Empty: checks if a Traversable is empty NonEmpty: checks if a Traversable is not empty Forall[P]: checks if the predicate P holds for all elements of a Traversable Exists[P]: checks if the predicate P holds for some elements of a Traversable Head[P]: checks if the predicate P holds for the first element of a Traversable Index[N, P]: checks if the predicate P holds for the element at index N of a sequence Init[P]: checks if the predicate P holds for all but the last element of a Traversable Last[P]: checks if the predicate P holds for the last element of a Traversable Tail[P]: checks if the predicate P holds for all but the first element of a Traversable Size[P]: checks if the size of a Traversable satisfies the predicate P MinSize[N]: checks if the size of a Traversable is greater than or equal to N MaxSize[N]: checks if the size of a Traversable is less than or equal to N generic Equal[U]: checks if a value is equal to U Eval[S]: checks if a value applied to the predicate S yields true ConstructorNames[P]: checks if the constructor names of a sum type satisfy P FieldNames[P]: checks if the field names of a product type satisfy P Subtype[U]: witnesses that the type of a value is a subtype of U Supertype[U]: witnesses that the type of a value is a supertype of U numeric Less[N]: checks if a numeric value is less than N LessEqual[N]: checks if a numeric value is less than or equal to N Greater[N]: checks if a numeric value is greater than N GreaterEqual[N]: checks if a numeric value is greater than or equal to N Positive: checks if a numeric value is greater than zero NonPositive: checks if a numeric value is zero or negative Negative: checks if a numeric value is less than zero NonNegative: checks if a numeric value is zero or positive Interval.Open[L, H]: checks if a numeric value is in the interval (L, H) Interval.OpenClosed[L, H]: checks if a numeric value is in the interval (L, H] Interval.ClosedOpen[L, H]: checks if a numeric value is in the interval [L, H) Interval.Closed[L, H]: checks if a numeric value is in the interval [L, H] string EndsWith[S]: checks if a String ends with the suffix S MatchesRegex[S]: checks if a String matches the regular expression S Regex: checks if a String is a valid regular expression StartsWith[S]: checks if a String starts with the prefix S Uri: checks if a String is a valid URI Url: checks if a String is a valid URL Uuid: checks if a String is a valid UUID Xml: checks if a String is well-formed XML XPath: checks if a String is a valid XPath expression Contributors and participation Alexandre Archambault (@alxarchambault) Dale Wijnand (@dwijnand) Frank S. Thomas (@fst9000) Jean-Rémi Desjardins (@jrdesjardins) John-Michael Reed Shohei Shimomura (@sm0kym0nkey) Vladimir Koshelev (@vlad_koshelev) Your name here :-) refined is a Typelevel project. This means we embrace pure, typeful, functional programming, and provide a safe and friendly environment for teaching, learning, and contributing as described in the Typelevel code of conduct. Projects using refined Please see the wiki for an incomplete list of projects and companies which use refined. If you are using the library and your project isn't listed yet, please add it. Performance concerns Using refined's macros for compile-time refinement has zero runtime overhead for reference types and only causes boxing for value types. Refer to RefineJavapSpec and InferJavapSpec for a detailed analysis of the runtime component of refinement types on the JVM. Related projects bond: Type-level validation for Scala F7: Refinement Types for F# LiquidHaskell: Refinement Types via SMT and Predicate Abstraction refined: Refinement types with static and runtime checking for Haskell. refined was inspired this library and even stole its name! refscript: Refinement Types for TypeScript Subtypes in Perl 6 License refined is licensed under the MIT license, available at http://opensource.org/licenses/MIT and also in the LICENSE file. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/fthomas/refined"	"Simple refinement types with compile- and runtime checking"	"true"
"Extensions"	"Resolvable ★ 30 ⧗ 56"	"https://github.com/resolvable/resolvable"	"A library to optimize fetching immutable data structures from several endpoints in several formats."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"31"	"10"	"1"	"GitHub - resolvable/resolvable: Fetching Scala objects from multiple endpoints Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 10 Star 31 Fork 1 resolvable/resolvable Code Issues 1 Pull requests 0 Pulse Graphs Fetching Scala objects from multiple endpoints http://resolvable.github.io/ 94 commits 1 branch 4 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v2.0.0-M6 v2.0.0-M5 v2.0.0-M4 v2.0.0-M3 Nothing to show New pull request Latest commit 1ac431f Jun 29, 2014 stanch = Republish after a clean Permalink Failed to load latest commit information. project Initial commit Nov 28, 2013 src Rename package, bump version, start refactoring tests Apr 19, 2014 .gitignore Rename package, bump version, start refactoring tests Apr 19, 2014 README.md Rename package, bump version, start refactoring tests Apr 19, 2014 build.sbt = Republish after a clean Jun 29, 2014 README.md Read the guide here: http://resolvable.github.io/ Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/resolvable/resolvable"	"A library to optimize fetching immutable data structures from several endpoints in several formats."	"true"
"Extensions"	"Scala Async ★ 639 ⧗ 4"	"https://github.com/scala/async"	"An asynchronous programming facility for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"677"	"64"	"59"	"GitHub - scala/async: An asynchronous programming facility for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 64 Star 677 Fork 59 scala/async Code Issues 20 Pull requests 2 Pulse Graphs An asynchronous programming facility for Scala 481 commits 10 branches 26 releases Fetching contributors Scala 98.7% Shell 1.3% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.10.x bump/0.9.5-SNAPSHOT bump/0.9.5-SNAPSHOT_2.10 bump/0.9.6-2.10 bump/0.9.6-2.11 master topic/m2 topic/seq-debugging topic/wip-futuresystem-extension tutorial Nothing to show v0.9.6-RC3 v0.9.6-RC2 v0.9.6-RC1_2.11 v0.9.5_2.11 v0.9.5_2.10 v0.9.5-RC1_2.11 v0.9.4_2.11 v0.9.4_2.10 v0.9.3_2.11 v0.9.3_2.10 v0.9.3-RC1_2.11 v0.9.3-RC1_2.10 v0.9.2_2.11 v0.9.2_2.10 v0.9.1_2.11 v0.9.1_2.10 v0.9.0_2.11 v0.9.0_2.10 v0.9.0-M6_2.11 v0.9.0-M6_2.10 v0.9.0-M5_2.11 v0.9.0-M5_2.10 v0.9.0-M4 v0.9.0-M3 v0.9.0-M2 v0.9.0-M1 Nothing to show New pull request Latest commit 4395afe Jan 19, 2016 retronym Merge pull request #151 from retronym/topic/late-expansion-fixes … Late expansion fixes Permalink Failed to load latest commit information. admin Support tag driven publishing Dec 18, 2014 pending/run/fallback0 Minimize the public API Nov 7, 2013 project Update SBT for the warn command, now assumed by sbt-extras Jul 27, 2015 src Various fixes to late expansion Jan 19, 2016 .gitignore Build maintainence Jul 2, 2013 .travis.yml Merge remote-tracking branch 'origin/2.10.x' into merge/2.10.x-to-mas… Jul 27, 2015 CONTRIBUTING.md Update documentation Nov 7, 2013 LICENSE Update copyright years. Jan 14, 2014 README.md Update README.md Jul 6, 2015 build.sbt Bump to 0.9.6-SNAPSHOT Jul 28, 2015 release.sh Fix substituion in release script Mar 10, 2014 sensitive.sbt.enc Support tag driven publishing Dec 18, 2014 README.md scala-async Note: this branch targets Scala 2.11.x, support for Scala 2.10.x has been moved to this branch. Quick start After adding a scala-async to your classpath, write your first async block: import ExecutionContext.Implicits.global import scala.async.Async.{async, await}  val future = async {   val f1 = async { ...; true }   val f2 = async { ...; 42 }   if (await(f1)) await(f2) else 0 } What is async? async marks a block of asynchronous code. Such a block usually contains one or more await calls, which marks a point at which the computation will be suspended until the awaited Future is complete. By default, async blocks operate on scala.concurrent.{Future, Promise}. The system can be adapted to alternative implementations of the Future pattern. Consider the following example: def slowCalcFuture: Future[Int] = ...             // 01 def combined: Future[Int] = async {               // 02   await(slowCalcFuture) + await(slowCalcFuture)   // 03 } val x: Int = Await.result(combined, 10.seconds)   // 05 Lines 1 defines an asynchronous method: it returns a Future. Line 2 begins an async block. During compilation, the contents of this block will be analyzed to identify the await calls, and transformed into non-blocking code. Control flow will immediately pass to line 5, as the computation in the async block is not executed on the caller's thread. Line 3 begins by triggering slowCalcFuture, and then suspending until it has been calculating. Only after it has finished, we trigger it again, and suspend again. Finally, we add the results and complete combined, which in turn will release line 5 (unless it had already timed out). It is important to note that while line 1-4 is non-blocking, it is not parallel. If we wanted to parallelize the two computations, we could rearrange the code as follows. def combined: Future[Int] = async {   val future1 = slowCalcFuture   val future2 = slowCalcFuture   await(future1) + await(future2) } Comparison with direct use of Future API This computation could also be expressed by directly using the higher-order functions of Futures: def slowCalcFuture: Future[Int] = ... val future1 = slowCalcFuture val future2 = slowCalcFuture def combined: Future[Int] = for {   r1 <- future1   r2 <- future2 } yield r1 + r2 The async approach has two advantages over the use of map and flatMap. The code more directly reflects the programmers intent, and does not require us to name the results r1 and r2. This advantage is even more pronounced when we mix control structures in async blocks. async blocks are compiled to a single anonymous class, as opposed to a separate anonymous class for each closure required at each generator (<-) in the for-comprehension. This reduces the size of generated code, and can avoid boxing of intermediate results. Comparison with CPS plugin The existing continuations (CPS) plugin for Scala can also be used to provide a syntactic layer like async. This approach has been used in Akka's Dataflow Concurrency CPS-based rewriting of asynchronous code also produces a closure for each suspension. It can also lead to type errors that are difficult to understand. How it works The async macro analyses the block of code, looking for control structures and locations of await calls. It then breaks the code into 'chunks'. Each chunk contains a linear sequence of statements that concludes with a branching decision, or with the registration of a subsequent state handler as the continuation. Before this analysis and transformation, the program is normalized into a form amenable to this manipulation. This is called the ""A Normal Form"" (ANF), and roughly means that: if and match constructs are only used as statements; they cannot be used as an expression. calls to await are not allowed in compound expressions. Identify vals, vars and defs that are accessed from multiple states. These will be lifted out to fields in the state machine object. Synthesize a class that holds: an integer representing the current state ID the lifted definitions an apply(value: Try[Any]): Unit method that will be called on completion of each future. The behavior of this method is determined by the current state. It records the downcast result of the future in a field, and calls the resume() method. the resume(): Unit method that switches on the current state and runs the users code for one 'chunk', and either: a) registers the state machine as the handler for the next future b) completes the result Promise of the async block, if at the terminal state. an apply(): Unit method that starts the computation. Limitations See the neg test cases for for constructs that are not allowed in a async block See the issue list for which of these restrictions are planned to be dropped in the future. See #13 for why await is not possible in closures, and for suggestions on ways to structure the code to work around this limitation. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala/async"	"An asynchronous programming facility for Scala."	"true"
"Extensions"	"Scala Blitz"	"http://scala-blitz.github.io/"	"A library to speed up Scala collection operations by removing runtime overheads during compilation, and a custom data-parallel operation runtime."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"155"	"21"	"18"	"GitHub - scala-blitz/scala-blitz: Scala framework for efficient sequential and data-parallel collections - Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 21 Star 155 Fork 18 scala-blitz/scala-blitz Code Issues 8 Pull requests 1 Pulse Graphs Scala framework for efficient sequential and data-parallel collections - http://scala-blitz.github.io/ 571 commits 10 branches 11 releases Fetching contributors Scala 96.2% C++ 2.0% C 1.2% Other 0.6% Scala C++ C Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.10 benchmark/scalameter experiment/fixedsize master optimized-block-arrays-support release-config topic/simple-trees-sizeEstimage2 topic/test-pc-blitz topic/typeclasses topic/wip-tbb-comparison Nothing to show tutorial/simple tutorial/impossibility tutorial/forkjoin tutorial/fixedstep scala-blitz_2.11.1_1.2-M1 scala-blitz_2.11.1-1.2 scala-blitz_2.11.0-M7-1.0-M2 scala-blitz_2.11.0-M7-1.0-M1 scala-blitz_2.11.0-1.0-M4 scala-blitz_2.10-1.0-M2 scala-blitz_2.10-1.0-M1 Nothing to show New pull request Latest commit 3085e3e Aug 4, 2014 axel22 Add `sbt-pgp` back. Permalink Failed to load latest commit information. c-comparison project scripts src .gitignore .tags .travis.yml LICENCE README.md Update README.md Apr 22, 2014 build.sbt tasks.TODO README.md ScalaBlitz Documentation at: http://scala-blitz.github.io/home/documentation/ Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-blitz/scala-blitz"	"A library to speed up Scala collection operations by removing runtime overheads during compilation, and a custom data-parallel operation runtime."	"true"
"Extensions"	"Scala Graph"	"http://www.scala-graph.org/"	"A Scala library with basic graph functionality that seamlessly fits into the Scala standard collections library."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"263"	"31"	"35"	"GitHub - scala-graph/scala-graph: Graph for Scala is intended to provide basic graph functionality seamlessly fitting into the Scala Collection Library. Like the well known members of scala.collection, Graph for Scala is an in-memory graph library aiming at editing and traversing graphs, finding cycles etc. in a user-friendly way. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 31 Star 263 Fork 35 scala-graph/scala-graph Code Issues 5 Pull requests 1 Pulse Graphs Graph for Scala is intended to provide basic graph functionality seamlessly fitting into the Scala Collection Library. Like the well known members of scala.collection, Graph for Scala is an in-memory graph library aiming at editing and traversing graphs, finding cycles etc. in a user-friendly way. 160 commits 8 branches 7 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1-Scala-2.10 DiHyperEdge garycoady-fix_github_9 kkasravi-master master scala.js scala_2.9 wcmjunior-master Nothing to show 1.9.2 1.9.1 1.9.0 1.8.1 1.7.3 1.7.0 01.06.01 Nothing to show New pull request Latest commit 3f880a9 Jul 10, 2016 Peter Empen changed maven organization to org.scala-graph Permalink Failed to load latest commit information. constrained/src (internal) Apr 16, 2016 core (internal) Apr 16, 2016 dot/src widened Iterable parameter type to Traversable for Graph constructors… Apr 10, 2016 json/src mod: the node type parameter of edge types is now covariant; fixes #40 Mar 13, 2016 misc mod: the node type parameter of edge types is now covariant; fixes #40 Mar 13, 2016 project changed maven organization to org.scala-graph Jul 10, 2016 .gitignore isDirected, isHyper and isMulti now also consider the graph content a… Apr 3, 2016 LICENSE initial drop from svn Jan 29, 2013 README.md link to scala-graph.org Sep 23, 2013 README.md Graph for Scala This is the source code repository and issue tracker for Graph for Scala. In case you prefer Subversion, go to Graph for Scala on Assembla. Questions or any feedback are appreciated. You are also welcome as a co-contributor. Have fun with Graph for Scala. Peter Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-graph/scala-graph"	"A Scala library with basic graph functionality that seamlessly fits into the Scala standard collections library."	"true"
"Extensions"	"Scalactic"	"http://www.scalactic.org/"	"Small library of utilities related to quality that helps keeping code clear and correct."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Scalactic Home | Quick Start | Install | User Guide | Scaladoc | SuperSafe | About Just released - Scalactic 2.2.6! And check out Scalactic 3.0.0-RC4, which includes Scala.js support and new org.scalactic.anyvals restricted value types! The Scalactic library is focused on constructs related to quality that are useful in both production code and tests. Although ScalaTest is tightly integrated with Scalactic, you can use Scalactic with any Scala project and any test framework. The Scalactic library has no dependencies other than Scala itself. The === and !== operators Scalactic provides a powerful === operator (and its complement, !==) that allows you to Customize equality for a type outside its class Get compiler errors for suspicious equality comparisons Compare numeric values for equality with a tolerance Normalize values before comparing them for equality For example, you can compare strings for equality after being normalized by forcing them to lowercase by customizing Equality explicitly, like this:      import org.scalactic._     import TripleEquals._     import StringNormalizations._     import Explicitly._      (""Hello"" === ""hello"") (after being lowerCased) // true     You can also define implicit Normalization strategies for types and access them by invoking norm methods:      import NormMethods._      implicit val strNormalization = lowerCased      ""WHISPER"".norm                                // ""whisper""     ""WHISPER"".norm === ""whisper""                  // true     The “after being lowerCased” syntax shown previously is provided by Scalactic' Explicitly DSL, which allows you to specify Equality explicitly. You can also define custom Equalitys implicitly:      implicit val strEquality =       decided by defaultEquality[String] afterBeing lowerCased      ""Hello"" === ""hello""                           // true     ""normalized"" === ""NORMALIZED""                 // true     You can compare numeric values for equality with a Tolerance, like this:      import Tolerance._      2.00001 === 2.0 +- 0.01                       // true     Or you could use TolerantNumerics define an implicit Equality[Double] that compares Doubles with a tolerance:      import TolerantNumerics._      implicit val dblEquality = tolerantDoubleEquality(0.01)      2.00001 === 2.0                               // true     A compiler error for an equality comparison that would always yield false looks like:      import TypeCheckedTripleEquals._      Some(""hi"") === ""hi""     error: types Some[String] and String do not adhere to the type constraint     selected for the === and !== operators; the missing implicit parameter is     of type org.scalactic.Constraint[Some[String],String]               Some(""hi"") === ""hi""                          ^     Or and Every Scalactic provides an “Either with attitude” named Or, designed for functional error handling. Or gives you more convenient chaining of map and flatMap calls (and for expressions) than Either and, when, combined with Every, enables you to accumulate errors. Every is an ordered collection of one or more elements. An Or is either Good or Bad. An Every is either One or Many. Here's an example of accumulating errors with Or and Every:  import org.scalactic._ import Accumulation._  case class Person(name: String, age: Int)  def parseName(input: String): String Or One[ErrorMessage] = {   val trimmed = input.trim   if (!trimmed.isEmpty)     Good(trimmed)   else     Bad(One(s""""""""${input}"" is not a valid name"""""")) }  def parseAge(input: String): Int Or One[ErrorMessage] = {   try {     val age = input.trim.toInt     if (age >= 0) Good(age) else Bad(One(s""""""""${age}"" is not a valid age""""""))   }   catch {     case _: NumberFormatException =>       Bad(One(s""""""""${input}"" is not a valid integer""""""))   } }  def parsePerson(inputName: String,       inputAge: String): Person Or Every[ErrorMessage] = {    val name = parseName(inputName)   val age = parseAge(inputAge)   withGood(name, age) { Person(_, _) } }  Here are some examples of parsePerson in action:  parsePerson(""Bridget Jones"", ""29"") // Result: Good(Person(Bridget Jones,29))  parsePerson(""Bridget Jones"", """") // Result: Bad(One("""" is not a valid integer))  parsePerson(""Bridget Jones"", ""-29"") // Result: Bad(One(""-29"" is not a valid age))  parsePerson("""", """") // Result: Bad(Many("""" is not a valid name, """" is not a valid integer))  Or offers several other ways to accumulate errors besides the withGood methods shown in the example above. See the documentation for Or for more information. Requirements and Snapshots Scalactic includes a Requirements trait that offers require, requireState, and requireNonNull methods for checking pre-conditions that give descriptive error messages extracted via a macro. Here are some examples:  val a = -1  require(a >= 0) // throws IllegalArgumentException: -1 was not greater than or equal to 0  requireState(a >= 0) // throws IllegalStateException: -1 was not greater than or equal to 0  val b: String = null  requireNonNull(a, b) // throws NullPointerException: b was null  Trait Snapshots offers a snap method that can help you make debug and log messages that include information about the values of variables:  val a = 1 val b = '2' val c = ""3""  snap(a, b, c) // Result: a was 1, b was '2', c was ""3""  And more (but not much more)... Scalactic also includes a TimesOnInt trait that allows you to perform side-effecting loops a specified number of times, like this:  import TimesOnInt._  3 times println(""hello "") // Output: hello hello hello  You can also define an alternate String forms for types using Prettifiers and create extractors for Throwables via the Catcher factory. And that's it: Scalactic is a small, very focused library. Why not give it a try? Just visit the Quick Start page. Scalactic is brought to you by Bill Venners, with contributions from several other folks. It is sponsored by Artima, Inc. ScalaTest is free, open-source software released under the Apache 2.0 license. Copyright © 2009-2016 Artima, Inc. All Rights Reserved."	"null"	"null"	"Small library of utilities related to quality that helps keeping code clear and correct."	"true"
"Extensions"	"Shapeless ★ 1492 ⧗ 0"	"https://github.com/milessabin/shapeless"	"A type class and dependent type based generic programming library for Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1634"	"120"	"261"	"GitHub - milessabin/shapeless: Generic programming for Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 120 Star 1,634 Fork 261 milessabin/shapeless Code Issues 53 Pull requests 5 Wiki Pulse Graphs Generic programming for Scala 1,676 commits 21 branches 31 releases 76 contributors Scala 99.9% Shell 0.1% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages jdk6-canary master migration-2.0.0-2.1.0 scala-2.9.x scala-2.10.x scala-2.11.x-defunct scalajs-2.10.x scalajs-2.11.x shapeless-1.2.3-scala-2.9.x shapeless-1.2.3 shapeless-1.2.4-scala-2.9.x shapeless-1.2.4-scala-2.11.x shapeless-1.2.4 shapeless-2.0.0-scala-2.10.x shapeless-2.0.0-scala-2.11.x shapeless-2.2.5-sjs-0.6.5 shapeless-2.3.0-sjs-0.6.8 shapeless-2.3.1-scala-2.12.0-M5 wip/fix-lint wip/smc Nothing to show shapeless-2.3.1 shapeless-2.3.1-RC1 shapeless-2.3.0 shapeless-2.3.0-RC4 shapeless-2.3.0-RC3 shapeless-2.3.0-RC2 shapeless-2.3.0-RC1 shapeless-2.2.5 shapeless-2.2.4 shapeless-2.2.3 shapeless-2.2.2 shapeless-2.2.1 shapeless-2.2.0 shapeless-2.2.0-RC6 shapeless-2.2.0-RC5 shapeless-2.2.0-RC4 shapeless-2.2.0-RC3 shapeless-2.2.0-RC2 shapeless-2.2.0-RC1 shapeless-2.1.0 shapeless-2.1.0-RC2 shapeless-2.1.0-RC1 shapeless-2.0.0 shapeless-2.0.0-M1 shapeless-1.2.3 shapeless-1.2.2 shapeless-1.2.1 shapeless-1.2.0 shapeless-1.1.0 shapeless-1.0.1 shapeless-1.0 Nothing to show New pull request Latest commit 2a31be4 Jul 4, 2016 milessabin No previous artefact for MiMa for 2.12.x. Permalink Failed to load latest commit information. core Add tests for singleton typeables after deserialization Jul 3, 2016 examples/src Add license/packages Jun 24, 2016 notes Fixed typo. May 13, 2016 project Updates for Scala 2.12.0-M5. Jul 4, 2016 scripts Updated README; added try-shapeless.sh script. May 30, 2016 .gitignore Updated Travis config. Mar 21, 2016 .jvmopts Revert ""Bumped heap to 3G."" Feb 15, 2016 .travis.yml Updates for Scala 2.12.0-M5. Jul 4, 2016 CONTRIBUTORS RecordArg, inverse of RecordArgs trait May 28, 2016 LICENSE Renamed LICENSE.txt -> LICENSE Apr 14, 2012 README.md Updated README; added try-shapeless.sh script. May 30, 2016 build.sbt No previous artefact for MiMa for 2.12.x. Jul 4, 2016 version.sbt Setting version to 2.3.2-SNAPSHOT May 13, 2016 README.md shapeless: generic programming for Scala shapeless is a type class and dependent type based generic programming library for Scala. It had its origins in several talks by Miles Sabin (@milessabin), given over the course of 2011, on implementing scrap your boilerplate and higher rank polymorphism in Scala. Since then it has evolved from being a resolutely experimental project into library which, while still testing the limits of what's possible in Scala, is being used widely in production systems wherever there are arities to be abstracted over and boilerplate to be scrapped. Projects which use shapeless There is a wide variety of projects which use shapeless in one way or another ... see the incomplete list of projects for ideas and inspiration. If you are using shapeless and your project isn't listed yet, please add it. Finding out more about the project The feature overview for shapeless-2.0.0 provides a very incomplete introduction to shapeless. Additional information can be found in subsequent release notes. If you are upgrading from shapeless-2.0.0 you will find the migration guide useful. We're not satisfied with the current state of the documentation and would love help in improving it. shapeless is part of the Typelevel family of projects. It is an Open Source project under the Apache License v2, hosted on github. Binary artefacts are published to the Sonatype OSS Repository Hosting service and synced to Maven Central. Most discussion of shapeless and generic programming in Scala happens on the shapeless Gitter channel. There is also a mailing list and IRC channel, but these are largely dormant now that most activity has moved to Gitter. Questions about shapeless are often asked and answered under the shapeless tag on StackOverflow. Some articles on the implementation techniques can be found on Miles's blog, and Olivera, Moors and Odersky, Type Classes as Object and Implicits is useful background material. Participation The shapeless project supports the Typelevel code of conduct and wants all of its channels (mailing list, Gitter, IRC, github, etc.) to be welcoming environments for everyone. Whilst shapeless is a somewhat ""advanced"" Scala library, it is a lot more approachable than many people think. Contributors are usually available to field questions, give advice and discuss ideas on the Gitter channel, and for people wanting to take their first steps at contributing we have a selection of open issues flagged up as being good candidates to take on. No contribution is too small, and guidance is always available. Using shapeless Binary release artefacts are published to the Sonatype OSS Repository Hosting service and synced to Maven Central. Snapshots of the master branch are built using Travis CI and automatically published to the Sonatype OSS Snapshot repository. Try shapeless with an Ammonite instant REPL The quickest way to get to a REPL prompt with the latest version of shapeless on the class path is to run the provided ""try shapeless"" script. This downloads and installs coursier and uses it to fetch the Ammonite REPL and the latest version of shapeless and drops you immediately into a REPL session to play around with, miles@frege:shapeless (master)$ scripts/try-shapeless.sh Loading... Welcome to the Ammonite Repl 0.5.8 (Scala 2.11.8 Java 1.8.0_51) @ 1 :: ""foo"" :: true :: HNil res0: Int :: String :: Boolean :: HNil = 1 :: foo :: true :: HNil @  shapeless-2.3.1 with SBT To include the Sonatype repositories in your SBT build you should add, resolvers ++= Seq(   Resolver.sonatypeRepo(""releases""),   Resolver.sonatypeRepo(""snapshots"") ) Builds are available for Scala 2.10.x, 2.11.x and for 2.12.0-M4. The main line of development for shapeless 2.3.1 is Scala 2.11.8 with Scala 2.10.x supported via the macro paradise compiler plugin. scalaVersion := ""2.11.8""  libraryDependencies ++= Seq(   ""com.chuusai"" %% ""shapeless"" % ""2.3.1"" ) If you are using Scala 2.10.x, you should also add the macro paradise plugin to your build, scalaVersion := ""2.10.6""  libraryDependencies ++= Seq(   ""com.chuusai"" %% ""shapeless"" % ""2.3.1"",   compilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.1.0"" cross CrossVersion.full) ) shapeless-2.3.1 with Maven shapeless is also available for projects using the Maven build tool via the following dependency, <dependency>   <groupId>com.chuusai</groupId>   <artifactId>shapeless_2.11</artifactId>   <version>2.3.1</version> </dependency> If you are using Scala 2.10.x, you should also add the macro paradise plugin to your build, <dependency>   <groupId>com.chuusai</groupId>   <artifactId>shapeless_2.10</artifactId>   <version>2.2.5</version> </dependency>  <plugins>   ...   <plugin>     ...     <configuration>       ...       <compilerPlugins>         <compilerPlugin>           <groupId>org.scala-lang.plugins</groupId>           <artifactId>macro-paradise_2.10</artifactId>           <version>2.1.0</version>         </compilerPlugin>       </compilerPlugins>       ...     </configuration>     ...   </plugin>   ... </plugins> Avoid SBT 0.13.6 Please be aware that SBT 0.13.6 has an issue related to its name hashing feature which when compiling with shapeless might cause SBT to loop indefinitely consuming all heap. If possible move to a more recent version of SBT. If you must use SBT 0.13.6 a workaround is to disable name hashing by adding, incOptions := incOptions.value.withNameHashing(false) to your settings. Older releases Please use a current release if possible. If unavoidable, you can find usage information for older releases on the shapeless wiki. Building shapeless shapeless is built with SBT 0.13.11 or later, and its master branch is built with Scala 2.11.8 by default but also cross-builds for 2.10.6 and 2.12.x. Contributors Alessandro Lacava alessandrolacava@gmail.com @lambdista Alexander Konovalov alex.knvl@gmail.com @alexknvl Alexandre Archambault alexandre.archambault@gmail.com @alxarchambault Alistair Johnson alistair.johnson@johnsonusm.com @AlistairUSM Alois Cochard alois.cochard@gmail.com @aloiscochard Andreas Koestler andreas.koestler@gmail.com @AndreasKostler Andrew Brett github@bretts.org @Ephemerix Arya Irani arya.irani@gmail.com @aryairani Ben Hutchison brhutchison@gmail.com @ben_hutchison Ben James ben.james@guardian.co.uk @bmjames Brian McKenna brian@brianmckenna.org @puffnfresh Brian Zeligson brian.zeligson@gmail.com @beezee Bryn Keller xoltar@xoltar.org @brynkeller Chris Hodapp clhodapp1@gmail.com @clhodapp Cody Allen ceedubs@gmail.com @fourierstrick Dale Wijnand dale.wijnand@gmail.com @dwijnand Daniel Urban urban.dani@gmail.com Dario Rexin dario.rexin@r3-tech.de @evonox Dave Gurnell d.j.gurnell@gmail.com @davegurnell David Barri japgolly@gmail.com @japgolly Denis Mikhaylov notxcain@gmail.com @notxcain Eugene Burmako xeno.by@gmail.com @xeno_by Filipe Nepomuceno filinep@gmail.com Frank S. Thomas frank@timepit.eu @fst9000 George Leontiev folone@gmail.com @folone Hamish Dickenson hamish.dickson@gmail.com @hamishdickson Howard Branch purestgreen@gmail.com @purestgreen Huw Giddens hgiddens@gmail.com Ievgen Garkusha ievgen@riskident.com Jason Zaugg jzaugg@gmail.com @retronym Jean-Remi Desjardins jeanremi.desjardins@gmail.com @jrdesjardins Jeff Wilde jeff@robo.ai Jisoo Park xxxyel@gmail.com @guersam Johannes Rudolph johannes.rudolph@gmail.com @virtualvoid Johnny Everson khronnuz@gmail.com @johnny_everson Joni Freeman joni.freeman@ri.fi @jonifreeman Joseph Price josephprice@iheartmedia.com Julien Tournay boudhevil@gmail.com @skaalf Jules Gosnell jules_gosnell@yahoo.com Kailuo Wang kailuo.wang@gmail.com @kailuowang Kenji Yoshida 6b656e6a69@gmail.com @xuwei_k Kevin Wright kev.lee.wright@gmail.com @thecoda Lars Hupel lars.hupel@mytum.de @larsr_h Mario Pastorelli mario.pastorelli@teralytics.ch @mapastr Matthew Taylor matthew.t@tbfe.net Mathias Doenitz mathias@spray.io @sirthias Michael Donaghy md401@srcf.ucam.org Michael Pilquist mpilquist@gmail.com @mpilquist Miles Sabin miles@milessabin.com @milessabin Neville Li neville@spotify.com @sinisa_lyh Nikolas Evangelopoulos nikolas@jkl.gr Oleg Aleshko olegych@tut.by @OlegYch Olivier Blanvillain olivier.blanvillain@gmail.com Olli Helenius liff@iki.fi @ollijh Owein Reese owreese@gmail.com @OweinReese Paolo G. Giarrusso p.giarrusso@gmail.com @blaisorblade Pascal Voitot pascal.voitot.dev@gmail.com @mandubian Pavel Chlupacek pavel.chlupacek@spinoco.com @pacmanius Peter Neyens peter.neyens@gmail.com @pneyens Peter Schmitz petrischmitz@gmail.com @peterschmitz_ Renato Cavalcanti renato@strongtyped.io @renatocaval Rob Norris rob_norris@mac.com @tpolecat Robert Hensing spam@roberthensing.nl Ryo Hongo ryoppy0516@gmail.com @ryoppy516 Sam Halliday sam.halliday@gmail.com @fommil Sarah Gerweck sarah.a180@gmail.com @SGerweck Sébastien Doeraene sjrdoeraene@gmail.com @sjrdoeraene Simon Hafner hafnersimon@gmail.com @reactormonk Stacy Curl stacy.curl@gmail.com @stacycurl Stephen Compall scompall@nocandysw.com @S11001001 Tin Pavlinic tin.pavlinic@gmail.com @triggerNZ Tom Switzer thomas.switzer@gmail.com @tixxit Tomas Mikula tomas.mikula@gmail.com @tomas_mikula Travis Brown travisrobertbrown@gmail.com @travisbrown Valentin Kasas valentin.kasas@gmail.com @ValentinKasas Valerian Barbot valerian.barbot@onzo.com @etaty Vladimir Matveev vladimir.matweev@gmail.com @netvlm Vladimir Pavkin vpavkin@gmail.com @vlpavkin Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/milessabin/shapeless"	"A type class and dependent type based generic programming library for Scala."	"true"
"Extensions"	"Stateless Future ★ 130 ⧗ 5"	"https://github.com/qifun/stateless-future"	"Asynchronous programming in fully featured Scala syntax."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"141"	"25"	"16"	"GitHub - qifun/stateless-future: Asynchronous programming in fully featured Scala syntax. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 25 Star 141 Fork 16 qifun/stateless-future Code Issues 2 Pull requests 1 Pulse Graphs Asynchronous programming in fully featured Scala syntax. 179 commits 2 branches 10 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 0.4.0 0.3.2 0.3.1 0.3.0 0.2.3 0.2.2 0.2.1 0.2.0 0.1.1 0.1 Nothing to show New pull request Latest commit d2f36d7 Oct 26, 2015 chank Merge pull request #13 from jvliwanag/patch-1 … Fix typo Permalink Failed to load latest commit information. project Upgrade sbt Nov 28, 2014 src/main/scala/com/qifun/statelessFuture Enable expression like `val (p1, p2, p3) = future.await` Jun 5, 2015 .gitignore Update .gitignore Apr 16, 2014 .travis.yml Enable travis Sep 11, 2014 LICENSE Initial commit Apr 12, 2014 NOTICE Rename project to stateless-future. Apr 18, 2014 README.md Fix typo Oct 26, 2015 build.sbt Bump version Jun 13, 2015 README.md Stateless Future Stateless Future is a set of Domain-specific language for asynchronous programming, in the pure functional flavor. Stateless Futures provide similar API to scala.concurrent.Future and scala.async, except Stateless Futures are simpler, cleaner, and more powerful than scala.concurrent.Future and scala.async. There was a continuation plugin for Scala. The continuation plugin also provided a DSL to define control flows like stateless-future or scala.async. I created the following table to compare the three DSL: stateless-future scala.concurrent.Future and scala.async scala.util.continuations Stateless Yes No Yes Threading-free Yes No Yes Exception handling in ""A-Normal Form"" Yes No No Tail call optimization in ""A-Normal Form"" Yes No No Pattern matching in ""A-Normal Form"" Yes Yes Yes, but buggy Lazy val in ""A-Normal Form"" No, because of some underlying scala.reflect bugs Only for those contain no await Yes, but buggy Usage Create a Stateless Future import com.qifun.statelessFuture.Future val randomDoubleFuture: Future.Stateless[Double] = Future {   println(""Generating a random Double..."")   math.random() }  A Stateless Future instance is lazy, only evaluated when you query it. Thus there is nothing printed when you create the Stateless Future. Read from a Stateless Future println(""I am going to read a random Double."") for (randomDouble <- randomDoubleFuture) {   println(s""Recevied $randomDouble."") }  Output: I am going to read a random Double. Generating a random Double... Recevied 0.19722960355012198.  Another Stateless Future that invokes the former Stateless Future twice. val anotherFuture = Future {   println(""I am going to read the first random Double."")   val randomDouble1 = randomDoubleFuture.await   println(s""The first random Double is $randomDouble1."")    println(""I am going to read the second random Double."")   val randomDouble2 = randomDoubleFuture.await   println(s""The second random Double is $randomDouble2."") }  println(""Before running the Future."") for (unit <- anotherFuture) {   println(""After running the Future."") }  Output: Before running the Future. I am going to read the first random Double. Generating a random Double... The first random Double is 0.10768210465170625. I am going to read the second random Double. Generating a random Double... The second random Double is 0.6134780449033244. After running the Future.  Note the magic await postfix, which invokes the the former Stateless Future randomDoubleFuture. It looks like a normal Scala method calls, but does not block any thread. A complex example with control structures import scala.concurrent.duration._ import scala.util.control.Exception.Catcher import com.qifun.statelessFuture.Future  val executor = java.util.concurrent.Executors.newSingleThreadScheduledExecutor  // Manually implements a Stateless Future, which is the asynchronous version of `Thread.sleep()` def asyncSleep(duration: Duration) = new Future.Stateless[Unit] {   import scala.util.control.TailCalls._   def onComplete(handler: Unit => TailRec[Unit])(implicit catcher: Catcher[TailRec[Unit]]) = {     executor.schedule(new Runnable {       def run() {         handler().result       }     }, duration.length, duration.unit)     done()   } }  // Without the keyword `new`, you have the magic version of `Future` constructor, // which enables the magic postfix `await`. val sleep10seconds = Future {   var i = 0   while (i < 10) {     println(s""I have slept $i times."")     // The magic postfix `await` invokes the asynchronous method `asyncSleep`.     // It looks like normal `Thread.sleep()`, but does not block any thread.     asyncSleep(1.seconds).await     i += 1   }   i }  // When `sleep10seconds` is running, it could report failures to this catcher implicit def catcher: Catcher[Unit] = {   case e: Exception => {     println(""An exception occured when I was sleeping: "" + e.getMessage)   } }  // A Stateless Future instance is lazy, only evaluating when you query it. println(""Before the evaluation of the Stateless Future `sleep10seconds`."") for (total <- sleep10seconds) {   println(""After the evaluation of the Stateless Future `sleep10seconds`."")   println(s""I slept $total times in total."")   executor.shutdown() }  Run it and you will see the output: Before evaluation of the Stateless Future `sleep10seconds`. I have slept 0 times. I have slept 1 times. I have slept 2 times. I have slept 3 times. I have slept 4 times. I have slept 5 times. I have slept 6 times. I have slept 7 times. I have slept 8 times. I have slept 9 times. After evaluation of the Stateless Future `sleep10seconds`. I slept 10 times in total.  Further Information There are two sorts of API to use a Stateless Future, the for-comprehensions style API and ""A-Normal Form"" style API. For-Comprehensions The for-comprehensions style API for stateless-future is like the for-comprehensions for scala.concurrent.Future. for (total <- sleep10seconds) {   println(""After evaluation of the Stateless Future `sleep10seconds`"")   println(s""I slept $total times in total."")   executor.shutdown() }  A notable difference between the two for-comprehensions implementations is the required implicit parameter. A scala.concurrent.Future requires an ExecutionContext, while a Stateless Future requires a Catcher. import scala.util.control.Exception.Catcher implicit def catcher: Catcher[Unit] = {   case e: Exception => {     println(""An exception occured when I was sleeping: "" + e.getMessage)   } }  A-Normal Form ""A-Normal Form"" style API for Stateless Futures is like the pending proposal scala.async, except Stateless Futures require less limitations than scala.async. val sleep10seconds = Future {   var i = 0   while (i < 10) {     println(s""I have slept $i times"")     // The magic postfix `await` invokes asynchronous method like normal `Thread.sleep()`,     // and does not block any thread.     asyncSleep(1.seconds).await     i += 1   }   i }  The Future function for Stateless Futures corresponds to async method in Async, and the await postfix to Stateless Futures corresponds to await method in Async. Design Regardless of the familiar veneers between Stateless Futures and scala.concurrent.Future, I have made some different designed choices on Stateless Futures. Statelessness The Stateless Futures are pure functional, thus they will never store result values or exceptions. Instead, Stateless Futures evaluate lazily, and they do the same job every time you invoke foreach or onComplete. The behavior of Stateless Futures is more like monads in Haskell than futures in Java. Also, there is no isComplete method in Stateless Futures. As a result, the users of Stateless Futures are forced not to share futures between threads, not to check the states in futures. They have to care about control flows instead of threads, and build the control flows by defining Stateless Futures. Threading-free Model There are too many threading models and implimentations in the Java/Scala world, java.util.concurrent.Executor, scala.concurrent.ExecutionContext, javax.swing.SwingUtilities.invokeLater, java.util.Timer, ... It is very hard to communicate between threading models. When a developer is working with multiple threading models, he must very carefully pass messages between threading models, or he have to maintain bulks of synchronized methods to properly deal with the shared variables between threads. Why does he need multiple threading models? Because the libraries that he uses depend on different threading modes. For example, you must update Swing components in the Swing's UI thread, you must specify java.util.concurrent.ExecutionServices for java.nio.channels.CompletionHandler, and, you must specify scala.concurrent.ExecutionContexts for scala.concurrent.Future and scala.async.Async. Oops! Think about somebody who uses Swing to develop a text editor software. He wants to create a state machine to update UI. He have heard the cool scala.async, then he uses the cool ""A-Normal Form"" expression in async to build the state machine that updates UI, and he types import scala.concurrent.ExecutionContext.Implicits._ to suppress the compiler errors. Everything looks pretty, except the software always crashes. Fortunately, stateless-future depends on none of these threading model, and cooperates with all of these threading models. If the poor guy tries Stateless Future, replacing async { } to stateless-future's Future { }, deleting the import scala.concurrent.ExecutionContext.Implicits._, he will find that everything looks pretty like before, and does not crash any more. That's why threading-free model is important. Exception Handling There were two Future implementations in Scala standard library, scala.actors.Future and scala.concurrent.Future. scala.actors.Futures are not designed to handling exceptions, since exceptions are always handled by actors. There is no way to handle a particular exception in a particular subrange of an actor. Unlike scala.actors.Futures, scala.concurrent.Futures are designed to handle exceptions. But, unfortunately, scala.concurrent.Futures provide too many mechanisms to handle an exception. For example: import scala.concurrent.Await import scala.concurrent.ExecutionContext import scala.concurrent.duration.Duration import scala.util.control.Exception.Catcher import scala.concurrent.forkjoin.ForkJoinPool val threadPool = new ForkJoinPool() val catcher1: Catcher[Unit] = { case e: Exception => println(""catcher1"") } val catcher2: Catcher[Unit] = {   case e: java.io.IOException => println(""catcher2"")   case other: Exception => throw new RuntimeException(other) } val catcher3: Catcher[Unit] = {   case e: java.io.IOException => println(""catcher3"")   case other: Exception => throw new RuntimeException(other) } val catcher4: Catcher[Unit] = { case e: Exception => println(""catcher4"") } val catcher5: Catcher[Unit] = { case e: Exception => println(""catcher5"") } val catcher6: Catcher[Unit] = { case e: Exception => println(""catcher6"") } val catcher7: Catcher[Unit] = { case e: Exception => println(""catcher7"") } def future1 = scala.concurrent.future { 1 }(ExecutionContext.fromExecutor(threadPool, catcher1)) def future2 = scala.concurrent.Future.failed(new Exception) val composedFuture = future1.flatMap { _ => future2 }(ExecutionContext.fromExecutor(threadPool, catcher2)) composedFuture.onFailure(catcher3)(ExecutionContext.fromExecutor(threadPool, catcher4)) composedFuture.onFailure(catcher5)(ExecutionContext.fromExecutor(threadPool, catcher6)) try { Await.result(composedFuture, Duration.Inf) } catch { case e if catcher7.isDefinedAt(e) => catcher7(e) }  Is any sane developer able to tell which catchers will receive the exceptions? There are too many concepts about exceptions when you work with scala.concurrent.Future. You have to remember the different exception handling strategies between flatMap, recover, recoverWith and onFailure, and the difference between scala.concurrent.Future.failed(new Exception) and scala.concurrent.future { throw new Exception }. scala.async does not make things better, because scala.async will produce a compiler error for every await in a try statement. Fortunately, you can get rid of all those concepts if you switch to stateless-future. There is neither catcher implicit parameter in flatMap or map in Stateless Futures, nor onFailure nor recover method at all. You just simply try, and things get done. See the examples to learn that. Tail Call Optimization Tail call optimization is an important feature for pure functional programming. Without tail call optimization, many recursive algorithm will fail at run-time, and you will get the well-known StackOverflowError. The Scala language provides scala.annotation.tailrec to automatically optimize simple tail recursions, and scala.util.control.TailCalls to manually optimize complex tail calls. stateless-future project is internally based on scala.util.control.TailCalls, and automatically performs tail call optimization in the magic Future blocks, without any additional special syntax. See this example. The example creates 500,000,000 stack levels recursively. And it just works, without any StackOverflowError or OutOfMemoryError. Note that if you port this example for scala.async it will throw an OutOfMemoryError or a TimeoutException. Installation Put these lines in your build.sbt if you use Sbt: libraryDependencies += ""com.qifun"" %% ""stateless-future"" % ""0.3.2""  stateless-future should work with Scala 2.10.3, 2.10.4, or 2.11.x. Known issues lazy vals in magic Future block are not supported, due to scala issue 8499. unapplySeqs in magic Future block are not supported, due to scala issue 8825. In some rare cases, if you create multiple val with same name in one Future block, the last val may be referred unexpectly. Some complex existential types may cause compiler errors, , due to scala issue 8500. Clone stateless-future-test and run the test cases to check these limitations. Links The API documentation Utilities Akka integration Tests and examples 明日歌 明日复明日， 明日何其多。 我生待明日， 万事成蹉跎。 文嘉／錢鶴灘 Future Song The Future flatMaps a Future. The Future tailcalls forever. My life to await the Future. It comes OutOfMemoryError. Wen Jia / Qian Hetan Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/qifun/stateless-future"	"Asynchronous programming in fully featured Scala syntax."	"true"
"Extensions"	"Twitter Util ★ 1484 ⧗ 0"	"https://github.com/twitter/util"	"General-purpose Scala libraries, including a future implementation and other concurrency tools."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1583"	"256"	"322"	"GitHub - twitter/util: Wonderful reusable code from Twitter Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 256 Star 1,583 Fork 322 twitter/util Code Issues 3 Pull requests 1 Pulse Graphs Wonderful reusable code from Twitter http://twitter.github.com/util 1,915 commits 11 branches 24 releases 108 contributors Scala 92.0% Java 6.5% Python 1.2% Mako 0.2% Shell 0.1% Makefile 0.0% Scala Java Python Mako Shell Makefile Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags 2.9.2 authfile develop eval2 exceptional_function_0 flags-patch gh-pages logging_queuing_handler master test_logging thrift_codec Nothing to show version-1.8.1 version-1.7.3 version-1.7.0 version-1.6.6 version-1.6.5 version-1.5.1 version-1.4.11 version-1.3.0 util-6.35.0 util-6.34.0 util-6.33.0 util-6.32.0 util-6.31.0 util-6.30.0 util-6.29.0 util-6.28.0 util-6.27.0 util-6.26.0 util-6.25.0 util-6.24.0 util-6.23.0 util-6.22.2 org=com.twitter,name=util,version=1.8.11 6.12.1 Nothing to show New pull request Latest commit 1b05cf1 Jul 11, 2016 johnynek committed with jenkins Problem … Scala added a Try that is basically isomorphic to Twitter's. Code using Twitter Try and scala.util.Try will almost certainly implement methods to convert between these types.  Solution  Add methods to convert to and from these two Trys.  Result  This will add a few methods which enable these conversions.  Signed-off-by: Ruben Oanta <roanta@twitter.com>  RB_ID=850261 Permalink Failed to load latest commit information. .github Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 codegen [util-test] Easy argument capture for Mockito mocks. Feb 23, 2015 project csl: Release CSL libraries Jul 7, 2016 util-app util-app: Fix bug parsing Java Float/Double flags Jul 6, 2016 util-benchmark util-core: Fewer allocations for AsyncSemaphore Jun 6, 2016 util-cache util-cache: Adds support for Caffeine caches May 23, 2016 util-class-preloader maven layout goes away Nov 30, 2015 util-codec util-codec: Stop depending on apache commons codec Jun 6, 2016 util-collection util-collections: rm apache collections!!! May 18, 2016 util-core Problem Jul 11, 2016 util-eval Eval: add Serializable to wrapping class Jun 27, 2016 util-events Enable warnings for unused imports for scala Mar 10, 2016 util-function csl: disable fatal warnings in java targets Jan 11, 2016 util-hashing util-codec: Stop depending on apache commons codec Jun 6, 2016 util-jvm twitter-server: expose applicationTime and tenuringThreshold metrics … Jul 8, 2016 util-lint twitter-server: Exclude Sharded Duplicate Clients and Make Rules Test… Jun 9, 2016 util-logging Cross build for Scala 2.12 Jun 6, 2016 util-reflect Problem Dec 21, 2015 util-registry twitter-server: Add filtering to /admin/registry.json Jun 20, 2016 util-security util-security: Add X509CertificateLoader and Introduce util-security Jun 20, 2016 util-stats util-stats: Fix typo in docs Jun 20, 2016 util-test source: fix more unused imports for scala 2.11.8 Mar 29, 2016 util-thrift Problem Dec 21, 2015 util-zk-common util-zk-common: Remove ServerSet Apr 18, 2016 util-zk-test Updating code ownership Jun 20, 2016 util-zk util-zk: Fix race condition in ZkAsyncSemaphore May 30, 2016 .gitignore add sbt-launch.jar to gitignores May 11, 2012 .travis.yml util, ostrich, scrooge, finagle, twitter-server: Update to use codeco… Jun 8, 2016 CHANGES csl: Release CSL libraries Jul 7, 2016 CONFIG.ini [split] s/GUILD/CSL/ for all CONFIG.ini's Jun 7, 2014 CONTRIBUTING.md Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 GROUPS [split] Use new git-review with simplified OWNERS/GROUPS May 18, 2012 LICENSE [split] Fixes race in channel.send(); fixes the meaning of filter(); … Apr 6, 2011 OWNERS Fix some lint errors in CONFIG.ini and OWNERS - missing newlines - tr… Feb 29, 2016 README.md csl: Release CSL libraries Jul 7, 2016 sbt Switch to Java 8 and Scala 2.11 May 9, 2016 updatedocs.bash util - Update scaladoc script Mar 7, 2016 README.md Twitter Util A bunch of idiomatic, small, general purpose tools. See the Scaladoc here. Status This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained. Please note that some sub-projects (including util-eval), classes, and methods may be deprecated, however. Using in your project An example SBT dependency string for the util-collection tools would look like this: val collUtils = ""com.twitter"" %% ""util-collection"" % ""6.35.0"" Units Time import com.twitter.conversions.time._  val duration1 = 1.second val duration2 = 2.minutes duration1.inMillis // => 1000L Space import com.twitter.conversions.storage._ val amount = 8.megabytes amount.inBytes // => 8388608L amount.inKilobytes // => 8192L Futures A Non-actor re-implementation of Scala Futures. import com.twitter.conversions.time._ import com.twitter.util.{Await, Future, Promise}  val f = new Promise[Int] val g = f.map { result => result + 1 } f.setValue(1) Await.result(g, 1.second) // => this blocks for the futures result (and eventually returns 2)  // Another option: g.onSuccess { result =>   println(result) // => prints ""2"" }  // Using for expressions: val xFuture = Future(1) val yFuture = Future(2)  for {    x <- xFuture   y <- yFuture } {   println(x + y) // => prints ""3"" } Collections LruMap The LruMap is an LRU with a maximum size passed in. If the map is full it expires items in FIFO order. Reading a value will move an item to the top of the stack. import com.twitter.util.LruMap  val map = new LruMap[String, String](15) // this is of type mutable.Map[String, String] Object Pool The pool order is FIFO. A pool of constants import scala.collection.mutable import com.twitter.util.{Await, SimplePool}  val queue = new mutable.Queue[Int] ++ List(1, 2, 3) val pool = new SimplePool(queue)  // Note that the pool returns Futures, it doesn't block on exhaustion. assert(Await.result(pool.reserve()) == 1) pool.reserve().onSuccess { item =>   println(item) // prints ""2"" } A pool of dynamically created objects Here is a pool of even-number generators. It stores 4 numbers at a time: import com.twitter.util.{Future, FactoryPool}  val pool = new FactoryPool[Int](4) {   var count = 0   def makeItem() = { count += 1; Future(count) }   def isHealthy(i: Int) = i % 2 == 0 } It checks the health when you successfully reserve an object (i.e., when the Future yields). Hashing util-hashing is a collection of hash functions and hashing distributors (eg. ketama). To use one of the available hash functions: import com.twitter.hashing.KeyHasher  KeyHasher.FNV1_32.hashKey(""string"".getBytes) Available hash functions are: FNV1_32 FNV1A_32 FNV1_64 FNV1A_64 KETAMA CRC32_ITU HSIEH  To use KetamaDistributor: import com.twitter.hashing.{KetamaDistributor, KetamaNode, KeyHasher}  val nodes = List(KetamaNode(""host:port"", 1 /* weight */, ""foo"" /* handle */)) val distributor = new KetamaDistributor(nodes, 1 /* num reps */) distributor.nodeForHash(""abc"".##) // => client Logging util-logging is a small wrapper around Java's built-in logging to make it more Scala-friendly. Using To access logging, you can usually just use: import com.twitter.logging.Logger private val log = Logger.get(getClass) This creates a Logger object that uses the current class or object's package name as the logging node, so class ""com.example.foo.Lamp"" will log to node com.example.foo (generally showing ""foo"" as the name in the logfile). You can also get a logger explicitly by name: private val log = Logger.get(""com.example.foo"") Logger objects wrap everything useful from java.util.logging.Logger, as well as adding some convenience methods: // log a string with sprintf conversion: log.info(""Starting compaction on zone %d..."", zoneId)  try {   ... } catch {   // log an exception backtrace with the message:   case e: IOException =>     log.error(e, ""I/O exception: %s"", e.getMessage) } Each of the log levels (from ""fatal"" to ""trace"") has these two convenience methods. You may also use log directly: import com.twitter.logging.Level log(Level.DEBUG, ""Logging %s at debug level."", name) An advantage to using sprintf (""%s"", etc) conversion, as opposed to: log(Level.DEBUG, s""Logging $name at debug level."") is that Java & Scala perform string concatenation at runtime, even if nothing will be logged because the log file isn't writing debug messages right now. With sprintf parameters, the arguments are just bundled up and passed directly to the logging level before formatting. If no log message would be written to any file or device, then no formatting is done and the arguments are thrown away. That makes it very inexpensive to include verbose debug logging which can be turned off without recompiling and re-deploying. If you prefer, there are also variants that take lazily evaluated parameters, and only evaluate them if logging is active at that level: log.ifDebug(s""Login from $name at $date."") The logging classes are done as an extension to the java.util.logging API, and so you can use the Java interface directly, if you want to. Each of the Java classes (Logger, Handler, Formatter) is just wrapped by a Scala class. Configuring In the Java style, log nodes are in a tree, with the root node being """" (the empty string). If a node has a filter level set, only log messages of that priority or higher are passed up to the parent. Handlers are attached to nodes for sending log messages to files or logging services, and may have formatters attached to them. Logging levels are, in priority order of highest to lowest: FATAL - the server is about to exit CRITICAL - an event occurred that is bad enough to warrant paging someone ERROR - a user-visible error occurred (though it may be limited in scope) WARNING - a coder may want to be notified, but the error was probably not user-visible INFO - normal informational messages DEBUG - coder-level debugging information TRACE - intensive debugging information Each node may also optionally choose to not pass messages up to the parent node. The LoggerFactory builder is used to configure individual log nodes, by filling in fields and calling the apply method. For example, to configure the root logger to filter at INFO level and write to a file: import com.twitter.logging._  val factory = LoggerFactory(   node = """",   level = Some(Level.INFO),   handlers = List(     FileHandler(       filename = ""/var/log/example/example.log"",       rollPolicy = Policy.SigHup     )   ) )  val logger = factory() As many LoggerFactorys can be configured as you want, so you can attach to several nodes if you like. To remove all previous configurations, use: Logger.clearHandlers() Handlers QueueingHandler Queues log records and publishes them in another thread thereby enabling ""async logging"". ConsoleHandler Logs to the console. FileHandler Logs to a file, with an optional file rotation policy. The policies are: Policy.Never - always use the same logfile (default) Policy.Hourly - roll to a new logfile at the top of every hour Policy.Daily - roll to a new logfile at midnight every night Policy.Weekly(n) - roll to a new logfile at midnight on day N (0 = Sunday) Policy.SigHup - reopen the logfile on SIGHUP (for logrotate and similar services) When a logfile is rolled, the current logfile is renamed to have the date (and hour, if rolling hourly) attached, and a new one is started. So, for example, test.log may become test-20080425.log, and test.log will be reopened as a new file. SyslogHandler Log to a syslog server, by host and port. ScribeHandler Log to a scribe server, by host, port, and category. Buffering and backoff can also be configured: You can specify how long to collect log lines before sending them in a single burst, the maximum burst size, and how long to backoff if the server seems to be offline. ThrottledHandler Wraps another handler, tracking (and squelching) duplicate messages. If you use a format string like ""Error %d at %s"", the log messages will be de-duped based on the format string, even if they have different parameters. Formatters Handlers usually have a formatter attached to them, and these formatters generally just add a prefix containing the date, log level, and logger name. Formatter A standard log prefix like ""ERR [20080315-18:39:05.033] jobs: "", which can be configured to truncate log lines to a certain length, limit the lines of an exception stack trace, and use a special time zone. You can override the format string used to generate the prefix, also. BareFormatterConfig No prefix at all. May be useful for logging info destined for scripts. SyslogFormatterConfig A formatter required by the syslog protocol, with configurable syslog priority and date format. Version 6.x Major version 6 introduced some breaking changes: Futures are no longer Cancellable; cancellation is replaced with a simpler interrupt mechanism. Time and duration implement true sentinels (similar to infinities in doubles). Time.now uses system time instead of nanotime + offset. The (dangerous) implicit conversion from a Duration to a Long was removed. Trys and Futures no longer handle fatal exceptions: these are propagated to the dispatching thread. Future interrupts Method raise on Future (def raise(cause: Throwable)) raises the interrupt described by cause to the producer of this Future. Interrupt handlers are installed on a Promise using setInterruptHandler, which takes a partial function: val p = new Promise[T] p.setInterruptHandler {   case exc: MyException =>     // deal with interrupt.. } Interrupts differ in semantics from cancellation in important ways: there can only be one interrupt handler per promise, and interrupts are only delivered if the promise is not yet complete. Time and Duration Like arithmetic on doubles, Time and Duration arithmetic is now free of overflows. Instead, they overflow to Top and Bottom values, which are analogous to positive and negative infinity. Since the resolution of Time.now has been reduced (and is also more expensive due to its use of system time), a new Stopwatch API has been introduced in order to calculate durations of time. It's used simply: import com.twitter.util.{Duration, Stopwatch} val elapsed: () => Duration = Stopwatch.start() which is read by applying elapsed: val duration: Duration = elapsed() Contributing The master branch of this repository contains the latest stable release of Util, and weekly snapshots are published to the develop branch. In general pull requests should be submitted against develop. See CONTRIBUTING.md for more details about how to contribute. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/util"	"General-purpose Scala libraries, including a future implementation and other concurrency tools."	"true"
"Misc"	"Ammonite-REPL"	"http://lihaoyi.github.io/Ammonite/#Ammonite-REPL"	"An improved Scala REPL: syntax highlighting, output formatting, multi-line input, and more."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Ammonite Ammonite Ammonite enables shell-like scripting in the Scala programming language. It is made of a few subprojects: Ammonite-REPL: A Modernized Scala REPL, with tons of bug fixes and features Ammonite-Ops: A Scala Library for convenient, rock-solid Filesystem Operations Ammonite-Shell: A modern replacement for the Bash system shell Depending on why you are here, click on the above links to jump straight to the documentation that interests you. For an overview of the project and it's motivation, check out this talk: Ammonite-REPL A Modernized Scala REPL Ammonite is an improved Scala REPL, re-implemented from first principles. It is much more featureful than the default REPL and comes with a lot of ergonomic improvements and configurability that may be familiar to people coming from IDEs or other REPLs such as IPython or Zsh. It can be combined with Ammonite-Ops to replace Bash as your systems shell, but also can be used alone as a superior version of the default Scala REPL, or as a debugging tool, or for many other fun and interesting things! Getting Started If you want to use Ammonite as a plain Scala shell, download the standalone executable: $ curl -L -o amm https://git.io/vo4w5 && chmod +x amm && ./amm  This will give you access to Ammonite for Scala: With Pretty Printing, Syntax Highlighting for input and output, Artifact Loading in-REPL, and all the other nice Features! If you want to use Ammonite as a filesystem shell, take a look at Ammonite-Shell. If you're not sure what to do with Ammonite, check out the REPL Cookbook for some fun ideas! If you want some initialization code available to the REPL, you can add it to your ~/.ammonite/predef.scala. If you have any questions, come hang out on the mailing list or gitter channel and get help! Ammonite-REPL in SBT You can also try out Ammonite in an existing SBT project, add the following to your build.sbt libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" % ""test"" cross CrossVersion.full  initialCommands in (Test, console) := """"""ammonite.repl.Main().run()""""""  After that, simple hit sbt projectName/test:console To activate the Ammonite REPL You can also pass a string to the run call containing any commands or imports you want executed at the start of every run. If you want Ammonite to be available in all projects, simply add the above snippet to a new file ~/.sbt/0.13/global.sbt. Note: Ammonite-REPL does not support Windows, even though Ammonite-Ops does. See #119 if you are interested in details or want to try your hand at making it work. Features Ammonite-REPL supports many more features than the default REPL, including: Artifact Loading @ import scalatags.Text.all._ error: not found: value scalatags  @ load.ivy(""com.lihaoyi"" %% ""scalatags"" % ""0.4.5"")  @ import scalatags.Text.all._ import scalatags.Text.all._  @ a(""omg"", href:=""www.google.com"").render res2: String = $tq <a href=""www.google.com"">omg</a> $tq Ammonite allows you to load artifacts directly from maven central by copy & pasting their SBT ivy-coordinate-snippet. In addition, you can also load in jars as java.io.Files to be included in the session or simple Strings to be executed using the load command. This makes Ammonite ideal for trying out new libraries or tools. You can pull down projects like Scalaz or Shapeless and immediately start working with them in the REPL: @ load.ivy(""com.chuusai"" %% ""shapeless"" % ""2.2.5"")  @ import shapeless._  @ (1 :: ""lol"" :: List(1, 2, 3) :: HNil) res2: Int :: String :: List[Int] :: HNil = 1 :: lol :: List(1, 2, 3) :: HNil  @ res2(1) res3: String = ""lol""  @ import shapeless.syntax.singleton._  @ 2.narrow res5: 2 = 2 Even non-trivial web frameworks like Finagle or Akka-HTTP can be simply pulled down and run in the REPL! @ load.ivy(""com.twitter"" %% ""finagle-httpx"" % ""6.26.0"")  @ import com.twitter.finagle._; import com.twitter.util._  @ var serverCount = 0  @ var clientResponse = 0  @ val service = new Service[httpx.Request, httpx.Response] { @   def apply(req: httpx.Request): Future[httpx.Response] = { @     serverCount += 1 @     Future.value( @       httpx.Response(req.version, httpx.Status.Ok) @     ) @   } @ }  @ val server = Httpx.serve("":8080"", service)  @ val client: Service[httpx.Request, httpx.Response] = Httpx.newService("":8080"")  @ val request = httpx.Request(httpx.Method.Get, ""/"")  @ request.host = ""www.scala-lang.org""  @ val response: Future[httpx.Response] = client(request)  @ response.onSuccess { resp: httpx.Response => @   clientResponse = resp.getStatusCode @ }  @ Await.ready(response)  @ serverCount res12: Int = 1  @ clientResponse res13: Int = 200  @ server.close() Ammonite-REPL is configured with a set of default resolvers but you can add your own @ load.ivy(""com.ambiata"" %% ""mundane"" % ""1.2.1-20141230225616-50fc792"") error: IvyResolutionException  @ import ammonite.repl._, Resolvers._  @ val oss = Resolver.Http( @   ""ambiata-oss"", @   ""https://ambiata-oss.s3-ap-southeast-2.amazonaws.com"", @   IvyPattern, @   false @ )  @ resolvers() = resolvers() :+ oss  @ load.ivy(""com.ambiata"" %% ""mundane"" % ""1.2.1-20141230225616-50fc792"")  @ import com.ambiata.mundane._ Pretty-printed output @ Seq.fill(10)(Seq.fill(3)(""Foo"")) res0: Seq[Seq[String]] = List(   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo""),   List(""Foo"", ""Foo"", ""Foo"") )  @ case class Foo(i: Int, s0: String, s1: Seq[String]) defined class Foo  @ Foo(1, """", Nil) res2: ${sessionPrefix}Foo = Foo(1, """", List())  @ Foo( @   1234567, @   ""I am a cow, hear me moo"", @   Seq(""I weigh twice as much as you"", ""and I look good on the barbecue"") @ ) res3: ${sessionPrefix}Foo = Foo(   1234567,   ""I am a cow, hear me moo"",   List(""I weigh twice as much as you"", ""and I look good on the barbecue"") ) Ammonite-REPL uses PPrint to display its output by default. That means that everything is nicely formatted to fit within the width of the terminal, and is copy-paste-able! By default, Ammonite truncates the pretty-printed output to avoid flooding your terminal. If you want to disable truncation, call show(...) on your expression to pretty-print it's full output. You can also pass in an optional height = ... parameter to control how much you want to show before truncation. Configurable Truncation @ Seq.fill(20)(100) res0: Seq[Int] = List(   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100, ...  @ show(Seq.fill(20)(100)) res1: ammonite.pprint.Show[Seq[Int]] = List(   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100,   100 )  @ show(Seq.fill(20)(100), height = 3) res2: ammonite.pprint.Show[Seq[Int]] = List(   100,   100, ...  @ pprintConfig() = pprintConfig().copy(height = 5 )  @ Seq.fill(20)(100) res4: Seq[Int] = List(   100,   100,   100,   100, ... Ammonite-REPL intelligently truncates your output when it's beyond a certain size. You can request for the full output to be printed on-demand, print a certain number of lines, or even change the implicit pprintConfig so subsequent lines all use your new configuration. Save/Load Session Ammonite allows you to save your work half way through, letting you discard and future changes and returning to the state of the world you saved. Defined some memory-hogging variable you didn't need? Loaded the wrong version of some third-party library? Reluctant to reload the REPL because reloading is slow? Fear not! With Ammonite, you can save your important work, do whatever you want later, and simply discard all the jars you loaded, variables you defined @ val veryImportant = 1 veryImportant: Int = 1  @ sess.save()  @ val oopsDontWantThis = 2 oopsDontWantThis: Int = 2  @ // Let's try this new cool new library  @ load.ivy(""com.lihaoyi"" %% ""scalatags"" % ""0.5.3"")  @ veryImportant res4: Int = 1  @ oopsDontWantThis res5: Int = 2  @ import scalatags.Text.all._  @ div(""Hello"").render res7: String = ""<div>Hello</div>""  @ // Oh no, maybe we don't want scalatags!  @ sess.load()  @ veryImportant res9: Int = 1  @ oopsDontWantThis error: not found: value oopsDontWantThis  @ import scalatags.Text.all._ error: not found: value scalatags """""") Apart from plain saves and loads, which simply discard everything after the most recent save, you can also provide a name to these functions. That lets you stop working on a branch, go do something else for a while, and be able to come back later to continue where you left off: @ val (x, y) = (1, 2) x: Int = 1 y: Int = 2  @ sess.save(""xy initialized"")  @ val z = x + y z: Int = 3  @ sess.save(""first z"")  @ sess.load(""xy initialized"")  @ val z = x - y z: Int = -1  @ sess.save(""second z"")  @ z res7: Int = -1  @ sess.load(""first z"")  @ z res9: Int = 3  @ sess.load(""second z"")  @ z res11: Int = -1             """""") Lastly, you have the sess.pop() function. Without any arguments, it behaves the same as sess.load(), reseting you to your last savepoint. However, you can pass in a number of session frames which you'd like to pop, allow you to reset your session to even earlier save points. sess.pop(2) would put you two save-points ago, sess.pop(3) would put you three save-points ago, letting you reach earlier save-points even if you did not give them names. Passing in a large number like sess.pop(999) would reset your session all the way until the start. Ammonite's save and load functionality is implemented via Java class-loaders. Superior Autocomplete The original Scala REPL provides no autocomplete except for the most basic scenarios of value.<complete>. In the Ammonite-REPL, you get the same autocomplete-anywhere support that you get in a modern IDE. @ Seq(1, 2, 3).map(x => x.) getClass            ##                  asInstanceOf        isInstanceOf toString            hashCode            equals              != ==                  %                   /                   * -                   +                   ^                   & |                   >=                  >                   <= <                   >>                  >>>                 << unary_-             unary_+             unary_~             toDouble toFloat             toLong              toInt               toChar toShort             toByte              compareTo           doubleValue ...  @ Futu scala.collection.parallel.FutureThreadPoolTasks scala.collection.parallel.FutureTasks scala.concurrent.impl.Future$PromiseCompletingRunnable scala.concurrent.impl.Future scala.concurrent.Future scala.concurrent.FutureTaskRunner scala.concurrent.Future$InternalCallbackExecutor scala.concurrent.Future$class java.util.concurrent.Future java.util.concurrent.FutureTask$WaitNode java.util.concurrent.FutureTask com.sun.corba.se.impl.orbutil.closure.Future  Neither of these examples work in the standard Scala REPL. Interrupting run-away execution with Ctrl-C @ while(true) () ... hangs ... ^Ctrl-C Interrupted!  @  The traditional Scala REPL doesn't handle runaway code, and gives you no option but to kill the process, losing all your work. Ammonite-REPL lets you interrupt the thread, stop the runaway-command and keep going. Compiler-crash Robustness @ val x = 1 x: Int = 1  @ /* trigger compiler crash */ trait Bar { super[Object].hashCode } error: java.lang.AssertionError: assertion failed  @ 1 + x res1: Int = 2 The default Scala REPL throws away all your work if the compiler crashes. This doesn't make any sense, because all the compiler is is a dumb String => Array[Byte] pipe. In the Ammonite, we simply swap out the broken compiler for a new one and let you continue your work. Other Fixes Apart from the above features, the Ammonite REPL fixes a large number of bugs in the default Scala REPL, including but not limited to: SI-6302 SI-8971 SI-9249 SI-4438 SI-8603 SI-6660 SI-7953 SI-6659 SI-8456 SI-1067 SI-8307 SI-9335 Editing Ammonite by default ships with a custom implementation of readline, which provides... Syntax Highlighting Ammonite syntax highlights both the code you're entering as well as any output being echoed in response. This should make it much easier to work with larger snippets of input. All colors are configurable, and you can easily turn off colors entirely via the Configuration. Stack traces are similarly highlighted, for easier reading: Multi-line editing You can use the Up and Down arrows to navigate between lines within your snippet. Enter only executes the code when you're on the last line of a multi-line snippet, meaning you can take your time, space out your code nicely, and fix any mistakes anywhere in your snippet. History is multi-line too, meaning re-running a multi-line snippet is trivial, even with tweaks. Long gone are the days where you're desperately trying to cram everything in a single line, or curse quietly when you notice a mistake in an earlier line you are no longer able to fix. No more painstakingly crafting a multi-line snippet, and then having to painstakingly fish it line by individual line out of the history so you can run it again! Desktop key-bindings You can use Alt-Left/Right to move forward/backwards by one word at a time or hold down Shift to select text to delete. These compose as you'd be used to: e.g. Shift-Up selects all the text between your current cursor and the same column one row up. Tab and Shift-Tab now work to block-indent and -dedent sections of code, as you'd expect in any desktop editor like Sublime Text or IntelliJ. This further enhances the multi-line editing experience, letting your nicely lay-out your more-complex REPL commands the same way you'd format code in any other editor. Console key-bindings All the readline-style navigation hotkeys like Ctrl-W to delete a word or Esc-Left/Right to navigate one word left/right still work. If you're comfortable with consoles like Bash, Python, IPython or even the default Scala console, you should have no trouble as all the exact same hotkeys work in Ammonite History Search Apart from browsing your command-line history with UP, you can also perform a history search by entering some search term and then pressing UP. That will pull up the most recent history line with that term in it, underlined. You can continue to press UP or DOWN to cycle through the matches, or Backspace or continue typing characters to refine your search to what you want. You can press TAB, or any other command character (LEFT, RIGHT, ...) to end the search and let you continue working with the currently-displayed command. Pressing ENTER will end the search and immediately submit the command to be run. You can also kick off a history search using Ctrl-R, and use Ctrl-R to cycle through the matches. Block Input To enter block input (many independent lines all at once) into the Ammonite-REPL, simply wrap the multiple lines in curly braces { ... }, and Ammonite will wait until you close it before evaluating the contents: @ { @   val x = 1 @   val y = 2 @   x + y @ } x: Int = 1 y: Int = 2 res0_2: Int = 3 As you can see, the contents of the { ... } block are unwrapped and evaluated as top-level statements. You can use this to e.g. declare mutually recursive functions or classes & companion-objects without being forced to squeeze everything onto a single line. If you don't want this un-wrapping behavior, simply add another set of curlies and the block will be evaluated as a normal block, to a single expression: @ {{ @   val x = 1 @   val y = 2 @   x + y @ }} res0: Int = 3 Undo & Redo The Ammonite command-line editor allows you to undo and re-do portions of your edits: Ctrl -: Undo last change Alt/Esc -: Redo last change Each block of typing, deletes, or navigation counts as one undo. This should make it much more convenient to recover from botched copy-pastes or bulk-deletions. Builtins The Ammonite REPL contains a bunch of built-in imports and definitions by default. This includes: Repl API: the way you can interact with the REPL programmatically and access things like it's history, modify it's prompt, etc. repl: the object representing the Repl API, aliased as repl rather than it's full name ammonite.repl.frontend.ReplBridge.repl. Although you can call them API methods directly (e.g. history) you can also call them via the repl object (e.g. repl.history) and you can use autocomplete or typeOf on the repl object to see what is available. Utilities: tools such as time, grep or browse that are independent from the REPL, but are extremely useful to have in it. Artifact Loading implicits to provide the SBT-like syntax All of these are imported by default into any Ammonite REPL, in order to provide a rich and consistent REPL experience. If you want to disable these imports and run the REPL with a clean namespace (with only the core implicits needed for result pretty-printing/type-printing to work) pass in defaultPredef = false to the REPL's Main API or --no-default-predef to the REPL from the command-line. Repl API Ammonite contains a range of useful built-ins implemented as normal functions. Everything inside the ReplAPI trait is imported by default and can be accessed directly by default to control the console. trait ReplAPI {   /**    * Exit the Ammonite REPL. You can also use Ctrl-D to exit    */   def exit = throw ReplExit(())   /**    * Exit the Ammonite REPL. You can also use Ctrl-D to exit    */   def exit(value: Any) = throw ReplExit(value)     /**    * Read/writable prompt for the shell. Use this to change the    * REPL prompt at any time!    */   val prompt: Ref[String]   /**    * The front-end REPL used to take user input. Modifiable!    */   val frontEnd: Ref[FrontEnd]    /**    * Display help text if you don't know how to use the REPL    */   def help: String    /**     * The last exception that was thrown in the REPL; `null` if nothing has     * yet been thrown. Useful if you want additional information from the     * thrown exception than the printed stack trace (e.g. many exceptions have     * additional metadata attached) or if you want to show the stack trace     * on an exception that doesn't normally print it (e.g. seeing the stack     * when a Ctrl-C interrupt happened) via `lastException.printStackTrace`.     */   def lastException: Throwable   /**    * History of commands that have been entered into the shell, including    * previous sessions    */   def fullHistory: History    /**    * History of commands that have been entered into the shell during the    * current session    */   def history: History    /**    * Get the `Type` object of [[T]]. Useful for finding    * what its methods are and what you can do with it    */   def typeOf[T: WeakTypeTag]: Type    /**    * Get the `Type` object representing the type of `t`. Useful    * for finding what its methods are and what you can do with it    *    */   def typeOf[T: WeakTypeTag](t: => T): Type      /**    * Tools related to loading external scripts and code into the REPL    */   def load: Load    /**    * resolvers to use when loading jars     */   def resolvers: Ref[List[Resolver]]    /**    * The colors that will be used to render the Ammonite REPL in the terminal    */   val colors: Ref[Colors]    /**    * Throw away the current scala.tools.nsc.Global and get a new one    */   def newCompiler(): Unit    /**    * Access the compiler to do crazy things if you really want to!    */   def compiler: scala.tools.nsc.Global    /**    * Show all the imports that are used to execute commands going forward    */   def imports: String   /**    * Controls how things are pretty-printed in the REPL. Feel free    * to shadow this with your own definition to change how things look    */   implicit val pprintConfig: Ref[pprint.Config]    implicit def derefPPrint(implicit t: Ref[pprint.Config]): pprint.Config = t()    implicit def tprintColors: pprint.TPrintColors    implicit def codeColors: CodeColors   /**    * Current width of the terminal    */   def width: Int   /**    * Current height of the terminal    */   def height: Int    def replArgs: Vector[ammonite.repl.Bind[_]]    /**    * Lets you configure the pretty-printing of a value. By default, it simply    * disables truncation and prints the entire thing, but you can set other    * parameters as well if you want.    */   def show[T: PPrint](implicit cfg: Config): T => Unit   def show[T: PPrint](t: T,                       width: Integer = 0,                       height: Integer = null,                       indent: Integer = null,                       colors: pprint.Colors = null)                      (implicit cfg: Config = Config.Defaults.PPrintConfig): Unit   /**     * Functions that can be used to manipulate the current REPL session:     * check-pointing progress, reverting to earlier checkpoints, or deleting     * checkpoints by name.     *     * Frames get pushed on a stack; by default, a saved frame is     * accessible simply by calling `load`. If you provide a name     * when `save`ing a checkpoint, it can later be `load`ed directly     * by providing the same name to `load`     *     * Un-named checkpoints are garbage collected, together with their     * classloader and associated data, when they are no longer accessible     * due to `restore`. Named checkpoints are kept forever; call `delete`     * on them if you really want them to go away.     */   def sess: Session } trait Session{   /**     * The current stack of frames     */   def frames: List[Frame]   /**     * Checkpoints your current work, placing all future work into its own     * frames. If a name is provided, it can be used to quickly recover     * that checkpoint later.     */   def save(name: String = """"): Unit    /**     * Discards the last frames, effectively reverting your session to     * the last `save`-ed checkpoint. If a name is provided, it instead reverts     * your session to the checkpoint with that name.     */   def load(name: String = """"): SessionChanged    /**     * Resets you to the last save point. If you pass in `num`, it resets     * you to that many savepoints since the last one.     */   def pop(num: Int = 1): SessionChanged   /**     * Deletes a named checkpoint, allowing it to be garbage collected if it     * is no longer accessible.     */   def delete(name: String): Unit }  trait LoadJar {    /**    * Load a `.jar` file or directory into your JVM classpath    */   def cp(jar: Path): Unit   /**    * Load a library from its maven/ivy coordinates    */   def ivy(coordinates: (String, String, String), verbose: Boolean = true): Unit } trait Load extends (String => Unit) with LoadJar{   /**    * Loads a command into the REPL and    * evaluates them one after another    */   def apply(line: String): Unit    /**    * Loads and executes the scriptfile on the specified path.    * Compilation units separated by `@\n` are evaluated sequentially.    * If an error happens it prints an error message to the console.    */    def exec(path: Path): Unit    def module(path: Path): Unit    def plugin: LoadJar  } All of these are also available as part of the repl object which is imported in scope by default. Utilities Apart from the core Builtins of the REPL, the Ammonite REPL also includes many helpers that are not strictly necessarily but are very useful in almost all REPL sessions. Here are a few of them time grep browse desugar The REPL also imports the pipe-operators from Ammonite-Ops by default to make it easy for you to use tools like grep interactively, and imports all the Builtins from the repl. These tools are useful but not strictly necessary; time bash$ time ls -a . .. .git .gitignore .idea .travis.yml LICENSE appveyor.yml build.sbt integration internals-docs ops project readme readme.md repl shell sshd target terminal  real	0m0.012s user	0m0.003s sys	0m0.005s haoyi-Ammonite@ time{ls!}  res0: (LsSeq, concurrent.duration.FiniteDuration) = (    "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd ,   25968654 nanoseconds ) Just as bash provides a time command that you can use to see how long a command took to run, Ammonite-Shell provides a time function which serves the same purpose. While the bash version spits out the time in an ad-hoc table format, stuck together with the output of the command, Ammonite-Shell's time instead returns a tuple containing the expression that was evaluated, and the time taken to evaluate it. grep bash$ ls -a . | grep re .gitignore readme readme.md repl haoyi-Ammonite@ ls! wd || grep! ""re""  res0: Seq[GrepResult] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) bash$ ls -a . | grep re .gitignore readme readme.md repl haoyi-Ammonite@ ls! wd |? grep! ""re""  res0: Seq[Path] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) Ammonite provides its own grep command, which lets you easily perform ad-hoc searches within a list. As shown above, Ammonite's grep can be used via || (flatMap) or |? (filter). In the case of ||, it displays the matches found, highlighted, with some additional context before and after the match. When used with |?, it simply returns the relevant items. In general, || is useful for manual exploration, while |? is useful in scripts where you want to deal with the list of matched results later. By default, Ammonite's grep matches a string as a literal. If you want to match via a regex, append a .r to the string literal to turn it into a regex: bash$ ls -a . | grep -G ""re[a-z]\+"" readme readme.md repl haoyi-Ammonite@ ls! wd || grep! ""re[a-z]+"".r  res0: Seq[GrepResult] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'readme,   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/""readme.md"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'repl ) Ammonite's grep isn't limited to ""filesystem""-y things; any collection of objects can be piped through grep! For example, here's grep being used to quickly search through the JVM system properties: haoyi-Ammonite@ // I'm only interested in OS-related properties, show them to me!  haoyi-Ammonite@ sys.props || grep! ""os|OS"".r  res0: collection.mutable.Iterable[GrepResult] = ArrayBuffer(   (""sun.os.patch.level"", ""unknown""),   (""os.arch"", ""x86_64""),   (""os.name"", ""Mac OS X""),   (""os.version"", ""10.11.5""),   (""http.nonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16""),   (""java.awt.printerjob"", ""sun.lwawt.macosx.CPrinterJob""),   (""awt.toolkit"", ""sun.lwawt.macosx.LWCToolkit""),   (""socksNonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16""),   (""ftp.nonProxyHosts"", ""local|*.local|169.254/16|*.169.254/16"") ) You can even use Ammonite's grep to dig through the methods of an object, even large messy objects with far-too-many methods to go over by hand hunting for what you want: haoyi-Ammonite@ typeOf(compiler).members.size // Too many methods to dig through!  res0: Int = 1567 haoyi-Ammonite@ // I forgot what I want but I think it has Raw in the name  haoyi-Ammonite@ typeOf(compiler).members || grep! ""Raw""  res1: Iterable[GrepResult] = List(   class RawTreePrinter,   method newRawTreePrinter,   method isRawParameter,   method isRaw,   method isRawType,   method isRawIfWithoutArgs,   method showRaw$default$7,   method showRaw$default$6,   method showRaw$default$5,   method showRaw$default$4,   method showRaw$default$3,   method showRaw$default$2,   method showRaw,   method showRaw,   method showRaw,   method showRaw ) In general, Ammonite's grep serves the same purpose of grep in the Bash shell: a quick and dirty way to explore large amounts of semi-structured data. You probably don't want to build your enterprise business logic on top of grep's string matching. While you're working, though, grep can be a quick way to find items of interest in collections of things (anything!) too large to sift through by hand, when you're not yet sure exactly what you want. browse browse is a utility that lets you open up far-too-large data structures in the less pager, letting you page through large quantities of text, navigating around it and searching through it, without needing to spam your terminal output with its contents and losing your earlier work to the output-spam. Simple call browse on whatever value you want, e.g. this 50 thousand line ls.rec result show above. If you're dealing with large blobs of data that you want to dig through manually, you might normally format it nicely, write it to a file, and open it in vim or less or an editor such as Sublime Text. browse makes that process quick and convenient. You can customize the browse call like you would a show call or pprint.pprintln call, e.g. setting an optional width, colors or indent. You can also choose a viewer program in case you don't want to use less: e.g. here's a command that would open it up in vim: haoyi-Ammonite@ browse(res0, viewer=""vim"", colors = pprint.Colors.BlackWhite)  Apart from using viewer=""vim"", we also set the colors to black and white because Vim by default doesn't display ANSI colors nicely. You can also pass in a Seq of strings to viewer if you want to pass additional flags to your editor, and of course use any other editor you would like such as ""emacs"" or ""nano"" or ""subl"" desugar desugar allows you to easily see what the compiler is doing with your code before it gets run. For example, in the above calls to desugar, you can see: List(...) being converted to List.apply(...) true -> false being converted to Predef.ArrayAssoc(true).$minus$greater(false) default.write$default, default.SeqishW, etc. being injected as implicits for comprehensions with if filters being converted into the relevant withFilter and map calls In general, if you are having trouble understanding the combination of implicit parameters, implicit conversions, macros, and other odd Scala features, desugar could you see what is left after all the magic happens. desugar only works in Scala 2.11.x and above, not in 2.10.x Script Files Ammonite defines a format that allows you to load external scripts into the REPL; this can be used to save common functionality so it can be used at a later date. In the simplest case, a script file is simply a sequence of Scala statements, e.g. // script.scala // print banner println(""Welcome to the XYZ custom REPL!!"")  // common imports import sys.process._ import collection.mutable  // common initialization code val x = 123 ...  Which you can then load into the REPL as desired: @ mutable.Seq(x) // doesn't work Compilation Failed Main.scala:122: not found: value mutable mutable.Seq(x) // doesn't work ^ Main.scala:122: not found: value x mutable.Seq(x) // doesn't work             ^ @ import ammonite.ops._ @ load.module(cwd / ""script.scala"") Welcome to the XYZ custom REPL!!  @ mutable.Seq(x) // works res1: mutable.Seq[Int] = ArrayBuffer(123)  By default, everything in a script is compiled and executed as a single block. That means that if you want to perform classpath-modifying operations, such as load.cp or load.ivy, its results will not be available within the same script if you want to use methods, values or packages defined in the loaded code. To make this work, break the script up into multiple compilation units with an @ sign, e.g. // print banner println(""Welcome to the XYZ custom REPL!!"")  load.ivy(""org.scalaz"" %% ""scalaz-core"" % ""7.1.1"")  @  // common imports import scalaz._ import Scalaz._  // common initialization code ... Ammonite provides two ways to load scripts, load.exec and load.module. With load.exec the script is executed like it was pasted in the REPL. Exec scripts can access all values previously defined in the REPL, and all side-effects are guaranteed to be applied. This is useful for one-off sets of commands. With load.module, the script is loaded like a Scala module. That means it can't access values previously defined in the REPL, but it is guaranteed to only execute once even if loaded many times by different scripts. If you want to execute the script code multiple times, put it in a function and call it after you load the script. Any scripts you load can themselves load scripts. You can also run scripts using the Ammonite executable from an external shell (e.g. bash): bash$ ./amm path/to/script.scala  All types, values and imports defined in scripts are available to commands entered in REPL after loading the script. You can also make an Ammonite script self-executable by using a shebang #!. This is an example script named hello. There is no need to add the .scala extension. The amm command needs to be in the PATH: #!/usr/bin/env amm  println(""hello world"") make it executable and run it from an external shell (e.g. bash): $ chmod +x /path/to/script $ /path/to/script  Script Arguments Often when calling a script from the external command- line (e.g. Bash), you need to pass arguments to configure its behavior. With Ammonite, this is done by defining a main method, e.g. // Args.scala val x = 1 import ammonite.ops._ def main(i: Int, s: String, path: Path = cwd) = {   println(s""Hello! ${s * i} ${path.relativeTo(cwd)}."") } When the script is run from the command line: ~/amm Args.scala 3 Moo The top-level definitions execute first (e.g. setting x), and then the main method is called with the arguments you passed in. Default arguments behave as you would expect (i.e. they allow you to omit it when calling) and arguments are parsed using the scopt.Read typeclass, which provides parsers for primitives like Int, Double, String, as well as basic data-structures like Seqs (taken as a comma-separated list) and common types like Paths. If you pass in the wrong number of arguments, or if an argument fails to deserialize, the script will fail with an exception. The main method does not get automatically called when you load.module or load.exec a script from within the Ammonite REPL. It gets imported into scope like any other method or value defined in the script, and you can just call it normally. Configuration Ammonite is configured via Scala code, that can live in the ~/.ammonite/predef.scala file, passed in through SBT's initialCommands, or passed to the command-line executable as --predef='...'. Anything that you put in predef.scala will be executed when you load the Ammonite REPL. This is a handy place to put common imports, setup code, or even call load.ivy to load third-party jars. The compilation of the predef is cached, so after the first run it should not noticeably slow down the initialization of your REPL. Some examples of things you can configure: @ // Set the shell prompt to be something else  @ repl.prompt() = "">""  @ // Change the terminal front end; the default is  @ // Ammonite on Linux/OSX and JLineWindows on Windows  @ repl.frontEnd() = ammonite.repl.frontend.FrontEnd.JLineUnix  @ repl.frontEnd() = ammonite.repl.frontend.FrontEnd.JLineWindows  @ repl.frontEnd() = ammonite.repl.frontend.AmmoniteFrontEnd()  @ // Changing the colors used by Ammonite; all at once:  @ repl.colors() = ammonite.repl.Colors.BlackWhite  @ repl.colors() = ammonite.repl.Colors.Default  @ // or one at a time:  @ repl.colors().prompt() = fansi.Color.Red  @ repl.colors().ident() = fansi.Color.Green  @ repl.colors().`type`() = fansi.Color.Yellow  @ repl.colors().literal() = fansi.Color.Magenta  @ repl.colors().prefix() = fansi.Color.Cyan  @ repl.colors().comment() = fansi.Color.Red  @ repl.colors().keyword() = fansi.Bold.On  @ repl.colors().selected() = fansi.Underlined.On  @ repl.colors().error() = fansi.Color.Yellow Refs By default, all the values you're seeing here with the () after them are Refs, defined as trait StableRef[T]{   /**    * Get the current value of the this [[StableRef]] at this instant in time    */   def apply(): T    /**    * Set the value of this [[StableRef]] to always be the value `t`    */   def update(t: T): Unit }  trait Ref[T] extends StableRef[T]{   /**    * Return a function that can be used to get the value of this [[Ref]]    * at any point in time    */   def live(): () => T    /**    * Set the value of this [[Ref]] to always be the value of the by-name    * argument `t`, at any point in time    */   def bind(t: => T): Unit } As you can see from the signature, you can basically interact with the Refs in two ways: either getting or setting their values as values, or binding their values to expressions that will be evaluated every time the Ref's value is needed. As an example of the latter, you can use bind to set your prompt to always include your current working directory repl.prompt.bind(wd.toString + ""@ "")  As is common practice in other shells. Further modifications to make it include e.g. your current branch in Git (which you can call through Ammonite's subprocess API or the current timestamp/user are similarly possible. Compiler Flags Apart from configuration of the rest of the shell through Refs, configuration of the Scala compiler takes place separately through the compiler's own configuration mechanism. You have access to the compiler as compiler, and can modify its settings as you see fit. Here's an example of this in action: @ // Disabling default Scala imports  @ List(1, 2, 3) + ""lol"" res0: String = ""List(1, 2, 3)lol""  @ compiler.settings.noimports.value = true  @ List(1, 2, 3) + ""lol"" // predef imports disappear error: not found: value List  @ compiler.settings.noimports.value = false  @ List(1, 2, 3) + ""lol"" res3: String = ""List(1, 2, 3)lol""  @ // Disabling Scala language-import enforcement  @ object X extends Dynamic error: extension of type scala.Dynamic needs to be enabled  @ compiler.settings.language.tryToSet(List(""dynamics""))  @ object X extends Dynamic defined object X  @ 1 + 1 // other things still work  @ // Enabling warnings (which are disabled by default)  @ List(1) match { case _: List[Double] => 2 } res7: Int = 2  @ compiler.settings.nowarnings.value = false  @ List(1) match { case _: List[Double] => 2 } warning: $fruitlessTypeTestWarningMessageBlahBlahBlah If you want these changes to always be present, place them in your ~/.ammonite/predef.scala. Embedding Ammonite The Ammonite REPL is just a plain-old-Scala-object, just like any other Scala object, and can be easily used within an existing Scala program. This is useful for things like interactive Debugging or hosting a Remote REPL to interact with a long-lived Scala process, or Instantiating Ammonite inside an existing program to serve as a powerful interactive console. Instantiating Ammonite To use Ammonite inside an existing Scala program, you need to first add it to your dependencies: libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" cross CrossVersion.full  Then instantate it with this code anywhere within your program: package ammonite.repl   object TestMain{   def main(args: Array[String]): Unit = {     Main.main(args ++ Array(""--home"", ""target/tempAmmoniteHome""))   } } You can configure the instantiated REPL by passing in arguments to the Main() call, e.g. to redirect the input/output streams or to run a predef to configure it further. Debugging Ammonite can be used as a tool to debug any other Scala program, by conveniently opening a REPL at any point within your program with which you can interact with live program data, similar to pdb/ipdb in Python. To do so, first add Ammonite to your classpath, e.g. through this SBT snippet: libraryDependencies += ""com.lihaoyi"" % ""ammonite-repl"" % ""0.6.2"" cross CrossVersion.full  Note that unlike the snippet given above, we leave out the % ""test"" because we may want ammonite to be available within the ""main"" project, and not just in the unit tests. Then, anywhere within your program, you can place a breakpoint via: package ammonite.integration object TestMain {   def main(args: Array[String]): Unit = {     val hello = ""Hello""     // Break into debug REPL with     ammonite.repl.Main(       predef = ""println(\""Starting Debugging!\"")""     ).run(       ""hello"" -> hello,       ""fooValue"" -> foo()     )   }   def foo() = 1 } And when your program reaches that point, it will pause and open up an Ammonite REPL with the values you provided it bound to the names you gave it. From there, you can interact with those values as normal Scala values within the REPL. Use Ctrl-D or exit to exit the REPL and continue normal program execution. Note that the names given must be plain Scala identifiers. Here's an example of it being used to debug changes to the WootJS webserver: In this case, we added the debug statement within the websocket frame handler, so we can inspect the values that are taking part in the client-server data exchange. You can also put the debug statement inside a conditional, to make it break only when certain interesting situations (e.g. bugs) occur. As you can see, you can bind the values you're interested in to names inside the debug REPL, and once in the REPL are free to explore them interactively. The debug() call returns : Any; by default, this is (): Unit, but you can also return custom values by passing in an argument to exit(...) when you exit the REPL. This value will then be returned from debug(), and can be used in the rest of your Scala application. Remote REPL Ammonite can also be used to remotely connect to your running application and interact with it in real-time, similar to Erlang's erl -remsh command. This is useful if e.g. you have multiple Scala/Java processes running but aren't sure when/if you'd want to inspect them for debugging, and if so which ones. With Ammonite, you can leave a ssh server running in each process. You can then and connect-to/disconnect-from each one at your leisure, working with the in-process Scala/Java objects and methods and classes interactively, without having to change code and restart the process to add breakpoints or instrumentation. To do this, add ammonite-sshd to your classpath, for example with SBT: libraryDependencies += ""com.lihaoyi"" % ""ammonite-sshd"" % ""0.6.2"" cross CrossVersion.full  Now add repl server to your application: import ammonite.sshd._ val replServer = new SshdRepl(   SshServerConfig(     address = ""localhost"", // or ""0.0.0.0"" for public-facing shells     port = 22222, // Any available port     username = ""repl"", // Arbitrary     password = ""your_secure_password"" // or """"   ) ) replServer.start()  And start your application. You will be able to connect to it using ssh like this: ssh repl@localhost -p22222 and interact with your running app. Invoke stop() method whenever you want to shutdown ammonite sshd server. Here for example sshd repl server is embedded in the Akka HTTP microservice example: Here we can interact with code live, inspecting values or calling methods on the running system. We can try different things, see which works and which not, and then put our final bits in application code. In this example app is located on local machine, but you are free to connect to any remote node running your code. Security notes: It is probably unsafe to run this server publicly (on host ""0.0.0.0"") in a production, public-facing application. Currently it doesn't supports key-based auth, and password-based auth is notoriously weak. Despite this, it is perfectly possible to run these on production infrastructure: simply leave the host set to ""localhost"", and rely on the machine's own SSH access to keep out unwanted users: you would first ssh onto the machine itself, and then ssh into the Ammonite REPL running on localhost. Typically most organizations already have bastions, firewalls, and other necessary infrastructure to allow trusted parties SSH access to the relevant machines. Running on localhost lets you leverage that and gain all the same security properties without having to re-implement them in Scala. REPL Cookbook The Ammonite Scala REPL is meant to be extended: you can load in arbitary Java/Scala modules from the internet via load.ivy. Using this third-party code, you extend the REPL to do anything you wish to do, and tools like Ammonite-Shell are simply modules like any other. Simple install Java, download Ammonite onto any Linux/OSX machine, and try out one of these fun snippets! HTTP Requests Scraping HTML GUI Applications Office Automation Image Processing Machine Learning HTTP Requests Ammonite does not come with a built-in way to make HTTP requests, but there are Java /Scala modules that do this quite well! Here's an example: Welcome to the Ammonite Repl @ load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.0"") :: loading settings :: :: resolving dependencies :: ... [SUCCESSFUL ] org.scalaj#scalaj-http_2.11;2.2.0!scalaj-http_2.11.jar (63ms)  @ import ammonite.ops._, scalaj.http._ import ammonite.ops._, scalaj.http._  @ val resp = Http(""https://api.github.com/repos/scala/scala"").asString resp: HttpResponse[String] = HttpResponse( {""id"":2888818,""name"":""scala"",""full_name"":""scala/scala"",""owner"": {""login"":""scala"",""id"":57059,""avatar_url"": ""https://avatars.githubusercontent.com/u/57059?v=3"",""gravatar_id"":"""", ""url"":""https://api.github.com/users/scala"",""html_url"":""https://github.com/scala"", ""followers_url"":""https://api.github.com/users/scala/followers"", ""following_url"":""https://api.github.com/users/scala/following{/other_user}"", ""gists_url"":""https://api.github.com/users/scala/gists{/gist_id}"", ""starred_url"":""https://api.github.com/users/scala/starred{/owner}{/repo}"", ""subscriptions_url"":""https://api.github.com/users/scala/subscriptions"", ...  @ val parsed = upickle.json.read(resp.body).asInstanceOf[upickle.Js.Obj] parsed: upickle.Js.Obj = Obj(   ArrayBuffer(     (""id"", Num(2888818.0)),     (""name"", Str(""scala"")),     (""full_name"", Str(""scala/scala"")),     (       ""owner"",       Obj(         ArrayBuffer(           (""login"", Str(""scala"")), ...  @ for((k, v) <- parsed.value) write(cwd/'target/'temp/k, upickle.json.write(v))  @ ls! cwd/'target/'temp res6: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/archive_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/assignees_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/blobs_url,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/temp/branches_url,, ...  In this example, we use the Scalaj HTTP library to download a URL, and we use uPickle and Ammonite-Ops to parse the JSON and write it into files. uPickle and Ammonite-Ops are bundled with the Ammonite REPL and are used internally, and while Scalaj HTTP isn't, we can simply load it from the public repositories via load.ivy. This is a small example, but it illustrates the potential: if you find yourself needing to scrape some website or bulk-download large quantities of data from some website's HTTP/JSON API, you can start doing so within a matter of seconds using Ammonite. The results are given to you in nicely structured data, and you can deal with them using any Java or Scala libraries or tools you are used to rather than being forced to munge around in Bash. Sometimes, you may find that you need to get data from somewhere without a nice JSON API, which means you'd need to fall back to Scraping HTML... Scraping HTML Not every website has an API, and not every website is meant to be accessed programmatically. That doesn't mean you can't do it! Using libraries like JSoup, you can quickly and easily get the computer to extract useful information from HTML that was meant to humans. Using the Ammonite REPL, you can do it interactively and without needing to set up annoying project scaffolding. @ load.ivy(""org.jsoup"" % ""jsoup"" % ""1.7.2"")  @ import org.jsoup._ // import Jsoup  @ import collection.JavaConversions._ // make Java collections easier to use  @ val doc = Jsoup.connect(""http://en.wikipedia.org/"").get()  @ doc.select(""h1"") res54: select.Elements = <h1 id=""firstHeading"" class=""firstHeading"" lang=""en"">Main Page</h1> @ doc.select(""h2"") // this is huge and messy res55: select.Elements = <h2 id=""mp-tfa-h2"" style=""margin:3px; background:#cef2e0; font-family:inherit; font-size:120%; font-weight:bold; border:1px solid #a3bfb1; text-align:left; color:#000; padding:0.2em 0.4em;""><span class=""mw-headline"" id=""From_today.27s_featured_article"">From today's featured article</span></h2> <h2 id=""mp-dyk-h2"" style=""margin:3px; background:#cef2e0; font-family:inherit; font-size:120%; font-weight:bold; border:1px solid #a3bfb1; text-align:left; color:#000; padding:0.2em 0.4em;""><span class=""mw-headline"" id=""Did_you_know..."">Did you know...</span></h2> ...  @ doc.select(""h2"").map(_.text) // but you can easily pull out the bits you want res56: collection.mutable.Buffer[String] = ArrayBuffer(   ""From today's featured article"",   ""Did you know..."",   ""In the news"",   ""On this day..."",   ""From today's featured list"",   ""Today's featured picture"",   ""Other areas of Wikipedia"",   ""Wikipedia's sister projects"",   ""Wikipedia languages"",   ""Navigation menu"" ) If you wanted to scrape headlines off some news-site or scrape video game reviews off of some gaming site, you don't need to worry about setting up a project and installing libraries and all that stuff. You can simply load libraries like Jsoup right into the Ammonite REPL, copy some example from their website, and start scraping useful information in less than a minute. GUI Applications The Ammonite REPL runs on the Java virtual machine, which means you can use it to create Desktop GUI applications like anyone else who uses Java! Here's an example of how to create a hello-world interactive desktop app using Swing @ {   import javax.swing._, java.awt.event._   val frame = new JFrame(""Hello World Window"")    val button = new JButton(""Click Me"")   button.addActionListener(new ActionListener{     def actionPerformed(e: ActionEvent) = button.setText(""You clicked the button!"")   })   button.setPreferredSize(new java.awt.Dimension(200, 100))   frame.getContentPane.add(button)   frame.pack()   frame.setVisible(true)    } This can be run inside the Ammonite REPL without installing anything, and will show the following window with a single button: When clicked, it changes text: Although this is just a small demo, you can use Ammonite yourself to experiment with GUI programming without needing to go through the hassle of setting up an environment and project and all that rigmarole. Just run the code right in the console! You can even interact with the GUI live in the console, e.g. running this snippet of code to add another action listener to keep count of how many times you clicked the button @ {   var count = 0   button.addActionListener(new ActionListener{     def actionPerformed(e: ActionEvent) = {       count += 1       frame.setTitle(""Clicks: "" + count)     }   })   }  Which immediately becomes visible in the title of the window: Even while you're clicking on the button, you can still access count in the console: @ count res12: Int = 6 This is a level of live interactivity which is traditionally hard to come by in the world of desktop GUI applications, but with the Ammonite REPL, it's totally seamless Office Automation Apart from writing code, you very often find yourself dealing with documents and spreadsheets of various sorts. This is often rather tedious. Wouldn't it be cool if you could deal with these things programmatically? It turns out that there are open-soure Java libraries such as Apache POI that let you do this, and with the Ammonite-REPL you can quickly and easily load these libraries and get to work on your favorite documents. Here's an example extracting some data from my old Resume, in .docx format: @ load.ivy(""org.apache.poi"" % ""poi-ooxml"" % ""3.13"")  @ import ammonite.ops._                  // Prepare to deal with some files @ import org.apache.poi.xwpf.usermodel._ // Bring Ms-Word APIs into scope @ import collection.JavaConversions._    // Make use of Java collections easier  @ val path = cwd/'repl/'src/'test/'resources/'testdata/""Resume.docx""  @ val docx = new XWPFDocument(new java.io.ByteArrayInputStream(read.bytes(path)))  @ docx.get<tab> getAllEmbedds                            getParagraphArray getAllPackagePictures                    getParagraphPos getAllPictures                           getParagraphs getBodyElements                          getParagraphsIterator getBodyElementsIterator                  getParent getClass                                 getPart getCommentByID                           getPartById getComments                              getPartType getDocument                              getPictureDataByID ...  @ docx.getParagraphs.map(_.getText) res28: collection.mutable.Buffer[String] = ArrayBuffer(   """""" Haoyi Li   """""",   """""" Education	Massachusetts Institute of Technology		Cambridge, MA   """""",   """""" Bachelor of Science degree in Computer Science, GPA 4.8/5.0	 Sep 2010 – Jun 2013   """""",   """""" Work	Dropbox		San Francisco, CA   """""" ...  @ docx.getHyperlinks.map(_.getURL) res27: Array[String] = Array(   ""http://vimeo.com/87845442"",   ""http://www.github.com/lihaoyi/scalatags"",   ""http://www.github.com/lihaoyi/scala.rx"",   ""http://www.github.com/lihaoyi/scala-js-fiddle"",   ""http://www.github.com/lihaoyi/metascala"",   ""https://www.github.com/lihaoyi/macropy"",   ""http://www.github.com/lihaoyi"",   ""http://www.github.com/lihaoyi"" ) As you can see, loading the Apache POI library is just a single command, reading in my resume file is just one or two more, and then you can immediate start exploring the document's data model to see what inside interests you. You even get tab-completion on the methods of the document, making it really easy for you to interactively explore all the things that a word document has to offer! This is just a small example, but you can easily do more things in the same vein: Apache POI lets you create/modify/save .docx files in addition to reading from them, meaning you can automatically perform batch operations on large numbers of documents. The library also provides mechanisms to load in Excel spreadsheets and Powerpoint slide decks, meaning you have easy, programmable access to the great bulk of any Microsoft-Office files you find yourself dealing with. Image Processing You can perform lots of image operations in Java. You can use BufferedImagee if you want to access the low-level details or read/write individual pixels, and using Java2D you can draw shapes, perform transforms, or do anything you could possibly want to do with the images. There are also simple libraries like Thumbnailator if you're doing basic things like renaming/resizing/rotating and don't need pixel-per-pixel access. This is an example of using Thumbnailator to resize a folder of images and put them somewhere else: @ load.ivy(""net.coobird"" % ""thumbnailator"" % ""0.4.8"")  @ import net.coobird.thumbnailator._ import net.coobird.thumbnailator._  @ val images = ls! cwd/'repl/'src/'test/'resources/'testdata/'images images: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/GettingStarted.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/Highlighting.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/repl/src/test/resources/testdata/images/SystemShell.png ) @ val dest = cwd/'target/'thumbnails dest: Path = /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails  @ mkdir! dest  @ for(i <- images) {     Thumbnails.of(i.toString).size(200, 200).toFile(dest/i.last toString)   }  @ val thumbnails = ls! dest thumbnails: LsSeq = LsSeq(   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/GettingStarted.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/Highlighting.png,   /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target/thumbnails/SystemShell.png )  @ images.map(_.size) // Original image files are pretty big res44: Seq[Long] = List(180913L, 208328L, 227570L)  @ thumbnails.map(_.size) // Thumbnailed image files are much smaller res45: Seq[Long] = List(11129L, 11790L, 11893L) Machine Learning The word ""Machine Learning"" sounds big and intimidating, like something you'd need to spend 6 years getting a PhD before you understand. What if you could ""do some machine-learning"" (whatever that means) in your spare time, in a minute or two? It turns out there are many Java libraries that can help you with basics, and with the Ammonite REPL getting started is easy. Here's one example of how you can get started using the OpenNLP project to do some natural-language processing in just a few minutes. The example was found online, and shows how to extract english names from a raw String using NLP: @ load.ivy(""org.apache.opennlp"" % ""opennlp-tools"" % ""1.6.0"") // load OpenNLP  @ // Turns out you need training data, use Scalaj-HTTP to download it  @ load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.0"")  @ val tokenDataUrl = ""http://opennlp.sourceforge.net/models-1.5/en-token.bin""  @ val tokenData = scalaj.http.Http(tokenDataUrl).asBytes tokenData: HttpResponse[Array[Byte]] = HttpResponse(   Array(     80,     75,     3,     4,     20, ...  @ import opennlp.tools.tokenize._ // let's get started with OpenNLP!  @ val str = ""Hi. How are you? This is Mike. Did you see book about Peter Smith?""  @ import java.io.ByteArrayInputStream  @ val tokenModel = new TokenizerModel(new ByteArrayInputStream(tokenData.body))  @ val tokenizer = new TokenizerME(tokenModel)  @ val tokens = tokenizer.tokenize(str) tplems: Array[String] = Array(   ""Hi"",   ""."",   ""How"",   ""are"",   ""you"" ...  @ import opennlp.tools.namefind._ // Turns out we need more test data...  @ val nameDataUrl = ""http://opennlp.sourceforge.net/models-1.5/en-ner-person.bin""  @ val nameData = Http(nameDataUrl).asBytes nameData: HttpResponse[Array[Byte]] = HttpResponse(   Array(     80,     75,     3,     4,     20, ...  @ val nameModel = new TokenNameFinderModel(new ByteArrayInputStream(nameData.body))  @ val nameFinder = new NameFinderME(nameModel) nameFinder: NameFinderME = opennlp.tools.namefind.NameFinderME@491eb5ef  @ val names = nameFinder.find(tokens) names: Array[opennlp.tools.util.Span] = Array([8..9) person, [15..17) person)  @ opennlp.tools.util.Span.spansToStrings(names, tokens) // Woohoo, names! res96: Array[String] = Array(""Mike"", ""Peter Smith"")  This took a while, but only in comparison to the earlier cookbook recipes: this one is still less than 20 steps, which is not bad for something that installs multiple third-party modules, pulls down training data off the internet, and then does natural language processing to extract the english names from a text blob! Obviously we did not go very deep into the field. If you did, it would definitely be a lot more reading and understanding than just blindly following tutorials like I did above, and you probably would find it worth the time to set up a proper project. Nevertheless, this quick 5-minute run through of how to perform the basics of NLP is a fun way to get started whether or not you decide to take it further, and is only possible because of the Ammonite REPL! Play Framework Server /**   * Single-file play framework application!   */ load.ivy(""com.typesafe.play"" %% ""play"" % ""2.5.0"") load.ivy(""com.typesafe.play"" %% ""play-netty-server"" % ""2.5.0"") load.ivy(""org.scalaj"" %% ""scalaj-http"" % ""2.2.1"")  @  import play.core.server._, play.api.routing.sird._, play.api.mvc._ import scalaj.http._ val server = NettyServer.fromRouter(new ServerConfig(   rootDir = new java.io.File("".""),   port = Some(19000), sslPort = None,   address = ""0.0.0.0"", mode = play.api.Mode.Dev,   properties = System.getProperties,   configuration = play.api.Configuration(     ""play.server.netty"" -> Map(       ""maxInitialLineLength"" -> 4096,       ""maxHeaderSize"" -> 8192,       ""maxChunkSize"" -> 8192,       ""log.wire"" -> false,       ""eventLoopThreads"" -> 0,       ""transport"" -> ""jdk"",       ""option.child"" -> Map()     )   ) )) {   case GET(p""/hello/$to"") => Action { Results.Ok(s""Hello $to"") } } try {   println(Http(""http://localhost:19000/hello/bar"").asString.body) }finally{   server.stop() } Ammonite's script-running capabilities can also be used as a way to set up lightweight Scala projects without needing SBT or an IDE to get started. For example, here is a single-file Play Framework test that Spins up a HTTP server and Makes a single HTTP request against it and prints the response Shuts down the server. And can be run via ./amm PlayFramework.scala Although this is just a hello world example, you can easily keep the server running (instead of exiting after a test request) and extend it with more functionality, possibly splitting it into multiple Script Files. Ammonite-Ops Rock-solid Filesystem Operations Ammonite-Ops is a library to make common filesystem operations in Scala as concise and easy-to-use as from the Bash shell, while being robust enough to use in large applications without getting messy. It lives in the same repo as the Ammonite REPL, but can easily be used stand-alone in a normal SBT/maven project. To get started with Ammonite-Ops, add this to your build.sbt: libraryDependencies += ""com.lihaoyi"" %% ""ammonite-ops"" % ""0.6.2"" And you're all set! Here's an example of some common operations you can do with Ammonite-Ops import ammonite.ops._  // Pick the directory you want to work with, // relative to the process working dir val wd = cwd/'ops/'target/""scala-2.11""/""test-classes""/'example2  // Delete a file or folder, if it exists rm! wd  // Make a folder named ""folder"" mkdir! wd/'folder  // Copy a file or folder to a particular path cp(wd/'folder, wd/'folder1) // Copy a file or folder *into* another folder at a particular path // There's also `cp.over` which copies it to a path and stomps over // anything that was there before. cp.into(wd/'folder, wd/'folder1)   // List the current directory val listed = ls! wd  // Write to a file without pain! Necessary // enclosing directories are created automatically write(wd/'dir2/""file1.scala"", ""package example\nclass Foo{}\n"") write(wd/'dir2/""file2.scala"", ""package example\nclass Bar{}\n"")  // Rename all .scala files inside the folder d into .java files ls! wd/'dir2 | mv{case r""$x.scala"" => s""$x.java""}  // List files in a folder val renamed = ls! wd/'dir2  // Line-count of all .java files recursively in wd val lineCount = ls.rec! wd |? (_.ext == ""java"") | read.lines | (_.size) sum  // Find and concatenate all .java files directly in the working directory ls! wd/'dir2 |? (_.ext == ""java"") | read |> write! wd/'target/""bundled.java"" These examples make heavy use of Ammonite-Ops' Paths, Operations and Extensions to achieve their minimal, concise syntax As you can see, Ammonite-Ops replaces the common mess of boilerplate: def removeAll(path: String) = {   def getRecursively(f: java.io.File): Seq[java.io.File] = {     f.listFiles.filter(_.isDirectory).flatMap(getRecursively) ++ f.listFiles   }   getRecursively(new java.io.File(path)).foreach{f =>     println(f)     if (!f.delete())       throw new RuntimeException(""Failed to delete "" + f.getAbsolutePath)   }   new java.io.File(path).delete } removeAll(""target/folder/thing"") With a single, sleek expression: rm! cwd/'target/'folder/'thing That handles the common case for you: recursively deleting folders, not-failing if the file doesn't exist, etc. Note: Ammonite-Ops supports Windows experimentally, even if Ammonite-REPL does not. That means you can use these convenient filesystem operations and commands in your Scala programs that run on Windows. Try it out and let me know if there are problems. Paths Ammonite uses strongly-typed data-structures to represent filesystem paths. The two basic versions are: Path: an absolute path, starting from the root RelPath: a relative path, not rooted anywhere Generally, almost all commands take absolute Paths. These are basically defined as: case class Path private[ops] (root: java.nio.file.Path, segments: Vector[String]) With a number of useful operations that can be performed on them. Absolute paths can be created in a few ways: // Get the process' Current Working Directory. As a convention // the directory that ""this"" code cares about (which may differ // from the cwd) is called `wd` val wd = cwd  // A path nested inside `wd` wd/'folder/'file  // A path starting from the root root/'folder/'file  // A path with spaces or other special characters wd/""My Folder""/""My File.txt""  // Up one level from the wd wd/up  // Up two levels from the wd wd/up/up Note that there are no in-built operations to change the `cwd`. In general you should not need to: simply defining a new path, e.g. val target = cwd/'target Should be sufficient for most needs. Above, we made use of the cwd built-in path. There are a number of Paths built into Ammonite: cwd: The current working directory of the process. This can't be changed in Java, so if you need another path to work with the convention is to define a wd variable. root: The root of the filesystem. home: The home directory of the current user. tmp()/tmp.dir(): Creates a temporary file/folder and returns the path. RelPaths RelPaths represent relative paths. These are basically defined as: case class RelPath private[ops] (segments: Vector[String], ups: Int) The same data structure as Paths, except that they can represent a number of ups before the relative path is applied. They can be created in the following ways: // The path ""folder/file"" val rel1 = 'folder/'file val rel2 = 'folder/'file  // The path ""file""; will get converted to a RelPath by an implicit val rel3 = 'file  // The relative difference between two paths val target = cwd/'target/'file assert((target relativeTo cwd) == 'target/'file)  // `up`s get resolved automatically val minus = cwd relativeTo target val ups = up/up assert(minus == ups) In general, very few APIs take relative paths. Their main purpose is to be combined with absolute paths in order to create new absolute paths. e.g. val target = cwd/'target/'file val rel = target relativeTo cwd val newBase = root/'code/'server assert(newBase/rel == root/'code/'server/'target/'file) up is a relative path that comes in-built: val target = root/'target/'file assert(target/up == root/'target) Note that all paths, both relative and absolute, are always expressed in a canonical manner: assert((root/'folder/'file/up).toString == ""/folder"") // not ""/folder/file/..""  assert(('folder/'file/up).toString == ""folder"") // not ""folder/file/.."" So you don't need to worry about canonicalizing your paths before comparing them for equality or otherwise manipulating them. Path Operations Ammonite's paths are transparent data-structures, and you can always access the segments and ups directly. Nevertheless, Ammonite defines a number of useful operations that handle the common cases of dealing with these paths: sealed trait BasePath{   type ThisType <: BasePath   /**     * The individual path segments of this path.     */   def segments: Seq[String]    /**     * Combines this path with the given relative path, returning     * a path of the same type as this one (e.g. `Path` returns `Path`,     * `RelPath` returns `RelPath`     */   def /(subpath: RelPath): ThisType    /**     * Relativizes this path with the given `base` path, finding a     * relative path `p` such that base/p == this.     *     * Note that you can only relativize paths of the same type, e.g.     * `Path` & `Path` or `RelPath` & `RelPath`. In the case of `RelPath`,     * this can throw a [[PathError.NoRelativePath]] if there is no     * relative path that satisfies the above requirement in the general     * case.     */   def relativeTo(target: ThisType): RelPath    /**     * This path starts with the target path, including if it's identical     */   def startsWith(target: ThisType): Boolean    /**     * The last segment in this path. Very commonly used, e.g. it     * represents the name of the file/folder in filesystem paths     */   def last: String    /**     * Gives you the file extension of this path, or the empty     * string if there is no extension     */   def ext: String }  object BasePath {    def invalidChars = Set('/')   def checkSegment(s: String) = {     def fail(msg: String) = throw PathError.InvalidSegment(s, msg)     def considerStr =       ""use the Path(...) or RelPath(...) constructor calls to convert them. ""      s.find(BasePath.invalidChars) match{       case Some(c) => fail(         s""[$c] is not a valid character to appear in a path segment. "" +           ""If you want to parse an absolute or relative path that may have "" +           ""multiple segments, e.g. path-strings coming from external sources"" +           considerStr       )       case None =>     }     def externalStr = ""If you are dealing with path-strings coming from external sources, ""     s match{       case """" =>         fail(           ""Ammonite-Ops does not allow empty path segments "" +             externalStr + considerStr         )       case ""."" =>         fail(           ""Ammonite-Ops does not allow [.] as a path segment "" +             externalStr + considerStr         )       case "".."" =>         fail(           ""Ammonite-Ops does not allow [..] as a path segment "" +             externalStr +             considerStr +             ""If you want to use the `..` segment manually to represent going up "" +             ""one level in the path, use the `up` segment from `ammonite.ops.up` "" +             ""e.g. an external path foo/bar/../baz translates into 'foo/'bar/up/'baz.""         )       case _ =>     }   }   def chunkify(s: java.nio.file.Path) = {     import collection.JavaConversions._     s.iterator().map(_.toString).filter(_ != ""."").toVector   } }   /**   * Represents a value that is either an absolute [[Path]] or a   * relative [[ResourcePath]], and can be constructed from   */ In this definition, ThisType represents the same type as the current path; e.g. a Path's / returns a Path while a RelPath's / returns a RelPath. Similarly, you can only compare or subtract paths of the same type. Apart from RelPaths themselves, a number of other data structures are convertible into RelPaths when spliced into a path using /: Strings Symbolss Array[T]s where T is convertible into a RelPath Seq[T]s where T is convertible into a RelPath Constructing Paths Apart from built-ins like cwd or root or home, you can also construct Ammonite's Paths from Strings, java.io.Files or java.nio.file.Paths: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world""  assert(   RelPath(relStr) == 'hello/'cow,   Path(absStr) == root/'hello/'world  )  // You can also pass in java.io.File and java.nio.file.Path // objects instead of Strings when constructing paths val relIoFile = new java.io.File(relStr) val absNioFile = java.nio.file.Paths.get(absStr)  assert(   RelPath(relIoFile) == 'hello/'cow,   Path(absNioFile) == root/'hello/'world,   Path(relIoFile, root/'base) == root/'base/'hello/'cow ) Trying to construct invalid paths fails with exceptions: val relStr = ""hello/.."" intercept[java.lang.IllegalArgumentException]{   Path(relStr) }  val absStr = ""/hello"" intercept[java.lang.IllegalArgumentException]{   RelPath(absStr) }  val tooManyUpsStr = ""/hello/../.."" intercept[PathError.AbsolutePathOutsideRoot.type]{   Path(tooManyUpsStr) } As you can see, attempting to parse a relative path with Path or an absolute path with RelPath throws an exception. If you're uncertain about what kind of path you are getting, you could use BasePath to parse it: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world"" assert(   FilePath(relStr) == 'hello/'cow,   FilePath(absStr) == root/'hello/'world ) This converts it into a BasePath, which is either a Path or RelPath. It's then up to you to pattern-match on the types and decide what you want to do in each case. You can also pass in a second argument to Path(..., base). If the path being parsed is a relative path, this base will be used to coerce it into an absolute path: val relStr = ""hello/cow/world/.."" val absStr = ""/hello/world"" val basePath: FilePath = FilePath(relStr) assert(   Path(relStr, root/'base) == root/'base/'hello/'cow,   Path(absStr, root/'base) == root/'hello/'world,   Path(basePath, root/'base) == root/'base/'hello/'cow ) For example, if you wanted the common behavior of converting relative paths to absolute based on your current working directory, you can pass in cwd as the second argument to Path(...). Apart from passing in Strings or java.io.Files or java.nio.file.Paths, you can also pass in BasePaths you parsed early as a convenient way of converting it to a absolute path, if it isn't already one. In general, Ammonite is very picky about the distinction between relative and absolute paths, and doesn't allow ""automatic"" conversion between them based on current-working-directory the same way many other filesystem APIs (Bash, Java, Python, ...) do. Even in cases where it's uncertain, e.g. you're taking user input as a String, you have to either handle both possibilities with BasePath or explicitly choose to convert relative paths to absolute using some base. While this adds some boilerplate, it should overall result in more robust filesystem code that doesn't contain bugs like this one. Operations Paths not aren't interesting on their own, but serve as a base to use to perform filesystem operations in a concise and easy to use way. Here is a quick tour of the core capabilities that Ammonite-Ops provides: import ammonite.ops._  // Let's pick our working directory val wd: Path = cwd/'ops/'target/""scala-2.11""/""test-classes""/'example3  // And make sure it's empty rm! wd mkdir! wd  // Reading and writing to files is done through the read! and write! // You can write `Strings`, `Traversable[String]`s or `Array[Byte]`s write(wd/""file1.txt"", ""I am cow"") write(wd/""file2.txt"", Seq(""I am cow\n"", ""hear me moo"")) write(wd/'file3, ""I weigh twice as much as you"".getBytes)  // When reading, you can either `read!` a `String`, `read.lines!` to // get a `Vector[String]` or `read.bytes` to get an `Array[Byte]` read! wd/""file1.txt""        ==> ""I am cow"" read! wd/""file2.txt""        ==> ""I am cow\nhear me moo"" read.lines! wd/""file2.txt""  ==> Vector(""I am cow"", ""hear me moo"") read.bytes! wd/""file3""      ==> ""I weigh twice as much as you"".getBytes  // These operations are mirrored in `read.resource`, // `read.resource.lines` and `read.resource.bytes` to conveniently read // files from your classpath: val resourcePath = resource/'test/'ammonite/'ops/'folder/""file.txt"" read(resourcePath).length        ==> 18 read.bytes(resourcePath).length  ==> 18 read.lines(resourcePath).length  ==> 1  // You can read resources relative to any particular class, including // the ""current"" class by passing in `getClass` val relResourcePath = resource(getClass)/'folder/""file.txt"" read(relResourcePath).length        ==> 18 read.bytes(relResourcePath).length  ==> 18 read.lines(relResourcePath).length  ==> 1  // You can also read `InputStream`s val inputStream = new java.io.ByteArrayInputStream(   Array[Byte](104, 101, 108, 108, 111) ) read(inputStream)           ==> ""hello""  // By default, `write` fails if there is already a file in place. Use // `write.append` or `write.over` if you want to append-to/overwrite // any existing files write.append(wd/""file1.txt"", ""\nI eat grass"") write.over(wd/""file2.txt"", ""I am cow\nHere I stand"")  read! wd/""file1.txt""        ==> ""I am cow\nI eat grass"" read! wd/""file2.txt""        ==> ""I am cow\nHere I stand""  // You can create folders through `mkdir!`. This behaves the same as // `mkdir -p` in Bash, and creates and parents necessary val deep = wd/'this/'is/'very/'deep mkdir! deep // Writing to a file also creates neccessary parents write(deep/'deeeep/""file.txt"", ""I am cow"")  // `ls` provides a listing of every direct child of the given folder. // Both files and folders are included ls! wd    ==> Seq(wd/""file1.txt"", wd/""file2.txt"", wd/'file3, wd/'this)  // `ls.rec` does the same thing recursively ls.rec! deep ==> Seq(deep/'deeeep, deep/'deeeep/""file.txt"")  // You can move files or folders with `mv` and remove them with `rm!` ls! deep  ==> Seq(deep/'deeeep) mv(deep/'deeeep, deep/'renamed_deeeep) ls! deep  ==> Seq(deep/'renamed_deeeep)  // `mv.into` lets you move a file into a // particular folder, rather than to particular path mv.into(deep/'renamed_deeeep/""file.txt"", deep) ls! deep/'renamed_deeeep ==> Seq() ls! deep  ==> Seq(deep/""file.txt"", deep/'renamed_deeeep)  // `mv.over` lets you move a file to a particular path, but // if something was there before it stomps over it mv.over(deep/""file.txt"", deep/'renamed_deeeep) ls! deep  ==> Seq(deep/'renamed_deeeep) read! deep/'renamed_deeeep ==> ""I am cow"" // contents from file.txt  // `rm!` behaves the same as `rm -rf` in Bash, and deletes anything: // file, folder, even a folder filled with contents rm! deep/'renamed_deeeep rm! deep/""file.txt"" ls! deep  ==> Seq()  // You can stat paths to find out information about any file or // folder that exists there val info = stat! wd/""file1.txt"" info.isDir  ==> false info.isFile ==> true info.size   ==> 20 info.name   ==> ""file1.txt""  // Ammonite provides an implicit conversion from `Path` to // `stat`, so you can use these attributes directly (wd/""file1.txt"").size ==> 20  // You can also use `stat.full` which provides more information val fullInfo = stat.full(wd/""file1.txt"") fullInfo.ctime: FileTime fullInfo.atime: FileTime fullInfo.group: GroupPrincipal In these definitions, Op1 and Op2 are isomorphic to Function1 and Function2. The main difference is that ops can be called in two ways: rm(filepath) rm! filepath  The latter syntax allows you to use it more easily from the command line, where remembering to close all your parenthesis is a hassle. Indentation signifies nesting, e.g. in addition to write! you also have write.append! and write.over! Operator Reference All of these operations are pre-defined and strongly typed, so feel free to jump to their implementation to look at what they do or what else is available. Here's a shortlist of the one that may interest you: ls! path [doc] returning Vector[Path], and ls.iter! path returning a Iterator[Path] ls.rec! path [doc] and ls.rec.iter! read! path [doc] returning a String, and read.lines! path and read.bytes! path returning Seq[String] and Array[Byte]. You can also use the various read! commands for Reading Resources or reading java.io.InputStreams write(path, contents), [doc], which lets you write Strings, Array[Byte]s, and Seqs of those rm! path, [doc], roughly Bash's rm -rf mv(src, dest), [doc] cp(src, dest), [doc], roughly Bash's cp -r exists! path, [doc] stat! path, [doc] stat.full! path, [doc] ln(src, dest), [doc] kill(9)! processId, [doc] In general, each operator has sensible/safe defaults: rm and cp are recursive rm ignores the file if it doesn't exist all operations that create a file or folder (mkdir, write, mv) automatically create any necessary parent directories write also does not stomp over existing files by default. You need to use write.over In general, this should make these operations much easier to use; the defaults should cover the 99% use case without needing any special flags or fiddling. Extensions Ammonite-Ops contains a set of extension methods on common types, which serve no purpose other than to make things more concise. These turn Scala from a ""relatively-concise"" language into one as tight as Bash scripts, while still maintaining the high level of type-safety and maintainability that comes with Scala code. Traversable These extensions apply to any Traversable: Seqs, Lists, Arrays, and others. things | f is an alias for things map f things || f is an alias for things flatMap f things |? f is an alias for things filter f things |& f is an alias for things reduce f things |! f is an alias for things foreach f These should behave exactly the same as their implementations; their sole purpose is to make things more concise at the command-line. Pipeable thing |> f is an alias for f(thing) This lets you flip around the function and argument, and fits nicely into the Ammonite's | pipelines. Callable f! thing is an alias for f(thing) This is another syntax-saving extension, that makes it easy to call functions without having to constantly be opening and closing brackets. It does nothing else. Chaining The real value of Ammonite is the fact that you can pipe things together as easily as you could in Bash. No longer do you need to write reams of boilerplate. to accomplish simple tasks. Some of these chains are listed at the top of this readme, here are a few more fun examples: // Move all files inside the ""py"" folder out of it ls! wd/""py"" | mv.all*{case d/""py""/x => d/x }  // Find all dot-files in the current folder val dots = ls! wd |? (_.last(0) == '.')  // Find the names of the 10 largest files in the current working directory ls.rec! wd | (x => x.size -> x) sortBy (-_._1) take 10  // Sorted list of the most common words in your .scala source files def txt = ls.rec! wd |? (_.ext == ""scala"") | read def freq(s: Seq[String]) = s groupBy (x => x) mapValues (_.length) toSeq val map = txt || (_.split(""[^a-zA-Z0-9_]"")) |> freq sortBy (-_._2) As you can see, you can often compose elaborate operations entirely naturally using the available pipes, without needing to remember any special flags or techniques. Here's another example: // Ensure that we don't have any Scala files in the current working directory // which have lines more than 100 characters long, excluding generated sources // in `src_managed` folders.  def longLines(p: Path) =   (p, read.lines(p).zipWithIndex |? (_._1.length > 100) | (_._2))  val filesWithTooLongLines = (   ls.rec! cwd |? (_.ext == ""scala"")               | longLines               |? (_._2.length > 0)               |? (!_._1.segments.contains(""src_managed"")) )  assert(filesWithTooLongLines.length == 0) Reading Resources In addition to manipulating paths on the filesystem, you can also manipulate resource paths in order to read resources off of the Java classpath. By default, the path used to load resources is absolute, using the Thread.currentThread().getContextClassLoader. You can also pass in a classloader explicitly to the resource call: val contents = read(resource/'test/'ammonite/'ops/'folder/""file.txt"") assert(contents.contains(""file contents lols""))  val cl = getClass.getClassLoader val contents2 = read(resource(cl)/'test/'ammonite/'ops/'folder/""file.txt"") assert(contents2.contains(""file contents lols"")) If you want to load resources relative to a particular class, pass in a class for the resource to be relative, or getClass to get something relative to the current class. val cls = classOf[test.ammonite.ops.Testing] val contents = read! resource(cls)/'folder/""file.txt"" assert(contents.contains(""file contents lols""))  val contents2 = read! resource(getClass)/'folder/""file.txt"" assert(contents2.contains(""file contents lols"")) In both cases, reading resources is performed as if you did not pass a leading slash into the getResource(""foo/bar"") call. In the case of ClassLoader#getResource, passing in a leading slash is never valid, and in the case of Class#getResource, passing in a leading slash is equivalent to calling getResource on the ClassLoader. Ammonite-Ops ensures you only use the two valid cases in the API, without a leading slash, and not the two cases with a leading slash which are redundant (in the case of Class#getResource, which can be replaced by ClassLoader#getResource) or invalid (a leading slash with ClassLoader#getResource) Note that you can only read! from paths; you can't write to them or perform any other filesystem operations on them, since they're not really files. Note also that resources belong to classloaders, and you may have multiple classloaders in your application e.g. if you are running in a servlet or REPL. Make sure you use the correct classloader (or a class belonging to the correct classloader) to load the resources you want, or else it might not find them. Spawning Subprocesses Ammonite-Ops provides easy syntax for anyone who wants to spawn sub-processes, e.g. commands like ls or git commit -am ""wip"". This is provided through the % and %% operators, which are used as follows: @ import ammonite.ops._ @ import ammonite.ops.ImplicitWd._ @ %ls build.sbt	log		ops		readme		repl		terminal echo		modules		project		readme.md	target		shell res2: Int = 0 @ %%('ls) res3: CommandResult = build.sbt echo log modules ops project readme readme.md repl target terminal ...  In short, % lets you run a command as you would in bash, and dumps the output to standard-out in a similar way, returning the return-code. This lets you run git commands, edit files via vim, open ssh sessions or even start SBT or Python shells right from your Scala REPL! %% on the other hand is intended for programmatic usage: rather than printing to stdout, it returns a CommandResult, which contains the standard output .out and standard error .err of the subprocess. These provide helper methods to retrieve the stdout or stderr as a list of lines val res = %%('ls, ""ops/src/test/resources/testdata"") assert(res.out.lines == Seq(""File.txt"", ""folder1"", ""folder2"")) Or as a single string: val res = %%('ls, ""ops/src/test/resources/testdata"") assert(res.out.string == ""File.txt\nfolder1\nfolder2\n"") Or as an array of bytes: if(Unix()){   val res = %%('echo, ""abc"")   val listed = res.out.bytes   //        assert(listed == ""File.txt\nfolder\nfolder2\nFile.txt"".getBytes)   listed.toSeq } %% throws an ShelloutException containig the CommandResult if the return-code is non-zero. val ex = intercept[ShelloutException]{ %%('ls, ""does-not-exist"") } val res: CommandResult = ex.result assert(   res.exitCode != 0,   res.err.string.contains(""No such file or directory"") ) In both cases, you end up with a CommandResult can then be used however you like. You can also use backticks to execute commands which aren't valid Scala identifiers, e.g. @ %`ssh-add` Enter passphrase for /Users/haoyi/.ssh/id_rsa:  Lastly, you can also pass arguments into these subprocess calls, as Strings, Symbols or Seqs of Strings: @ %git 'branch   gh-pages   history * master   speedip res4: Int = 0  @ %%('git, 'branch) res5: CommandResult =   gh-pages   history * master   speedip  @ %%('git, checkout, ""master"") Already on 'master' res6: CommandResult = M	readme/Index.scalatex Your branch is up-to-date with 'origin/master'.  @ %git(""checkout"", 'master) M	readme/Index.scalatex Already on 'master' Your branch is up-to-date with 'origin/master'. res8: Int = 0  @ val stuff = List(""readme.md"", ""build.sbt"") stuff: List[String] = List(""readme.md"", ""build.sbt"") @ %('ls, '"".gitignore"", stuff) .gitignore	build.sbt	readme.md Ammonite-Ops currently does not provide many convenient ways of piping together multiple processes, but support may come in future if someone finds it useful enough to implement. % calls subprocesses in a way that is compatible with a normal terminal. That means you can easily call things like %vim to open a text editor, %python to open up a Python terminal, or %sbt to open up the SBT prompt! @ %python Python 2.7.6 (default, Sep  9 2014, 15:04:36) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> print ""Hello %s%s"" % (""World"", ""!""*3) Hello World!!! >>> ^D res3: Int = 0  @ %sbt [info] Loading global plugins from /Users/haoyi/.sbt/0.13/plugins [info] Updating {file:/Users/haoyi/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to haoyi (in build file:/Users/haoyi/) >  %% does not do this. Environment Variables Ammonite lets you pass in environment variables to subprocess calls; just pass them in as named arguments when you invoke the subprocess ia % or %%: val res0 = %%('bash, ""-c"", ""echo \""Hello$ENV_ARG\"""", ENV_ARG=12) assert(res0.out.lines == Seq(""Hello12""))  val res1 = %%('bash, ""-c"", ""echo \""Hello$ENV_ARG\"""", ENV_ARG=12) assert(res1.out.lines == Seq(""Hello12""))  val res2 = %%('bash, ""-c"", ""echo 'Hello$ENV_ARG'"", ENV_ARG=12) assert(res2.out.lines == Seq(""Hello$ENV_ARG""))  val res3 = %%('bash, ""-c"", ""echo 'Hello'$ENV_ARG"", ENV_ARG=123) assert(res3.out.lines == Seq(""Hello123"")) Invoking Files You can invoke files on disk using % and %% the same way you can invoke shell commands: val res: CommandResult =   %%(root/'bin/'bash, ""-c"", ""echo 'Hello'$ENV_ARG"", ENV_ARG=123)  assert(res.out.string.trim == ""Hello123"") Current Working Directory In Ammonite the current working directory is not a side-effect unlike in bash. Instead it is an argument to the command you are invoking. It can be passed in explicitly or implicitly. val res1 = %.ls()(cwd) // explicitly // or implicitly import ammonite.ops.ImplicitWd._ val res2 = %ls Note how passing it inexplicitly, you need to use a . before the command-name in order for it to parse properly. That's a limitation of the Scala syntax that isn't likely to change. Another limitation is that when invoking a file, you need to call .apply explicitly rather than relying on the plain-function-call syntax: if(Unix()){   val output = %%.apply(scriptFolder/'echo_with_wd, 'HELLO)(root/'usr)   assert(output.out.lines == Seq(""HELLO /usr"")) } Ammonite-Shell Replacing Bash for the 21st Century Ammonite-Shell is a rock-solid system shell that can replace Bash as the interface to your operating system, using Scala as the primary command and scripting language, running on the JVM. Apart from system operations, Ammonite-Shell provides the full-range of Java APIs for usage at the command-line, including loading libraries from Maven Central. Why would you want to use Ammonite-Shell instead of Bash? Possible reasons include: You can never remember the syntax to write an if-statement in Bash You are sick of googling the same set of inconsistent, ad-hoc commands over and over: ""obviously you need the flag -nrk 7 to sort by file size!"" You've seen Bash's dynamic/sloppy nature fail hard, and don't want your future work to fall victim to the same bugs You think that technology has improved in the last 38 years and a modern systems shell should be better than the shells of our forefathers If none of these apply to you, then likely you won't be interested. If any of these bullet points strikes a chord, then read on to get started. For more discussion about why this project exists, take a look at the presentation slides for Beyond Bash: shell scripting in a typed, OO language, presented at Scala by the Bay 2015, or check out the section on Design Decisions & Tradeoffs. Getting Ammonite-Shell To begin using Ammonite-Shell, simply download the default predef.scala to configure your REPL to be a usable systems shell before downloading the Ammonite-REPL executable (below): $ mkdir ~/.ammonite && curl -L -o ~/.ammonite/predef.scala https://git.io/vo4wx $ curl -L -o amm https://git.io/vo4w5 && chmod +x amm && ./amm  You can then start using Ammonite as a replacement for Bash: Shell Basics Ammonite-Shell isn't backwards compatible with Bash. It isn't even the same language, giving you access to all of Scala instead of the quirky Bash scripting language. Nevertheless, lots of things you'd expect in Bash turn up in Ammonite-Shell: Working Directory bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite haoyi-Ammonite@ wd  res0: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite Bash's pwd is instead called wd. Instead of being a subprocess that prints to stdout, wd is simply a variable holding the working directory. As you can see, the path syntax is also different: as an absolute path, wd must start from root and the path segments must be quoted as Scala ""string""s or 'symbols. Apart from that, however, it is basically the same. The documentation about Paths goes over the syntax and semantics of Paths in more detail. You can navigate around the filesystem using cd!, instead of Bash's cd: bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite bash$ cd target bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite/target bash$ cd .. bash$ pwd /Users/haoyi/Dropbox (Personal)/Workspace/Ammonite haoyi-Ammonite@ wd  res0: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite haoyi-Ammonite@ cd! 'target  res1: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'target haoyi-target@ wd  res2: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/'target haoyi-target@ cd! up  res3: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite haoyi-Ammonite@ wd  res4: Path = root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite Listing Files bash$ ls LICENSE appveyor.yml build.sbt integration internals-docs ops project readme readme.md repl shell sshd target terminal haoyi-Ammonite@ ls!  res0: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  Bash's ls syntax is tweaked slightly to become ls!. Apart from that, it basically does the same thing. Listing files in other folders behaves similarly: bash$ ls project Constants.scala build.properties build.sbt project target haoyi-Ammonite@ ls! 'project  res0: LsSeq =  ""Constants.scala""   ""build.properties""  ""build.sbt""         'project            'target  bash$ ls project/target config-classes resolution-cache scala-2.10 streams haoyi-Ammonite@ ls! 'project/'target  res0: LsSeq =  ""config-classes""    ""resolution-cache""  ""scala-2.10""        'streams  Again, we have to use the quoted 'symbol/""string"" syntax when defining Paths, but otherwise it behaves identically. You can press <tab> at any point after a / or halfway through a file-name to auto-complete it, just like in Bash. Listing recursively is done via ls.rec, instead of find: bash$ find ops/src/main ops/src/main ops/src/main/scala ops/src/main/scala/ammonite ops/src/main/scala/ammonite/ops ops/src/main/scala/ammonite/ops/Extensions.scala ops/src/main/scala/ammonite/ops/FileOps.scala ops/src/main/scala/ammonite/ops/Model.scala ops/src/main/scala/ammonite/ops/package.scala ops/src/main/scala/ammonite/ops/Path.scala ops/src/main/scala/ammonite/ops/PathUtils.scala ops/src/main/scala/ammonite/ops/Shellout.scala haoyi-Ammonite@ ls.rec! 'ops/'src/'main  res0: LsSeq =  'scala                                            'scala/'ammonite/'ops/""Model.scala"" 'scala/'ammonite                                  'scala/'ammonite/'ops/""Path.scala"" 'scala/'ammonite/'ops                             'scala/'ammonite/'ops/""PathUtils.scala"" 'scala/'ammonite/'ops/""Extensions.scala""          'scala/'ammonite/'ops/""Shellout.scala"" 'scala/'ammonite/'ops/""FileOps.scala""             'scala/'ammonite/'ops/""package.scala""  ls, ls.rec and other commands are all functions defined by Ammonite-Ops. Filesystem Operations Ammonite-Shell uses Ammonite-Ops to provide a nice API to use filesystem operations. The default setup will import ammonite.ops._ into your Ammonite-REPL, gives the nice path-completion shown above, and also provides some additional command-line-friendly functionality on top of the default Ammonite-Ops commands: bash$ mkdir target/test bash$ echo ""hello"" > target/test/hello.txt bash$ cat target/test/hello.txt hello bash$ ls target/test hello.txt bash$ cp target/test/hello.txt target/test/hello2.txt bash$ ls target/test hello.txt hello2.txt bash$ mv target/test/hello.txt target/test/hello3.txt bash$ ls target/test hello2.txt hello3.txt bash$ rm -rf target/test haoyi-Ammonite@ mkdir! 'target/'test   haoyi-Ammonite@ write('target/'test/""hello.txt"", ""hello"")   haoyi-Ammonite@ ls! 'target/'test  res2: LsSeq =  ""hello.txt""  haoyi-Ammonite@ cp('target/'test/""hello.txt"", 'target/'test/""hello2.txt"")   haoyi-Ammonite@ ls! 'target/'test  res4: LsSeq =  ""hello.txt""   ""hello2.txt""  haoyi-Ammonite@ mv('target/'test/""hello.txt"", 'target/'test/""hello3.txt"")   haoyi-Ammonite@ ls! 'target/'test  res6: LsSeq =  ""hello2.txt""  ""hello3.txt""  haoyi-Ammonite@ rm! 'target/'test   Piping Ammonite allows piping similar to how Bash does it. Unlike Bash, Ammonite has a variety of pipes you can use that do different things: things | f is an alias for things map f things || f is an alias for things flatMap f things |? f is an alias for things filter f things |& f is an alias for things reduce f things |! f is an alias for things foreach f For example, this is how you can get the dot-files in the current directory: bash$ ls -a | grep ""^\."" . .. .git .gitignore .idea .travis.yml haoyi-Ammonite@ ls! cwd |? (_.last(0) == '.')  res0: Seq[Path] = List(   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".git"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".gitignore"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".idea"",   root/'Users/'haoyi/""Dropbox (Personal)""/'Workspace/'Ammonite/"".travis.yml"" ) Here, we're using the |? pipe, which basically performs a filter on the paths coming in on the left. In this case, we're checking that for each path, the first character of the last segment of that path is the character '.'. This is slightly more verbose than Bash the bash equivalent shown above, but not by too much. Here is how to find the largest 3 files in a given directory tree: bash$ find ./repl/src -ls | sort -nrk 7 | head -3 169433236        8 -rw-r--r--    1 haoyi            DROPBOX\Domain Users     3977 Jun 15 12:32 ./repl/src/main/scala/ammonite/repl/Repl.scala 169430434       24 -rw-r--r--    1 haoyi            DROPBOX\Domain Users     8492 Jun 15 12:28 ./repl/src/test/scala/ammonite/repl/session/ImportTests.scala 169430433       24 -rw-r--r--    1 haoyi            DROPBOX\Domain Users    10727 Jun 15 12:28 ./repl/src/main/scala/ammonite/repl/interp/Preprocessor.scala haoyi-Ammonite@ ls.rec! wd/'repl/'src | (x => x.size -> x.last) sortBy (-_._1) take 3  res0: Seq[(Long, String)] = List((340324L, ""Resume.docx""), (227570L, ""SystemShell.png""), (208328L, ""Highlighting.png"")) And lastly, here is how to performa recursive line count of all the Scala files in your current directory tree: bash$ find ./ops/src/main -name '*.scala' | xargs wc -l      143 ./ops/src/main/scala/ammonite/ops/Extensions.scala      418 ./ops/src/main/scala/ammonite/ops/FileOps.scala      137 ./ops/src/main/scala/ammonite/ops/Model.scala      119 ./ops/src/main/scala/ammonite/ops/package.scala      369 ./ops/src/main/scala/ammonite/ops/Path.scala      101 ./ops/src/main/scala/ammonite/ops/PathUtils.scala      194 ./ops/src/main/scala/ammonite/ops/Shellout.scala     1481 total haoyi-Ammonite@ ls.rec! wd/'ops/'src/'main |? (_.ext == ""scala"") | read.lines | (_.size) sum  res0: Int = 1483 For more examples of how to use Ammonite's pipes, check out the section on Extensions and Chaining Subprocesses Ammonite provides a convenient way to spawn subprocesses using the % and %% commands: %cmd(arg1, arg2): Spawn a subprocess with the command cmd and command-line arguments arg1, arg2. print out any stdout or stderr, take any input from the current console, and return the exit code when all is done. %%cmd(arg1, arg2): Spawn a subprocess similar to using %, but return the stdout of the subprocess as a String, and throw an exception if the exit code is non-zero. For example, this is how you use the bash command to run a standalone bash script in Bash and Ammonite: bash$ bash ops/src/test/resources/scripts/echo HELLO HELLO haoyi-Ammonite@ %bash('ops/'src/'test/'resources/'scripts/'echo, ""HELLO"")  HELLO  Note that apart from quoting each path segment as a 'symbol, we also need to quote ""HELLO"" as a string. That makes things slightly more verbose than a traditional shell, but also makes it much clearer when arguments are literals v.s. variables. If you are only passing a single argument, or no arguments, Scala allows you to leave off parentheses, as shown: bash$ git branch   389   gh-pages   import-hooks * master   subprocess haoyi-Ammonite@ %git 'branch    389   gh-pages   import-hooks * master   subprocess  bash$ date Wed Jun 15 13:08:19 SGT 2016 haoyi-Ammonite@ %date  Wed Jun 15 13:08:26 SGT 2016  You can use Ammonite-Ops' support for Spawning Subprocesses to call any external programs, even interactive ones like Python or SBT! @ %python Python 2.7.6 (default, Sep  9 2014, 15:04:36) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> print ""Hello %s%s"" % (""World"", ""!""*3) Hello World!!! >>> ^D res3: Int = 0  @ %sbt [info] Loading global plugins from /Users/haoyi/.sbt/0.13/plugins [info] Updating {file:/Users/haoyi/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to haoyi (in build file:/Users/haoyi/) >  Scripting Ammonite-Shell uses Scala as its command and scripting language. Although the commands seem short and concise, you have the full power of the language available at any time. This lets you do things that are difficult or unfeasible to do when using a traditional shell like Bash. Scala Scripting Since Ammonite-Shell runs Scala code, you can perform math: haoyi-Ammonite@ (1 + 2) * 3  res0: Int = 9 haoyi-Ammonite@ math.pow(4, 4)  res1: Double = 256.0 Assign things to values (vals): haoyi-Ammonite@ val x = (1 + 2) * 3  x: Int = 9 haoyi-Ammonite@ x + x  res1: Int = 18 Define re-usable functions: haoyi-Ammonite@ def addMul(x: Int) = (x + 2) * 3  defined function addMul haoyi-Ammonite@ addMul(5)  res1: Int = 21 haoyi-Ammonite@ addMul(5) + 1  res2: Int = 22 haoyi-Ammonite@ addMul(5 + 1)  res3: Int = 24 Or make use of mutable vars, conditionals or loops: haoyi-Ammonite@ var total = 0  total: Int = 0 haoyi-Ammonite@ for(i <- 0 until 100){ if (i % 2 == 0) total += 1 }   haoyi-Ammonite@ total  res2: Int = 50 Typed Values In Ammonite-Shell, everything is a typed value and not just a stream of bytes as is the case in Bash. That means you can assign them to variables and call methods on them just like you can in any programming language: haoyi-Ammonite@ val files = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ val count = files.length  count: Int = 18 As is the case in Scala, you can annotate types. haoyi-Ammonite@ val files: LsSeq = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ val count: Int = files.length  count: Int = 18 This is often not required (e.g. in the earlier example), since Scala has type inference, but it may make your code clearer. Furthermore, if you make a mistake, having types annotated will help the compiler give a more specific error message. The fact that variables are typed means if you try to perform the wrong operation on a variable, you get an error even before the code runs: haoyi-Ammonite@ val files = ls! wd  files: LsSeq =  "".git""              'LICENSE            ""internals-docs""    ""readme.md""         'target "".gitignore""        ""appveyor.yml""      'ops                'repl               'terminal "".idea""             ""build.sbt""         'project            'shell "".travis.yml""       'integration        'readme             'sshd  haoyi-Ammonite@ ls + 123  cmd1.scala:1: type mismatch;  found   : Int(123)  required: String val res1 = ls + 123                 ^ Compilation Failed The fact that Ammonite-Shell uses typed, structured values instead of byte streams makes a lot of things easier. For example, all the common data structures like Arrays and Maps are present: haoyi-Ammonite@ val numbers = Array(1, 3, 6, 10)  numbers: Array[Int] = Array(1, 3, 6, 10) haoyi-Ammonite@ numbers(0)  res1: Int = 1 haoyi-Ammonite@ numbers(3)  res2: Int = 10 haoyi-Ammonite@ numbers.sum  res3: Int = 20 haoyi-Ammonite@ numbers(3) = 100   haoyi-Ammonite@ numbers.sum  res5: Int = 110 haoyi-Ammonite@ val scores = Map(""txt"" -> 5, ""scala"" -> 0)  scores: Map[String, Int] = Map(""txt"" -> 5, ""scala"" -> 0) haoyi-Ammonite@ scores(""txt"")  res7: Int = 5 Naturally, these data structures are typed too! Trying to put the wrong sort of value inside of them results in compilation errors before the code gets a chance to run: haoyi-Ammonite@ val numbers = Array(1, 3, 6, 10)  numbers: Array[Int] = Array(1, 3, 6, 10) haoyi-Ammonite@ val myValue = ""3""  myValue: String = ""3"" haoyi-Ammonite@ numbers(myValue) // Doesn't work  cmd2.scala:1: type mismatch;  found   : String  required: Int val res2 = numbers(myValue) // Doesn't work                    ^ Compilation Failed haoyi-Ammonite@ numbers(1) = myValue // Also doesn't work  cmd2.scala:1: type mismatch;  found   : String  required: Int val res2 = numbers(1) = myValue // Also doesn't work                         ^ Compilation Failed haoyi-Ammonite@  // Need to convert the string to an Int  haoyi-Ammonite@ numbers(myValue.toInt)  res2: Int = 10 haoyi-Ammonite@ numbers(1) = myValue.toInt   haoyi-Ammonite@ numbers(1) = ""2"".toInt   In general, apart from the filesystem-specific commands, you should be able to do anything you would expect to be able to do in a Scala shell or Java project. This documentation isn't intended to be a full tutorial on the Scala language, check out the Scala Documentation if you want to learn more! Scala/Java APIs Apart from the pipe operators described in the earlier section on Piping, Ammonite-Shell allows you to call any valid Scala method on any value; it's just Scala after all! Here's an example using normal Scala collection operations to deal with a list of files, counting how many files exist for each extension: haoyi-Ammonite@ val allFiles = ls.rec! 'ops/'src/'test/'resources  allFiles: LsSeq =  'scripts 'test 'testdata 'scripts/'echo 'scripts/'echo_with_wd 'test/'ammonite 'testdata/""File.txt"" 'testdata/'folder1 'testdata/'folder2 'test/'ammonite/'ops 'testdata/'folder1/""Yoghurt Curds Cream Cheese.txt"" 'testdata/'folder2/'folder2a 'testdata/'folder2/'folder2b 'test/'ammonite/'ops/'folder 'testdata/'folder2/'folder2a/""I am.txt"" 'testdata/'folder2/'folder2b/""b.txt"" 'test/'ammonite/'ops/'folder/""file.txt""  haoyi-Ammonite@ val extensionCounts = allFiles.groupBy(_.ext).mapValues(_.length)  extensionCounts: Map[String, Int] = Map(""txt"" -> 5, """" -> 12) Any Java APIs are likewise available: haoyi-Ammonite@ System.out.println(""Hello from Java!"")  Hello from Java!  haoyi-Ammonite@ import java.util._  import java.util._ haoyi-Ammonite@ val date = new Date()  date: Date = Wed Jun 15 13:10:57 SGT 2016 haoyi-Ammonite@ date.getDay()  res3: Int = 3 In fact, Ammonite-Shell allows you to ask for any published third-party Java/Scala library for usage in the shell, and have them downloaded, automatically cached, and made available for use. e.g. we can load popular libraries like Google Guava and using it in the shell: haoyi-Ammonite@ import com.google.common.collect.ImmutableBiMap // Doesn't work  cmd0.scala:1: object google is not a member of package com import com.google.common.collect.ImmutableBiMap // Doesn't work            ^ Compilation Failed haoyi-Ammonite@ load.ivy(""com.google.guava"" % ""guava"" % ""18.0"") // Load from Maven Central  :: loading settings :: url = jar:file:/Users/haoyi/Dropbox%20(Personal)/Workspace/Ammonite/repl/target/scala-2.11/ammonite-repl-0.6.2-2.11.8!/org/apache/ivy/core/settings/ivysettings.xml :: resolving dependencies :: com.google.guava#guava-caller;working 	confs: [default] 	found com.google.guava#guava;18.0 in central 	found com.google.code.findbugs#jsr305;1.3.9 in chain-resolver  haoyi-Ammonite@ import com.google.common.collect.ImmutableBiMap // Works now  import com.google.common.collect.ImmutableBiMap // Works now haoyi-Ammonite@ val bimap = ImmutableBiMap.of(1, ""one"", 2, ""two"", 3, ""three"")  bimap: ImmutableBiMap[Int, String] = {1=one, 2=two, 3=three} haoyi-Ammonite@ bimap.get(1)  res3: String = ""one"" haoyi-Ammonite@ bimap.inverse.get(""two"")  res4: Int = 2 Or Joda Time: haoyi-Ammonite@ load.ivy(""joda-time"" % ""joda-time"" % ""2.8.2"")   :: loading settings :: url = jar:file:/Users/haoyi/Dropbox%20(Personal)/Workspace/Ammonite/repl/target/scala-2.11/ammonite-repl-0.6.2-2.11.8!/org/apache/ivy/core/settings/ivysettings.xml :: resolving dependencies :: joda-time#joda-time-caller;working 	confs: [default] 	found joda-time#joda-time;2.8.2 in central 	found org.joda#joda-convert;1.2 in central  haoyi-Ammonite@ import org.joda.time.{DateTime, Period, Duration}  import org.joda.time.{DateTime, Period, Duration} haoyi-Ammonite@ val dt = new DateTime(2005, 3, 26, 12, 0, 0, 0)  dt: DateTime = 2005-03-26T12:00:00.000+08:00 haoyi-Ammonite@ val plusPeriod = dt.plus(Period.days(1))  plusPeriod: DateTime = 2005-03-27T12:00:00.000+08:00 haoyi-Ammonite@ dt.plus(new Duration(24L*60L*60L*1000L))  res4: DateTime = 2005-03-27T12:00:00.000+08:00 See the section on Artifact Loading to learn more. Writing/Loading Scripts You can write scripts in the same way you write commants, and load them using the load.script(...) and load.module(...) methods. To read more about this, check out the documentation on Script Files. Design Decisions & Tradeoffs Ammonite-Shell takes a fundamentally different architecture from traditional shells, or even more-modern shell-alternatives. Significant differences include: The command & scripting language is a standard, well-known application language (Scala) rather than one specially-designed for the shell The shell runs on the JVM, and can execute or integrate-with arbitrary Java/JVM code or libraries. In this section we'll examine each of these decisions and their consequences in turn. As the incumbents in this space, we'll be looking at traditional system shells like Bash, Zsh or Fish, as well as popular non-system REPLs like the Python/IPython REPL. Scala as the Language The use of Scala as the command & scripting language is unusual among shells, for many reasons. Firstly, most shells implement their own, purpose built language: Bash, Zsh, Fish, and even more obscure ones like Xonsh each implement their own language. Secondly, all of these languages are extremely dynamic, and apart from those most popular languages with REPLs (Python, Ruby, Javascript, ...) tend to be dynamical, interpreted languages. Scala falls at the opposite end of the spectrum: statically typed and compiled. Scala brings many changes over using traditional dynamic, interpreted REPL languages: The code being entered in the shell takes time to compile. The first command easily takes 3-4 to compile, and even when the compiler is ""warm"" there is a 0.2-0.3 second delay before any command begins executing. Once compiled, the code runs extremely fast: compute-intensive code easily runs 50-100x faster in Scala vs Python or Bash, once you've paid the cost of compiling it. Many mistakes get caught even before a command begins executing. This is less valuable for small commands that execute quickly, but for slower commands processing more data, it is nice to get an error after 0.2s of compilation rather than 10s into execution. Apart from the differences between Scala and dynamic languages (Python, Ruby, etc.) for REPL usage, Scala is even further away from the sort of ad-hoc, ultra-dynamic languages most often associated with traditional shells (Bash, sh, zsh, etc.). In particular: Scala provides a set of proper data structures for you to work with. Rather than just byte-streams, you have unicode Strings, proper numbers like Int or Double, absolute Paths and relative RelPaths, Arrays, Maps, Iterators and all sorts of other handy data-structures. Many commands return objects which have fields: this sounds simple until you realize that none of bash/zsh/fish behave this way. Scala is a general-purpose language: you can do math, you can work with Strings, you can write non-trivial algorithms quickly and easily. While this is not surprising coming from a Python REPL, these simple tasks are difficult-to-impossible in traditional system shells like Bash. Scala runs most code in the same process. While you can shell-out to subprocesses in Ammonite using the % syntax, most commands like ls! and rm! are simple functions living in-process rather than in separate processes. This reduces the overhead as compared to spawning new processes each time, but does cause some additional risk: if a command causes the process to crash hard, the entire shell fails. In bash, only that command would fail. The latter set of tradeoffs would be also present in many of the shell-replacements written in dynamic languages, like Xonsh which is written in Python. The earlier set, on the other hand, are pretty unique to Ammonite using Scala. There are both positive and negative points in this list. Running on the JVM Running Ammonite directly on the JVM again is very different from how most shells work: most have their own scripting language, and their own interpreter. Most are implemented in C. What is it like running your code directly as bytecode on the JVM? Here are some of the negatives: You get JVM boot time; although some of the initial several-second delay is due to the Scala compiler's slowness, some of it is also due to the cost of JVM classloading and initialization. While a hello-world JVM project loads instantly, one which uses a large number of class-files takes longer. In contrast, shells written in C load basically instantly. You get the JVM bloat: Ammonite, implemented in only a few thousand lines of code, wraps up to become a 30mb .jar file. That's already larger than most other shells out there, and gets >100mb larger if you bundle the JVM along with it! In general, the JVM class-file format is bloated and inefficient, and there is no way to exclude to numerous un-needed parts of the JVM during the initial download. Project Jigsaw will help with this when it finally lands in Java 9. Ammonite uses hundreds of megabytes (~500mb at last count) of memory, again orders-of-magnitude more than an interpreter written in C or Python. The JVM has traditionally been a very pointer-heavy, memory-intensive platform for running code, and it shows. Project Valhalla would help with this, also scheduled to land in Java 9. In general, the JVM has traditionally been used as a server-side platform for long-runing services, and its slow-startup and bloated disk/memory footprints are a symptom of that. Running on the JVM also has some upsides, though: Ammonite code runs ridiculously fast, once you've paid 0.2-0.3s for its compilation. 50x faster than Python or Bash! This is not a trivial multiplier, and really makes a different if you're dealing with non-trivial data sets: a computation that takes a minute in Ammonite might take an hour if done at the Python REPL! This means that a moderately-large computation which may require special tools, libraries or optimizations to perform in Python might be trivial to implement naively in Ammonite while still enjoying reasonable speed. Ammonite can make use of any JVM APIs, and there are a lot of them! The JVM is a general-purpose platform for a general purpose language (Java), and thus has APIs for doing all sorts of things: dealing with dates and times, math, networks, threads, and many other things. While you may not always need all of these capabilities, it is nice to have them at your disposal where necessary. Ammonite can make use of any Java/JVM libraries, and the excelent infrastructure used by Java developers to download and manage them! Any library is just a load.ivy away. Need to parse Python source into an AST? Load Jython and just do it. Need a high-performance web server? Load Akka-HTTP. Need some data someone stored in a YAML file? Load SnakeYAML and parse it. You don't even need to download and manage these libraries yourself: just load.ivy them from Ammonite, and Java's excellent dependency-management infrastructure will download them (along with any transitive dependencies!), cache them locally, and make them available to your code. There are both pros and cons with running Ammonite on the JVM: we gain its heavy startup/memory overhead, but also get access to its high-performance JIT, massive ecosystem of available packages. Goals of Ammonite-Shell Overall, Ammonite-Shell blurs the line between a ""programming language REPL"" like IPython or Ruby's IRB and a ""system shell"" like Bash or Zsh. Like system shells, Ammonite-Shell provides concise filesystem operations, path-completion, and easy spawning of subprocesses. Like programming language REPLs, it provides a full-fledged, general-purpose language for you to use, rather than a crippled cut-down command-language that is available in most system shells. The goal is to provide something general enough to use as both a system shell and a general-purpose programming language. Traditionally, there has always been some tension when deciding between these: Should I write this script in Bash? I'm already using Bash as my shell It's getting complicated, I can't follow the logic in Bash. Should I re-write it in Python? If I write it in Python, I'll need to deal with argument-parsing and forwarding between Bash and Python and Bash, which is annoying Since my scripts are in Python, should I use Python/IPython as my shell instead of Bash when dealing with these things? But if I use Python/IPython as my shell, basic filesystem operations becomes impossible Traditionally, there really has been no good answer to this dilemma: whether you use Bash or Python to write your scripts, whether you use Bash or Python as your shell, there is always something frustrating about the set-up. With Ammonite-Shell, there is no dilemma. You can use the same concise, general-purpose language for your shell as you would for your scripts, large or small. In Ammonite-Shell, you can concisely deal with files at the command-line with the same language you use to write maintainable scripts, large or small, and the same language that you use to write rock-solid application code. Reference Community Ammonite is primarily maintained by Li Haoyi, with a lot of help from Laszlo Mero over the summer through Google Summer of Code, and help from many other contributors. We have an active Gitter channel and a mailing list: Gitter Channel Mailing List Talks I've also given a number of talks about Ammonite at conferences: Rock-solid Shell Scripting in Scala, at Scala Exchange 2015 Shell scripting in a typed, OO language, at New Object Oriented Languages 2015. Beyond Bash: shell scripting in a typed, OO language, at Scala by the Bay 2015. Slides Scaladoc Here's the Scaladoc for the various projects: Ammonite-Ops Ammonite-Terminal Ammonite-Repl Ammonite-Shell Ammonite-Sshd Although it's best to read the documentation on this page to learn how to use these projects, the Scaladoc is still useful as a reference. Changelog 0.6.2 Fix #403: Errors are silently swallowed in 0.6.1 0.6.1 Fixed #400: load.exec in predef.scala causes NPE in 0.5.9 or 0.6.0 Fixed #398: by default, using Ammonite inside a SBT project uses the old `~/.ammonite/` storage folder, not the `InMemory` storage system. This restores the pre-0.5.9 behavior Fix regression causing BACKSPACE to not work while performing a history-search More internal refactoring 0.6.0 Made browse use the process current working directory if there's no implicit path in scope, since most times it doesn't matter Make the welcome banner ""Welcome to the Ammonite Repl..."" customizable Fixed bug where triggering autocomplete resulted in a broken REPL session 0.5.9 Introduced the desugar helper to Ammonite-REPL, letting you easily see what the compiler is transforming your code into before it gets run. Prefix _root_ to imports from packages, which should reduce the chance of naming collisions Improved source locations for error messages: now failures in scripts have a filename matching the name of the script (instead of Main.scala), and line numbers matching the original line numbers (instead of the line numbers in the synthetic code) thanks to Abhishek Kumar Failures in scripts run using Ammonite from the command line or via load.module should show only the meaningful error and not irrelevant internal stacktraces Wrapper names are now greatly simplified; now the names of wrapper objects for scripts match the name of the script (e.g. MyScript) rather than based on the code hash (e.g. cache5a8890cd7c2ab07eea3fe8d605c7e188) Placed most synthetic code into packages; loaded scripts go into ammonite.scripts and code entered at the REPL goes in ammonite.session. Wrote some basic Internals Documentation in case people want to read about the internal workings of Ammonite in a way that's easier than digging through tons of code. Changed the interface for Embedding Ammonite to make configuring the Ammonite REPL before invoking it programmatically much more consistent. Moved tools such as grep, time, browse from Ammonite-Shell into the base Ammonite-REPL, and made them imported by default, so everyone can enjoy them by default. Imported the various pipe operations from Ammonite-Ops: aliasing .map as |, .filter as |?, etc. to make it more convenient to use tools like grep from the base REPL Massive internal refactors to try and clean up the Ammonite codebase and get it ready for future work; if you find any bugs please report them! Fixed #393: REPL requires two carriage returns to move to a new line Compiler settings set in the predef now get preserved when the session starts, and when replacing compilers, thanks to Rob Norris Fixed #395: Fixed resolver pattern for local ivy, thanks to Aish Fenton Fixed the kill command in Ammonite-Ops, thanks to 杨博 Unknown Ansi escape codes now have their '\u001b' escape character removed, rather than messing up the REPL rendering 0.5.8 write has been generalized to work on any combination of Array, Traversable and Iterator. e.g. write(foo: Iterator[Iterator[Array[String]]]) write no longer inserts newlines between items by default. Introduced the browse helper to Ammonite-Shell, letting you easily open up large data structures in external editors like Vim or Emacs to browse them without spamming the console Improved the error messages for invalid Path segments to make them more specific and suggest alternatives to what a user is trying to do. Broke out the FilePath sub-trait from the BasePath trait, to differentiate those BasePaths are filesystem paths and can be constructed from java.nio.file.Path or java.io.Files (RelPath and Path)from those which can't (ResourcePath) Path.makeTemp has been renamed tmp() and tmp.dir() Arrow-keys now work properly in the previously odd case where they were creating \u033O{A,B,C,D}"" codes instead of \u033[{A,B,C,D} codes Converted all string-encoding methods to take a scala.io.Codec instead of a String or Charset, letting you pass in either of those types and having it be implicitly converted. 0.5.7 Improved performance of various read! commands to be competitive with java.nio (#362) read! and read.lines! now take an optional charset, passed via read(file, charSet: String) or read.lines(file, charSet: String) which defaults to ""utf-8"" Make read! resource read from the Thread.currentThread.getContextClassLoader by default, fixing #348 Re-organize Reading Resources in Ammonite-Ops to allow proper handling of absolute and relative resources by passing in Classs or ClassLoaders Make read! work on InputStreams Renamed InputPath to Readable, a more appropiate name now that it works on two different non-path entities (resources and InputStreams) Bump uPickle and PPrint to 0.3.9 Now published for Scala 2.11.8, thanks to Clark Kampfe 0.5.6 Fixed #341: stack overflow when lsing large directories Fixed regression preventing you from running scripts via ./amm using relative paths #353 Ammonite should be more robust when interacting with other compiler plugins Fixed #352: imports now don't get improperly collapsed, and defining a value called repl no longer borks your session. Improved readline-emulation of AmmoniteFrontEnd: Ctrl-T and Alt-T now properly transpose characters and words, and the kill-ring now properly aggregates multiple consecutive kills. Added asserts to rm cp and mv to prevent you from removing the root folder, or copying/moving folders into themselves. Command-line-undo via Ctrl - and redo via Esc/Alt - are now supported. Page-up and Page-down (fn-up and fn-down on Macs) scrolls through history when used at the start/end of input, allowing you to use page-up/page-down to quickly scroll through history with lots of multi-line blocks. 0.5.5 Experimental support for Ammonite-Ops in Windows! I haven't tested it but basic CI passes here, so try it out and let me know if there are problems (#120) Changes around Ammonite-Ops's definition of Paths: they now wrap a java.nio.file.Path (#346), and thus can be used on Windows, on multiple drives, or with virtual filesystems such as JimFS. Construction of Paths from various types (Strings, java.nio.file.Path, java.io.File) is much more well behaved & consistent now. See Constructing Paths for details. read.resource! root/'foo is now read! resource/'foo Parser improvements which fix bugs when trying to write some multi-line snippets #343 cp and mv now have .into and .over modes #206 Wrapping content is automatically shifted onto a new line, to avoid problems when copying and pasting #205 Thrown exceptions are now made available to you in the REPL under the lastException variable #289, in case you need more metadata from it or you want the stack-trace of a non-printed-by-default exception (e.g. ThreadDeath due to Ctrl-C). Thanks to coderabhishek! Fixed #280: Ammonite REPL confused by singleton types 0.5.4 Improve the pretty-printing for Range.Inclusive #337, thanks to Saheb More fixes to shadowing behavior of types and vals #199 Pressing BACKSPACE now drops you out of history-browsing properly, preserving your edits if you then press ENTER without entering any other characters #338 0.5.3 Added support for multi-line prompts, thanks to thirstycrow Fix #312 lsing empty directory gives error, thanks to coderabhishek Implemented History Search, also known as reverse-i-search/Ctrl-R Fixed #325: error due to function types with by-name parameters Fixed #258: java.util.NoSuchElementException: head of empty list in Ammonite-REPL Fixed #198: NoSuchElementException thrown in REPL when using a type alias to refer to a shapeless coproduct Warnings can now be enabled with the flag compiler.settings.nowarn.value = false Stopped Ivy from spitting out countless useless unknown resolver null warnings when resolving dependencies (#144) Fixed edge cases around import shadowing and sequencing (#199, #248) Started the REPL Cookbook, examples of using the Ammonite REPL to do useful work. 0.5.2 Fixed #80: Support artifact resolvers to load libraries not published to maven central, by Eric Torreborre Fixed #310: java.util.NoSuchElementException: None.get on Shift+Tab without selection, by senia-psm 0.5.1 Fix performance regression causing slowness when C&Ping large snippets #274 Added the ability to pass Script Arguments to Ammonite scripts from an external command line (e.g. bash) #277 Iterator-returning commands like ls!! ls.rec!! and read.lines!! have been renamed ls.iter! ls.rec.iter! and read.lines.iter!, to help cut down on cryptic operators Added the time command to Ammonite-Shell, roughly equivalent to the bash time command, allowing easy timing of simple commands ls.rec now exposes basic configurability of the recursion, allowing you to skip directories or controlling the pre/post-order of results Paths and RelPaths no longer permit ""."" or "".."" as a path segment #215 Paths and RelPaths now us Vector[String] instead of Seq[String] as the segments data-structure, promising more consistent semantics Pretty-printing of the results of ls! now properly gets truncated when too large #209 Cross-build for 2.10.6 #282 Refactor of the CommandResult type being returned from the %% operator, to now properly capture the raw byte output, stdout, stderr, exit code. See Spawning Subprocesses for details. #207 Added a new grep command. Added support for word navigation with Ctrl+Arrow in Linux (#217), thanks to Ian McIntosh Moved the initialization calls from ammonite.repl.Repl.run and ammonite.repl.Repl.debug into ammonite.repl.Main.run and ammonite.repl.Main.debug 0.5.0 Fixed def<tab> auto-complete crasher #257, thanks to Matthew Edwards! Fixed input-height bug around multi-line selection that would case the prompt to fly up the console Multi-line-select tab-indent (and shift-tab-dedent) now works! Pressing [Enter] now only submits the input if your cursor is at the bottom Added a powerful Save/Load Session API, letting you save your work at any point in time and return to it later. Compiled-code-caches are now properly invalidated when you change project-code while using Ammonite as a REPL for an existing SBT project. Simplify the way shelling to to run files as subprocesses works, and align with documentation (#234) 0.4.9 Update to fastparse/scalaparse 0.3.1 Fix for perennial classpath problems, thanks to Johannes Rudolph! Fix wildcard-imports from Java libraries like Joda or Guava #213 Added an MIT license Slightly more robust tab completion #252, thanks to Sanjiv Sahayam! Properly handly EOF in standard input #242, thanks to Patrick Premont! 0.4.8 Swapped to G1 garbage collector to reduce unnecessary memory footprint Allow splicing Seq[String]s into subprocess arguments Fix source packaging which was causing problems with ensime Allow shebang line to make Ammonite scripts more conveniently executable Robustify line-breaking-logic 0.4.7 Path-completion now works when using Ammonite as a filesystem shell Ammonite's filesystem functionality (cd!, wd, path-completion) has been pulled out of Ammonite-REPL, and is now available separately as Ammonite-Shell. Improve the pretty-printing of the ls and ls.rec commands Ammonite can now be used as a Remote REPL into an already-running Scala process, letting you SSH in to poke around at any time while it's running, thanks to Viacheslav Blinov Fix execution of files via symbols in the current working directory. Load.ivy now properly attempts to load artifacts from the local ~/.ivy/cache, ~/.ivy/local and ~/.m2 folders, before fetching from maven central Wrote up a good amount of documentation for Ammonite-Shell: using Ammonite as a Bash replacement 0.4.6 Provide a way of Invoking Files and passing Environment Variables Documented existing approach for setting Compiler Flags Fixed a bug in the readline re-implementation causing barely-full lines in the terminal to mess up cursor positioning and line re-drawing Remove cache1234567890abcdef1234567890abcdef objects from the autocomplete list, because they're not helpful Trim all the useless members of Any from the default import lists. Fix a file-handle-leak for most usages of read.lines and ls/ls.rec Fix bugs #186, #152, #149, #180 0.4.5 Fix for running Ammonite using OpenJDK, thanks to Johannes Rudolph Support for HOME and END keys, thanks to Johannes Rudolph Fix for incorrect syntax highlighting (#159) Support for loading compiler plugins, thanks to Alexandre Archambault You can now use Ammonite as a Debugging tool like Python's pdb, placing an interactive breakpoint anywhere within a normal Scala application 0.4.4 Lots and lots of terminal improvements, courtesy of Erik Osheim Only the last @-delimited block in a script loaded via load.module gets its names dumped into the REPL's environment now, letting you create some semblance of hygiene, thanks to Laszlo Mero 0.4.3 Remove embarassing debug println left behind in autocomplete code Fix pathSeparator so Ammonite-REPL is at least basically-runnable on windows, although buggy Update to more robust version of pprint to fix #140 0.4.2 Fix #139: Can't fix typos? Fix bad wrapping of long lines in ammonite-repl 0.4.1 Fix crasher running the REPL on new machines 0.4.0 Re-added support for 2.10.x, minus features that don't work in it (e.g. scope-aware type-printing) Added a standalone distributable that comes bundled with Scala 2.10.4 or 2.11.7, letting you quickly load and experiment with libraries without SBT User input now has Syntax Highlighting by default! Exception stack traces are now highlighted as well, to make them easier to read Pretty-printing has been extracted into a separate project, and aside from that is greatly improved. Many more common cases (e.g. sealed trait hierarchies) are now pretty-printed rather than falling back to toString Exposed the show function by default, letting you pretty-print any value with custom configuration (wrapping-width, truncation-height, colors, ...) Fixed cases where PPrint/TPrint was causing compilation errors Persistent data is now stored in a ~/.ammonite folder. This includes ~/.ammonite/history, ~/.ammonite/predef.scala, and various cache, thanks to Laszlo Mero You can now define a ~/.ammonite/predef.scala Configuration file which will be executed the first thing when the Ammonite REPL is loaded. This is useful for common imports, load.ivying libraries, or other configuration for your REPL Added the ability to load arbitrary Script Files via load.exec and load.module, thanks to Laszlo Mero Configuration that was previously passed into the REPLs constructor is now done in-REPL, Multi-line editing and other features via a custom terminal interface that should behave just like readline, but with added conveniences. Removed the ability to reload classes; using load.ivy no longer causes all existing values to be lazily recomputed. Added the cd! and wd built-ins to make working with filesystem operations via Ammonite-Ops more pleasant Evaluated values of type Unit are no longer echo-ed to the user Performance improvements to the startup time of the REPL, with more to come Third-party library resolution via load.ivy is now cached after the first call, letting you e.g. load libraries in your ~/.ammonite/predef.scala without waiting for the slow ivy-resolution every startup Standardized the use of Refs for configuration, including the ability to bind them ""live"" to the value of an expression. Allows you to trivially spawn subprocesses, letting you run git commands, edit files via vim, open ssh sessions or even start SBT or Python shells right from your Scala REPL 0.3.2 Fix pretty-printing of higher-kinded types. Drop support for 2.10.x; ammonite is 2.11.x-only now 0.3.1 Many of the collection PPrints are much lazier and will avoid stringifying the whole collection if its going to get truncated anyway. Types now get printed semi-qualified (depending on what's in scope), with simple highlighting. You can define custom TPrint[T]s to provide custom printing for any type. Operator-named two-param generic types are now printed infix by default. 0.3.0 allow predef parameter to be passed into Main.run() call, letting you configure initialization commands or imports Compilation errors in expressions no longer show synthetic code in the message Ivy module loading now lets you configure verbosity level Defining macros in the REPL and using them in subsequent lines now works Output lines are now truncated past a certain length, which is configurable, thanks to Laszlo Mero 0.2.9 Lots of improvements to Ctrl-C and Ctrl-D handling, to make it behave more like other REPLs 0.2.8 Fix #47: PPrint derivation fails with nested case class Fix #14: Exception when trying to use Ammonite REPL #15 by cross building against Scala 2.10.{3,4,5} and 2.11.{3,4,5,6} Autocomplete results are sorted alphabetically (Fixed #42) Fix #39: nothing echoed on multiple import Importing things from Java packages now works properly Capture Exceptions and expose them to repl as repl.lastException including exceptions causing Failures"	"null"	"null"	"An improved Scala REPL: syntax highlighting, output formatting, multi-line input, and more."	"true"
"Misc"	"Fansi"	"https://github.com/lihaoyi/fansi"	"Scala/Scala.js library for manipulating Fancy Ansi colored strings"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"71"	"2"	"3"	"GitHub - lihaoyi/fansi: Scala/Scala.js library for manipulating Fancy Ansi colored strings Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 71 Fork 3 lihaoyi/fansi Code Issues 3 Pull requests 1 Pulse Graphs Scala/Scala.js library for manipulating Fancy Ansi colored strings 26 commits 2 branches 3 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 0.1.3 0.1.2 0.1.1 Nothing to show New pull request Latest commit 0db9baa Jun 23, 2016 lihaoyi committed on GitHub Create LICENSE Permalink Failed to load latest commit information. docs Delete attrs.png May 26, 2016 fansi/shared/src 0.1.3 Jun 3, 2016 project cross-compile for Scala.js, set up travis build May 26, 2016 .gitignore first commit May 26, 2016 .travis.yml add some perf tests and tweaks May 26, 2016 LICENSE Create LICENSE Jun 23, 2016 build.sbt 0.1.3 Jun 3, 2016 readme.md 0.1.3 Jun 3, 2016 readme.md Fansi 0.1.3 ""com.lihaoyi"" %% ""fansi"" % ""0.1.3"" ""com.lihaoyi"" %%% ""fansi"" % ""0.1.3"" // Scala.js Fansi is a Scala library to make it easy to deal with fancy colored Ansi strings within your command-line programs. While ""normal"" use of Ansi escapes with java.lang.String, you find yourself concatenating colors: val colored = Console.RED + ""Hello World Ansi!"" + Console.RESET To build your colored string. This works the first time, but is error prone on larger strings: e.g. did you remember to put a Console.RESET where it's necessary? Do you need to end with one to avoid leaking the color to the entire console after printing it? Furthermore, some operations are fundamentally difficult or error-prone with this approach. For example, val colored: String = Console.RED + ""Hello World Ansi!"" + Console.RESET  // How to efficiently get the length of this string on-screen? We could try // using regexes to remove and Ansi codes, but that's slow and inefficient.  // And it's easy to accidentally call `colored.length` and get a invalid length val length = ???   // How to make the word `World` blue, while preserving the coloring of the  // `Ansi!` text after? What if the string came from somewhere else and you  // don't know what color that text was originally? val blueWorld = ???   // What if I want to underline ""World"" instead of changing it's color, while // still preserving the original color? val underlinedWorld = ???  // What if I want to apply underlines to ""World"" and the two characters on  // either side, after I had already turned ""World"" blue? val underlinedBlue = ??? While simple to describe, these tasks are all error-prone and difficult to do using normal java.lang.Strings containing Ansi color codes. This is especially so if, unlike the toy example above, colored is coming from some other part of your program and you're not sure what or how-many Ansi color codes it already contains. With Fansi, doing all these tasks is simple, error-proof and efficient: val colored: fansi.Str = fansi.Color.Red(""Hello World Ansi!"") // Or fansi.Str(""Hello World Ansi!"").overlay(fansi.Color.Red)   val length = colored.length // Fast and returns the non-colored length of string  val blueWorld = colored.overlay(fansi.Color.Blue, 6, 11)  val underlinedWorld = colored.overlay(fansi.Underlined.On, 6, 11)  val underlinedBlue = blueWorld.overlay(fansi.Underlined.On, 4, 13) And it just works: Why Fansi? Unlike normal java.lang.Strings with Ansi escapes embedded inside, fansi.Str allows you to perform a range of operations in an efficient manner: Extracting the non-Ansi plainText version of the string Get the non-Ansi length Concatenate colored Ansi strings without worrying about leaking colors between them Applying colors to certain portions of an existing fansi.Str, and ensuring that the newly-applied colors get properly terminated while existing colors are unchanged Splitting colored Ansi strings at a plainText index Rendering to colored java.lang.Strings with Ansi escapes embedded, which can be passed around or concatenated without worrying about leaking colors. These are tasks which are possible to do with normal java.lang.String, but are tedious, error-prone and typically inefficient. Often, you can get by with adding copious amounts of Console.RESETs when working with colored java.lang.Strings, but even that easily results in errors when you RESET too much and stomp over colors that already exist: fansi.Str allows you to perform these tasks safely and easily: Fansi is also very efficient: fansi.Str uses just 3x as much memory as java.lang.String to hold all the additional formatting information. Its operations are probably about the same factor slower, as they are all implemented using fast arraycopys and while-loops similar to java.lang.String. That means that - unlike fiddling with Ansi-codes using regexes - you generally do not need to worry about performance when dealing with fansi.Strs. Just treat them as you would java.lang.Strings: splitting them, substringing them, and applying or removing colors or other styles at-will. Fansi was originally a part of the Ammonite REPL, but is now a standalone zero-dependency library anyone can use if they want to easily and efficiently deal with colored Ansi strings. Using Fansi The main operations you need to know are: fansi.Str(raw: CharSequence): fansi.String, to construct colored Ansi strings from a java.lang.String, with or without existing Ansi color codes inside it. fansi.Str, the primary data-type that you will use to pass-around colored Ansi strings and manipulate them: concatenating, splitting, applying or removing colors, etc. fansi.Attrs are the individual modifications you can make to an fansi.Str's formatting. Examples are: fansi.Bold.{On, Off} fansi.Reversed.{On, Off} fansi.Underlined.{On, Off} fansi.Color.* fansi.Back.* fansi.Attr.Reset fansi.Attrs represents a group of zero or more fansi.Attrs. These that can be passed around together, combined via ++ or applied to fansi.Strs all at once. Any individual fansi.Attr can be used when fansi.Attrs is required, as can fansi.Attrs.empty. Using any of the fansi.Attr or fansi.Attrs mentioned above, e.g. fansi.Color.Red, using fansi.Color.Red(""hello world ansi!"") to create a fansi.Str with that text and color, or fansi.Str(""hello world ansi!"").overlay(fansi.Color.Blue, 6, 11) .render to convert a fansi.Str back into a java.lang.String with all necessary Ansi color codes within it Digging Deeper If you want to dig into deeper, there are a few more APIs you can use: getColors and getChars methods on fansi.Str to extract the raw data for your own use fansi.Str.fromArrays to piece it back together This allows you to perform fast, mutable array operations on the color/character arrays if you know what you're doing and want to perform operations that are inconvenient or slow to do through fansi.Str's immutable API. For example, if you want to do a bunch of work with colored strings and then at-the-end render everything to HTML, you can manually walk over the color/character arrays yourself and decide where to print HTML tags to give the text colors. fansi.Str currently has a relatively skeletal API: it is slightly smaller than what java.lang.String has, and definitely much less than what is available on scala.RichString's extension methods. Feel free to implement your own custom operations using fromArrays if you can't find what you want on fansi.Str, or send a patch if you think it's arguably general enough to be included in Fansi itself. fansi.Attrs.emitAnsiCodes Lets you manually emit the different java.lang.Strings that correspond to changes in color in an Ansi string. For example, if you want to emit the Ansi codes that correspond to the transition from ""No Color"" to ""Red"", you can use fansi.Attrs.emitAnsiCodes(0, fansi.Color.Red.applyMask) // ""\u001b[31m"" Or the Ansi codes from ""Red"" to ""No Color"" fansi.Attrs.emitAnsiCodes(fansi.Color.Red.applyMask, 0) // ""\u001b[39m"" Or for any other combination of attributes val attrs = fansi.Color.Red ++ fansi.Back.Blue ++ fansi.Underlined.On fansi.Attrs.emitAnsiCodes(0, attrs.applyMask) // ""\u001b[31m\u001b[44m\u001b[4m"" You can also pass in an errorMode when parsing a string via ansi.Str(...) to tell Fansi how to behave if it finds Ansi escapes it can't handle. You have the options: fansi.ErrorMode.Throw is the default, to throw an exception and fail the parse if it sees an Ansi escape it does not recognize. fansi.ErrorMode.Sanitize to remove the escape character but leave the remnants of the escape-sequence in the result that people can see fansi.ErrorMode.Strip to remove those escape sequences entirely so that no trace of them remains in the final result Scaladoc 0.1.3 Changelog 0.1.3 Fixed a bug in substring that incorrectly threw an out of bounds exception for end == length Exposed the fansi.Attrs.emitAnsiCodes function Renamed Attrs.empty to Attrs.Empty for consistency with all the others 0.1.2 Removed infinite-loop if parsing strings with Ansi escapes that are not recognized by Fansi Added fansi.ErrorMode parameter, to control behavior when un-recognized Ansi escapes are found. 0.1.1 Doubled the speed of the .render operation Doubled the speed of the .overlay operation Added the .overlayAll method on fansi.Str, to allow you to quickly apply multiple overlays onto the same string 0.1.0 First release Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/fansi"	"Scala/Scala.js library for manipulating Fancy Ansi colored strings"	"true"
"Misc"	"Miniboxing"	"https://github.com/miniboxing/miniboxing-plugin"	"A Scala compiler plugin that improves program performance -- - Less boxes"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"87"	"15"	"15"	"GitHub - miniboxing/miniboxing-plugin: Miniboxing is a program transformation that improves the performance of Scala generics when used with primitive types. It can speed up generic collections by factors between 1.5x and 22x, while maintaining bytecode duplication to a minimum. You can easily add miniboxing to your sbt project: Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 87 Fork 15 miniboxing/miniboxing-plugin Code Issues 43 Pull requests 0 Wiki Pulse Graphs Miniboxing is a program transformation that improves the performance of Scala generics when used with primitive types. It can speed up generic collections by factors between 1.5x and 22x, while maintaining bytecode duplication to a minimum. You can easily add miniboxing to your sbt project: http://scala-miniboxing.org 1,138 commits 10 branches 3 releases Fetching contributors Java 50.0% Scala 49.7% Shell 0.3% Java Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: wip Switch branches/tags Branches Tags 2.12.0-M1 master mbarray-wip sbt-scoverage-snapshot tuples wip-benchmarks-oops-interp wip-coursier wip-interpreter wip-mbarrays wip Nothing to show v0.4-M4 v0.4-M3 oopsla15 Nothing to show New pull request Latest commit 7cd4276 Mar 13, 2016 VladUreche Closes #30: Store special overrides in a list Permalink Failed to load latest commit information. components Closes #30: Store special overrides in a list Mar 13, 2016 docs Errata Jan 28, 2016 licenses Specializing classloader Mar 5, 2013 project Fix missing `compileTimeOnly` annotation on 2.10 May 14, 2015 sandbox updated cp-test Mar 27, 2015 scripts Ported 890c230 from branch 2.12.0-M1 May 3, 2015 tests Closes #30: Store special overrides in a list Mar 13, 2016 .gitignore Ignore release script Nov 11, 2015 .travis.yml Revert ""Build on JDK6"" Jan 28, 2016 LICENSE.md Unstuck the build Sep 25, 2013 README.md Added covecov badge May 14, 2015 build.sbt Closes #228: Remove only non-unique bridges Feb 6, 2016 mb-scala Updated scripts Feb 21, 2014 mb-scalac Updated scripts Feb 21, 2014 README.md Miniboxing Plugin for Scala Miniboxing is a Scala compiler transformation that improves the performance of generics for primitive numeric types. Unlike specialization, miniboxing reduces the amount of generated bytecode without sacrificing execution speed, therefore being capable of transforming the Scala collections hierachy. Check out the scala-miniboxing.org website. Why use Miniboxing? Short answer: because it matches the performance of specialization, without the bytecode blowup. For the Tuple3 class: case class Tuple3[@specialized +T1, @specialized +T2, @specialized +T3](_1: T1, _2: T2, _3: T3)  Specialization generates 1000 classes. Just change @specialized to @miniboxed and you get only 8 classes. Long answer: Aside from reducing the bytecode size, the miniboxing technique improves several other aspects of specialization: miniboxing-specialized classes don't inherit generic fields (see SI-3585); miniboxing-specialized classes can inherit from their miniboxing-specialized parents (see SI-8405 and this restriction). To see the benchmarks we performed, have a look at the OOPSLA 2013 paper we just prepared. They include performance evaluations, bytecode size comparisons and many more. Also, the docs directory contains a series of papers and presentations which explain many aspects of the miniboxing transformation. Using the Plugin At this point, the miniboxing plugin is not production-ready, although it can compile spire. However, we do publish a nightly maven artifact on Sonatype, so anyone can try the current transformation. To get started, have a look at the example project we prepared. Mind the gap: there are still many bugs and known limitations, so you're in for a thrill! Also, don't hesitate to add bugs to the tracker, good reductions that can be easily reproduced are highly appreciated! Hacking the Plugin The wiki is a good place to start looking into installing, testing, benchmarking and hacking on the miniboxing plugin. Also have a look at the docs directory, which contains some good resources. The development branches are: master is always stable, usually outdated wip is pretty stable, usually has the last resonably stable developments topic/erasure-rebase contains the most recent developments, but expect tests to be broken most of the time Repository organization wip is the working branch, most current version of the plugin master is usually a bit behind wip, but should be stable (alpha-stable, not production-ready!) sbt is used for compilation and testing the repository contains several sbt sub-projects: components/plugin - the actual Scala compiler plugin components/runtime - the runtime support for the transformed code components/classloader - the classloader used for runtime class specialization tests/benchmarks - the benchmarks for the project tests/correctness - the tests for the plugin transformation docs - documents released as the development goes on Questions? If you have any question, you can contact me at vlad dot ureche at epfl dot ch. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/miniboxing/miniboxing-plugin"	"A Scala compiler plugin that improves program performance -- - Less boxes"	"true"
"Misc"	"see the project web site"	"http://scala-miniboxing.org"	"A Scala compiler plugin that improves program performance -- - Less boxes"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"87"	"15"	"15"	"GitHub - miniboxing/miniboxing-plugin: Miniboxing is a program transformation that improves the performance of Scala generics when used with primitive types. It can speed up generic collections by factors between 1.5x and 22x, while maintaining bytecode duplication to a minimum. You can easily add miniboxing to your sbt project: Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 87 Fork 15 miniboxing/miniboxing-plugin Code Issues 43 Pull requests 0 Wiki Pulse Graphs Miniboxing is a program transformation that improves the performance of Scala generics when used with primitive types. It can speed up generic collections by factors between 1.5x and 22x, while maintaining bytecode duplication to a minimum. You can easily add miniboxing to your sbt project: http://scala-miniboxing.org 1,138 commits 10 branches 3 releases Fetching contributors Java 50.0% Scala 49.7% Shell 0.3% Java Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: wip Switch branches/tags Branches Tags 2.12.0-M1 master mbarray-wip sbt-scoverage-snapshot tuples wip-benchmarks-oops-interp wip-coursier wip-interpreter wip-mbarrays wip Nothing to show v0.4-M4 v0.4-M3 oopsla15 Nothing to show New pull request Latest commit 7cd4276 Mar 13, 2016 VladUreche Closes #30: Store special overrides in a list Permalink Failed to load latest commit information. components Closes #30: Store special overrides in a list Mar 13, 2016 docs Errata Jan 28, 2016 licenses Specializing classloader Mar 5, 2013 project Fix missing `compileTimeOnly` annotation on 2.10 May 14, 2015 sandbox updated cp-test Mar 27, 2015 scripts Ported 890c230 from branch 2.12.0-M1 May 3, 2015 tests Closes #30: Store special overrides in a list Mar 13, 2016 .gitignore Ignore release script Nov 11, 2015 .travis.yml Revert ""Build on JDK6"" Jan 28, 2016 LICENSE.md Unstuck the build Sep 25, 2013 README.md Added covecov badge May 14, 2015 build.sbt Closes #228: Remove only non-unique bridges Feb 6, 2016 mb-scala Updated scripts Feb 21, 2014 mb-scalac Updated scripts Feb 21, 2014 README.md Miniboxing Plugin for Scala Miniboxing is a Scala compiler transformation that improves the performance of generics for primitive numeric types. Unlike specialization, miniboxing reduces the amount of generated bytecode without sacrificing execution speed, therefore being capable of transforming the Scala collections hierachy. Check out the scala-miniboxing.org website. Why use Miniboxing? Short answer: because it matches the performance of specialization, without the bytecode blowup. For the Tuple3 class: case class Tuple3[@specialized +T1, @specialized +T2, @specialized +T3](_1: T1, _2: T2, _3: T3)  Specialization generates 1000 classes. Just change @specialized to @miniboxed and you get only 8 classes. Long answer: Aside from reducing the bytecode size, the miniboxing technique improves several other aspects of specialization: miniboxing-specialized classes don't inherit generic fields (see SI-3585); miniboxing-specialized classes can inherit from their miniboxing-specialized parents (see SI-8405 and this restriction). To see the benchmarks we performed, have a look at the OOPSLA 2013 paper we just prepared. They include performance evaluations, bytecode size comparisons and many more. Also, the docs directory contains a series of papers and presentations which explain many aspects of the miniboxing transformation. Using the Plugin At this point, the miniboxing plugin is not production-ready, although it can compile spire. However, we do publish a nightly maven artifact on Sonatype, so anyone can try the current transformation. To get started, have a look at the example project we prepared. Mind the gap: there are still many bugs and known limitations, so you're in for a thrill! Also, don't hesitate to add bugs to the tracker, good reductions that can be easily reproduced are highly appreciated! Hacking the Plugin The wiki is a good place to start looking into installing, testing, benchmarking and hacking on the miniboxing plugin. Also have a look at the docs directory, which contains some good resources. The development branches are: master is always stable, usually outdated wip is pretty stable, usually has the last resonably stable developments topic/erasure-rebase contains the most recent developments, but expect tests to be broken most of the time Repository organization wip is the working branch, most current version of the plugin master is usually a bit behind wip, but should be stable (alpha-stable, not production-ready!) sbt is used for compilation and testing the repository contains several sbt sub-projects: components/plugin - the actual Scala compiler plugin components/runtime - the runtime support for the transformed code components/classloader - the classloader used for runtime class specialization tests/benchmarks - the benchmarks for the project tests/correctness - the tests for the plugin transformation docs - documents released as the development goes on Questions? If you have any question, you can contact me at vlad dot ureche at epfl dot ch. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/miniboxing/miniboxing-plugin"	"A Scala compiler plugin that improves program performance -- - Less boxes"	"true"
"Misc"	"Openquant ★ 73 ⧗ 0"	"https://github.com/openquant"	"A Scala open source quantitative trading platform"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Open Quant · GitHub Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This organization Open Quant Open source quantitative trading software https://medium.com/@openquant openquant@larroy.com Repositories People 0 Filters Sources Forks Scala 4 2 YahooFinanceScala A non-blocking Yahoo Finance Scala client Updated Jun 25, 2016 Scala 1 1 common common lib Updated Jun 25, 2016 0 0 chartzfx pretty technical analisys charts Updated May 6, 2016 Scala 1 0 historical historical data management and data source adapters Updated Apr 16, 2016 Scala 2 0 ibquoteprovider Quote provider adapter for Interactive Brokers Updated Apr 9, 2016 0 0 documentation Documentation, guides and blog posts Updated Mar 5, 2016 Scala 1 0 quoter Manage database of historical quotes for backtesting Updated Oct 31, 2015 0 People This organization has no public members. You must be a member to see who’s a part of this organization. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/openquant"	"A Scala open source quantitative trading platform"	"true"
"Misc"	"REPLesent ★ 237 ⧗ 3"	"https://github.com/marconilanna/REPLesent"	"A presentation tool built inside the Scala REPL. Runs code straight from your slides with a single keystroke."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"270"	"13"	"25"	"GitHub - marconilanna/REPLesent: A neat little tool to build presentations using the Scala REPL Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 13 Star 270 Fork 25 marconilanna/REPLesent Code Issues 0 Pull requests 4 Pulse Graphs A neat little tool to build presentations using the Scala REPL 66 commits 1 branch 11 releases 6 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v1.1 v1.0 v0.9 v0.8 v0.7 v0.6 v0.5 v0.4 v0.3 v0.2 v0.1 Nothing to show New pull request Latest commit b9b9469 May 2, 2016 Krever committed with marconilanna Slides reloading (#7) … * allow reloading slides from file  * readme update Permalink Failed to load latest commit information. project Update plugins Feb 1, 2016 test Minor formatting Aug 6, 2015 .gitignore Initial commit Feb 1, 2015 .travis.yml Travis CI integration Feb 4, 2015 CHANGELOG.md Merge pull request #4 from noelmarkham/master Mar 4, 2015 LICENSE.txt Initial commit Feb 2, 2015 README.md Slides reloading (#7) May 2, 2016 REPLesent.scala Slides reloading (#7) May 2, 2016 REPLesent.txt Emoji support Feb 25, 2015 build.sbt Update ScalaTest to 2.2.6 Feb 1, 2016 emoji.txt Update emoji.txt Feb 1, 2016 tour.txt Quick tour sample screencast Feb 19, 2015 README.md REPLesent represent verb depict, portray, render, delineate, show, illustrate. symbolize, stand for, personify, typify, embody. point out, state, present, put forward. REPLesent is a neat little powerful tool to build presentations using the Scala REPL. Conceptualized and originally implemented while waiting almost two hours for my plane to be de-iced, it was introduced during the Northeast Scala Symposium 2015 in Boston. While clearly not a Powerpoint Keynote replacement, REPLesent is a good option for training sessions and technical talks featuring live coding. Its old-school looks were considered very cool by many conference attendants. Features Easy to write slides: a simple plain text file with minimal markup The full arsenal of navigation options: next, previous, first, last, jump to Builds (incremental slides) Slide number / total Text alignment: left, right, centered, flushed ANSI colors Horizontal rulers (thanks, @daviscabral) Syntax highlighting Run code straight from slides directly in the REPL with a single keystroke. No other presentation tool can do that for you! And the #1 requested feature: emoji! :-) I mean, 😄 Quick Tour Getting Started REPLesent is distributed as a single .scala file and has no dependencies. REPLesent was designed to be used only in conjunction with the Scala REPL. It is not meant to be compiled as a standalone application (build.sbt is only for running the unit tests). We recommend you install and use the full Scala distribution (the scala command) instead of just the sbt console. Scala 2.11.4 or later and JDK 7 or later are the preferred versions. If you are using Windows, you need to run scala in ANSICON or another console that supports ANSI escape codes. REPLesent will not render correctly in Windows' standard cmd console. To get started, download and save to the same folder the files REPLesent.scala and REPLesent.txt. The file emoji.txt is optional, only needed to enable emoji support. First, create an alias: alias REPLesent='scala -Dscala.color -language:_ -nowarn -i REPLesent.scala' Open the REPL and enter the two statements below: $ REPLesent Loading REPLesent.scala... defined class REPLesent  Welcome to Scala version 2.11.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_31). Type in expressions to have them evaluated. Type :help for more information.  scala> val replesent = REPLesent(80, 25, intp=$intp) replesent: REPLesent = REPLesent(80,25,REPLesent.txt)  scala> import replesent._ import replesent._ Do not forget to replace 80, 25 with the actual size of your terminal window. The intp=$intp incantation is required to hook onto the REPL to run slide code. Once all is done, type f, press ENTER, and follow the on-screen instructions. Experimental screen size auto-detection: For Unix-like systems, including Mac OS X and Linux, you can omit the screen size. Does not work for Microsoft Windows systems. Quick Reference Guide Initialization options Parameter Type Description Default width Int Terminal width in columns 0 (Unix: autodetect, Windows: 80) height Int Terminal height in rows 0 (Unix: autodetect, Windows: 25) input String The path to the presentation file ""REPLesent.txt"" slideCounter Boolean Whether to show the slide number false slideTotal Boolean Whether to show the total number of slides false intp A hook to the Scala REPL No default, use magic value $intp Navigation commands Command Shortcut Symbolic alias Description next n > Go to next build/slide previous p < Go back to previous build/slide redraw z Redraw the current build/slide reload y Reload the slides from file Next N >> Go to next slide Previous P << Go back to previous slide i next i n Advance i slides i previous i p Go back i slides i go i g Go to slide i first f |< Go to first slide last l >| Go to last slide Last L >>| Go to last build of last slide run r !! Execute code that appears on slide blank b Blank screen help h ? This help message Separators and delimiters Separator Description --- Separates slides -- Separates builds ``` Delineates Scala code Text alignment Command Description << Left-flushed text < Left-aligned text | Centered text > Right-aligned text >> Right-flushed text A space separating the alignment command from the text is mandatory. ANSI colors Escape code Result \x Foreground color, where x is one of: red, green, blue, cyan, magenta, yellow, black, white \X Background color, where capital X is one of the same as above \* Bold \_ Underscore \! Reverse colors \s Resets to normal Horizontal rulers Command Description / A ruler across the slide length // A ruler across the entire screen width An optional pattern may be specified immediately following the forward slash. Unicode characters and ANSI color escapes (as above) are supported. Emoji To enable emoji support, you will need a copy of the emoji.txt file. Almost all shortcuts listed by the Emoji cheat sheet are supported. You can look at emoji.txt for the definitive list of supported emoji. Emojis can be combined with horizontal rulers and, depending on your system fonts, ANSI colors, too. Emoji has only be tested on Mac OS X Terminal.app, YMMV. Thanks Davis Z. Cabral for implementing horizontal ruler support. I don't care if it is used, I just want it to be useful License Copyright 2015 Marconi Lanna Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/marconilanna/REPLesent"	"A presentation tool built inside the Scala REPL. Runs code straight from your slides with a single keystroke."	"true"
"Misc"	"scala-ssh ★ 164 ⧗ 2"	"https://github.com/sirthias/scala-ssh"	"Remote shell access via SSH for your Scala applications"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"168"	"15"	"40"	"GitHub - sirthias/scala-ssh: Remote shell access via SSH for your Scala applications Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 168 Fork 40 sirthias/scala-ssh Code Issues 11 Pull requests 1 Pulse Graphs Remote shell access via SSH for your Scala applications 87 commits 1 branch 6 releases 13 contributors Scala 96.8% DIGITAL Command Language 1.7% Shell 1.5% Scala DIGITAL Command Language Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.7.0 v0.6.4 v0.6.3 v0.6.2 v0.6.0 v0.5.0 Nothing to show New pull request Latest commit f7dc7a3 May 17, 2016 sirthias Update README Permalink Failed to load latest commit information. lib Initial commit Oct 27, 2011 notes Prepare for 0.6.3 release Oct 26, 2012 project Updated sshj, bouncycastle, scala and sbt versions Jun 22, 2015 src Fix connection hanging after upload/download Jun 19, 2015 .gitignore Initial commit Oct 27, 2011 .travis.yml = upgrade SBT and dependencies, remove support for Scala 2.9 Oct 16, 2014 CHANGELOG = prepare for 0.7.0 release Oct 16, 2014 LICENSE Initial commit Oct 27, 2011 README.rst Update README May 17, 2016 build.sbt Updated sshj, bouncycastle, scala and sbt versions Jun 22, 2015 README.rst scala-ssh is a Scala library providing remote shell access via SSH. It builds on SSHJ to provide the following features: Remote execution of one or more shell commands Access to stdin, stdout, stderr and exitcode of remote shell commands Authentication via password or public key Host key verification via known_hosts file or explicit fingerprint Convenient configuration of remote host properties via config file, resource or directly in code Scala-idiomatic API MAINTENANCE STATE This project is currently unmaintained. However there is a fork at https://github.com/veact/scala-ssh whose owner have announced that they will continue development and maintenance. (See also issue https://github.com/sirthias/scala-ssh/issues/35) Installation The latest release is 0.7.0 and is built against Scala 2.10 and Scala 2.11. It is available from Maven Central. If you use SBT you can pull in the scala-ssh artifacts with: libraryDependencies += ""com.decodified"" %% ""scala-ssh"" % ""0.7.0""  SSHJ uses SLF4J for logging, so you might want to also add logback to your dependencies: libraryDependencies += ""ch.qos.logback"" % ""logback-classic"" % ""1.1.2""  Additionally, in many cases you will need the following two artifacts, which provide additional cypher and compression support: libraryDependencies ++= Seq(   ""org.bouncycastle"" % ""bcprov-jdk16"" % ""1.46"",   ""com.jcraft"" % ""jzlib"" % ""1.1.3"" )  Usage The highest-level API element provided by scala-ssh is the SSH object. You use it like this: SSH(""example.com"") { client =>   client.exec(""ls -a"").right.map { result =>     println(""Result:\n"" + result.stdOutAsString())   } }  This establishes an SSH connection to host example.com and gives you an SshClient instance that you can use to execute one or more commands on the host. SSH.apply has a second (optional) parameter of type HostConfigProvider, which is essentially a function returning a HostConfig instance for a given hostname. A HostConfig looks like this: case class HostConfig(   login: SshLogin,   hostName: String = """",   port: Int = 22,   connectTimeout: Option[Int] = None,   connectionTimeout: Option[Int] = None,   commandTimeout: Option[Int] = None,   enableCompression: Boolean = false,   hostKeyVerifier: HostKeyVerifier = ...,   sshjConfig: Config = ... )  It provides all the details required for properly establishing an SSH connection. If you don't provide an explicit HostConfigProvider the default one will be used. For every hostname you pass to the SSH.apply method this default HostConfigProvider expects a file ~/.scala-ssh/{hostname}, which contains the properties of a HostConfig in a simple config file format (see below for details). The HostResourceConfig object gives you alternative HostConfigProvider implementations that read the host config from classpath resources. If the file ~/.scala-ssh/{hostname} (or the classpath resource {hostname}) doesn't exist scala-ssh looks for more general files (or resources) in the following way: As long as the first segment of the host name (up to the first .) contains one or more digits replace the rightmost of these with X and look for a respectively named file or resource. Repeat until no digits left. Drop all characters up to (and including) the first . from the host name and look for a respectively named file or resource. Repeat from 1. as long as there are characters left. This means that for a host with name node42.tier1.example.com the following locations (either under ~/.scala-ssh/ or the classpath, depending on the HostConfigProvider) are tried: node42.tier1.example.com node4X.tier1.example.com nodeXX.tier1.example.com tier1.example.com tierX.example.com example.com com Host Config File Format A host config file is a UTF8-encoded text file containing key = value pairs, one per line. Blank lines and lines starting with a # character are ignored. This is an example file: # simple password-based config login-type = password username = bob password = 123 command-timeout = 5000 enable-compression = yes  These key are defined: login-type required, can be either password or keyfile host-name optional, if not given the name of the config file is assumed to be the hostname port optional, the default value is 22 username required password required for login-type password, ignored otherwise keyfile optionally specifies the location of the user keyfile to use with login-type keyfile, if not given the default files ~/.ssh/id_rsa and ~/.ssh/id_dsa are tried, ignored for login-type password, if the filename starts with a + the file is searched in addition to the default locations, if the filename starts with classpath: it is interpreted as the name of a classpath resource holding the private key passphrase optionally specifies the passphrase for the keyfile, if not given the keyfile is assumed to be unencrypted, ignored for login-type password connect-timeout optionally specifies the number of milli-seconds that a connection request has to succeed in before triggering a timeout error, default value is 'no timeout' connection-timeout optionally specifies the number of milli-seconds that an idle connection is held open before being closed due due to idleness, default value is 'no timeout' command-timeout optionally specifies the number of milli-seconds that a pending response to an issued command is waited for before triggering a timeout error, default value is 'no timeout' enable-compression optionally adds zlib compression to preferred compression algorithms, there is no guarantee that it will be successfully negotiatied, requires jzlib on the classpath (see 'installation' chapter) above, default is 'no' fingerprint optionally specifies the fingerprint of the public host key to verify in standard SSH format (e.g. 4b:69:6c:72:6f:79:20:77:61:73:20:68:65:72:65:21), if not given the standard ~/.ssh/known_hosts or ~/.ssh/known_hosts2 files will be searched for a matching entry, fingerprint verification can be entirely disabled by setting fingerprint = any Troubleshoting Java Cryptography Extension Policy Files To use this library it might be neccessary that you install the Java Cryptography Extension Policy Files from the JDK additional downloads section. Make sure they are installed, especially if you encounter exceptions like this: net.schmizz.sshj.common.SSHRuntimeException: null at net.schmizz.sshj.common.Buffer.readPublicKey(Buffer.java:432) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.kex.AbstractDHG.next(AbstractDHG.java:108) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.KeyExchanger.handle(KeyExchanger.java:352) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.TransportImpl.handle(TransportImpl.java:487) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.Decoder.decode(Decoder.java:107) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.Decoder.received(Decoder.java:175) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.transport.Reader.run(Reader.java:61) ~[sshj-0.12.0.jar:na] Caused by: java.security.GeneralSecurityException: java.security.spec.InvalidKeySpecException: key spec not recognised at net.schmizz.sshj.common.KeyType$3.readPubKeyFromBuffer(KeyType.java:146) ~[sshj-0.12.0.jar:na] at net.schmizz.sshj.common.Buffer.readPublicKey(Buffer.java:430) ~[sshj-0.12.0.jar:na] ... 6 common frames omitted Caused by: java.security.spec.InvalidKeySpecException: key spec not recognised at org.bouncycastle.jcajce.provider.asymmetric.util.BaseKeyFactorySpi.engineGeneratePublic(Unknown Source) ~[bcprov-jdk15on-1.52.jar:1.52.0] at org.bouncycastle.jcajce.provider.asymmetric.ec.KeyFactorySpi.engineGeneratePublic(Unknown Source) ~[bcprov-jdk15on-1.52.jar:1.52.0] at java.security.KeyFactory.generatePublic(KeyFactory.java:334) ~[na:1.8.0_05] at net.schmizz.sshj.common.KeyType$3.readPubKeyFromBuffer(KeyType.java:144) ~[sshj-0.12.0.jar:na] ... 7 common frames omitted License scala-ssh is licensed under APL 2.0. Patch Policy Feedback and contributions to the project, no matter what kind, are always very welcome. However, patches can only be accepted from their original author. Along with any patches, please state that the patch is your original work and that you license the work to the scala-ssh project under the project’s open source license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sirthias/scala-ssh"	"Remote shell access via SSH for your Scala applications"	"true"
"Misc"	"Scalan ★ 58 ⧗ 10"	"https://github.com/scalan/scalan"	"A framework for development of domain-specific compilers in Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"70"	"9"	"18"	"GitHub - scalan/scalan: Generic framework for development of domain-specific compilers in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 9 Star 70 Fork 18 scalan/scalan Code Issues 80 Pull requests 1 Pulse Graphs Generic framework for development of domain-specific compilers in Scala https://groups.google.com/d/forum/scalan 2,408 commits 51 branches 12 releases Fetching contributors Scala 98.6% C++ 1.2% Other 0.2% Scala C++ Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags abstract-elems backend-external-cpp-libraries backend-tupled-functions bugfix-elem-names bugfix-issue-130 bugfix/field-pa concrete-implementations config fhpc fix-crosscompilation graphviz-node-order include-lms-backend-in-root issue-26 kernels la-development lms-bridge-test-only lms-bridge lua-backend-tmp lua-backend master misc-fixes mutable-collections new-method-call processutil-split-stderr quasiquote-weird remove-lazy-elems scala-2.11 scala-2.11.8 scala-compiler-crash scalac-ClassFormatError si-9114-again strange-type-error topic-effects-backend topic-hotspot topic-new-frontend topic-newrewriting topic-typer topic/analysis topic/analyzing topic/containers topic/dynamic-upgrade topic/improvements topic/isorules topic/stddecls topic/structs-api topic/structs-backend topic/structs topic/wrapping-structs-compiler type-descs update-lua-codegen weird-error Nothing to show v0.2.11 v0.2.10 v0.2.9 v0.2.8 v0.2.7 v0.2.6 v0.2.5 v0.2.4 v0.2.3 v0.2.2 v0.2.1 v0.2.0 Nothing to show New pull request Latest commit adc4124 Jul 12, 2016 alexeyr Add a customization point to emitFunction Permalink Failed to load latest commit information. collections/src [Collections] renaming CompoundCollection to JuggedCollection Jun 13, 2016 common/src Separate stderr output from stdout in ProcessUtil.launch Jun 30, 2016 core Add a customization point to emitFunction Jul 12, 2016 docs Merge remote-tracking branch 'github/master' Oct 8, 2014 effects/src/test/scala/scalan/monads [Monads] explicit type for Reader Jun 13, 2016 examples/src [Collections] renaming CompoundCollection to JuggedCollection Jun 13, 2016 graphs/src [LA,Collections] refactoring of Collections, Matrices and Vectors Jun 10, 2016 linear-algebra/src Update boilerplate Jun 29, 2016 lms-backend Separate stderr output from stdout in ProcessUtil.launch Jun 30, 2016 lua-backend/core/src Extend LuaCodegen Jun 14, 2016 meta/src Move SQL-related code to scalan-sql Jul 6, 2016 pointers/src/main/scala/scalan/pointers Make Elem and Cont extend TypeDesc directly Jun 3, 2016 project Generate scalan.plugins.extraClassPath Jun 9, 2016 .gitignore Extract C++ runtime's headers automatically Jun 10, 2016 .travis.jvmopts move to Travis trusty environment Dec 18, 2015 .travis.yml move to Travis trusty environment Dec 18, 2015 DEVELOPMENT.md add simple transformer Nov 27, 2014 LICENSE.txt Initial public version of Scalan-meta Aug 29, 2014 README.md [Core] inferredFun added Jul 5, 2016 build.sbt Compile Java annotations for JVM 1.7 Jul 6, 2016 new-feature-branch.sh Improved push-github script Dec 3, 2014 push-github.sh Simplify push-github.sh Jun 6, 2015 version.sbt Setting version to 0.3.0-SNAPSHOT Dec 14, 2015 README.md Scalan Compilation Framework Intro Scalan is a framework for domain-specific compilation in Scala. It allows you to write high-level Scala programs and compile them to efficient low-level code (in any language supported by the existing backends) by applying domain-specific compilation techniques. Scalan is based on Polymorphic Embedding and LMS-like staging. However, in contrast to LMS, Scalan doesn't rely on Scala-virtualized and works with Scala 2.10+ compiler. In conjunction with Scalanizer Scalan can be used to develop domain-specific JIT compilers for hot-spot optimization in Scala. (To get started checkout Scala Days video and demonstration project) One of the distinguishing feature of Scalan is Isomorphic Specialization a new specialization algorithm and technique which allows to perform cross-domain translations of programs. Thus it is possible to construct compilation pipelines with gradual lowering of domain-specific abstractions. Please visit Scalan Google Group for Scalan-related discussions. See also Contributions below and get involved. Building the project and running tests SBT is required to build Scalan. See SBT documentation for installation and usage instructions. There is also an improved runner which takes care of downloading correct SBT version and adds some extra features. The project consists of several subprojects, including the aggregate project scalan. One of the subprojects, scalan-lms-backend currently depends on a fork of LMS located at http://github.com/scalan/virtualization-lms-core, branch scalan-develop. If you want to use it, you need to clone and build this dependency first, since it isn't published in a public repository. The tests are split into unit tests (which can be run with the usual test SBT command) and integration tests (it:test) which actually generate a program (using some backend) and test the generated code. As of this writing, the only backends(codegens) available are Scala- and C++-based LMS backends, both defined in the scalan-lms-backend subproject. If you want to create your own project depending on Scalan, you should use publishLocal SBT command to publish Scalan artifacts to your local Ivy repository and add dependencies as usual: def liteDependency(name: String) = ""com.huawei.scalan"" %% name % ""0.2.9-SNAPSHOT""  lazy val core = liteDependency(""scalan-core"") lazy val library = liteDependency(""scalan-library"") lazy val meta = liteDependency(""scalan-meta"")  lazy val myProject = Project(""myProjectName"").settings(   // or core, core % ""test"" classifier ""tests"" if you only need scalan-core   libraryDependencies ++= Seq(library, library % ""test"" classifier ""tests"") )  lazy val myMeta = Project(""myMetaProjectName"").settings(libraryDependencies += meta) ""test"" dependencies allow reuse of Scalan's existing test infrastructure and aren't necessary if you don't need it. See Extending Scalan below for an explanation of myMeta. If you also need to depend on scalan-lms-backend, you have to add Scala-Virtualized to the settings block above. See Maven Repository for the latest Scala-Virtualized version; as of this writing, 2.11.2 is the only version which can be used: settings(...,   libraryDependencies += liteDependency(""scalan-lms-backend""),   scalaVersion := ""2.11.2"",   scalaOrganization := ""org.scala-lang.virtualized"") Alternately, you can use project references to Scalan in your build instead of libraryDependencies if you want changes to Scalan to be immediately visible to your project without a publishLocal step. Stability Currently we are quite far from 1.0 and breaking changes can happen. In such a case we usually publish a new release. Writing programs With the introduction of Scalanizer as a new frontend, the role of Scalan is shifted one level down in the middle part of the compilation pipeline, but it still can be used for development of new EDSLs without Scalanizer. Scalan is a library in Scala, so all standard rules of Scala syntax apply and knowledge of Scala is assumed in this manual. However, Scalan supports only a limited subset of Scala types and operations. This is not a conceptual limitation and will improve overtime especially with the help of Scalanizer plugin. Note that if you write a program which is legal Scala, but not legal Scalan, there are currently no error messages or warnings. This is one of the reasons and rationale for creating Scalanizer where such errors can easily be handled. Types All Scalan values have type Rep[A] for some Scala type A. Currently A can be one of the following types: Base types: Boolean, Byte, Int, Long, Float, Double, String. Pairs (A, B). Larger tuples are are represented as pairs nested to the right, so e.g. instead of Rep[(Int, Int, Boolean)] you write Rep[(Int, (Int, Boolean))]. This might look a bit annoying for a DSL front-end, but it actually pays-off at middle-end where generic transformations are implemented. Sums Either[A, B] (can also be written as A | B). Functions A => B. Functions with multiple arguments are emulated by functions taking a tuple (or by currying), such as Rep[((A, B)) => C] (note double parentheses). Arrays Array[A] (in community-edition subproject). Traits and classes added by DSL developers (see Extending Scalan section below). Note that nested Rep is not allowed (i.e. you cannot write something like this Rep[(Rep[Int], Double)]), but it is possible to nest Rep in user defined classes (see Extending Scalan). There are type aliases for some Rep types, such as type IntRep = Rep[Int], type Arr[A] = Rep[Array[A]], etc. You can use aliases freely and the only rule for type aliases is that they don't introduce nested Reps described above. Scala values of type A can be converted to Rep[A] implicitly (or explicitly using toRep method if desired). Like in LMS they become constants of the next stage. (Const[A] nodes of intermediate representation, IR). However, in current version it's not possible to convert from Rep[A] to A as it would mean running the corresponding IR. Operations Scalan supports the usual arithmetic, logical, ordering, etc. operations. They are added by implicit conversions on Rep. Note that x + y where x: Int and y: Rep[Int] doesn't compile; write toRep(x) + y or y + x instead. There are currently no implicit widening conversions from Rep[Int] to Rep[Long], from Rep[Float] to Rep[Double], etc. Methods like toInt and toDouble should be used instead. val x: Rep[Int] = 1  x + 3 As in Scala, arithmetical operations on Rep[T] require an implicit Numeric[T] (Fractional[T] for /) to be in scope, and ordering operations require an Ordering[T]. Equality is written === and inequality !==. Note that accidental use of == or != will likely compile (since Boolean can be implicitly converted to Rep[Boolean]) but produce wrong results! Unfortunately this is a deficiency of polymorphic embedding and can only be cured by Scalanizer with automatic virtualization. Conditional expression is IF (cond) THEN branch1 ELSE branch2, where cond: Rep[Boolean]. THEN is optional, and ELSEIF can be used in place of ELSE. Because IF, THEN and ELSE are methods, they are parsed differently from normal Scala conditionals if, then and else. Namely, THEN and ELSE shouldn't start new lines, e.g. IF (true)   THEN true   ELSE false is parsed by scalac as 3 separate expressions and doesn't typecheck. Instead write IF (true) THEN true ELSE false or IF (true)   THEN {     true   } ELSE {     false   } For tuples the usual Scala methods _1, _2, etc. are available. Tuples can be constructed and pattern-matched using Pair and Tuple objects. val a0 = 1 val b0 = 2 val c0 = false val tuple: Rep[(Int, (Int, Boolean))] = Tuple(a0, b0, c0)  val Tuple(a1, b1, c1) = tuple val c2 = tuple._3 Currently tuples up to size 8 are supported. Scalan function values can be pure or they can have effectful operations. Effectful operation are reified in the IR using LMS style Reflect/Reify nodes. Scalan function values are of type Rep[A=>B] and are not the same as Scala functions (or lambdas) of type Rep[A] => Rep[B]. They can be obtained from pure Scala functions which take and return Rep by an implicit conversion fun. By default if you apply Scalan function like val y = f(x) the function is inlined in the point of application. If the function shouldn't be inlined, funGlob method can be used instead. val f: Rep[Int => Int] = { x: Rep[Int] => x + 1 } // implicit conversion val f1 = fun { x: Rep[Int] => x + 1 } // explicit conversion val f2 = fun((_: Rep[Int]) + 1) // using underscore val f3 = funGlob({ x: Rep[Int] => x + 1 }) Loops can be written as from(startingState).until(isMatch)(step), where isMatch and step are pure Scala functions which take the same number and types of arguments as passed to from and return Rep[Boolean] and a tuple as above respectively. val collatz = from(start).until(_ === 1) { n => IF (n % 2 === 0) THEN (n / 2) ELSE (n * 3 + 1) } Methods can be defined using def keyword, as usual. Normally all arguments and return values will have Rep types, but this isn't required. User types User-Defined Types, UDTs, (abstractions like Vector[T] and concrete implementations like DenseVector[T]) are introduced as part of a module - logical group of Scala traits usually in a single file. Such modules in Scalan are also called DSLs to emphasise their domain specificity (VectorsDsl, MatricesDsl, etc). For such user defined types (like Vector[T]) there is an implicit conversion from Rep[Vector[T]] to Vector[T], so all methods/fields of Vector[T] are available on Rep[Vector[T]]. This is of cause true for other user defined types not only for Vector. If you need to add your own types see Extending Scalan below. There are certain rules how to declare UDTs that should be obeyed in order to make UDTs first-class citizens in Scalan framework. It's also possible to create regular Scala classes with Rep fields, but you won't be able to stage these classes directly (i.e. obtain a Rep[MyClass]) because necessary boilerplate will not be generated by scalan-meta for such declarations. Program structure and example Your program needs to extends ScalanDsl trait (along with any traits describing the DSLs you use). Here is a very simple example program: trait HelloScalan extends MatricesDsl {   lazy val run = fun { p: Rep[(Array[Array[Double]], Array[Double])] =>     val Pair(m, v) = p     val width = m(0).length     val matrix: Matrix[Double] = CompoundMatrix(Collection(m.map { r: Arr[Double] => DenseVector(Collection(r))}), width)     val vector: Vector[Double] = DenseVector(Collection(v))     (matrix * vector).items.arr   }   // example input   val matrix = Array(Array(1.0, 2.0), Array(3.0, 5.0))   val vector = Array(2.0, 3.0)   val input = (matrix, vector) } It can be seen to be very close to a usual Scala program, except for use of Rep type constructor and fun method. Note that run takes core types as argument and returns core types, not matrices and vectors themselves. This example is available in the repository. Please raise an issue if you find it isn't up-to-date! Now, there are two ways in which Scalan can work with this program: Sequential mode Run it without optimizations in order to ensure it works as desired and debug if necessary. This is done by mixing in ScalanCommunityDslSeq (and Seq versions of any additional DSLs used by your program): // to run: scalan-lms-backend/it:runMain HelloScalanSeq object HelloScalanSeq extends HelloScalan with MatricesDslSeq {   def result = run(input)    def main(args: Array[String]) = {     println(result.mkString("",""))   } } In this mode, Scalan's behavior is very simple: Rep[A] is the same type as A, and fun returns its argument, so you can mentally erase all Rep and fun. However, the structure of Scalan programs inhibits some of Scala's own optimization opportunities, so it should be expected to run somewhat slower than an equivalent Scala program. Staged mode Compile it to produce optimized code by mixing in ScalanCommunityDslExp (and Exp versions of any additional DSLs) and a compiler trait. // to run: scalan-lms-backend/it:runMain HelloScalanExp object HelloScalanExp {   // allows use of standard Scala library, commented out to make tests faster   // override val defaultCompilerConfig = CompilerConfig(Some(""2.11.7""), Seq.empty)    val program = new HelloScalan with MatricesDslExp    val compiler = new CommunityLmsCompilerScala(program)   import compiler._   import compiler.scalan._    def result = {     // output directory     val dir = new File(""it-out"")     val compiled = compiler.buildExecutable(       dir,       // generated class name       ""HelloScalan1"",       // function to compile       run,       // write .dot files containing graph IR with default settings       GraphVizConfig.default)     // not necessary if you just want to generate     // and compile the program     execute(compiled, input)   }    def main(args: Array[String]): Unit = {     println(result.mkString("",""))   } } Running this program will generate HelloScalan.scala and HelloScalan.jar in the given directory. HelloScalan class will have a apply((Array[Array[Double]], Array[Double])): Array[Double] method which corresponds to the run function. You can add either the source code or the jar to your own programs and call the apply method from them. Note that generated code depends only on the Scala standard library, not on Scalan. If it's acceptable for your program to depend on Scalan, it can also call Backend.execute method which loads the generated class and invokes the apply method. In this mode Rep[A] represents a value of type A in the generated code. Any values of non-Rep Scala types which appear in the Scalan program aren't represented directly. Scalan aggressively applies optimizations such as dead code elimination, common sub-expression elimination, and function inlining independently of backend. The backend can, of course, include its own optimizations as well (a major one in the LMS backend is loop fusion). Understanding Scalan code Scalan uses a variant of cake pattern for code organization. Namely, it is composed of a set of traits such as Base, Elems, etc. which define a component API (and helper methods using this API). Each component has two implementations ComponentNameSeq and ComponentNameExp which are used in sequential and staged mode respectively. There is a trait combining all components called Scalan, and corresponding ScalanSeq and ScalanExp traits. These traits are used as self-types for the components and their implementations, which allows each component to depend on all others. There are also ScalanCtxSeq and ScalanCtxExp which extend ScalanSeq and ScalanExp with implementations. scalan-library subproject adds more components. Their combination with Scalan is called ScalanCommunity (also with Seq and Exp versions). It also defines some DSLs which work the same as components, and ScalanCommunityDsl combines ScalanCommunity and all DSLs. Important types to understand are: Element[A] is a reified type representation. For all legal Scalan types Rep[A] an Element[A] instance exists and should be available implicitly. Exp[A] is Rep[A] in staged mode. As said above, it represents a staged value (i.e. a value in generated code) of type A or, more strictly speaking, an identifier of such a value. Def[A] is a definition of an Exp[A]. Extending Scalan New methods can be added to existing types using implicit conversions: implicit class Norm(arr: Rep[Array[Double]]) {   def norm: Rep[Double] = Math.sqrt((arr *^ arr).sum) }  val x: Rep[Array[Double]] = ... x.norm Normal Scala rules apply. It's also possible to add new user types to Scalan. As a simple example, we consider points on a plane. We have a trait describing the interface, and a class describing an implementation (though in this case there is only one implementation, the split is still required). They are contained in the trait Points which serves as a DSL component. trait Points { self: PointsDsl =>   trait Point {     def x: Rep[Double]     def y: Rep[Double]     def distance(other: Rep[Point]): Rep[Double] = Math.sqrt((x - other.x)*(x - other.x) + (y - other.y)*(y - other.y))   }   // trait PointCompanion // uncomment to add methods for companion object    abstract class PointImpl(val x: Rep[Double], val y: Rep[Double]) extends Point   // trait PointImplCompanion }  // generated automatically if absent; // you can add methods which shouldn't be staged or which have different // implementations in Seq and Exp contexts here  // trait PointsDsl extends impl.PointsAbs // trait PointsDslSeq extends impl.PointsSeq // trait PointsDslExp extends impl.PointsExp This obviously doesn't compile yet, because of references to non-existent classes in the impl package. They are boilerplate code which must be generated using the meta subproject. Currently this unfortunately has to be done manually by running SBT command meta/run <configurations>, where <configurations> is one or more configuration defined in BoilerplateTool.scala. This has to be done when a DSL file is added or changed, or after changes in the meta subproject itself. Projects which depend on Scalan and add their own DSLs will normally also have a meta subproject with a dependency on scalan-meta. Note that methods in Point must have Rep in argument types and return value type. If Point had a type parameter, it would also have methods asserting existence of Element instances. It may seem some of these empty traits are unnecessary, but they serve as extension points. E.g. any methods added to PointCompanion will be available on Point companion objects. See scalan.linalgebra.Vectors for a larger example. Adding new primitive operations, core types, or backends to Scalan is possible, but the API is currently not stable. Contributions Please feel free to open an issue if you notice a bug, have an idea for a feature, or have a question about the code. Minor pull requests (typos, bug fixes and so on) are gladly accepted; for anything larger please raise an issue first. Issues with the low-hanging fruit label should be easy to fix if you want something to get started with. If you want to start working on an issue (existing or one you just raised), please leave a comment to avoid effort duplication. Issues that someone is already working on are labelled in progress. See also Scalanizer - a Scala plugin which allows to capture Scala ASTs and translate it into Scalan. Scalanizer Demo - a simple project that demonstrates how to use Scalanizer, declare hot-spot regions and generate efficient kernels for JVM and native execution. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalan/scalan"	"A framework for development of domain-specific compilers in Scala"	"true"
"Misc"	"ScalaSTM"	"https://nbronson.github.io/scala-stm/"	"Software Transaction Memory for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"ScalaSTM — Library-Based Software Transactional Memory for Scala ScalaSTM Start Here Welcome Introduction Quick Start Cheat Sheet Waiting Maps + Sets Exceptions F.A.Q. Getting it Releases Snapshots Source License Examples Quick Start Indexed Map Philosophers Reference ScalaDoc API Benchmarking Expert Group Semantics Mailing List   Sponsors Welcome Welcome from the creators of Scala STM. We’ve built a lightweight software transactional memory for Scala, inspired by the STMs in Haskell and Clojure while taking advantage of Scala’s power and performance. ScalaSTM is a single JAR with no dependencies, and includes An API that supports multiple STM implementations A reference implementation based on CCSTM Scalable concurrent sets and maps (with fast snapshots) that can be used inside or outside transactions ScalaSTM provides a mutable cell called a Ref. If you build a shared data structure using immutable objects and Ref-s, then you can access it from multiple threads or actors. No synchronized, no deadlocks or race conditions, and good scalability. Included are concurrent sets and maps, and we also have an easier and safer replacement for wait and notifyAll. Learn more… News 2014 Apr 17 — ScalaSTM version 0.7 published for 2.11 2014 Apr 7 — ScalaSTM version 0.7 published for 2.11.0-RC4 2014 Mar 20 — ScalaSTM version 0.7 published for 2.11.0-RC3 2014 Mar 7 — ScalaSTM version 0.7 published for 2.11.0-RC1 2014 Feb 4 — ScalaSTM version 0.7 published for 2.11.0-M8 2014 Jan 12 — ScalaSTM version 0.7 published for 2.11.0-M7 2013 Sep 15 — ScalaSTM version 0.7 published for 2.11.0-M5 2013 Jul 28 — ScalaSTM version 0.7 published for 2.11.0-M4 2013 May 22 — ScalaSTM version 0.7 published for 2.11.0-M3 2013 Jan 12 — ScalaSTM version 0.7 published for 2.11.0-M1 2012 Dec 21 — ScalaSTM version 0.7 released, including debugger support via TxnDebuggable cooperation with Scala 2.10’s thread pools additional convenience methods on Ref, Ref.View, and TxnLocal 2012 Oct 16 — 0.7-SNAPSHOT: Improved support for interactive debuggers via scala.concurrent.stm.TxnDebuggable 2012 Oct 16 — Maven artifacts are now published under the “org.scala-stm” group id (publishing to “org.scala-tools” will continue for current versions), and are now synced automatically with the Maven central repo 2012 Jul 22 — ScalaSTM version 0.6 released, including enhancements to the Java convenience layer uses of scala.actors.threadpool.TimeUnit replaced by java.util.concurrent.TimeUnit 2012 Feb 2 — ScalaSTM version 0.5 released, including Java convenience layer scala.concurrent.stm.japi.STM 2011 Nov 9 — ScalaSTM version 0.4 released, including CommitBarrier, support for group commit STMBench7 benchmark support Better implementation selection 2011 Sep 5 — ScalaSTM 0.3 and 0.4-SNAPSHOT for Scala 2.9.1 released 2011 Jul 8 — 0.4-SNAPSHOT: STMBench7 support. See Benchmarking 2011 May 12 — ScalaSTM version 0.3 for 2.9.0 released 2011 May 8 — ScalaSTM version 0.3 for 2.9.0.RC4 released 2011 May 6 — ScalaSTM version 0.3 for 2.9.0.RC3 released 2011 Mar 26 — ScalaSTM version 0.3 released, a pure-Scala software TM library. Included Support for Scala 2.9.0.RC1 Timeouts for retry 2011 Feb 4 — 0.3-SNAPHOT: Bug fixes. 2011 Jan 15 — 0.3-SNAPHOT: Timeouts for retry. See Waiting : Timeouts 2011 Jan 2 — 0.3-SNAPHOT: Bug fixes. 2010 Dec 27 — ScalaSTM version 0.2 released, a library-based software transactional memory (STM) for Scala. Included Performance improvements and bug fixes Better TSet, TMap and TxnLocal Transaction statistics 2010 Dec 16 — 0.2-SNAPSHOT: Bug fix for while-committing handlers (#3). TxnLocal can now be read and written from while-preparing and while-committing handlers. 2010 Dec 8 — 0.2-SNAPSHOT: Substantial performance improvements, especially for nested atomic blocks. TMap.View and TSet.View are integrated with Scala collection class hierarchy. 2010 Dec 6 — ScalaSTM version 0.1 released, a library-based software transactional memory (STM) for Scala. Get it… Copyright © 2012-2014"	"null"	"null"	"Software Transaction Memory for Scala"	"true"
"Misc"	"YahooFinanceScala ★ 3 ⧗ 2"	"https://github.com/openquant/YahooFinanceScala"	"Get stock data from Yahoo Finance using Akka http."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"4"	"1"	"2"	"GitHub - openquant/YahooFinanceScala: A non-blocking Yahoo Finance Scala client Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 4 Fork 2 openquant/YahooFinanceScala Code Issues 1 Pull requests 0 Pulse Graphs A non-blocking Yahoo Finance Scala client 14 commits 1 branch 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.2 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. project src .gitignore .travis.yml CHANGELOG.md LICENSE README.md TODO.md build.sbt README.md YahooFinanceScala A non-blocking Yahoo Finance Scala client based on Akka streams. Add YahooFinanceScala to your build.sbt file libraryDependencies ++= Seq(  ""com.larroy.openquant"" %% ""yahoofinancescala"" % ""0.2"" ) Check it out in Central Usage $ sbt console import scala.concurrent.duration.Duration import openquant.yahoofinance.{YahooFinance, Quote, Fundamentals} import akka.actor.ActorSystem import java.time.ZonedDateTime import scala.concurrent.Await  implicit val system = ActorSystem()  val yahooFinance = new YahooFinance() val quotes: IndexedSeq[Quote] = Await.result(yahooFinance.quotes(""MSFT"", Some(ZonedDateTime.now().minusDays(5))), Duration.Inf) // Quote(2016-04-01T00:00-04:00[America/New_York],55.049999,55.57,55.610001,54.57,24298600,55.57) val fundamentals: IndexedSeq[Fundamentals] = Await.result(yahooFinance.fundamentals(""IBM""), Duration.Inf) // fundamentals: IndexedSeq[openquant.yahoofinance.Fundamentals] = Vector(Fundamentals(true,IBM,International Business Machines)) Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/openquant/YahooFinanceScala"	"Get stock data from Yahoo Finance using Akka http."	"true"
"Android"	"Android SDK Plugin for SBT ★ 438 ⧗ 1"	"https://github.com/pfn/android-sdk-plugin"	"An sbt plugin that adds tasks for developing Android applications."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"491"	"50"	"64"	"GitHub - scala-android/sbt-android: An easy-to-use sbt plugin for working with all Android projects Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 50 Star 491 Fork 64 scala-android/sbt-android Code Issues 6 Pull requests 3 Wiki Pulse Graphs An easy-to-use sbt plugin for working with all Android projects 1,044 commits 5 branches 159 releases 22 contributors Java 73.2% Scala 26.6% Other 0.2% Java Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.2.x 1.4.x master sbt-0.11 sbt-0.12 Nothing to show sbt-android-gradle-1.2.1 sbt-0.12-0.8.2 sbt-0.12-0.8.0 sbt-0.12-0.7.8 sbt-0.12-0.7.7 sbt-0.12-0.7.6 sbt-0.12-0.7.5 sbt-0.12-0.7.4 sbt-0.12-0.7.3 sbt-0.12-0.7.1 sbt-0.12-0.7.0 sbt-0.12-0.6.1 sbt-0.12-0.3.12 sbt-0.11-0.7.8 sbt-0.11-0.7.7 sbt-0.11-0.7.6 sbt-0.11-0.7.5 sbt-0.11-0.7.4 sbt-0.11-0.7.3 sbt-0.11-0.7.1 sbt-0.11-0.7.0 sbt-0.11-0.6.1 sbt-0.11-0.3.12 android-gradle-build-1.2.0 android-gradle-build-1.1.12 android-gradle-build-1.1.11 android-gradle-build-1.1.10 android-gradle-build-1.1.9 android-gradle-build-1.1.8 android-gradle-build-1.1.7 android-gradle-build-1.1.6 android-gradle-build-1.1.5 android-gradle-build-1.1.4 android-gradle-build-1.1.3 android-gradle-build-1.1.2 android-gradle-build-1.1.1 android-gradle-build-1.1.0 android-gradle-build-1.0.5 android-gradle-build-1.0.3 android-gradle-build-1.0.2 android-gradle-build-1.0.1 android-gradle-build-1.0.0 android-gradle-build-0.9.3 android-gradle-build-0.9.2 android-gradle-build-0.9.1 android-gradle-build-0.9 android-gradle-build-0.8 android-gradle-build-0.7 android-gradle-build-0.6 android-gradle-build-0.5 android-gradle-build-0.4 android-gradle-build-0.3 android-gradle-build-0.2 android-gradle-build-0.1 1.6.7 1.6.6 1.6.5 1.6.4 1.6.3 1.6.2 1.6.1 1.6.0 1.5.20 1.5.19 1.5.18 1.5.17 1.5.16 1.5.15 1.5.14 1.5.13 1.5.12 1.5.11 1.5.10 1.5.9 1.5.8 1.5.7 1.5.6 1.5.5 1.5.4 1.5.3 1.5.2 1.5.1 1.5.0 1.4.15 1.4.14 1.4.13 1.4.12 1.4.11 1.4.10 1.4.9 1.4.8 1.4.7 1.4.6 1.4.5 1.4.4 1.4.3 1.4.2 1.4.1 1.4.0 1.3.24 Nothing to show New pull request Latest commit 2551814 Jul 15, 2016 pfn Fix PR link Permalink Failed to load latest commit information. gradle-build/src/main improve 00-gradle-generated.sbt format May 27, 2016 gradle-model/src/main/java/com/hanhuy/gradle/discovery android-gradle-build 0.4: hack to include PackagingOptions Aug 25, 2015 gradle-plugin/src/main/java/com/hanhuy/gradle/discovery android-gradle-build 0.4: hack to include PackagingOptions Aug 25, 2015 project quote injars for mainDexClasses if windows only Aug 21, 2015 resources automatically setTag add TypedViewHolder.from Jul 12, 2016 sbt-test Do not run protify-compatibility on travis Jul 15, 2016 src Add viewholder entry for rootId when present Jul 12, 2016 .gitignore 1.0.6: fix getLocalJars in LibraryDependencies Sep 18, 2013 .travis.yml account for configs when including included layout views Jul 8, 2016 CHANGES.md Fix PR link Jul 15, 2016 GRADLE.md sbt-android 1.6.4; sbt-android-gradle 1.2.1 Jun 10, 2016 LICENSE add license file Apr 4, 2012 README.md 1.6.7 Jul 11, 2016 build.sbt fix #279 better error when included layout is missing Jul 12, 2016 README.md Build Android Projects Using SBT Current version is 1.6.7 (Change log) Auto-import from gradle using sbt-android-gradle NOTE: 1.6.0 is the last version published using addSbtPlugin(""com.hanhuy.sbt"" % ""android-sdk-plugin"" % ""1.6.0""), all future updates can be accessed by using addSbtPlugin(""org.scala-android"" % ""sbt-android"" % VERSION) Description This is an easy-to-use plugin for existing and newly created android projects. It is tested and developed against 0.13.6+. The plugin supports all android projects configurations. 3rd party libraries can be included by placing them in libs, or they can be added by using sbt's libraryDependencies feature. This build plugin is 100% compatible with the standard Android build system due to the use of the same underlying com.android.tools.build:builder implementation. NOTE: proguard 5.1 does not like old versions of scala. Projects that wish to use Proguard 5.1 or newer with Scala should use scalaVersion := ""2.11.5"" or newer. For compatible scala projects and java-based projects which wish to use proguard 5.1 (to fix issues around generic types being removed from base-classes) a workaround is to add this setting into your build.sbt: proguardVersion := ""5.1"". See proguard bug #549 and SI-8931 NOTE: support-v4 22.2.x triggers compilation errors, see #173 and SI-7741 Support and Help The first line of support is reading this README, beyond that, help can be found on the #sbt-android IRC channel on Freenode, or the scala-android/sbt-android gitter Example projects A growing collection of tests can be found under sbt-test/android-sdk-plugin/. These projects are examples of how to use the plugin in various configurations. Testing the plugin can be run via sbt scripted, they require a device or emulator to be running in order to pass. All tests have auto-generated build.properties and auto_plugins.sbt files that set the current version of sbt and the sbt-android to use for testing. Usage Install sbt (from scala-sbt.org or use your local packaging system like macports, brew, etc.) (OPTIONAL) Install the plugin globally by adding the following line in the file ~/.sbt/0.13/plugins/android.sbt: addSbtPlugin(""org.scala-android"" % ""sbt-android"" % ""1.6.7"")  Set the environment variable ANDROID_HOME pointing to the path where the Android SDK is installed. If ANDROID_HOME is not set, an Android SDK will be installed automatically at ~/.android/sbt/sdk. If any components are missing from your SDK, they will be installed automatically. (OPTIONAL) Set ANDROID_NDK_HOME if NDK building is desired and an NDK already installed. If neither are set, or an NDK is not installed, an NDK will be installed to ~/.android/sbt/sdk/ndk-bundle automatically if an NDK build is detected (Android.mk and friends) Create a new android project using gen-android if the plugin is installed globally Instead of creating a new project, one can also do sbt gen-android-sbt to make sure everything is properly setup in an existing project. (N/A if globally configured) Create a directory named project within your project and add the file project/plugins.sbt, in it, add the following line: addSbtPlugin(""org.scala-android"" % ""sbt-android"" % ""1.6.7"")  Create or edit the file named build.sbt and add the following line, (automatically performed if using gen-android) : androidBuild  Now you will be able to run SBT, some available commands in sbt are: compile Compiles all the sources in the project, java and scala Compile output is automatically processed through proguard if there are any Scala sources, otherwise; it can be enabled manually. android:package-release Builds a release APK and signs it with a release key if configured android:package-debug Builds a debug APK and signs it using the debug key android:package Builds an APK for the project of the last type selected, by default debug android:test run instrumented android unit tests android:install Install the application to device android:run Install and run the application on-device android:uninstall Uninstall the application from device Any task can be repeated continuously whenever any source code changes by prefixing the command with a ~. ~ android:package-debug will continuously build a debug build any time one of the project's source files is modified. If you want sbt-android to automatically sign release packages add the following lines to local.properties (or any file.properties of your choice that you will not check in to source control): key.alias: KEY-ALIAS key.alias.password: PASSWORD (optional, defaults to key.store.password) key.store: /path/to/your/.keystore key.store.password: KEYSTORE-PASSWORD key.store.type: pkcs12 (optional, defaults to jks) Advanced Usage IDE integration The recommended IDE is IntelliJ, not Android Studio. However Android Studio can be used with some massaging (i.e install the Scala Plugin). When loading a project into IntelliJ, it is required that the Android and Scala plugins are installed To ensure proper building, configure the IDE Run command to execute an SBT android:package task instead of Make (remove the make entry); this is found under Run Configurations The SBT plugin for IntelliJ is the one from orfjackal/idea-sbt-plugin The Scala plugin is still required for non-Scala projects in order to edit sbt build files from inside the IDE. IntelliJ 14 and newer now includes native support for importing projects from sbt-android. The process generally works well, however there are still several caveats: The idea-sbt-plugin is still required to actually perform the build aar resources do not show up in editor or autocomplete automatically They can be added manually, but must be added everytime the project is refreshed from SBT (SBT toolwindow -> Refresh) To add: Project Structure -> Modules -> + -> Import Module $HOME/.android/sbt/exploded-aars/AAR-PACKAGE-FOLDER Create from existing sources Next all the until to the Finish button, finish. Go to the Dependencies tab for the Module you want to be able to access the AAR resources, click + -> Module Dependency Select the newly added AAR module above, and it will now be visible. Steps 5 and 6 will need to be repeated any time the build description is refreshed (SBT toolwindow -> refresh) Consuming apklib and aar artifacts from other projects Optionally use apklib() or aar() specifying apklib() and aar() are only necessary if there are multiple filetypes for the dependency, such as jar, etc. libraryDependencies += apklib(""groupId"" % ""artifactId"" % ""version"", ""optionalArtifactFilename"") Basically, wrap the typical dependency specification with either apklib() or aar() to consume the library If aars or apklibs are duplicately included in a multi-project build, specify transitiveAndroidLibs := false apklib and aar that transitively depend on apklib and aar will automatically be processed. To disable set transitiveAndroidLibs := false Sometimes library projects and apklibs will incorrectly bundle android-support-v4.jar, to rectify this, add this setting, repeat for any other incorrectly added jars: unmanagedJars in Compile ~= { _ filterNot (_.data.getName startsWith ""android-support-v4"") } Using the google gms play-services aar: libraryDependencies +=   ""com.google.android.gms"" % ""play-services"" % ""VERSION""  Generating apklib and/or aar artifacts To specify that your project will generate and publish either an aar or apklib artifact simply change the android.Plugin.androidBuild line to one of the variants that will build the desired output type. For apklib use android.Plugin.androidBuildApklib For aar use android.Plugin.androidBuildAar Alternatively, use android.Plugin.buildAar and/or android.Plugin.buildApklib in addition to any of the variants above In build.sbt, add android.Plugin.buildAar and/or android.Plugin.buildApklib on a new line. It could also be specified, for example, like so: android.Plugin.androidBuild ++ android.Plugin.buildAar Multi-project builds See multi-project build examples in the test cases for an example of configuration. androidBuild(...) should be used to specify all dependent library-projects All sub-projects in a multi-project build must specify exportJars := true. Android projects automatically set this variable. When using multi-project builds in Scala, where library projects have scala code, but the main project(s) do(es) not, you will need to specify that proguard must run. To do this, the following must be set for each main project: proguardScala := true Configuring sbt-android by editing build.sbt Add configuration options according to the sbt style: useProguard := true to enable proguard. Note: if you disable proguard for scala, you must specify uses-library on a pre-installed scala lib on-device or enable multi-dex. Configurable keys can be discovered by typing android:<tab> at the sbt shell Configuring proguard, some options are available proguardOptions ++= Seq(""-dontobfuscate"", ""-dontoptimize"") - will tell proguard not to obfuscute nor optimize code (any valid proguard option is usable here) proguardConfig ... can be used to replace the entire proguard config included with sbt-android On-device testing, use android:test and see Android Testing Fundamentals Unit testing with robolectric and Junit (use the test task), see how it works in the robo-junit-test test case Be sure to set fork in Test := true otherwise the classloading black magic in robolectric will fail. Device Management The commands devices and device are implemented. The former lists all connected devices. The latter command is for selecting a target device if there is more than one device. If there is more than one device, and no target is selected, all commands will execute against the first device in the list. android:install, android:run and android:test are tasks that can be used to install, run and test the built apk on-device, respectively. Full list of sbt-android added commands, all commands have full tab completion when possible. adb-ls <path> adb-cat <file> adb-rm <file> adb-pull <file> [destination] adb-push <file> <destination> adb-shell <command> adb-runas <command> adb-kill[/project] logcat [-p pid] [-s tags] [options...] logcat-grep [-p pid] [regex] pidcat[/project] [partial pkg] [TAGs...] pidcat-grep[/project] [partial pkg] [regex] gen-android <platform> <package> <name> gen-android-sbt device <serial> devices adb-screenon adb-wifi adb-reboot [recovery|bootloader] variant[/project] [buildType] [flavor] variant-reset[/project] android-install <package> android-update <all|package> android-license <license-id> TODO / Known Issues autolibs do not properly process apklib and aar resources. If anything in an autolib uses resources from such a library, the answer is to create a standard multi-project build configuration rather than utilize autolibs. autolibs can be disabled by manually configuring localProjects Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/pfn/android-sdk-plugin"	"An sbt plugin that adds tasks for developing Android applications."	"true"
"Android"	"Gradle Android Scala Plugin ★ 270 ⧗ 0"	"https://github.com/saturday06/gradle-android-scala-plugin"	"A gradle plugin that allows you to use Scala with Android"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"287"	"40"	"35"	"GitHub - saturday06/gradle-android-scala-plugin: gradle-android-scala-plugin adds scala language support to official gradle android plugin Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 40 Star 287 Fork 35 saturday06/gradle-android-scala-plugin Code Issues 36 Pull requests 2 Pulse Graphs gradle-android-scala-plugin adds scala language support to official gradle android plugin 403 commits 10 branches 0 releases 5 contributors Groovy 45.4% Java 28.4% Scala 26.2% Groovy Java Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.2 1.2.1 1.3 1.3.1 1.3.2 1.4 1.5 android-plugin-1.5.0 gh-pages master Nothing to show Nothing to show New pull request Latest commit 11dd96a Apr 13, 2016 saturday06 Merge pull request #81 from faqqe/android-plugin-2.0.0 … Android plugin 2.0.0 Permalink Failed to load latest commit information. buildSrc Add com.android.build.threadPoolSize=5 to test/sample tasks Aug 31, 2015 gradle/wrapper Upgrade gradle to 2.12 Apr 12, 2016 sample Upgrade gradle to 2.12 Apr 12, 2016 src Make compatible with android gradle plugin 2.0.0 Apr 12, 2016 .gitignore Restore gradle wrapper Mar 29, 2015 .travis.yml Modernize .travis.yml Sep 8, 2015 LICENSE Initial commit Feb 5, 2014 README.md android plugin 1.3.1 Aug 31, 2015 build.gradle Upgrade gradle to 2.12 Apr 12, 2016 gradlew Upgrade gradle to 2.12 Apr 12, 2016 gradlew.bat Upgrade gradle to 2.12 Apr 12, 2016 settings.gradle Double quotations Jul 11, 2014 README.md gradle-android-scala-plugin gradle-android-scala-plugin adds scala language support to official gradle android plugin. See also sample projects at https://github.com/saturday06/gradle-android-scala-plugin/tree/master/sample Table of Contents generated with DocToc Supported versions Installation 1. Add buildscript's dependency 2. Apply plugin 3. Add scala-library dependency 4. Put scala source files 5. Implement a workaround for DEX 64K Methods Limit 5.1. Option 1: Use ProGuard 5.2. Option 2: Use MultiDex 5.2.1. Setup application class if you use customized one Configuration Complete example of build.gradle with manually configured MultiDexApplication Changelog Supported versions Scala Gradle Android Plugin compileSdkVersion buildToolsVersion 2.11.7 2.2.1 1.1.3, 1.2.3, 1.3.1 21, 22, 23 21.1.2, 22.0.1 2.10.5 2.2.1 1.1.3, 1.2.3, 1.3.1 21, 22, 23 21.1.2, 22.0.1 If you want to use older build environment, please try android-scala-plugin-1.3.2 Installation 1. Add buildscript's dependency build.gradle buildscript {     dependencies {         classpath ""com.android.tools.build:gradle:1.3.1""         classpath ""jp.leafytree.gradle:gradle-android-scala-plugin:1.4""     } } 2. Apply plugin build.gradle apply plugin: ""com.android.application"" apply plugin: ""jp.leafytree.android-scala"" 3. Add scala-library dependency The plugin decides scala language version using scala-library's version. build.gradle dependencies {     compile ""org.scala-lang:scala-library:2.11.7"" } 4. Put scala source files Default locations are src/main/scala, src/androidTest/scala. You can customize those directories similar to java. build.gradle android {     sourceSets {         main {             scala {                 srcDir ""path/to/main/scala"" // default: ""src/main/scala""             }         }          androidTest {             scala {                 srcDir ""path/to/androidTest/scala"" // default: ""src/androidTest/scala""             }         }     } } 5. Implement a workaround for DEX 64K Methods Limit The Scala Application generally suffers DEX 64K Methods Limit. To avoid it we need to implement one of following workarounds. 5.1. Option 1: Use ProGuard If your project doesn't need to run androidTest, You can use proguard to reduce methods. Sample proguard configuration here: proguard-rules.txt -dontoptimize -dontobfuscate -dontpreverify -dontwarn scala.** -ignorewarnings # temporary workaround; see Scala issue SI-5397 -keep class scala.collection.SeqLike {     public protected *; }  From: hello-scaloid-gradle 5.2. Option 2: Use MultiDex Android comes with built in support for MultiDex. You will need to use MultiDexApplication from the support library, or modify your Application subclass in order to support versions of Android prior to 5.0. You may still wish to use ProGuard for your production build. Using MultiDex with Scala is no different than with a normal Java application. See the Android Documentation and MultiDex author's Documentation for details. It is recommended that you set your minSdkVersion to 21 or later for development, as this enables an incremental multidex algorithm to be used, which is significantly faster. build.gradle repositories {     jcenter() }  android {     defaultConfig {         multiDexEnabled true     } }  dependencies {     compile ""org.scala-lang:scala-library:2.11.7""     compile ""com.android.support:multidex:1.0.1"" } Change application class. AndroidManifest.xml <?xml version=""1.0"" encoding=""utf-8""?> <manifest xmlns:android=""http://schemas.android.com/apk/res/android"" package=""jp.leafytree.sample"">     <application android:name=""android.support.multidex.MultiDexApplication""> </manifest> If you use customized application class, please read next section. To test MultiDexApplication, custom instrumentation test runner should be used. See also https://github.com/casidiablo/multidex/blob/publishing/instrumentation/src/com/android/test/runner/MultiDexTestRunner.java build.gradle android {     defaultConfig {         testInstrumentationRunner ""com.android.test.runner.MultiDexTestRunner""     } }  dependencies {     compile ""org.scala-lang:scala-library:2.11.7""     compile ""com.android.support:multidex:1.0.1""     androidTestCompile ""com.android.support:multidex-instrumentation:1.0.1"", { exclude module: ""multidex"" } } 5.2.1. Setup application class if you use customized one Since application class is executed before multidex configuration, Writing custom application class has stll many pitfalls. The application class must extend MultiDexApplication or override Application#attachBaseContext like following. MyCustomApplication.scala package my.custom.application  import android.app.Application import android.content.Context import android.support.multidex.MultiDex  object MyCustomApplication {   var globalVariable: Int = _ }  class MyCustomApplication extends Application {   override protected def attachBaseContext(base: Context) = {     super.attachBaseContext(base)     MultiDex.install(this)   } } You need to remember: NOTE: The following cautions must be taken only on your android Application class, you don't need to apply this cautions in all classes of your app The static fields in your application class will be loaded before the MultiDex#installbe called! So the suggestion is to avoid static fields with types that can be placed out of main classes.dex file. The methods of your application class may not have access to other classes that are loaded after your application class. As workaround for this, you can create another class (any class, in the example above, I use Runnable) and execute the method content inside it. Example:   override def onCreate = {     super.onCreate      val context = this     new Runnable {       override def run = {         variable = new ClassNeededToBeListed(context, new ClassNotNeededToBeListed)         MyCustomApplication.globalVariable = 100       }     }.run   } This section is copyed from README.md for multidex project Configuration You can configure scala compiler options as follows: build.gradle tasks.withType(ScalaCompile) {     // If you want to use scala compile daemon     scalaCompileOptions.useCompileDaemon = true     // Suppress deprecation warnings     scalaCompileOptions.deprecation = false     // Additional parameters     scalaCompileOptions.additionalParameters = [""-feature""] } Complete list is described in http://www.gradle.org/docs/current/dsl/org.gradle.api.tasks.scala.ScalaCompileOptions.html Complete example of build.gradle with manually configured MultiDexApplication build.gradle buildscript {     repositories {         mavenCentral()     }      dependencies {         classpath ""com.android.tools.build:gradle:1.3.1""         classpath ""jp.leafytree.gradle:gradle-android-scala-plugin:1.4""     } }  repositories {     jcenter() }  apply plugin: ""com.android.application"" apply plugin: ""jp.leafytree.android-scala""  android {     compileSdkVersion ""android-22""     buildToolsVersion ""22.0.1""      defaultConfig {         targetSdkVersion 22         testInstrumentationRunner ""com.android.test.runner.MultiDexTestRunner""         versionCode 1         versionName ""1.0""         multiDexEnabled true     }      productFlavors {         dev {             minSdkVersion 21 // To reduce compilation time         }          prod {             minSdkVersion 8         }     }      sourceSets {         main {             scala {                 srcDir ""path/to/main/scala"" // default: ""src/main/scala""             }         }          androidTest {             scala {                 srcDir ""path/to/androidTest/scala"" // default: ""src/androidTest/scala""             }         }     } }  dependencies {     compile ""org.scala-lang:scala-library:2.11.7""     compile ""com.android.support:multidex:1.0.1""     androidTestCompile ""com.android.support:multidex-instrumentation:1.0.1"", { exclude module: ""multidex"" } }  tasks.withType(ScalaCompile) {     scalaCompileOptions.deprecation = false     scalaCompileOptions.additionalParameters = [""-feature""] } Changelog 1.4 Support android plugin 1.1.3. Manual configuration for dex task is now unnecessary (contributed by sgrif) 1.3.2 Fix unexpected annotation processor's warnings 1.3.1 Support android plugin 0.12.2 1.3 Incremental compilation support in scala 2.11 1.2.1 Fix binary compatibility with JDK6 1.2 Incremental compilation support in scala 2.10 / Flavors support 1.1 MultiDexApplication support 1.0 First release Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/saturday06/gradle-android-scala-plugin"	"A gradle plugin that allows you to use Scala with Android"	"true"
"Android"	"Macroid ★ 403 ⧗ 0"	"https://github.com/47deg/macroid"	"A modular functional UI language for Android."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"438"	"52"	"29"	"GitHub - 47deg/macroid: A modular functional UI language for Android Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 438 Fork 29 47deg/macroid Code Issues 19 Pull requests 4 Pulse Graphs A modular functional UI language for Android http://macroid.github.io 376 commits 4 branches 14 releases 9 contributors Scala 98.4% Shell 1.3% JavaScript 0.3% Scala Shell JavaScript Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags +core-add-contexts-view ft-refactor-concurrency master v1 Nothing to show v2.0.0-M5 v2.0.0-M4 v2.0.0-M3 v2.0.0-M2 v2.0.0-M1 v2.0.0-20150427 v2.0.0-20140412 v2.0.0-20140401 v2.0.0-20140322 v2.0.0-20140312 v2.0.0-20130330 v1.1.0-20131212 v1.0.1 v1.0.0 Nothing to show New pull request Latest commit 93c5e31 May 26, 2016 raulraja Merge pull request #88 from 47deg/=all-new-snapshot … New 2.0.0-M6-SNAPSHOT Permalink Failed to load latest commit information. macroid-akka = all: add sbt-scalariform Feb 14, 2016 macroid-core/src +core: Adding implicit ContextWrapper for Views Mar 1, 2016 macroid-docs +core: Adding implicit ContextWrapper for Views Mar 1, 2016 macroid-viewable = all: migrate macros to macro-compat Feb 16, 2016 project Fixes travis file Apr 19, 2016 .gitignore =all: Prepares for Sonatype releases Apr 19, 2016 .travis.yml Create automatically snapshot in master Apr 19, 2016 README.md New 2.0.0-M6-SNAPSHOT May 23, 2016 build.sbt New 2.0.0-M6-SNAPSHOT May 23, 2016 rings.tar.enc =all: Prepares for Sonatype releases Apr 19, 2016 README.md Macroid — a Scala GUI DSL for Android Macroid is the most badass modular functional user interface creation language for Android, implemented with Scala macros. Some people say it’s also a dead noodle. Striving to be focused on one thing (GUI), Macroid promotes composability and high-level abstractions. Prerequisites: Scala 2.10.x or 2.11.x, Android API 9+. Latest version: 2.0.0-M5 Snapshot version: 2.0.0-M6-SNAPSHOT License: MIT. What does it look like How is it different from... Detailed guide Mailing list For more info head to http://macroid.github.io! Installation libraryDependencies ++= Seq(   aar(""org.macroid"" %% ""macroid"" % ""2.0.0-M5"")  If you want to use the SNAPSHOT version you need to add the Sonatype SNAPSHOT repo resolvers +=   ""Sonatype OSS Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots""  libraryDependencies ++= Seq(   aar(""org.macroid"" %% ""macroid"" % ""2.0.0-M6-SNAPSHOT"")  Contributing All contributions are welcome (and encouraged)! Commit messages Macroid’s commit message structure is inspired by the Spray project. The message has the following format: [=|+|!] [core|viewable|akka|docs|all]: <Actual message>.  = means there are no API changes + means added functionality ! means breaking changes (source or binary) Example: ! core: Receive UI actions in mapUi & co (fix #48)  mapUi, flatMapUi, ... now operate on UI actions, rather than simple thunks. For example, the new type signature for mapUi is (A ⇒ Ui[B]) ⇒ Future[B].  Following this convention greatly simplifies writing the changelogs. Documentation Although this is not crucial, updating the docs under macroid-docs together with the code changes might save some time in the future, and thus is highly appreciated. It can be done in the same commit. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/47deg/macroid"	"A modular functional UI language for Android."	"true"
"Android"	"Scaloid ★ 1906 ⧗ 0"	"https://github.com/pocorall/scaloid"	"Less painful Android development with Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1973"	"156"	"163"	"GitHub - pocorall/scaloid: Scaloid makes your Android code easy to understand and maintain. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 156 Star 1,973 Fork 163 pocorall/scaloid Code Issues 15 Pull requests 0 Wiki Pulse Graphs Scaloid makes your Android code easy to understand and maintain. http://blog.scaloid.org/ 892 commits 2 branches 31 releases Fetching contributors Scala 99.4% Java 0.6% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 4.2 4.1 4.0 4.0-RC2 4.0-RC1 3.6.1 3.6 3.5 3.4 3.3 3.2.1 3.2 3.1 3.0 3.0-M2 3.0-M1 2.5 2.3 2.2 2.1 2.0 2.0-RC2 2.0-RC1 1.1 1.0 0.9 0.8 0.7.1 0.7 0.6 0.5 Nothing to show New pull request Latest commit 3624348 Jul 5, 2016 pocorall Fix #147 Permalink Failed to load latest commit information. project Configure external links in ScalaDoc Apr 16, 2016 scaloid-common/src Fix #147 Jul 5, 2016 scaloid-support-v4/src/main Change type parameter names Aug 9, 2015 scaloid-util/src/main/scala/org/scaloid/util Fix util.Styles.white() Jun 25, 2015 .gitignore fix a missed wildcard May 25, 2013 LICENSE.txt add LICENSE.txt Nov 15, 2014 README.md Fix #139 Feb 12, 2016 README.md Simpler Android Scaloid is a library that simplifies your Android code. It makes your code easy to understand and maintain by leveraging Scala language. For example, the code block shown below: val button = new Button(context) button.setText(""Greet"") button.setOnClickListener(new OnClickListener() {   def onClick(v: View) {     Toast.makeText(context, ""Hello!"", Toast.LENGTH_SHORT).show()   } }) layout.addView(button) is reduced to: SButton(""Greet"", toast(""Hello!"")) Benefits Write elegant Android software Simplicity is number one principle, keeps programmability and type-safety. Easy to use Check the quick start guide Compatible with your legacy code You can use both Scaloid and plain-old Java Android API. You can gradually improve your legacy code. Production quality Not a toy project. The creator of Scaloid uses it to build a millionth downloaded app. Demos Fork one of this to start a new project: Hello world of Scaloid for sbt (recommended, it builds faster) Hello world of Scaloid for maven Hello world of Scaloid for gradle Learn how Scaloid can be used in action: Scaloid port of apidemos app List of projects using Scaloid Tutorial by Gaston Hillar - part 1 and part 2 Contents Core design principle UI Layout without XML Layout context Styles for programmers Automatic layout converter Lifecycle management Asynchronous task processing Implicit conversions Shorter listeners Database cursor Traits Smarter logging Improved getters/setters Classes Concise dialog builder Beauty ArrayAdapter Dynamically Preferences Read in blog Binding services concisely Read in blog Other links Quick start guide API doc Blog Twitter FAQs FAQs about Scala on Android Inside Scaloid We are hiring! Core design principle ""Being practically simple"" is number one principle of Scaloid. Most frequently used things should be written shorter, like Huffman coding. To do this, I first observed Android programs I wrote, and thought that which part of the code is more fundamental than others. For example, what is the most essential part of buttons? Buttons should have some visible things on it, such as title or image, so the buttons are created like this: SButton(""Hello""). The second essential part is doing something when it is pressed: SImageButton(R.drawable.hello, toast(""World!"")). What should be the third one? The answer might not the same for every people, but I think that repetition frequency of press-and-hold action is nice: SButton(""Add"", n += 1, 500) increases n for every 500 milliseconds when the user holds the button. UI Layout without XML Android SDK leverages XML to build UI layouts. However, XML is considered still a bit verbose, and lacks programmability. Scaloid composes UI layout in Scala DSL style, therefore achieve both clarity and programmability. For example, suppose a legacy XML layout as shown below: <LinearLayout xmlns:android=""http://schemas.android.com/apk/res/android""         android:orientation=""vertical"" android:layout_width=""match_parent""         android:layout_height=""wrap_content"" android:padding=""20dip"">     <TextView android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:text=""Sign in""             android:layout_marginBottom=""25dip"" android:textSize=""24.5sp""/>     <TextView android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:text=""ID""/>     <EditText android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:id=""@+id/userId""/>     <TextView android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:text=""Password""/>     <EditText android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:id=""@+id/password""             android:inputType=""textPassword""/>     <Button android:layout_width=""match_parent""             android:layout_height=""wrap_content"" android:id=""@+id/signin""             android:text=""Sign in""/>     <LinearLayout android:orientation=""horizontal""             android:layout_width=""wrap_content""             android:layout_height=""wrap_content"">         <Button android:text=""Help"" android:id=""@+id/help""                 android:layout_width=""match_parent"" android:layout_height=""wrap_content""/>         <Button android:text=""Sign up"" android:id=""@+id/signup""                 android:layout_width=""match_parent"" android:layout_height=""wrap_content""/>     </LinearLayout> </LinearLayout> is reduced to: new SVerticalLayout {   STextView(""Sign in"").textSize(24.5.sp).<<.marginBottom(25.dip).>>   STextView(""ID"")   SEditText()   STextView(""Password"")   SEditText() inputType TEXT_PASSWORD   SButton(""Sign in"")   new SLinearLayout {     SButton(""Help"")     SButton(""Sign up"")   }.wrap.here }.padding(20.dip) The layout description shown above is highly programmable. You can easily wire your logic into the layout: new SVerticalLayout {   STextView(""Sign in"").textSize(24.5.sp).<<.marginBottom(25.dip).>>   STextView(""ID"")   val userId = SEditText()   STextView(""Password"")   val pass = SEditText() inputType TEXT_PASSWORD   SButton(""Sign in"", signin(userId.text, pass.text))   new SLinearLayout {     SButton(""Help"", openUri(""http://help.url""))     SButton(""Sign up"", openUri(""http://signup.uri""))   }.wrap.here }.padding(20.dip) Because a Scaloid layout description is plain Scala code, it is type-safe. Automatic layout converter This converter turns an Android XML layout into a Scaloid layout: http://layout.scaloid.org Migration tip Scaloid is fully compatible with legacy xml layout files. You can access a widget described in xml layout as: onCreate {   setContentView(R.layout.main)   val name = find[EditText](R.id.name)   // do something with `name` } Responsive layout Basically, a layout written in Scaloid is just an ordinary Scala code, so you can just freely composite the layout according to the device configuration: import org.scaloid.util.Configuration._  if(long) SButton(""This button is shown only for a long screen ""   + ""dimension (""+ width + "", "" + height + "")"") if(landscape) new SLinearLayout {   SButton(""Buttons for"")   SButton(""landscape layout"")   if(dpi <= HDPI) SButton(""You have a high resolution display!"") }.here Please refer to this blog post for more detail: Syntactic sugar for multiple device configuration Further readings about Scaloid layout Accessing widgets in view class Layout context In-depth tutorial on styles Styles for programmers Lifecycle management With Android API, Registering and unregistering BroadcastReceiver can be done as: var connectivityListener: BroadcastReceiver = null  def onResume() {   super.onResume()   // ...   connectivityListener = new BroadcastReceiver {     def onReceive(context: Context, intent: Intent) {      doSomething()     }   }    registerReceiver(connectivityListener, new IntentFilter(ConnectivityManager.CONNECTIVITY_ACTION)) }  def onPause() {   unregisterReceiver(connectivityListener)   // ...   super.onPause() } In Scaloid, the directly equivalent code is: broadcastReceiver(ConnectivityManager.CONNECTIVITY_ACTION) {   doSomething() } Scaloid has highly flexible resource register/unregister management architecture. If this code is written in services, registering and unregistering is done in onCreate and onDestroy respectively. If the same code is in activities, registering and unregistering is done in onResume and onPause respectively. This is just a default behavior. You can override a preference that determine when the register/unregister preforms. Overriding it is simple as well: broadcastReceiver(ConnectivityManager.CONNECTIVITY_ACTION) {   doSomething() }(this, onStartStop) Then, the receiver is registered onStart, and unregistered onStop. onDestroy can be called in many times! You can declare onDestroy behaviors in many places. This simplifies resource management significantly. Suppose you want to open a stream from a file: def openFileStream(file: File): InputStream = {   val stream = new FileInputStream(file)   onDestroy(stream.close()) // automatically closed when the Activity is destroyed!!   stream } onDestroy is a method that adds a function into the job list triggered when the activity is destroyed. So, we just get a stream from openFileStream() and forget about releasing it. Other lifecycle states (onCreate, onResume, onStop and so on) can be treated in the same way. Further reading: Refer to this blog post for more details. Asynchronous task processing Android API provides runOnUiThread() only for class Activity. Scaloid provides a Scala version of runOnUiThread() for anywhere other than Activity. Instead of: activity.runOnUiThread {   new Runnable() {     def run() {       debug(""Running only in Activity class"")     }   } } In Scaloid, use it like this: runOnUiThread(debug(""Running in any context"")) Running a job asynchronously and notifying the UI thread is a very frequently used pattern. Although Android API provides a helper class AsyncTask, implementing such a simple idea is still painful, even when we use Scala: new AsyncTask[String, Void, String] {   def doInBackground(params: Array[String]) = {     doAJobTakeSomeTime(params)   }    override def onPostExecute(result: String) {     alert(""Done!"", result)   } }.execute(""param"") Using scala.concurrent.Future, the asynchronous job shown above can be rewritten like this: Future {   val result = doAJobTakeSomeTime(params)   runOnUiThread(alert(""Done!"", result)) } When you don't want to build sophisticate UI interactions, but just want to display something by calling a single Scaloid method (e.g. alert, toast, and spinnerDialog), Scaloid handles runOnUiThread for you. Therefore, the code block shown above is reduced to: Future {   alert(""Done!"", doAJobTakeSomeTime(params)) } It is a great win as it exposes your idea clearly. Just like we thrown away AsyncTask, we can also eliminate all other Java helpers for asynchronous job, such as AsyncQueryHandler and AsyncTaskLoader. Compare with the original Java code and a Scala port of ApiDemos example app. Using Future is just an example of asynchronous task processing in Scaloid. You can freely use any modern task management utilities. Further reading: Refer to this blog post for an important consideration when using Future in Android. Implicit conversions Scaloid employs several implicit conversions. Some of the available implicit conversions are shown below: Uri conversion String => Uri The functions such as play ringtones play() or open URIs openUri() takes an instance of Uri as a parameter. However, we frequently have URIs as a String. Scaloid implicitly converts String into Uri. Therefore, you can freely use String when you play a ringtone: play(""content://media/internal/audio/media/50"") , open a URI: openUri(""http://scaloid.org"") , or wherever you want. Alternatively, you can specify the conversion as: val uri:Uri = ""http://scaloid.org"".toUri Unit conversion Units dip and sp can be converted into the pixel unit. val inPixel:Int = 32.dip val inPixel2:Int = 22.sp Reversely, pixel unit can also be converted into dip and sp unit. val inDip:Double = 35.px2dip val inSp:Double = 27.px2sp Resource IDs Scaloid provides several implicit conversions that convert from Int type resource ID to CharSequence, Array[CharSequence], Array[String], Drawable and Movie. For example: def toast(msg:CharSequence) = ...  toast(R.string.my_message) // implicit conversion works! Although Scaloid provides these conversions implicitly, explicit conversion may be required in some context. In this case, methods r2... are provided for the Int type: warn(""Will display the content of the resource: "" + R.string.my_message.r2String) Currently, r2Text, r2TextArray, r2String, r2StringArray, r2Drawable and r2Movie is provided. Further reading: Why implicit conversion of Resource ID is cool? Context as an implicit parameter Many methods in the Android API require an instance of a class Context. Providing this for every method call results in clumsy code. We employ an implicit parameter to eliminate this. Just declare an implicit value that represents current context: implicit val ctx = ... or just extend trait SContext, which defines it for you. Then the code that required Context becomes much simpler, for example: Intent new Intent(context, classOf[MyActivity]) is reduced to: SIntent[MyActivity] When a method takes an Intent as a first parameter in which we want to pass the newly created intent object, the parameter can be omitted. For example: startService(new Intent(context, classOf[MyService])) stopService(new Intent(context, classOf[MyService])) is reduced to: startService[MyService] stopService[MyService] or val intent = // initialize the intent and put some attributes on it intent.start[MyActivity] An intent that has a long list of extra attributes: new Intent().putExtra(""valueA"", valueA).putExtra(""valueB"", valueB).putExtra(""valueC"", valueC) is reduced to: new Intent().put(valueA, valueB, valueC) Toast toast(""hi, there!"") If you want a longer toast: longToast(""long toast"") Dialog ProgressDialog.show(context, ""Dialog"", ""working..."", true) is reduced to: spinnerDialog(""Dialog"", ""working..."") When you call toast, longToast or spinnerDialog from non-UI thread, you don't have to mind about threading. The toast example shown above is equivalent to the following Java code: activity.runOnUiThread(new Runnable() {     public void run() {         Toast.makeText(activity, ""hi, there!"", Toast.LENGTH_SHORT).show();     } }); Pending intent PendingIntent.getActivity(context, 0, new Intent(context, classOf[MyActivity]), 0) PendingIntent.getService(context, 0, new Intent(context, classOf[MyService]), 0) is reduced to: pendingActivity[MyActivity] pendingService[MyService] Open URIs This opens a web browser (or another view assigned to the http protocol). openUri(""http://scaloid.org"") System services Getting system service objects become much simpler. The following legacy code: val vibrator = context.getSystemService(Context.VIBRATOR_SERVICE).asInstanceOf[Vibrator] vibrator.vibrate(500) is reduced to: vibrator.vibrate(500) Under the hood, Scaloid defines a function vibrator like this: def vibrator(implicit ctx: Context) = ctx.getSystemService(Context.VIBRATOR_SERVICE).asInstanceOf[Vibrator] All the system service accessors available in Android API level 8 are defined (e.g. audioManager, alarmManager, notificationManager, etc.). The name of a system service accessor is the same as its class name, except that the first character is lowercased. Enriched Implicit classes Suppose an Android class Foo, for example, Scaloid defines an implicit conversion Foo => RichFoo. The class RichFoo defines additional methods for more convenient access to Foo. This is a common pattern in Scala to extend existing API (see pimp-my-library pattern). This section describes various features added on existing Android API classes. Listeners Android API defines many listener interfaces for callback notifications. For example, View.OnClickListener is used to be notified when a view is clicked: find[Button](R.id.search).setOnClickListener(new View.OnClickListener {   def onClick(v:View) {     openUri(""http://scaloid.org"")   } }) Scaloid provides a shortcut that dramatically reduces the length of the code: find[Button](R.id.search).onClick(openUri(""http://scaloid.org"")) All other listener-appending methods such as .onKey(), .onLongClick(), and .onTouch() are defined. Some conventions we employed for method naming are: We omit set..., add..., and ...Listener from the method name, which is less significant. For example, .setOnKeyListener() becomes .onKey(). Every method has two versions of parameters overridden. One is a lazy parameter, and another is a function which has full parameters defined in the original Android API. For example, these two usages are valid: button.onClick(info(""touched"")) button.onClick((v:View) => info(""touched a button ""+v)) Methods add... is abbreviated with a method += if it is not a listener-appender. For example, layout.addView(button) becomes layout += button. Multiple method listeners Methods beforeTextChanged(), onTextChanged(), and afterTextChanged() are defined in RichTextView, which can be implicitly converted from TextView. It is more convenient than using TextWatcher directly. For example: inputField.beforeTextChanged(saveTextStatus()) is equivalent to: inputField.addTextChangedListener(new TextWatcher {   def beforeTextChanged(s: CharSequence, start: Int, before: Int, count: Int) {     saveTextStatus()   }    def onTextChanged(p1: CharSequence, p2: Int, p3: Int, p4: Int) {}    def afterTextChanged(p1: Editable) {} }) Also, we override beforeTextChanged() with full parameters defined in the original listener: inputField.beforeTextChanged((s:CharSequence, _:Int, _:Int) => saveText(s)) Other listeners in Android API can also be accessed in this way. Layout context In Android API, layout information is stored into a View object via the method View.setLayoutParams(ViewGroup.LayoutParams). A specific type of parameter passing into that method is determined by a the type of ...Layout object which contains the View object. For example, let us see some Java code shown below: LinearLayout layout = new LinearLayout(context); Button button = new Button(context); button.setText(""Click""); LinearLayout.LayoutParams params = new LinearLayout.LayoutParams(); params.weight = 1.0f;  // sets some value button.setLayoutParams(params); layout.addView(button); Because the button is appended into the LinearLayout, the layout parameter must be LinearLayout.LayoutParams, otherwise a runtime error might be occurred. Meanwhile, Scaloid eliminate this burden, while still preserving rigorous typing of LayoutParams. The code shown below is equivalent to the previous Java code: val layout = new SLinearLayout {   SButton(""Click"").<<.Weight(1.0f).>> } In the anonymous constructor of 'SLinearLayout', Scaloid provides an implicit function called ""layout context"". This affects a return type of the method << defined in the class SButton. If we use SFrameLayout as a layout context, the method << returns FrameLayout.LayoutParams, which does not have Weight method. Therefore, the code below results a syntax error. val layout = new SFrameLayout {   SButton(""Click"").<<.Weight(1.0f).>>   // Syntax error on Weight() } Compared with XML layout description, Scaloid layout is simple and type-safe. The method << is overloaded with parameters <<(width:Int, height:Int) which assigns the size of the view component. For example: SButton(""Click"").<<(40.dip, WRAP_CONTENT) Operator new and method apply Usually, View components are referenced multiple times in an Activity. For example: lazy val button = new SButton() text ""Click"" onCreate {   contentView = new SLinearLayout {     button.here   } } // ... uses the button somewhere in other methods (e.g. changing text or adding listeners) Prefixed classes in Scaloid (e.g. SButton) have a companion object that implements apply methods that create a new component. These methods also append the component to the layout context that enclose the component. Therefore, the code block from the above example: button = new SButton() text ""Click"" button.here is equivalent to: button = SButton(""Click"") Because the apply methods access to the layout context, it cannot be called outside of the layout context. In this case, use the new operator instead. Method >> As we noted, the method << returns an object which is a type of ViewGroup.LayoutParams: val params = SButton(""Click"").<<   // type LayoutParams This class provides some setters for chaining: val params = SButton(""Click"").<<.marginBottom(100).marginLeft(10)   // type LayoutParams if we want use the SButton object again, Scaloid provides >> method returning back to the object: val button = SButton(""Click"").<<.marginBottom(100).marginLeft(10).>>   // type SButton Nested layout context When the layout context is nested, inner-most layout's context is applied: val layout = new SFrameLayout {   new SLinearLayout {     SButton(""Click"").<<.Weight(1.0f).>>   // in context of SLinearLayout   }.here } Methods fill, wrap, wf and fw When we get a LayoutParams from <<, the default values of width and height properties are width = FILL_PARENT and height = WRAP_CONTENT. You can override this when you need it: SButton(""Click"").<<(FILL_PARENT, FILL_PARENT) This is a very frequently used idiom. Therefore we provide further shorthand: SButton(""Click"").<<.fill If you want the View element to be wrapped, SButton(""Click"").<<(WRAP_CONTENT, WRAP_CONTENT) This is also shortened as: SButton(""Click"").<<.wrap Similarly, <<(WRAP_CONTENT, FILL_PARENT) and <<(FILL_PARENT, WRAP_CONTENT) can also be shortend as <<.wf and <<.fw respectively. Because there are so many occurences <<.wrap.>> pattern in actual Android code, it is allowed to remove .<< and .>> in this case: SButton(""Click"").wrap    // returns SButton type This pattern also usable for .fill, .fw and .wf methods. Styles for programmers Naming conventions Scaloid follows the naming conventions of XML attributes in the Android API with some improvements. For XML attributes, layout related properties are prefixed with layout_ and as you might have guessed, Scaloid does not need it. For boolean attributes, the default is false. However, Scaloid flags it as true when the attribute is declared explicitly without any parameter. For example: new SRelativeLayout {   STextView(""hello"").<<.centerHorizontal.alignParentBottom.>> } Scaloid omits unnecessary =""true"" for the attribute centerHorizontal. Equivalent XML layout description for TextView is: <TextView     android:id=""@+id/helloText""     android:layout_width=""fill_parent""     android:layout_height=""wrap_content""     android:layout_centerHorizontal=""true""     android:layout_alignParentBottom=""true""     android:text=""hello""/> For layout methods named with four directions (e.g. ...Top, ...Right, ...Bottom and ...Left), Scaloid provides additional methods that specifies all properties at once. For example, Because Android XML layout defines margin... properties(marginTop(v:Int), marginRight(v:Int), marginBottom(v:Int) and marginLeft(v:Int)), Scaloid provides additional margin(top:Int, right:Int, bottom:Int, left:Int) and margin(amount:Int) methods that can be used as: STextView(""hello"").<<.margin(5.dip, 10.dip, 5.dip, 10.dip) or STextView(""hello"").<<.margin(10.sp)  // assigns the same value for all directions Android SDK introduced styles to reuse common properties on XML layout. We repeatedly pointed out that XML is verbose. To apply styles in Scaloid, you do not need to learn any syntax or API library, because Scaloid layout is an ordinary Scala code. Just write a code that work as styles. Basic: Assign it individually Suppose the following code that repeats some properties: SButton(""first"").textSize(20.dip).<<.margin(5.dip).>> SButton(""prev"").textSize(20.dip).<<.margin(5.dip).>> SButton(""next"").textSize(20.dip).<<.margin(5.dip).>> SButton(""last"").textSize(20.dip).<<.margin(5.dip).>> Then we can define a function that applies these properties: def myStyle = (_: SButton).textSize(20.dip).<<.margin(5.dip).>> myStyle(SButton(""first"")) myStyle(SButton(""prev"")) myStyle(SButton(""next"")) myStyle(SButton(""last"")) Still not satisfying? Here we have a shorter one: def myStyle = (_: SButton).textSize(20.dip).<<.margin(5.dip).>> List(""first"", ""prev"", ""next"", ""last"").foreach(title => myStyle(SButton(title))) Advanced: CSS-like stylesheet Scaloid provides SViewGroup.style(View => View) method to provide more generic component styling. The parameter is a function which receives a view requested for styling, and returns a view which is finished applying the style. Then the example in the previous subsection becomes: style {   case b: SButton => b.textSize(20.dip).<<.margin(5.dip).>> }  SButton(""first"") SButton(""prev"") SButton(""next"") SButton(""last"") Note that individually applying myStyle is reduced. Let us see another example: style {   case b: SButton => b.textColor(Color.RED).onClick(toast(""Bang!""))   case t: STextView => t.textSize(10.dip)   case v => v.backgroundColor(Color.YELLOW) }  STextView(""I am 10.dip tall"") STextView(""Me too"") STextView(""I am taller than you"").textSize(15.dip) // overriding SEditText(""Yellow input field"") SButton(""Red alert!"") Similar to CSS, you can assign different styles for each classes using Scala pattern matching. Unlike Android XML styles or even CSS, Scaloid can assign some actions to the component (see onclick(toast(...))), or can do anything that you imagine. Also, you can easily override the property individually, as shown in the example above. Last thing that you may missed: These are type-safe. If you made a mistake, compiler will check it for you. Further readings: Accessing widgets in view class In-depth tutorial on styles Traits Trait UnregisterReceiver When you register BroadcastReceiver with Context.registerReceiver() you have to unregister it to prevent memory leak. Trait UnregisterReceiver handles these chores for you. All you need to do is append the trait to your class. class MyService extends SService with UnregisterReceiver {   def func() {     // ...     registerReceiver(receiver, intentFilter)     // Done! automatically unregistered at UnregisterReceiverService.onDestroy()   } } Trait SActivity Instead of findViewById(R.id.login).asInstanceOf[Button] use a shorthand: find[Button](R.id.login) Although we provide this shorthand, Scaloid recommends programmatically laying out UI, not with XML. Activity as an implicit parameter Similar to the implicit context, an Activity typed implicit parameter is also required for some methods. Therefore, you have to define an activity as an implicit value: implicit val ctx: Activity = ... Because the class Activity is a subclass of Context, it can also be an implicit context. When you extend SActivity, object this is assigned as the implicit activity by default. Here we show some example cases of using the implicit activity: Automatically allocate a unique View ID Often, Views are required to have an ID value. Although Android API document specifies that the ID need not be unique, allocating unique ID is virtually mandatory in practice. Scaloid provides a package scope function getUniqueId, which returns Int type ID that is not allocated by any existing View components for given implicit activity. val newUniqueIdForCurrentActivity = getUniqueId Using this, Scaloid also extended View class to add a method uniqueId, that assigns a new unique ID if it is not already allocated. val uniqueIdOfMyView = myView.uniqueId One of the good use case of uniqueId is SRelativeLayout. Some of the methods in this layout context, such as below, above, leftOf and rightOf, takes another View object as an anchor: new SRelativeLayout {   val btn = SButton(R.string.hi)   SButton(""There"").<<.below(btn) } Here we show the implementation of the below function: def below(anchor: View)(implicit activity: Activity) = {   addRule(RelativeLayout.BELOW, anchor.uniqueId)   this } A new unique ID is assigned to the anchor if it is not assigned already, and passes it to addRule function. Logging Unlike other logging frameworks, Android Logging API requires a String tag for every log call. We eliminate this by introducing an implicit parameter. Define an implicit value type of LoggerTag as shown: implicit val loggerTag = LoggerTag(""MyAppTag"") or, extend trait TagUtil or SContext which defines the tag by default. Then you can simply log like this: warn(""Something happened!"") Other functions for every log level (verbose(), debug(), info(), warn(), error() and wtf()) are available. info(""hello "" + world) A String parameter passed with info() is a by-name parameter, so it is evaluated only if the logging is possible. Therefore, the example shown above is equivalent to: val tag = ""MyAppTag"" if(Log.isLoggable(tag, Log.INFO)) Log.i(tag, ""hello "" + world) Scala getters and setters You can use any of the setters listed below: obj.setText(""Hello"") Java bean style obj.text = ""Hello"" Assignment style obj text ""Hello"" DSL style obj.text(""Hello"") Method calling style Compared to Java style getters and setters, for example: new TextView(context) {   setText(""Hello"")   setTextSize(15) } that of Scala style clearly reveals the nature of the operations as shown below: new STextView {   text = ""Hello""   textSize = 15 } Or, you can also chain the setters: new STextView text ""Hello"" textSize 15 which is a syntactic sugar for: new STextView.text(""Hello"").textSize(15) We recommend ""assignment style"" and ""DSL style"". Use assignment style when you emphasize that you are assigning something, or use DSL style when the code length of the assignee is short and needs to be chained. Note: Using .apply(String) method on object STextView, you can further reduce the code above like this: STextView(""Hello"") textSize 15 Further readings: Return value of setters Prefixed classes Sweet-little sugar Classes Class AlertDialogBuilder A Scala-style builder for AlertDialog. new AlertDialogBuilder(R.string.title, R.string.message) {   neutralButton() }.show() This displays an alert dialog with given string resources. We provide an equivalent shortcut: alert(R.string.title, R.string.message) Also you can build a more complex dialog: new AlertDialogBuilder(""Exit the app"", ""Do you really want to exit?"") {   positiveButton(""Exit"", finishTheApplication())   negativeButton(android.R.string.cancel) }.show() The code above is equivalent to: new AlertDialog.Builder(context)   .setTitle(""Exit the app"")   .setMessage(""Do you really want to exit?"")   .setPositiveButton(""Exit"", new DialogInterface.OnClickListener {     def onClick(dialog: DialogInterface, which: Int) {       finishTheApplication()     }   })   .setNegativeButton(android.R.string.cancel, new DialogInterface.OnClickListener {     def onClick(dialog: DialogInterface, which: Int) {       dialog.cancel()     }   }).show() When you call show() or alert from non-UI thread, you don't have to mind about threading. Class SArrayAdapter Suppose you want to let the user selects a string from spinner, and larger font should be displayed in the dropdown list. Then the plain-old Android code is consisted of a chunk of XML and its wiring: <?xml version=""1.0"" encoding=""utf-8""?> <TextView xmlns:android=""http://schemas.android.com/apk/res/android""     style=""?android:attr/spinnerDropDownItemStyle""     android:id=""@+id/spinner_textview""     android:layout_width=""fill_parent""     android:layout_height=""wrap_content""     android:textSize=""25.dip"" /> val adapter = new ArrayAdapter(context, android.R.layout.simple_spinner_item, Array(""One"", ""Two"", ""Three"")) adapter.setDropDownViewResource(R.layout.spinner_dropdown) In Scaloid, a directly equivalent code is: SArrayAdapter(""One"", ""Two"", ""Three"").dropDownStyle(_.textSize(25.dip)) If you want to let the text color in the spinner be blue, use the style method: SArrayAdapter(""Quick"", ""Brown"", ""Fox"").style(_.textColor(Color.BLUE)) Can it be simpler? Class LocalService Android Developer Guide on service binding says that we have to write more than 60 lines of code to define and bind an in-process service. With Scaloid, you can concisely define and access local service as shown below: class MyService extends LocalService {   private val generator = new Random()    def getRandomNumber() = generator.nextInt(100) }  class Activity extends SActivity {   val random = new LocalServiceConnection[MyService]    def onButtonClick(v:View) {     random( s => toast(""number: "" + s.getRandomNumber()))   } } Further reading: Refer to this blog post to see why this is awesome in compared with the existing method. Class Preferences SharedPreference can be accessed in this way: val executionCount = preferenceVar(0) // default value 0 val ec = executionCount() // read executionCount() = ec + 1 // write executionCount.remove() // remove Further reading: Type-safe SharedPreference A simple example: Prompt user to rate your app Extending View class Often we need to define a custom view widget for a specific requirement. To do this, we define a class that inherits android.widget.View class or its subclass (e.g. TextView and Button). To enable Scaloid extensions for this custom widget, you can define a class as follows: class MyView(implicit ctx: Context) extends View(ctx) with TraitView[MyView] {   def basis = this    // custom code for MyView here } Let's make it together! Scaloid is an Apache licensed project. If you have any idea to improve Scaloid, feel free to open issues or post patches. If you want look into inside of Scaloid, this document would be helpful: Inside Scaloid List of projects using Scaloid We are hiring! The company behind Scaloid, onsquare is hiring Scala developers. We are building a music app and other amazing products. We extensively uses Scaloid in our product, and probably it is the best reference of Scaloid application. For more information about our company, please refer to our website http://o-n2.com . Please send us your CV via email if you are interested in working at onsqure. We are located at Incheon, Korea. pocorall@gmail.com Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/pocorall/scaloid"	"Less painful Android development with Scala."	"true"
"HTTP"	"Dispatch ★ 337 ⧗ 6"	"https://github.com/dispatch/reboot"	"Library for asynchronous HTTP interaction. It provides a Scala vocabulary for Java’s."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"344"	"37"	"94"	"GitHub - dispatch/reboot: Dispatch with AsyncHttpClient as the underlying library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 37 Star 344 Fork 94 dispatch/reboot Code Issues 21 Pull requests 7 Pulse Graphs Dispatch with AsyncHttpClient as the underlying library 470 commits 21 branches 21 releases 21 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 0.11.3 Switch branches/tags Branches Tags 0.9.x 0.11.0-scala-2.11 0.11.3 either executor_ref filter-projections guarantor jsoup league lift-json master module-prefixing pathspec proj promise_repeat retry scala210 sippy str_charsets tagsoup useragent Nothing to show v0.11.3 v0.11.2 0.11.2 0.11.1 0.11.0 0.10.0 0.10.0-beta2 0.9.5 0.9.4 0.9.3 0.9.2 0.9.1 0.9.0 0.9.0-beta2 0.9.0-beta1 0.9.0-alpha6 0.9.0-alpha5 0.9.0-alpha4 0.9.0-alpha3 0.9.0-alpha2 0.9.0-alpha1 Nothing to show New pull request Latest commit 760620e May 30, 2015 n8han Drop Scala 2.9 support … It's time!  Also fix an sbt deprecation warning. Permalink Failed to load latest commit information. core Merge branch '0.11.3' of github.com:dispatch/reboot into 0.11.3 May 29, 2015 docs Include logging dependency in documentation Apr 17, 2015 json4sjackson async-http-client-1.9 Mar 2, 2015 json4snative async-http-client-1.9 Mar 2, 2015 jsoup Merge branch 'master' of github.com:dispatch/reboot into 0.11.3 May 29, 2015 liftjson Drop Scala 2.9 support May 30, 2015 notes 0.11.2 Aug 10, 2014 project Drop Scala 2.9 support May 30, 2015 tagsoup ls versions Aug 10, 2014 ufcheck upgrade json4s 3.2.9 -> 3.2.10 and unfiltered test dep from 0.7.1 -> … May 14, 2014 .gitignore request verbs Dec 11, 2011 .travis.yml Merge branch 'master' of github.com:dispatch/reboot into 0.11.3 May 29, 2015 LICENSE.txt license to reboot Dec 3, 2012 README.markdown add travis indicator to readme Aug 10, 2014 README.markdown Dispatch Reboot Dispatch reboot is a rewrite of the Dispatch library for HTTP interaction in Scala, using async-http-client as its underlying transport. For more info, see the Dispatch documentation site. Mailing List There's a mailing list for Dispatch. Please mail the list before opening github issues for problems that are not obvious, reproducible bugs. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/dispatch/reboot"	"Library for asynchronous HTTP interaction. It provides a Scala vocabulary for Java’s."	"true"
"HTTP"	"async-http-client"	"https://github.com/AsyncHttpClient/async-http-client"	"Library for asynchronous HTTP interaction. It provides a Scala vocabulary for Java’s."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2986"	"284"	"902"	"GitHub - AsyncHttpClient/async-http-client: Asynchronous Http and WebSocket Client library for Java Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 284 Star 2,986 Fork 902 AsyncHttpClient/async-http-client Code Issues 29 Pull requests 2 Wiki Pulse Graphs Asynchronous Http and WebSocket Client library for Java 3,347 commits 9 branches 183 releases 109 contributors Java 99.9% Other 0.1% Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.7.x 1.8.x 1.9.x gh-pages master netty-snapshot netty4.1 new-netty-4.1 test-refactoring Nothing to show async-http-client-project-2.0.10 async-http-client-project-2.0.9 async-http-client-project-2.0.8 async-http-client-project-2.0.7 async-http-client-project-2.0.6 async-http-client-project-2.0.5 async-http-client-project-2.0.4 async-http-client-project-2.0.3 async-http-client-project-2.0.2 async-http-client-project-2.0.1 async-http-client-project-2.0.0 async-http-client-project-2.0.0-alpha27 async-http-client-project-2.0.0-alpha26 async-http-client-project-2.0.0-alpha25 async-http-client-project-2.0.0-alpha24 async-http-client-project-2.0.0-alpha23 async-http-client-project-2.0.0-alpha22 async-http-client-project-2.0.0-alpha21 async-http-client-project-2.0.0-alpha20 async-http-client-project-2.0.0-alpha19 async-http-client-project-2.0.0-alpha18 async-http-client-project-2.0.0-alpha17 async-http-client-project-2.0.0-alpha16 async-http-client-project-2.0.0-alpha15 async-http-client-project-2.0.0-alpha14 async-http-client-project-2.0.0-alpha13 async-http-client-project-2.0.0-alpha12 async-http-client-project-2.0.0-alpha11 async-http-client-project-2.0.0-alpha10 async-http-client-project-2.0.0-alpha9 async-http-client-project-2.0.0-RC21 async-http-client-project-2.0.0-RC20 async-http-client-project-2.0.0-RC19 async-http-client-project-2.0.0-RC18 async-http-client-project-2.0.0-RC17 async-http-client-project-2.0.0-RC16 async-http-client-project-2.0.0-RC15 async-http-client-project-2.0.0-RC14 async-http-client-project-2.0.0-RC13 async-http-client-project-2.0.0-RC12 async-http-client-project-2.0.0-RC11 async-http-client-project-2.0.0-RC10 async-http-client-project-2.0.0-RC9 async-http-client-project-2.0.0-RC8 async-http-client-project-2.0.0-RC7 async-http-client-project-2.0.0-RC6 async-http-client-project-2.0.0-RC5 async-http-client-project-2.0.0-RC4 async-http-client-project-2.0.0-RC3 async-http-client-project-2.0.0-RC2 async-http-client-project-2.0.0-RC1 async-http-client-1.9.38 async-http-client-1.9.37 async-http-client-1.9.36 async-http-client-1.9.35 async-http-client-1.9.34 async-http-client-1.9.33 async-http-client-1.9.32 async-http-client-1.9.31 async-http-client-1.9.30 async-http-client-1.9.29 async-http-client-1.9.28 async-http-client-1.9.27 async-http-client-1.9.26 async-http-client-1.9.25 async-http-client-1.9.24 async-http-client-1.9.23 async-http-client-1.9.22 async-http-client-1.9.21 async-http-client-1.9.20 async-http-client-1.9.19 async-http-client-1.9.18 async-http-client-1.9.17 async-http-client-1.9.16 async-http-client-1.9.15 async-http-client-1.9.14 async-http-client-1.9.13 async-http-client-1.9.12 async-http-client-1.9.11 async-http-client-1.9.10 async-http-client-1.9.9 async-http-client-1.9.8 async-http-client-1.9.7 async-http-client-1.9.6 async-http-client-1.9.5 async-http-client-1.9.4 async-http-client-1.9.3 async-http-client-1.9.2 async-http-client-1.9.1 async-http-client-1.9.0 async-http-client-1.9.0-BETA24 async-http-client-1.9.0-BETA23 async-http-client-1.9.0-BETA22 async-http-client-1.9.0-BETA21 async-http-client-1.9.0-BETA20 async-http-client-1.9.0-BETA19 async-http-client-1.9.0-BETA18 async-http-client-1.9.0-BETA17 async-http-client-1.9.0-BETA16 async-http-client-1.9.0-BETA15 Nothing to show New pull request Latest commit 1b52e32 Jul 15, 2016 slandelle Upgrade Netty 4.0.39 Permalink Failed to load latest commit information. client Request realm should have precedence over config one, close #1211 Jul 14, 2016 extras [maven-release-plugin] prepare for next development iteration Jul 4, 2016 netty-bp [maven-release-plugin] prepare for next development iteration Jul 4, 2016 travis Increase max number of open files Jun 6, 2016 .gitignore Minor clean up Dec 18, 2012 .travis.yml Reorganize Travis scripts Jun 3, 2016 LICENSE-2.0.txt Add LICENCE/README, get rid of IDEA file Feb 16, 2010 MIGRATION.md Fix compressionEnforced doc in migration guide, close #1050 Dec 2, 2015 README.md Update sample code to use Builder Apr 27, 2016 pom.xml Upgrade Netty 4.0.39 Jul 15, 2016 README.md Async Http Client (@AsyncHttpClient on twitter) Javadoc Getting started, and use WebSockets The Async Http Client library's purpose is to allow Java applications to easily execute HTTP requests and asynchronously process the HTTP responses. The library also supports the WebSocket Protocol. The Async HTTP Client library is simple to use. I's built on top of Netty and currently requires JDK8. Latest version: Installation First, in order to add it to your Maven project, simply download from Maven central or add this dependency: <dependency>     <groupId>org.asynchttpclient</groupId>     <artifactId>async-http-client</artifactId>     <version>LATEST_VERSION</version> </dependency> Usage Then in your code you can simply do import org.asynchttpclient.*; import java.util.concurrent.Future;  AsyncHttpClient asyncHttpClient = new DefaultAsyncHttpClient(); Future<Response> f = asyncHttpClient.prepareGet(""http://www.example.com/"").execute(); Response r = f.get(); Note that in this case all the content must be read fully in memory, even if you used getResponseBodyAsStream() method on returned Response object. You can also accomplish asynchronous (non-blocking) operation without using a Future if you want to receive and process the response in your handler: import org.asynchttpclient.*; import java.util.concurrent.Future;  AsyncHttpClient asyncHttpClient = new DefaultAsyncHttpClient(); asyncHttpClient.prepareGet(""http://www.example.com/"").execute(new AsyncCompletionHandler<Response>(){      @Override     public Response onCompleted(Response response) throws Exception{         // Do something with the Response         // ...         return response;     }      @Override     public void onThrowable(Throwable t){         // Something wrong happened.     } }); (this will also fully read Response in memory before calling onCompleted) You can also mix Future with AsyncHandler to only retrieve part of the asynchronous response import org.asynchttpclient.*; import java.util.concurrent.Future;  AsyncHttpClient asyncHttpClient = new DefaultAsyncHttpClient(); Future<Integer> f = asyncHttpClient.prepareGet(""http://www.example.com/"").execute(    new AsyncCompletionHandler<Integer>(){      @Override     public Integer onCompleted(Response response) throws Exception{         // Do something with the Response         return response.getStatusCode();     }      @Override     public void onThrowable(Throwable t){         // Something wrong happened.     } });  int statusCode = f.get(); which is something you want to do for large responses: this way you can process content as soon as it becomes available, piece by piece, without having to buffer it all in memory. You have full control on the Response life cycle, so you can decide at any moment to stop processing what the server is sending back: import org.asynchttpclient.*; import java.util.concurrent.Future;  AsyncHttpClient c = new DefaultAsyncHttpClient(); Future<String> f = c.prepareGet(""http://www.example.com/"").execute(new AsyncHandler<String>() {     private ByteArrayOutputStream bytes = new ByteArrayOutputStream();      @Override     public STATE onStatusReceived(HttpResponseStatus status) throws Exception {         int statusCode = status.getStatusCode();         // The Status have been read         // If you don't want to read the headers,body or stop processing the response         if (statusCode >= 500) {             return STATE.ABORT;         }     }      @Override     public STATE onHeadersReceived(HttpResponseHeaders h) throws Exception {         Headers headers = h.getHeaders();          // The headers have been read          // If you don't want to read the body, or stop processing the response          return STATE.ABORT;     }      @Override     public STATE onBodyPartReceived(HttpResponseBodyPart bodyPart) throws Exception {          bytes.write(bodyPart.getBodyPartBytes());          return STATE.CONTINUE;     }      @Override     public String onCompleted() throws Exception {          // Will be invoked once the response has been fully read or a ResponseComplete exception          // has been thrown.          // NOTE: should probably use Content-Encoding from headers          return bytes.toString(""UTF-8"");     }      @Override     public void onThrowable(Throwable t) {     } });  String bodyResponse = f.get(); Configuration Finally, you can also configure the AsyncHttpClient via its AsyncHttpClientConfig object: AsyncHttpClientConfig cf = new DefaultAsyncHttpClientConfig.Builder()     .setProxyServer(new ProxyServer.Builder(""127.0.0.1"", 38080)).build();  AsyncHttpClient c = new DefaultAsyncHttpClient(cf); WebSocket Async Http Client also support WebSocket by simply doing: WebSocket websocket = c.prepareGet(getTargetUrl())       .execute(new WebSocketUpgradeHandler.Builder().addWebSocketListener(           new WebSocketTextListener() {            @Override           public void onMessage(String message) {           }            @Override           public void onOpen(WebSocket websocket) {               websocket.sendTextMessage(""..."").sendMessage(""..."");           }            @Override           public void onClose(WebSocket websocket) {               latch.countDown();           }            @Override           public void onError(Throwable t) {           }       }).build()).get(); User Group Keep up to date on the library development by joining the Asynchronous HTTP Client discussion group Google Group Contributing Of course, Pull Requests are welcome. Here a the few rules we'd like you to respect if you do so: Only edit the code related to the suggested change, so DON'T automatically format the classes you've edited. Respect the formatting rules: Indent with 4 spaces Your PR can contain multiple commits when submitting, but once it's been reviewed, we'll ask you to squash them into a single one Regarding licensing: You must be the original author of the code you suggest. You must give the copyright to ""the AsyncHttpClient Project"" Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/AsyncHttpClient/async-http-client"	"Library for asynchronous HTTP interaction. It provides a Scala vocabulary for Java’s."	"true"
"HTTP"	"Finch.io ★ 635 ⧗ 1"	"https://github.com/finagle/finch"	"Purely Functional REST API atop of."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"735"	"52"	"115"	"GitHub - finagle/finch: Scala combinator library for building Finagle HTTP services Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 735 Fork 115 finagle/finch Code Issues 31 Pull requests 9 Wiki Pulse Graphs Scala combinator library for building Finagle HTTP services https://gitter.im/finagle/finch 1,083 commits 6 branches 20 releases 62 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master vk/content-type-wrapper vk/encode-with-charset vk/fix-encode-as vk/statet Nothing to show 0.11.0-M1 0.10.0 0.9.3 0.9.2 0.9.1 0.9.0 0.8.0 0.7.0 0.6.0 0.5.0 0.4.0 0.3.0 0.2.0 0.1.6 0.1.5 0.1.4 0.1.3 0.1.2 0.1.1 0.1.0 Nothing to show New pull request Latest commit f215721 Jul 14, 2016 vkostyukov committed on GitHub Merge pull request #613 from finagle/vk/no-charset-for-empty-responses … No charset for empty responses Permalink Failed to load latest commit information. argonaut/src Encoding with charset Jul 13, 2016 benchmarks/src Use Rerunnable Mar 28, 2016 circe/src Encoding with charset Jul 13, 2016 core/src No charset for empty responses Jul 14, 2016 docs Prepare 0.11-M1 release Jul 13, 2016 examples/src/main/scala/io/finch Encoding with charset Jul 13, 2016 jackson/src Encoding with charset Jul 13, 2016 json-test/src/main/scala/io/finch/test/json Encoding with charset Jul 13, 2016 json4s/src Encoding with charset Jul 13, 2016 oauth2/src Use Rerunnable Mar 28, 2016 playjson/src Encoding with charset Jul 13, 2016 project Add note about sbt-scoverage version Mar 17, 2016 sprayjson/src Encoding with charset Jul 13, 2016 test/src/main/scala/io/finch/test Suite-typed self for ServiceIntegrationSuite Jul 10, 2016 .gitignore improve DecodeRequest API to return Try instead of Option Feb 3, 2015 .travis.yml Replace our benchmark with wrk Mar 14, 2016 CONTRIBUTING.md Finch uses Waffle Jul 23, 2015 LICENSE Fix #46 (Licensing) Jul 17, 2014 NOTICE #297: Remove headers Aug 2, 2015 README.md Prepare 0.11-M1 release Jul 13, 2016 build.sbt ToResponse for Response Jul 13, 2016 finch-logo.png Add new logo Dec 9, 2014 scalastyle-config.xml Sort imports Nov 15, 2015 README.md Finch is a thin layer of purely functional basic blocks atop of Finagle for building composable HTTP APIs. Its mission is to provide the developers simple and robust HTTP primitives being as close as possible to the bare metal Finagle API. Badges Modules Finch uses multi-project structure and contains of the following modules: finch-core - the core classes/functions finch-argonaut - the JSON API support for the Argonaut library finch-jackson - the JSON API support for the Jackson library finch-json4s - the JSON API support for the JSON4S library finch-circe - the JSON API support for the Circe library finch-playjson - The JSON API support for the PlayJson library - Not published yet. Will be available for 0.11.0 version. finch-test - the test support classes/functions finch-oauth2 - the OAuth2 support backed by the finagle-oauth2 library Installation Every Finch module is published at Maven Central. Use the following sbt snippet ... for the stable release: libraryDependencies ++= Seq(   ""com.github.finagle"" %% ""[finch-module]"" % ""0.11.0-M1"" ) for the SNAPSHOT version: resolvers += Resolver.sonatypeRepo(""snapshots"")  libraryDependencies ++= Seq(   ""com.github.finagle"" %% ""[finch-module]"" % ""0.12.0-SNAPSHOT"" changing() ) Hello World! This ""Hello World!"" example is built with the 0.12.0-SNAPSHOT version of finch-core. import io.finch._ import com.twitter.finagle.Http  val api: Endpoint[String] = get(""hello"") { Ok(""Hello, World!"") }  Http.server.serve("":8080"", api.toServiceAs[Plain.Text]) See examples sub-project for more complete examples. Performance We use wrk to load test Finch+Circe against Finagle+Jackson to get some insight on how much overhead, an idiomatic Finch application written in a purely functional way, involves on top of Finagle/Jackson. The results are quite impressive (for a pre-1.0 version): Finch performs on 85% of Finagle's throughput. Benchmark Run 1 Run 2 Run 3 Finagle + Jackson 33867.56 req/s 43781.26 req/s 43854.92 req/s Finch + Circe 27126.25 req/s 36720.75 req/s 37191.58 req/s Finch is also load tested against a number of Scala HTTP frameworks and libraries as par of the TechEmpower benchmark. The most recent round showed that Finch performs really well there, scoring a second place across all the Scala libraries. Documentation A comprehensive documentation may be found in the docs/ folder The latest Scaladoc is available at http://finagle.github.io/finch/docs Adopters Despegar Earnest Globo.com Glopart Hotel Urbano Konfettin JusBrasil Sabre Labs Spright SoFi Qubit QuizUp Lookout Project September Submit a pull-request to include your company/project into the list Contributing There are plenty of ways to contribute into Finch: Give it a star Join the Gitter room and leave a feedback or help with answering users' questions Submit a PR (there is an issue label ""easy"" for newcomers) Be cool and wear a Finch T-Shirt The Finch project supports the Typelevel code of conduct and wants all of its channels (Gitter, GitHub, etc.) to be welcoming environments for everyone. License Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this software except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/finagle/finch"	"Purely Functional REST API atop of."	"true"
"HTTP"	"Finagle"	"https://github.com/twitter/finagle"	"Purely Functional REST API atop of."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"4865"	"534"	"920"	"GitHub - twitter/finagle: A fault tolerant, protocol-agnostic RPC system Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 534 Star 4,865 Fork 920 twitter/finagle Code Issues 17 Pull requests 10 Pulse Graphs A fault tolerant, protocol-agnostic RPC system http://twitter.github.io/finagle 4,608 commits 27 branches 60 releases 179 contributors Scala 84.5% Java 13.1% Python 0.8% Ruby 0.5% Thrift 0.4% Shell 0.3% Other 0.4% Scala Java Python Ruby Thrift Shell Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags 2.9.2 b3_compatible_tracing copy_channel_buffer_after_decode_line detached-delayed develop divergent-master-2014-12-11 finagle-power-pack-2000 finagle_1_0_x fix-travis fix_race_in_service_to_channel gh-pages gzip_ssl kestrel lahosken-pages master memcache-long native netty new_ostrich ostrich_3 ostrich4 retry_strategies serialization ssl_track_cipher stats_refactoring svc thrift_client_reply_refactor Nothing to show version-1.2.3 version-1.2.2 version-1.1.32 version-1.1.30 version-1.1.29 version-1.1.28 version-1.1.27 version-1.1.26 version-1.1.24 version-1.1.18 version-1.1.17 version-1.1.15 version-1.1.14 version-1.1.13 version-1.1.12 version-1.1.11 version-1.1.10 version-1.1.9 version-1.1.8 version-1.1.7 version-1.1.4 version-1.1.3 version-1.1.2 version-1.1.1 version-1.0.21 version-1.0.20 version-1.0.19 version-1.0.16 version-1.0.15 version-1.0.12 version-1.0.11 version-1.0.9 finagle-6.36.0 finagle-6.35.0 finagle-6.34.0 finagle-6.33.0 finagle-6.31.0 finagle-6.30.0 finagle-6.29.0 finagle-6.28.0 finagle-6.27.0 finagle-6.26.0 finagle-6.25.0 finagle-6.24.0 6.22.0 6.20.0 6.18.0 6.17.0 6.16.0 6.15.0 6.14.0 6.13.1 6.13.0 6.12.2 6.12.1 6.11.1 6.10.0 6.9.0 6.8.1 6.8.0 Nothing to show New pull request Latest commit 23157ca Jul 9, 2016 yinquanteo committed with jenkins Revert ""finagle/finagle-memcached: Decoder on Buf"" … Reverting because it causes ads editor download to fail  This reverts commit dfa87f2fb8785381c06bc58f70b027c81283f2b5.  RB_ID=850427 TBR=true Permalink Failed to load latest commit information. .github Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 bin Make (some of) CI Green Again May 16, 2016 doc csl: Release CSL libraries Jul 7, 2016 finagle-benchmark-thrift/src/main/thrift finagle-benchmark: Move the thrift out to work around the nested obje… Oct 12, 2015 finagle-benchmark finagle-core: Delegate Trace.isActivelyTracing decision to the underl… Jun 27, 2016 finagle-commons-stats source: fix more unused imports for scala 2.11.8 Mar 29, 2016 finagle-core finagle-{core, memcached}: Simplify construction of FailureAccrualFac… Jul 8, 2016 finagle-example finagle-redis: Remove deprecated methods on Client Jun 27, 2016 finagle-exception maven layout goes away Nov 30, 2015 finagle-exp [Finagle-exp; DarkTrafficFilter] Change DarkTrafficFilter to forward … Jun 6, 2016 finagle-http-compat finagle-http: Request and Response don't proxy netty Req/Rep Apr 11, 2016 finagle-http finagle-http: add IoEngineImpl to registry Jun 27, 2016 finagle-http2 finagle-http2: Turn off Http2TransporterTests too Jul 6, 2016 finagle-integration finagle-http: Removes duplicates for StreamTransport versions May 16, 2016 finagle-kestrel Revert ""finagle/finagle-memcached: Decoder on Buf"" Jul 11, 2016 finagle-mdns Fix some Finagle ""Unused import"" warnings May 30, 2016 finagle-memcached Revert ""finagle/finagle-memcached: Decoder on Buf"" Jul 11, 2016 finagle-mux Problem Jul 6, 2016 finagle-mysql finagle-mysql: use direct executor for Caffeine in PrepareCacheTest Jul 8, 2016 finagle-native source: fix more unused imports for scala 2.11.8 Mar 29, 2016 finagle-netty4-http finagle-http: add IoEngineImpl to registry Jun 27, 2016 finagle-netty4 finagle-core: Remove Call of setEnableSessionCreation on SSLEngines Jun 27, 2016 finagle-ostrich4 Unrevert c.t.f.Address and fix finagle-memcached Feb 29, 2016 finagle-redis finagle-redis: Replace CB with Buf in sorted set commands Jun 27, 2016 finagle-serversets finagle: Removed all of the guava caches from finagle Jun 9, 2016 finagle-spdy finagle: Override Codec.protocolLibraryName Jun 7, 2016 finagle-stats finagle-stats: tag flaky tests Jun 27, 2016 finagle-stream finagle: Override Codec.protocolLibraryName Jun 7, 2016 finagle-thrift Problem / Solution Jun 7, 2016 finagle-thriftmux Problem Jul 6, 2016 finagle-toggle finagle-toggle: Make JSON description optional Jul 6, 2016 finagle-zipkin-core finagle-core: Delegate Trace.isActivelyTracing decision to the underl… Jun 27, 2016 finagle-zipkin finagle-zipkin: Accept an explicit client name while constructing a … Jun 29, 2016 project csl: Release CSL libraries Jul 7, 2016 site finagle-site: add link to fintrospect Feb 4, 2016 .gitignore fix broken WatermarkPool link in doc, also add *.pyc to .gitignore Feb 23, 2015 .mailmap add a .mailmap Dec 16, 2011 .travis.yml finagle: Ensure TravisCI configuration is exhaustive Jul 6, 2016 ADOPTERS.md Add Dwango to adopters Jun 1, 2016 CHANGES finagle-{core, memcached}: Simplify construction of FailureAccrualFac… Jul 8, 2016 CONFIG.ini Fix some lint errors in CONFIG.ini and OWNERS - missing newlines - tr… Feb 29, 2016 CONTRIBUTING.md Twitter OSS: Add ISSUE_TEMPLATE Jul 7, 2016 CONTRIBUTORS [split] finagle-doc: beginnings of a finagle user's guide Feb 21, 2013 GROUPS [split] Use new git-review with simplified OWNERS/GROUPS May 18, 2012 LICENSE Project scaffolding Oct 18, 2010 OWNERS Goodbye May 11, 2015 README.md util, ostrich, scrooge, finagle, twitter-server: Update to use codeco… Jun 8, 2016 link-netty.sh [split] finagle-core: introduce Transports and Dispatchers Apr 12, 2012 pushsite.bash finagle: Documentation Generation Cleanup Dec 3, 2015 sbt scrooge-sbt-plugin, finagle: Handle compiling for multiple languages … Jun 29, 2016 README.md Finagle Status This project is used in production at Twitter (and many other organizations), and is being actively developed and maintained. Getting involved Website: https://twitter.github.io/finagle/ Source: https://github.com/twitter/finagle/ Mailing List: finaglers@googlegroups.com IRC: #finagle on Freenode Finagle is an extensible RPC system for the JVM, used to construct high-concurrency servers. Finagle implements uniform client and server APIs for several protocols, and is designed for high performance and concurrency. Most of Finagle’s code is protocol agnostic, simplifying the implementation of new protocols. For extensive documentation, please see the user guide and API documentation websites. Documentation improvements are always welcome, so please send patches our way. Adopters The following are a few of the companies that are using Finagle: Foursquare ING Bank Pinterest SoundCloud Tumblr Twitter For a more complete list, please see our adopter page. If your organization is using Finagle, consider adding a link there and sending us a pull request! Contributing We feel that a welcoming community is important and we ask that you follow Twitter's Open Source Code of Conduct in all interactions with the community. The master branch of this repository contains the latest stable release of Finagle, and weekly snapshots are published to the develop branch. In general pull requests should be submitted against develop. See CONTRIBUTING.md for more details about how to contribute. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/twitter/finagle"	"Purely Functional REST API atop of."	"true"
"HTTP"	"Http4s ★ 427 ⧗ 1"	"https://github.com/http4s/http4s"	"A minimal, idiomatic Scala interface for HTTP."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"504"	"52"	"106"	"GitHub - http4s/http4s: A minimal, idiomatic Scala interface for HTTP Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 504 Fork 106 http4s/http4s Code Issues 53 Pull requests 8 Pulse Graphs A minimal, idiomatic Scala interface for HTTP http://http4s.org/ 2,459 commits 29 branches 46 releases 44 contributors Scala 99.9% Other 0.1% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages lenient-blaze-parser master optics release-0.13.x release-0.14.x topic/SlowStageBug topic/UriLiterals topic/async-http-client-websockets topic/aws topic/default-host topic/delimited-encoders topic/ec-to-es topic/expect topic/free-monad-servers topic/fs2 topic/incremental-json-encoders topic/managed topic/mock-client topic/oauth2 topic/path-unapply topic/rho topic/scalatra topic/scalatramacros topic/scalazVersion_SettingKey topic/source-writer topic/sse wip/jetty-readert wip/multipart Nothing to show v0.14.1 v0.14.0 v0.13.3 v0.13.2 v0.13.1 v0.13.0 v0.12.4 v0.12.3 v0.12.2 v0.12.1 v0.12.0 v0.11.3 v0.11.2 v0.11.1 v0.11.0 v0.10.1 v0.10.0 v0.9.3 v0.9.2 v0.9.1 v0.9.0 v0.8.6 v0.8.5 v0.8.4 v0.8.3 v0.8.2 v0.8.1 v0.8.0 v0.7.0 v0.6.5 v0.6.4 v0.6.3 v0.6.2 v0.6.1 v0.6.0 v0.5.4 v0.5.3 v0.5.2 v0.5.1 v0.5.0 v0.4.2 v0.4.1 v0.4.0 v0.3.0 v0.2.0 v0.1.0 Nothing to show New pull request Latest commit d83fb71 Jul 14, 2016 rossabaker committed on GitHub Merge pull request #644 from gwils/finalize-case-classes … Make all case classes final where possible. Fixes #643 Permalink Failed to load latest commit information. argonaut/src Make all case classes final where possible. Fixes #643 Jul 6, 2016 async-http-client/src Use Option[ExecutorService] for clients Mar 21, 2016 bench/src/main/scala/org/http4s/bench Replace last scalameter bench with jmh Apr 14, 2016 bin Build against the latest scalazes Jun 14, 2016 blaze-client/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 blaze-core/src Account for ByteVectors now being measured in Long Jun 10, 2016 blaze-server/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 circe/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 client/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 core/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 docs Fix link to 0.15 docs Jun 19, 2016 dsl/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 examples Make all case classes final where possible. Fixes #643 Jul 5, 2016 jawn/src s/RequestFailure/MessageFailure/g Feb 12, 2016 jetty/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 json4s-jackson/src Massive refactoring of the build. Jun 26, 2015 json4s-native/src Massive refactoring of the build. Jun 26, 2015 json4s/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 load-test/src/it Massive refactoring of the build. Jun 26, 2015 project Merge branch 'master' into topic/omnibus-upgrade Jun 15, 2016 scala-xml/src s/RequestFailure/MessageFailure/g Feb 12, 2016 server/src Merge pull request #644 from gwils/finalize-case-classes Jul 14, 2016 servlet/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 tomcat/src Make all case classes final where possible. Fixes #643 Jul 5, 2016 twirl/src EntityDecoder has `strict: Boolean` parameter Nov 13, 2015 .gitignore Merge remote-tracking branch 'jonoabroad/issue/179' into topic/multip… Mar 24, 2016 .travis.yml Omnibus dependency upgrades Jun 14, 2016 CHANGELOG.md Release v0.14.1 Jun 15, 2016 CONTRIBUTING.md Establish the http4s Code of Conduct. Feb 18, 2016 LICENSE Apache 2 license, as some of our forebears chose. Apr 15, 2014 README.md Joining Typelevel as an incubator project Apr 12, 2016 build.sbt Bump to v0.15.0-SNAPSHOT Jun 15, 2016 codecov.yml `status` needs to be in `coverage` block May 16, 2016 README.md Http4s Http4s is a minimal, idiomatic Scala interface for HTTP services. Http4s is Scala's answer to Ruby's Rack, Python's WSGI, Haskell's WAI, and Java's Servlets. val service = HttpService {     case GET -> Root / ""hello"" =>       Ok(""Hello, better world."")   } Learn more at http4s.org. http4s is proud to be a Typelevel incubator project. We are dedicated to providing a harassment-free community for everyone, and ask that the community adhere to the code of conduct. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/http4s/http4s"	"A minimal, idiomatic Scala interface for HTTP."	"true"
"HTTP"	"Netcaty ★ 11 ⧗ 68"	"https://github.com/ngocdaothanh/netcaty"	"Simple net test client/server for Netty and Scala lovers."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"11"	"3"	"0"	"GitHub - ngocdaothanh/netcaty: Simple net test client/server for Netty and Scala lovers Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 11 Fork 0 ngocdaothanh/netcaty Code Issues 1 Pull requests 0 Pulse Graphs Simple net test client/server for Netty and Scala lovers 24 commits 2 branches 2 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 1.4 1.2 Nothing to show New pull request Latest commit 3c2900b Jul 3, 2014 ngocdaothanh Fix #7 Update Netty from 4.0.20 to 4.0.21 Permalink Failed to load latest commit information. dev Add dev May 21, 2014 project Update SBT Jun 20, 2014 src Fix #5 Add TCP client and server Jun 20, 2014 .gitignore Add build.sbt May 21, 2014 CHANGELOG.rst Fix #7 Update Netty from 4.0.20 to 4.0.21 Jul 3, 2014 MIT-LICENSE Add MIT license May 21, 2014 README.rst Fix #7 Update Netty from 4.0.20 to 4.0.21 Jul 3, 2014 build.sbt Fix #7 Update Netty from 4.0.20 to 4.0.21 Jul 3, 2014 README.rst This Scala library is convenient for creating network server and client, useful for quickly writing server and client tests. This library doesn't try to be robust. If you want long running robust server or client, you should try other things. This library is small, the only dependency is Netty. Features HTTP: Server. Server can start at random open port, very useful for tests. It also automatically handles ""Expect 100 Continue"" requests. Client. Can handle chunks up to 16 MB. HTTPS. TCP: Server. Client. Be familiar with Netty Netcaty Scaladoc To create and inspect requests/responses, you should be familiar with things in package io.netty.handler.codec.http and io.netty.buffer in Netty Javadoc. req and res in the examples below are: req: FullHttpRequest res: FullHttpResponse HTTP server Start server at port 9000: netcaty.Http.respondOne(9000, { case (req, res) =>   // res is an empty 200 OK response.   // Modify it to respond what you want. })  respondOnce returns after the port has been bounded so you don't need to manually call Thread.sleep(someTime) to wait for the server to be started. The server runs on a separate thread. It sends only one response and after that stops immediately. If you don't want to stop the server after one response: val server = netcaty.Http.respond(9000, { case (req, res) =>   // res is an empty 200 OK response.   // Modify it to respond what you want. })  // Later: server.stop()  Port 0 means Netcaty will start server at a random open port. This is very useful for writing tests. To get the real port, call server.getPort. HTTP client Sync mode: // Create a FullHttpRequest import io.netty.handler.codec.http.{DefaultFullHttpRequest, HttpMethod, HttpVersion} val req = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, ""/path"")  // req will be automatically released val res = netcaty.Http.request(""localhost"", 9000, req)  // Use res ...  // Must manually release after using res.release()  Async mode: // Create a FullHttpRequest import io.netty.handler.codec.http.{DefaultFullHttpRequest, HttpMethod, HttpVersion} val req = new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.GET, ""/path"")  // req and res will be automatically released netcaty.Http.request(""localhost"", 9000, req, { res =>   ... })  TCP server You must know beforehand the length of the request. In a controlled environment like tests, that's not a big drawback. To listen on port 9000, receive exactly 123 bytes, then respond: netcaty.Tcp.respondOne(9000, 123, { requestBytes =>   // Return bytes to respond   ""Hello World"".getBytes })  val server = netcaty.Http.respond(9000, 123, { requestBytes =>   // Return bytes to respond   ""Hello World"".getBytes })  // Later: server.stop()  TCP client You must know beforehand the length of the response. In a controlled environment like tests, that's not a big drawback. Sync mode: val responseBytes = netcaty.Tcp.request(""localhost"", 9000, requestBytes)  Async mode: netcaty.Tcp.request(""localhost"", 9000, requestBytes, { responseBytes =>   ... })  HTTPS and TCP over SSL In the above examples, just replace netcaty.Http and netcaty.Tcp with netcaty.Https and netcaty.Tcps. Server: uses dummy certificate. Client: acepts all certificates. Use with SBT Supported Scala versions: 2.10.x, 2.11.x libraryDependencies += ""tv.cntt"" % ""netcaty"" %% ""1.4""  Netcaty uses Netty 4. Javassist can boost Netty 4 speed. Optionally, you can add: libraryDependencies += ""org.javassist"" % ""javassist"" % ""3.18.2-GA""  Netcat For more simple problems, maybe you don't need to use additionaly library. You can use Netcat, like this: import scala.sys.process._  object Http {   def async(fun: => Unit) {     val t = new Thread(new Runnable { def run { fun } })     t.start()   }    //----------------------------------------------------------------------------    def serveRaw(port: Int, lines: Seq[String]) {     val raw = lines.mkString(""\r\n"")     (Seq(""echo"", ""-n"", raw) #| Seq(""sh"", ""-c"", ""nc -l "" + port)).!   }    def serveContent(port: Int, contentType: String, content: String) {     val contentLength = content.getBytes.length     serveRaw(port, Seq(       ""HTTP/1.1 200 OK"",       s""Content-Type: $contentType"",       s""Content-Length: $contentLength"",       """",       content     ))   }    def asyncServeRaw(port: Int, lines: Seq[String]) {     async { serveRaw(port, lines) }   }    def asyncServeContent(port: Int, contentType: String, content: String) {     async { serveContent(port, contentType, content) }   }    //----------------------------------------------------------------------------    def requestRaw(host: String, port: Int, lines: Seq[String]): String = {     val raw = lines.mkString("""", ""\r\n"", ""\r\n\r\n"")     // ""-i 1"" delays 1s, slowering the tests.     // But without it the result will be empty.     (Seq(""echo"", ""-n"", raw) #| s""nc -i 1 $host $port"").!!   }    def get(host: String, port: Int, path: String): String = {     requestRaw(host, port, Seq(       s""GET $path HTTP/1.1"",       s""Host: $host:$port""     ))   } }  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ngocdaothanh/netcaty"	"Simple net test client/server for Netty and Scala lovers."	"true"
"HTTP"	"Newman ★ 237 ⧗ 0"	"https://github.com/stackmob/newman"	"A REST DSL that tries to take the best from Dispatch, Finagle and Apache HttpClient. See for rationale."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"239"	"35"	"48"	"GitHub - stackmob/newman: A REST Client Made For Humans Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 35 Star 239 Fork 48 stackmob/newman Code Issues 36 Pull requests 7 Pulse Graphs A REST Client Made For Humans 581 commits 9 branches 40 releases 15 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: develop Switch branches/tags Branches Tags apache-daemon-threads b/content-type-header b/performance b/spray-protected-headers develop master release/new-sbt-version spray-close-conn url-builder-implicits Nothing to show v1.3.5 v1.3.4 v1.3.3 v1.3.2 v1.3.1 v1.3.0 v1.2.3 v1.2.2 v1.2.1 v1.2.0 v1.1.0 v1.0.0 v0.23.0 v0.22.0 v0.21.0 v0.20.0 v0.19.1 v0.19.0 v0.18.0 v0.17.0 v0.16.0 v0.15.0 v0.14.2 v0.14.1 v0.14.0 v0.13.0 v0.12.0 v0.11.0 v0.10.1 v0.10.0 v0.9.2 v0.9.1 v0.9.0 v0.8.3 v0.8.2 v0.8.1 v0.8.0 v0.7.0 v0.6.0 v0.4.1 Nothing to show New pull request Latest commit 6a28aa4 Dec 2, 2015 2rs2ts Merge pull request #129 from stackmob/update-notice … Update README Permalink Failed to load latest commit information. project update the sbt version Jul 11, 2014 src Merge pull request #102 from AlekseyReviakin/issue99 Aug 20, 2014 .gitignore ignore DS_Store Feb 20, 2013 .travis.yml sign artifacts as part of the release process Oct 25, 2013 CHANGELOG.md Bump to 0.14.2 Apr 12, 2013 LICENSE.txt add apache license Jan 4, 2013 README.md Update README Nov 19, 2015 build.sbt update in versions of akka and spray Jul 11, 2014 scalastyle-config.xml Fixed a few Scalastyle warnings Mar 1, 2013 version.sbt Setting version to 1.3.6-SNAPSHOT Dec 5, 2013 README.md Newman This fork is no longer maintained An active fork can be found at https://github.com/megamsys/newman/ Newman is StackMob's HTTP client. We named it after the Seinfeld Character. And wrote a post explaining our motivation for building this library here. Newman supports the following basic features: Making HTTP requests and receiving responses Serializing and deserializing request and response bodies Serializing and deserializing requests and responses for replay or caching In memory response caching with TTL expiry ETag HTTP caching To add it to your project, use this for Maven: <dependency>   <groupId>com.stackmob</groupId>   <artifactId>newman_${scala.version}</artifactId>   <version>1.3.5</version> </dependency> or the equivalent for sbt: libraryDependencies += ""com.stackmob"" %% ""newman"" % ""1.3.5"" Basic Usage import com.stackmob.newman._ import com.stackmob.newman.dsl._ import scala.concurrent._ import scala.concurrent.duration._ import java.net.URL  implicit val httpClient = new ApacheHttpClient //execute a GET request val url = new URL(""http://google.com"") val response = Await.result(GET(url).apply, 1.second) //this will throw if the response doesn't return within 1 second println(s""Response returned from ${url.toString} with code ${response.code}, body ${response.bodyString}"") The DSL Newman comes with a DSL which is inspired by Dispatch, but uses mostly english instead of symbols. This DSL is the recommended way to build requests, and the above example in ""Basic Usage"" uses the DSL to construct a GET request. To start using the DSL, simply import com.stackmob.newman.dsl._. The functions of interest in the DSL are uppercase representations of the HTTP verbs: def GET(url: URL)(implicit client: HttpClient) def POST(url: URL)(implicit client: HttpClient) def PUT(url: URL)(implicit client: HttpClient) def DELETE(url: URL)(implicit client: HttpClient) def HEAD(url: URL)(implicit client: HttpClient) Notice that each method takes an implicit HttpClient, so you must declare your own implicit before you use any of the above listed DSL methods, or pass one explicitly. Each method listed above returns a Builder, which works in concert with the implicit methods defined in the DSL package to let you build up a request and then execute it. Executing Requests The most important method on com.stackmob.newman.HttpRequest is def apply: Future[HttpResponse]. A few notes on this method: It returns immediately after the request is started It returns a scala.concurrent.Future that will be complete immediately after the executing request is complete If you want to schedule some action to happen after the response is available, use onComplete or a similar callback The Future can also fail with an exception, which you can react to with onFailure If you need to block your code from proceeding until the HttpResponse is available, use Await.result. We recommend refactoring your blocking code to remove Await.result calls it possible, since latencies are unpredictable, subject to network conditions, etc... Serializing Newman comes with built in support for serializing HttpRequests and HttpResponses to Json. To serialize either, simply call the toJson(prettyPrint: Boolean = false): String method on the HttpRequest or HttpResponse. And to deserialize, call HttpRequest.fromJson(json: String): Result[HttpRequest] or HttpResponse.fromJson(json: String): Result[HttpResponse] to deserialize the HttpRequest or HttpResponse, respectively. ETag Support Newman comes with an implementation of HttpClient called ETagAwareHttpClient. This implementation requires an underlying ""raw"" HttpClient to execute requests to a server, but it also requires an implementation of HttpResponseCacher. It uses this HttpResponseCacher to check the cache for a response corresponding to a given request. If it finds one and that response has an ETag header in it, the ETagAwareHttpClient automatically sends an If-None-Match header to the server containing that ETag. In this case, if the server responds with a 304 NOT MODIFIED response code, then ETagAwareHttpClient will return the cached version. In all other cases, ETagAwareHttpClient will cache and return the new response. Usage With the DSL Using ETagAwareHttpClient is very similar to the basic usage above. Following demonstrates how to use the client with a (built-in) in-memory cache implementation. import com.stackmob.newman.{ETagAwareHttpClient, ApacheHttpClient} import com.stackmob.newman.caching.InMemoryHttpResponseCacher import com.stackmob.newman.dsl._ import java.net.URL  //change this implementation to your own if you want to use Memcached, Redis, etc val cache = new InMemoryHttpResponseCacher val rawHttpClient = new ApacheHttpClient //eTagClient will be used in the DSL to construct & execute requests below implicit val eTagClient = new ETagAwareHttpClient(rawHttpClient, cache)  val url = new URL(""http://stackmob.com"") //since the cacher is empty, this will issue a request to stackmob.com without an If-None-Match header val res1 = GET(url).apply //assuming res1 contained an ETag and stackmob.com fully supports ETag headers, //stackmob.com will return a 304 response code in this request and res2 will come from the cache val res2 = GET(url).apply Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/stackmob/newman"	"A REST DSL that tries to take the best from Dispatch, Finagle and Apache HttpClient. See for rationale."	"true"
"HTTP"	"here"	"https://www.paypal-engineering.com/2014/02/13/hello-newman-a-rest-client-for-scala/"	"A REST DSL that tries to take the best from Dispatch, Finagle and Apache HttpClient. See for rationale."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Hello Newman: A REST Client for Scala | PayPal Engineering Blog Engineering Search for: Hello Newman: A REST Client for Scala By Aaron Schlesinger February 13, 2014 Finagle httpClient Newman Open Source scala Hi everyone, I’m Aaron. I came to PayPal from the StackMob team recently and while I was at StackMob, I co-created the Newman project. Our original goal was to unify high quality HTTP clients on the JVM together into a single, idiomatic Scala interface. I believe we’ve accomplished that, and we’ve moved on to higher level features to make it a great choice to talk to a RESTful service. I recently gave a Newman talk at the Scala Bay Meetup in Mountain View, CA. A big thanks to everyone who came. I really appreciated all the great questions and feedback! For those who missed my talk, I’ll give a recap here as well as describe Newman in more detail, and talk about some future plans. You can also check out all code sample and slides from the talk at https://github.com/arschles/newman-example. Background & Motivation At StackMob, we ran a service oriented architecture to power our products. To build out that architecture we ran a distributed system composed of many services inside our firewall. Every service in the system used HTTP for transport and JSON for serialization to communicate over the wire. The challenge we faced was this: How do we easily and flexibly send and receive JSON data over HTTP in Scala? We had the same challenge for building servers and clients. When we began investigating the existing pool of HTTP clients, we turned to the massive JVM community for high quality clients that could handle our needs. We found a lot of them! I’ll highlight two clients with which we gained significant experience. Apache HttpClient When we looked at the Apache foundation, we found the HttpClient project. As expected, we found HttpClient to be very high quality. We used this library for a lot of our initial work, but we found a usability problem – it took too much code to do a simple request. The below code shows setup and execution logic for a GET request: /**  * set up a connection manager and client.  * you'd normally only do this once in your module or project.  */ val connManager: ClientConnectionManager = {   val cm = new PoolingClientConnectionManager()   cm.setDefaultMaxPerRoute(maxConnectionsPerRoute)   cm.setMaxTotal(maxTotalConnections)   cm } val httpClient: AbstractHttpClient = {   val client = new DefaultHttpClient(connManager)   val httpParams = client.getParams   HttpConnectionParams.setConnectionTimeout(httpParams, connectionTimeout)   HttpConnectionParams.setSoTimeout(httpParams, socketTimeout)   client }  /**  * now make the actual GET request  */ val req = new HttpGet val url = new URL(""http://paypal.com”) req.setURI(url.toURI) val headers: List[(String, String)] = ??? headers.foreach { tup: (String, String) =>   if(!headerName.equalsIgnoreCase(""Content-Type"")) req.addHeader(tup._1, tup._2) } val body: Array[Byte] = Array(‘a’.toByte, ‘b’.toByte, ‘c’.toByte) //oops, sending a request body with a GET request doesn't make sense req.setEntity(new ByteArrayEntity(body))  val resp = httpClient.execute(req) Twitter Finagle Finagle is Twitter’s core library for building distributed systems. The company has built almost all of their distributed systems infrastructure on top of this library. Furthermore, it represents a major abstraction that one of its creators has called services. See this paper for more. Finagle is built atop the Netty project, so we expected Finagle to handle high concurrency workloads, which was important in many of our use cases. Also, we had used Netty directly to build some of our servers and found it’s stable and has a good community. With Finagle we found a similar pattern. For more on Finagle and Netty at Twitter, check out the recent Twitter blog posts. Building HTTP clients with Finagle required less overall code than with the Apache library, but is still somewhat involved. The following setup an execution code for the same GET request as above: //Set up the client. It's bound to one host. host = ""http://paypal.com/"" val url = new URL(host) val client = ClientBuilder()   .codec(Http())   .hosts(host) //there are more params you can set here   .build()  //Execute the request. //Make sure the request is going to the same host //as the client is bound to val headers: Map[String, String] = ??? val method: Method = new HttpGet() //this is an org.jboss.netty.buffer.ChannelBuffer val channelBuf: ChannelBuffer = ???  val req = RequestBuilder()   .url(url)   .addHeaders(headers)   //oops, sending a request body with a GET request doesn't make sense   .build(method, Some(channelBuf)) val respFuture: Future[HttpResponse] = client.apply(req)  respFuture.ensure {   client.close() //don’t forget! } In Summary In our search, we looked at other libraries as well, but found common patterns with all of them: HTTP libraries on the JVM tend to be very stable and well tested, or built atop very stable and well tested core libraries. You usually have to write setup and cleanup code. It usually takes at least 5 lines of code to execute a request. The plain Java libraries (obviously) require you to write non-idiomatic Scala. Overall, the libraries we found required us to remember a lot of code, common patterns and sometimes implementation details. With so much to remember, we decided to either commit to a single library or write a wrapper around each that we wanted to use. In Comes Newman Newman started as an informal wrapper around Apache HttpClient. As our overall codebase grew and evolved, we needed to use new clients and knew we needed to formalize our original wrapper into a stable interface to wrap all the messy details of each implementation. We began with the core interface and two implementations: ApacheHttpClient and FinagleHttpClient. After we deployed code using our first Newman clients, we found more benefits to the core abstraction: Safety – We iterated on the interface and used Scala’s powerful type system to enforce various rules of HTTP and REST. We’re now at a point where our users can’t compile code that attempts to execute various types of invalid HTTP requests. Performance – Behind the interface, we added various levels of caching and experimented with connection pooling mechanisms, timeouts, and more to extract the best performance from Newman based on our workloads. We didn’t have to change any code on the other side of the abstraction. Concurrency – Regardless of the underlying implementation, executing a request returns standard Scala Futures that contain the response. This pattern helps ensure that code doesn’t block on downstream services. It also ensures we can interoperate with other Scala frameworks like Akka or Spray. The Scala community has a lot of great literature on Futures, so I’ll defer to those resources instead of repeating things. The Reactive Manifesto begins to explain some reasoning behind Futures (and more!) and the standard Scala documentation on Futures shows some usage patterns. Extensibility – Our environments and workloads change, so our clients must also. To effect the change we need, we just need to switch clients with one line of code. We also made the core client interface in Newman very easy to extend, so we can implement a new client quickly and have more time to focus on getting the performance correct. Higher Level Features We had our basic architecture figured out and tested, and it looks like this: A few notes about this architecture: HttpClient is heavy – it handles various caching tasks, complex concurrency tasks (running event loops and maintaining thread pools, for example), and talking to the network. HttpClient creates HttpRequests – each HttpRequest is very small and light. It contains a pointer back to the client that created it, so it’s common to have many requests for one client. HttpRequest creates Future[HttpResponse] – the Future[HttpResponse] is tied to the HttpClient that is executing the request. That Future will be completed when the response comes back into the client. With this architecture, we had proven to ourselves in production that we had a consistent, safe and performant HTTP client library. Our ongoing task now is to build features that make building and running systems easier for everyone who uses Newman. Here are a few higher level features that Newman has now: Caching – Newman has an extensible caching mechanism that plugs into its clients. You define your caching strategy (when to cache) and backend (how and where to store cached data) by implementing interfaces. You can then plug them in to a caching HttpClient as necessary. Also, with this extensible caching system, it’s possible to build cache hierarchies. We’ve so far built an ETag and a simple read-through caching strategy and an in-memory caching backend. All ship with Newman. JSON – As I mentioned at the beginning of this post, we use JSON extensively as our data serialization format over the wire, so we built it into Newman as a first class feature. Newman enables full serialization and deserialization to/from any type. Since JSON operations are built into the request and response interfaces, all client implementations get JSON functionality “for free.” DSL – We built a domain specific language into Newman that makes even complex requests possible to create and execute in one line of code. The same goes for reading, deserializing, and decoding for handling responses. The DSL is standard Scala and provides more type safety on top of core Newman. Newman DSL code has become canonical. The Result Newman abstracts away the basics of RPC. For example, we were able to replace 10+ lines of code with the following (excluding imports and comments in both cases): implicit val client = new ApacheHttpClient() //or swap out for another GET(url(http, ""paypal.com"")).addHeaders(""hello"" -> ""readers"").apply This code has more safety features than what it replaced in most cases and the setup and teardown complexities are written once and encapsulated inside the client. We have been pleased with Newman so far and anticipate that next steps will make Newman more powerful and useful for everyone. The Future We have a long list of plans for Newman. Our roadmap is open on GitHub at https://github.com/stackmob/newman/issues?state=open. The code is also open source, licensed under Apache 2.0. Read the code, file issues, request features, and submit pull requests at http://github.com/stackmob/newman. Finally, if similar distributed systems work excites you, we build very large scale, high availability distributed systems and we’re hiring. If you’re interested, send me a GitHub, Twitter or LinkedIn message. Regardless, happy coding. – Aaron Schlesinger – https://github.com/arschles, https://twitter.com/arschles, http://www.linkedin.com/profile/view?id=15144078     Aaron Schlesinger Twitter Github I am a Sr. Member of the Technical Staff on the platform team here at PayPal. I like studying concurrent systems and concepts, and I pay special attention to how that world applies to distributed systems. In my free time, I enjoy playing soccer and running. Post navigation ← The Dust Bowl Inside the PayPal Mobile SDK 2.0 → Follow @paypaleng Recent Posts Node.JS Single Page Apps — handling cookies disabled mode June 1, 2016 Open Source javascript offerings from PayPal Checkout! June 1, 2016 Securing your JS apps w/ Stateless CSRF June 1, 2016 Benching Microbenchmarks May 31, 2016 squbs: packaging and deployment instructions to run on AWS nodes May 12, 2016 Code at PayPal PayPal PayPal Developer Copyright © 1999-2016 PayPal. All rights reserved."	"null"	"null"	"A REST DSL that tries to take the best from Dispatch, Finagle and Apache HttpClient. See for rationale."	"true"
"HTTP"	"scalaj-http ★ 380 ⧗ 0"	"https://github.com/scalaj/scalaj-http"	"Simple scala wrapper for HttpURLConnection (including OAuth support)."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"430"	"30"	"76"	"GitHub - scalaj/scalaj-http: Simple scala wrapper for HttpURLConnection.  OAuth included. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 30 Star 430 Fork 76 scalaj/scalaj-http Code Issues 5 Pull requests 0 Pulse Graphs Simple scala wrapper for HttpURLConnection. OAuth included. 185 commits 3 branches 34 releases 13 contributors Scala 83.3% Java 15.9% Shell 0.8% Scala Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.3.x gh-pages master Nothing to show 2.3.0 2.2.2 2.2.1 2.2.0 2.1.0 2.0.0 1.1.6 1.1.5 1.1.4 1.1.2 1.1.1 1.1.0 1.0.1 1.0.0 0.3.16 0.3.14 0.3.13 0.3.12 0.3.10 0.3.9 0.3.8 0.3.6 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 0.2.9 0.2.5 0.2.4 0.2.3 0.2.2 0.2.1 Nothing to show New pull request Latest commit 667e8a1 Apr 2, 2016 hoffrocket 2.3.0 release Permalink Failed to load latest commit information. project updated docs and sbt version Nov 21, 2013 src Follow redirects across schemes Apr 2, 2016 .gitignore upgraded sbt to .11.2 and commons-codec to 1.5 Feb 28, 2012 .travis.yml maybe fix travis builds? Apr 2, 2016 CHANGELOG 2.3.0 release Apr 2, 2016 LICENSE.txt additional note around gzip/deflate Nov 28, 2014 README.md 2.3.0 release Apr 2, 2016 build.sbt 2.3.0 release Apr 2, 2016 root-doc.txt scaladocs Nov 28, 2014 sbt Fix for new sbt-launch location Oct 24, 2015 README.md Simplified Http This is a fully featured http client for Scala which wraps java.net.HttpURLConnection Features: Zero dependencies Cross compiled for Scala 2.9.3, 2.10 and 2.11 OAuth v1 request signing Automatic support of gzip and deflate encodings from server Easy to add querystring or form params. URL encoding is handled for you. Multipart file uploads Non-Features: Async execution The library is thread safe. HttpRequest and HttpResponse are immutable. So it should be easy to wrap in an execution framework of your choice. Works in Google AppEngine and Android environments. Note: 2.x.x is a new major version which is both syntactically and behaviorally different than the 0.x.x version. Previous version is branched here: https://github.com/scalaj/scalaj-http/tree/0.3.x Big differences: Executing the request always returns a HttpResponse[T] instance that contains the response-code, headers, and body Exceptions are no longer thrown for 4xx and 5xx response codes. Yay! Http(url) is the starting point for every type of request (post, get, multi, etc) You can easily create your own singleton instance to set your own defaults (timeouts, proxies, etc) Sends ""Accept-Encoding: gzip,deflate"" request header and decompresses based on Content-Encoding (configurable) Redirects are no longer followed by default. Use .option(HttpOptions.followRedirects(true)) to change. Installation in your build.sbt libraryDependencies +=  ""org.scalaj"" %% ""scalaj-http"" % ""2.3.0"" maven <dependency>   <groupId>org.scalaj</groupId>   <artifactId>scalaj-http_${scala.version}</artifactId>   <version>2.3.0</version> </dependency>  If you're including this in some other public library. Do your users a favor and change the fully qualified name so they don't have version conflicts if they're using a different version of this library. The easiest way to do that is just to copy the source into your project :) Usage Simple Get import scalaj.http._  val response: HttpResponse[String] = Http(""http://foo.com/search"").param(""q"",""monkeys"").asString response.body response.code response.headers response.cookies Immutable Request Http(url) is just shorthead for a Http.apply which returns an immutable instance of HttpRequest. You can create a HttpRequest and reuse it: val request: HttpRequest = Http(""http://date.jsontest.com/"")  val responseOne = request.asString val responseTwo = request.asString Additive Request All the ""modification"" methods of a HttpRequest are actually returning a new instance. The param(s), option(s), header(s) methods always add to their respective sets. So calling .headers(newHeaders) will return a HttpRequest instance that has newHeaders appended to the previous req.headers Simple form encoded POST Http(""http://foo.com/add"").postForm(Seq(""name"" -> ""jon"", ""age"" -> ""29"")).asString OAuth v1 Dance and Request Note: the .oauth(...) call must be the last method called in the request construction import scalaj.http.{Http, Token}  val consumer = Token(""key"", ""secret"") val response = Http(""https://api.twitter.com/oauth/request_token"").postForm(Seq(""oauth_callback"" -> ""oob""))   .oauth(consumer).asToken  println(""Go to https://api.twitter.com/oauth/authorize?oauth_token="" + response.body.key)  val verifier = Console.readLine(""Enter verifier: "").trim  val accessToken = Http(""https://api.twitter.com/oauth/access_token"").postForm.   .oauth(consumer, token, verifier).asToken  println(Http(""https://api.twitter.com/1.1/account/settings.json"").oauth(consumer, accessToken.body).asString) Parsing the response Http(""http://foo.com"").{asString, asBytes, asParams} Those methods will return an HttpResponse[String | Array[Byte] | Seq[(String, String)]] respectively Advanced Usage Examples Parse the response InputStream directly val response: HttpResponse[Map[String,String]] = Http(""http://foo.com"").execute(parser = {inputStream =>   Json.parse[Map[String,String]](inputStream) }) Post raw Array[Byte] or String data and get response code Http(url).postData(data).header(""content-type"", ""application/json"").asString.code Post multipart/form-data Http(url).postMulti(MultiPart(""photo"", ""headshot.png"", ""image/png"", fileBytes)).asString You can also stream uploads and get a callback on progress: Http(url).postMulti(MultiPart(""photo"", ""headshot.png"", ""image/png"", inputStream, bytesInStream,    lenWritten => {     println(s""Wrote $lenWritten bytes out of $bytesInStream total for headshot.png"")   })).asString Stream a chunked transfer response (like an event stream) Http(""http://httpbin.org/stream/20"").execute(is => {   scala.io.Source.fromInputStream(is).getLines().foreach(println) }) note that you may have to wrap in a while loop and set a long readTimeout to stay connected Send https request to site with self-signed or otherwise shady certificate Http(""https://localhost/"").option(HttpOptions.allowUnsafeSSL).asString Do a HEAD request Http(url).method(""HEAD"").asString Custom connect and read timeouts These are set to 1000 and 5000 milliseconds respectively by default Http(url).timeout(connTimeoutMs = 1000, readTimeoutMs = 5000).asString Get request via a proxy val response = Http(url).proxy(proxyHost, proxyPort).asString Other custom options The .option() method takes a function of type HttpURLConnection => Unit so you can manipulate the connection in whatever way you want before the request executes. Change the Charset By default, the charset for all param encoding and string response parsing is UTF-8. You can override with charset of your choice: Http(url).charset(""ISO-8859-1"").asString Create your own HttpRequest builder You don't have to use the default Http singleton. Create your own: object MyHttp extends BaseHttp (   proxyConfig: Option[Proxy] = None,   options: Seq[HttpOptions.HttpOption] = HttpConstants.defaultOptions,   charset: String = HttpConstants.utf8,   sendBufferSize: Int = 4096,   userAgent: String = ""scalaj-http/1.0"",   compress: Boolean = true ) Full API documentation scaladocs here Dealing with annoying java library issues Overriding the Access-Control, Content-Length, Content-Transfer-Encoding, Host, Keep-Alive, Origin, Trailer, Transfer-Encoding, Upgrade, Via headers Some of the headers are locked by the java library for ""security"" reasons and the behavior is that the library will just silently fail to set them. You can workaround by doing one of the following: Start your JVM with this command line parameter: -Dsun.net.http.allowRestrictedHeaders=true or, do this first thing at runtime: System.setProperty(""sun.net.http.allowRestrictedHeaders"", ""true"") Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalaj/scalaj-http"	"Simple scala wrapper for HttpURLConnection (including OAuth support)."	"true"
"HTTP"	"Scalaxb ★ 198 ⧗ 5"	"https://github.com/eed3si9n/scalaxb"	"An XML data-binding tool for Scala that supports W3C XML Schema (xsd) and Web Services Description Language (wsdl) as the input file."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"203"	"20"	"75"	"GitHub - eed3si9n/scalaxb: scalaxb is an XML data binding tool for Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 20 Star 203 Fork 75 eed3si9n/scalaxb Code Issues 76 Pull requests 6 Wiki Pulse Graphs scalaxb is an XML data binding tool for Scala. http://scalaxb.org/ 953 commits 24 branches 32 releases Fetching contributors Scala 94.6% Java 5.2% Other 0.2% Scala Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master release/0.6.6 release/0.6.7 release/0.6.8 release/0.6.9 release/0.7.0 release/0.7.1 release/0.7.2 release/1.0.0 release/1.0.1 release/1.0.2 release/1.1.0 release/1.1.1 release/1.1.2 release/1.2.0 release/1.2.1 release/1.3.0 release/1.4.0 sbt0.10 sbt0.12 scalaxb2 topic/lens try/explicit Nothing to show 1.4.0 1.3.0 1.2.1 1.2.0 1.1.2 1.1.1 1.1.0 1.0.2 1.0.1 1.0.0 0.7.3 0.7.2 0.6.8 0.6.7 0.6.6 0.6.5 0.6.4 0.6.3 0.6.2 0.6.1 0.6.0 0.5.4 0.5.3 0.5.2 0.5.1 0.5.0 0.4.0 0.3.0 0.2.0 0.1.0 0.0.3 0.0.2 Nothing to show New pull request Latest commit a9b221a Apr 28, 2016 eed3si9n Merge pull request #372 from alexdupre/dispatch-0.11.4 … Add support for dispatch 0.11.4 and bump scalaxb version. Permalink Failed to load latest commit information. cli Add support for dispatch 0.11.4 and bump scalaxb version. Apr 28, 2016 integration/src/test failed test dummy fixing because of bad response of external soap ser… Feb 18, 2016 mvn-scalaxb note on building maven plugin Jul 20, 2015 notes notes Jul 20, 2015 project handling implicit and explicit soap headers + tests Feb 18, 2016 sbt-scalaxb/src Fixes plugin Jul 20, 2015 src/main/conscript/scalaxb bumping up to 1.4.0 Jul 20, 2015 web aggregated build.sbt into build.scala Oct 2, 2011 .gitignore Add .cache to .gitignore (generated by Scala IDE plugin for Eclipse). Mar 30, 2012 .travis.yml scripted Jul 19, 2015 INSTALL.md launchconfig updates Mar 17, 2012 LICENSE applying the MIT license. Feb 23, 2010 README.md Added Gitter badge Jul 20, 2015 build.sbt Add support for dispatch 0.11.4 and bump scalaxb version. Apr 28, 2016 README.md scalaxb scalaxb is an XML data-binding tool for Scala that supports W3C XML Schema (xsd) and Web Services Description Language (wsdl) as the input file. From schema documents scalaxb will generate Scala source files containing case classes to represent the data and typeclass instances to turn XML documents into an object, and the object back to XML. Status The latest is 1.4.0. Some things may not work. I'd really appreciate if you could run it against your favorite xsd file and let me know the result. Modules There are currently four ways of running scalaxb: command line app scalaxb sbt plugin sbt-scalaxb maven plugin mvn-scalaxb web API scalaxb-heroku hosted on heroku sbt-scalaxb To call scalaxb from sbt 0.13.x, put this in your project/scalaxb.sbt: resolvers += Resolver.sonatypeRepo(""public"")  addSbtPlugin(""org.scalaxb"" % ""sbt-scalaxb"" % ""X.X"")  and this in scalaxb.sbt: scalaxbSettings  packageName in scalaxb in Compile := ""xxx""  sourceGenerators in Compile <+= scalaxb in Compile  dispatchVersion in scalaxb in Compile := ""0.11.1""  command line app scalaxb See INSTALL.md. mvn-scalaxb See mvn-scalaxb. Documents Further info is available at scalaxb.org. Bug Reporting If you're having problem with scalaxb, please take a moment and read issue reporting guideline. Licensing It's the MIT License. See the file called LICENSE. Contacts mailing list @scalaxb Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/eed3si9n/scalaxb"	"An XML data-binding tool for Scala that supports W3C XML Schema (xsd) and Web Services Description Language (wsdl) as the input file."	"true"
"HTTP"	"Spray"	"http://spray.io/"	"Actor-based library for http interaction."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2362"	"198"	"560"	"GitHub - spray/spray: A suite of scala libraries for building and consuming RESTful web services on top of Akka: lightweight, asynchronous, non-blocking, actor-based, testable Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 198 Star 2,362 Fork 560 spray/spray Code Issues 67 Pull requests 18 Wiki Pulse Graphs A suite of scala libraries for building and consuming RESTful web services on top of Akka: lightweight, asynchronous, non-blocking, actor-based, testable http://spray.io 2,652 commits 26 branches 67 releases 76 contributors Scala 63.7% HTML 17.8% CSS 12.3% JavaScript 6.1% Shell 0.1% Scala HTML CSS JavaScript Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags feature/integrate-spray-json feature/json-interpolator feature/openssl-engine-1.0-M7 fix-xxe master release/1.0 release/1.1 release/1.2-M8 release/1.2 release/1.3 ticket/780 w/simplify-deflate w/ssl-note w/852-encode-ChunkedResponseStart-with-empty-entity w/861-support-transfer-encoding-identity w/879-fix-request-level-error-message w/903 w/936-fix-NoEncoding w/959-fix-PathMatcher-NumberMatcher w/984-fix-directives-map wip-954-mathias wip-965-mathias wip-977-mathias wip-992-mathias wip-1006-mathias wip-1019-mathias Nothing to show wip/spdy2/hacky-client-server-push v1.3.3 v1.3.2 v1.3.1_2.11 v1.3.1 v1.3.0 v1.3-RC4 v1.3-RC1 v1.3-M2 v1.3-M1 v1.2.3 v1.2.2 v1.2.1 v1.2.0 v1.2-RC4 v1.2-RC3 v1.2-RC2 v1.2-RC1 v1.2-M8 v1.1.3 v1.1.2 v1.1.1 v1.1.0 v1.1-RC4 v1.1-RC3 v1.1-RC2 v1.1-RC1 v1.1-M8 v1.1-M7 v1.1-M6 v1.1-M5 v1.1-M4.2 v1.1-M4.1 v1.1-M4 v1.0.1 v1.0.0 v1.0-RC4 v1.0-RC3 v1.0-RC2 v1.0-RC1 v1.0-M8.3 v1.0-M8.2 v1.0-M8.1 v1.0-M8 v1.0-M7 v1.0-M6 v1.0-M5 v1.0-M4.2 v1.0-M4.1 v1.0-M4 v1.0-M3.1 v1.0-M3 v1.0-M2 v1.0-M2.x v1.0-M1 v0.9.0 v0.9.0-RC4 v0.9.0-RC3 v0.9.0-RC2 v0.9.0-RC1 v0.8.0 v0.8.0-RC3 v0.8.0-RC2 v0.8.0-RC1 v0.7.0 v0.5.0 old-release-1.0.x Nothing to show New pull request Latest commit b473d9e Jul 13, 2016 sirthias committed on GitHub Merge pull request #1119 from ametrocavich/master … Fix typo Permalink Failed to load latest commit information. docs = site: add scala.world talk reference Oct 22, 2015 examples = examples: remove misleading `CloseAll` in client examples, fixes #921 May 18, 2015 notes Rename domain name from spray.cc to spray.io, rename packages from 'c… Oct 12, 2012 project = site: fix missing APIdoc link Mar 24, 2015 site/src = site: extend production logging timestamps with milli seconds Sep 18, 2015 spray-caching/src Update copyright year to 2015 Mar 8, 2015 spray-can-tests/src/test/scala/spray/can = can: adding test proving effectiveness of the latest fix also for #964 Mar 24, 2015 spray-can/src/main Merge pull request #1033 from spray/wip-965-mathias Mar 24, 2015 spray-client/src Update copyright year to 2015 Mar 8, 2015 spray-http/src Fix typo Jul 13, 2016 spray-httpx/src Merge pull request #1024 from spray/fix-xxe Mar 24, 2015 spray-io-tests/src/test Update copyright year to 2015 Mar 8, 2015 spray-io/src/main Merge pull request #994 from briandignan/bugfix-dnsQueryOfIpWhenSslEn… Mar 23, 2015 spray-routing-tests/src/test = routing: fix tests broken with last commit May 17, 2015 spray-routing/src/main = routing: fix typos in RejectionHandler error messages May 16, 2015 spray-servlet/src Update copyright year to 2015 Mar 8, 2015 spray-testkit/src Update copyright year to 2015 Mar 8, 2015 spray-util/src Update copyright year to 2015 Mar 8, 2015 .gitignore Improve .gitignore, in particular exclude Eclipse project files (closes Mar 30, 2013 .jvmopts = build: use travis docker-based environment Dec 19, 2014 .travis.yml = build: upgrade .travis.yml to Scala 2.10.4 Feb 10, 2015 CHANGELOG = project: update CHANGELOG for 1.x.3 release Mar 24, 2015 CONTRIBUTING.md = build: add CONTRIBUTING.md to show in github pull requests Jul 21, 2013 LICENSE Update copyright year to 2015 Mar 8, 2015 README.markdown = project: add Gitter badge to README Feb 10, 2015 README.markdown Documentation Please see http://spray.io/ for all documentation Code Climate Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/spray/spray"	"Actor-based library for http interaction."	"true"
"HTTP"	"Tubesocks ★ 7 ⧗ 143"	"https://github.com/softprops/tubesocks"	"Library supporting bi-directional communication with websocket servers."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"7"	"4"	"2"	"GitHub - softprops/tubesocks: A comfortable and fashionable way to have bi-directional conversations with modern web servers. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 4 Star 7 Fork 2 softprops/tubesocks Code Issues 1 Pull requests 0 Pulse Graphs A comfortable and fashionable way to have bi-directional conversations with modern web servers. 46 commits 2 branches 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master refactor Nothing to show 0.1.0 Nothing to show New pull request Latest commit 3b43bb2 Jul 10, 2014 softprops we are on 2.10 now so we can use promises Permalink Failed to load latest commit information. notes build info & async http update Dec 22, 2012 project upgrade to bintray-sbt 0.1.2 Jul 10, 2014 src we are on 2.10 now so we can use promises Jul 10, 2014 .gitignore it works Jun 20, 2012 .travis.yml more mem for travis Jul 10, 2014 LICENSE it works Jun 21, 2012 README.md travis badge Jul 10, 2014 build.sbt prefer no-op logging as a default slf4j impl, small build tweeks for … Jul 10, 2014 README.md tubesocks A comfortable and fashionable way to have bi-directional conversations with modern web servers. Tubesocks is a snug little interface that wraps async http client which supports an emerging standard protocol for pushing messages to clients and responding over an open connection. Here is an echo client. import tubesocks._ Sock.uri(""ws://host"") {   case Message(m, s) => s.send(m) } install sbt By hand (cut & paste) libraryDependencies += ""me.lessis"" %% ""tubesocks"" % ""0.1.0"" The civilized way ls-install tubesocks  usage import tubesocks._ Sock.uri(""ws://host.com"") {   case Open(s) => s.send(""I'm here"")   case Message(t, s) => println(""server says %s"" format t)   case Close(s) => println(""we're done"") } Do you prefer configuring your own transmission? import tubesocks._ Sock.configure({ b =>   b.setWebSocketIdleTimeoutInMs(2 * 60 * 1000) })(new URI(""ws://host.com"")) {   case Message(t, s) => s.send(""thanks for the message"") } Doug Tangren (softprops) 2012-2013 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/softprops/tubesocks"	"Library supporting bi-directional communication with websocket servers."	"true"
"Semantic Web"	"Banana-RDF ★ 178 ⧗ 7"	"https://github.com/banana-rdf/banana-rdf"	"Scala-friendly abstractions for RDF and Linked Data technologies. Supports Jena, Sesame and native Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"184"	"32"	"45"	"GitHub - banana-rdf/banana-rdf: Banana RDF Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 32 Star 184 Fork 45 banana-rdf/banana-rdf Code Issues 82 Pull requests 8 Wiki Pulse Graphs Banana RDF 1,227 commits 13 branches 3 releases 19 contributors Scala 68.6% JavaScript 30.1% Java 1.2% Shell 0.1% Scala JavaScript Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: series/0.8.x Switch branches/tags Branches Tags contributing-proposal fix/build-scalajs fix/node-extractors fix/sbt-scalajs-0.6.0 ldp revert-282-ntriplesFixes scalajs series/0.7.x series/0.8.x topic/ldpatch wip/cats wip/n3-js-model wip/publication-settings Nothing to show v0.8.1 v0.8.0 nomo-turtle-parser Nothing to show New pull request Latest commit 76933e0 Sep 6, 2015 InTheNow Merge pull request #297 from InTheNow/topic/validate-examples … Validate examples and turn warnings into errors Permalink Failed to load latest commit information. N3.js/src idea 14.1.1 fixes and sjs 0.6.3 May 24, 2015 bin minor setup changes Jan 29, 2015 io/ntriples/common/src/main/scala/org/w3/banana/io NTriples Writer Tests pass. Renamed one test. Jan 31, 2015 jena/src Validate examples and turn warnings into errors Sep 6, 2015 jsonld.js/src idea 14.1.1 fixes and sjs 0.6.3 May 23, 2015 ldpatch/src #139 -- move source files according to the Java convention; add the e… Oct 1, 2014 misc Validate examples and turn warnings into errors Sep 6, 2015 plantain moved to akka-http 1.0-RC3 and removed redundant imports May 24, 2015 project Validate examples and turn warnings into errors Sep 6, 2015 rdf-test-suite Validate examples and turn warnings into errors Sep 6, 2015 rdf eleminate the trait TryInstances to fix #263 May 24, 2015 sesame/src Validate examples and turn warnings into errors Sep 6, 2015 .gitignore Validate examples and turn warnings into errors Sep 6, 2015 .history git ignores .history Jun 23, 2014 .travis-jvmopts Validate examples and turn warnings into errors Sep 6, 2015 .travis.yml Validate examples and turn warnings into errors Sep 6, 2015 CONTRIBUTING.md Chained methods can fit on one line too. [closes #276] May 28, 2015 README.md fix link to test suite Jun 8, 2015 build.sbt Validate examples and turn warnings into errors Sep 6, 2015 version.sbt Setting version to 0.8.2-SNAPSHOT Mar 15, 2015 README.md banana-rdf An RDF library in Scala banana-rdf is a library for RDF, SPARQL and Linked Data technologies in Scala. It can be used with existing libraries without any added cost. There is no wrapping involved: you manipulate directly the real objects. We currently support Jena, Sesame and Plantain, a pure Scala implementation. Features banana-rdf emphasizes type-safety and immutability, so it can come with some cost when the underlying implementation is very mutable (I'm looking at you, Jena and Sesame). We try to keep a clear distinction between the core concepts and the enhanced syntax that Scala can give us. RDF itself is defined as a record of types. Implementations just have to plug their own types. And because types alone are not enough, we introduce the RDFOps typeclass, which defines the mandatory operations that an RDF implementation must implement. SparqlOps does the same for SPARQL. With banana-rdf, you get Diesel, a nice DSL to build and navigate within pointed graphs (graphs with a pointer to an inner node). You also get an abstraction for graph stores (GraphStore), which do not have to be SPARQL engines (SparqlEngine). Of course, you can serialize and deserialize most of the RDF syntaxes as well as JSON-LD (RDFa will come soon). banana-rdf introduces the concept of binders, which let you bridge the Scala and RDF worlds. Most of the common datastructures are already available, and you can even map your own classes. Unlike usual ORM techniques, this does not rely on annotation or reflection. Until we write thorough documentation, the best place to understand what you can do is to go through the test suite. How to start geeking You only need a recent version of Java, that's all: $ git clone git@github.com:w3c/banana-rdf.git $ cd banana-rdf $ sbt It's also easy to just build specific target platforms: $ sbt +banana_js/test    # for javascript only  $ sbt +banana_jvm/test   # for jvm only ( note: scala-js compilation uses more memory. see travis.yml ) IDE Setup banana-rdf works with both eclipse and IntelliJ IDEA. global.sbt Independent of your preferred IDE, optionally the add the following line to ~/.sbt/0.13/global.sbt to prevent the generation of empty source directories:     unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  Eclipse Eclipse should work ""out of the box"" with the addition of the following global settings: In ~/.sbt/0.13/global.sbt:     unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  In ~/.sbt/0.13/plugins/build.sbt     addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""2.5.0"")  To generate eclipse project files, just run the command: $ sbt eclipse IntelliJ IDEA IntelliJ IDEA works with just one global change: In ~/.sbt/0.13/plugins/build.sbt     addSbtPlugin(""com.github.mpeltonen"" % ""sbt-idea"" % ""1.6.0"")  To generate IntelliJ project files, just run the command: $ sbt gen-idea Community For discussions that don't fit in the issues tracker, you may try either the w3c banana-rdf mailing list, for longer discussions the banana-rdf irc channel on freenode using a dedicated IRC client connecting to irc://irc.freenode.net:6667/banana-rdf or using the freenode html interface, for quick real time socialising Code of Conduct Banana-RDF contributors all agree to follow the W3C Code of Ethics and Professional Conduct. If you want to take action, feel free to contact Alexandre Bertails alexandre@bertails.org. You can also contact W3C Staff as explained in W3C Procedures. Licence This source code is made available under the W3C Licence. This is a business friendly license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/banana-rdf/banana-rdf"	"Scala-friendly abstractions for RDF and Linked Data technologies. Supports Jena, Sesame and native Scala."	"true"
"Semantic Web"	"rdfp ★ 1 ⧗ 226"	"https://github.com/jannvck/rdfp"	"RDF stream processing framework in Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"1"	"1"	"0"	"GitHub - jannvck/rdfp: RDF stream processing framework in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 1 Fork 0 jannvck/rdfp Code Issues 1 Pull requests 0 Pulse Graphs RDF stream processing framework in Scala 15 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. data src README.md build.sbt log4j.properties README.md rdfp RDF stream processing framework written in Scala. Works with Sesame and Jena and has a small footprint. Supports on-the-fly Lucene indexing. RDFStreamProcessingTest.scala contains several test cases which may serve as examples on how to use. The framework is designed to process very large datasets and has been used successfully to process datasets of the german national library containing more than 100 billion triples. It can process RDF streams in parallel since the Akka actors model has been applied. Installation Use Maven: <dependency>     <groupId>com.github.jannvck</groupId>     <artifactId>rdfp_2.11</artifactId>     <version>1.0</version> </dependency>  Or with SBT add to your build.sbt file: libraryDependencies += ""com.github.jannvck"" % ""rdfp_2.11"" % ""1.0""  Or clone the repository and run sbt in the project root folder. Enter 'test' to run the tests, 'compile' to compile the project or 'doc' to generate API documentation with scaladoc. Use the tests to verify all is running correctly. Getting Started The basic building blocks of a RDF stream processor in rdfp are matchers, sources and sinks. While sources are mandatory, sinks are optional. To process RDF triples you have to define matchers, that define which triples will be processed. A matcher consists of a match condition, definition of what to store and how to transform it: trait Matcher[S] {   def matches: S => Boolean   def handle: S => Option[_]   def transform: S => Option[Set[S]] } The following examples match upon a certain URI on the subject, store the object of a matched triple and apply no transformation. With Jena: val jenaMatcher = new SetMatcher(     (t: Triple) => t.getSubject().hasURI(someURI), // match condition     (t: Triple) => Some(t.getObject()), // element to store in this Matcher's list     (t: Triple) => Some(Set(t))) // triple to store, the place for transformations Or with sesame: val sesameMatcher = new SetMatcher(     (s: SesameStatement) => s.getSubject().stringValue().equals(someURI), // match condition     (s: SesameStatement) => Some(s.getObject()), // element to store in this Matcher's list     (s: SesameStatement) => Some(Set(s))) // triple to store, the place for transformations To start processing the RDF source, set up a RDFStreamProcessor instance. An RDFStreamProcessor consists of source, sink and a list of matchers: trait RDFStreamProcessor[S] {   val source: Any   val sink: S => Unit   val matchers: List[Matcher[S]]   … } Implementations of RDFStreamProcessors exist for Jena and Sesame. With Jena: new JenaRDFStreamProcessor(     ""/path/to/dataset"", // source     (t: Triple) => None, // sink     List(jenaMatcher)).trigger With Sesame: SesameRDFStreamProcessor(         () => FileUtils.openResourceFile(""/path/to/dataset""),         (s: SesameStatement) => None,         List(sesameMatcher)).trigger Parallel processing You can easily create multiple processors to run in parallel as the following example illustrates with a Sesame RDF stream processor. The producer-consumer pattern has been applied. The file src/test/scala/RDFStreamProcessingTest.scala contains some tests which demonstrate how to use multiple processors. val consumers = Set[ActorRef]() val consumedStatements = ListBuffer[Statement[SesameStatement]]() val processor = SesameRDFStreamProcessor(     () => FileUtils.openResourceFile(Dataset),     (s: SesameStatement) => consumers.foreach((c: ActorRef) => c ! Statement(s)), // send matched triple to all consumers     List(sesameMatcher)) val producer0 = processor.producer val producer1 = processor.producer consumers += processor.consumer((s: Statement[SesameStatement]) => consumedStatements += s) // do something with the statement producer0 ! Start producer1 ! Start Persistence Special classes exist to store matched elements in a SQL database by using the H2 database. These classes are useful when dealing with RDF streams. They can be used for general storage and are, by modular design, not dependent on the rest of the rdfp code. Implemenatations are available for string keys and values. PersistentMap, Map[K, V] simple key-value SQL storage of arbitrary serializable objects. PersistentMapSet, Map[K, Set[V]] forms a mapping between an arbitrary serializable key object and a corresponding set containing arbitrary serializable objects PersistentNestedMapSet, Map[K1, Map[K2, Set[V]]] mapping // find all people lazy val smwPersons = new PersistentSetMatcher[SesameStatement, Value](     ""jdbc:h2:"" + path + ""/tmp/rdfp.smw.persons"",     (s: SesameStatement) => RDFType.equals(s.getPredicate().toString()) && PersonURI.equals(s.getObject().toString()), // match condition     (s: SesameStatement) => Some(s.getSubject()), // element to store in this Matcher's list     (s: SesameStatement) => None) // triple to store, the place for transformations Change Listeners To watch for changes on components of statments as triples pass along, the SesameRDFStreamProcessor extends the ComponentChangeHandler trait. trait ComponentChangeListener[Component, Subject <: Component, Predicate <: Component, Object <: Component] {     def onComponentChange(e: ComponentChangeEvent[Component, Subject, Predicate, Object]): Unit } The received event ComponentChangeEvent is implemented by the case classes SubjectChange, PredicateChange and ObjectChange which carry the previous element and the new one. Blank nodes The SesameRDFStreamProcessor extends the BNodeHandler trait which allows to keep track of blank nodes. For example it allows to keep track of the root node of the blank node subgraph. Arbitrary transformations of the subgraphs are possible. The following example will flatten a subgraph containing blank nodes: implicit val bNMapping = (s: SesameStatement, h: BNodeHandler[Value, Resource, Value, SesameStatement]) => {       if (s.getSubject().isInstanceOf[BNode])         Set(factory.createStatement(h.root.getOrElse(s.getSubject()), s.getPredicate(), s.getObject()))       else         Set[SesameStatement]()     } This blank node mapping will have the following effect: <someSubject> <somePredicate> <b_node0> <b_node0> <someOtherPredicate> <someObject>  Will yield two triples: <someSubject> <somePredicate> <someObject> <someSubject> <someOtherPredicate> <someObject>  Where someSubject is the root node. Lucene indexing A IndexingSesameRDFStreamProcessor will by default use a StandardAnalyzer, and DefaultSimilarity. It uses a ComponentChangeHandler and has a special method onNewRootSubject which is called whenever the subject in the RDF stream changes. Subgraphs containing blank nodes are automatically handled (by flattening the subgraph - see previous section) The code snippet below processes an RDF stream and creates an index with Lucene on-the-fly. By default, a new subject will be mapped to a document in the Lucene index. This can be changed by overriding the onNewRootSubject method. implicit val idxDirectory = FSDirectory.open(new File(""data/lucene.idx"")) val proc = new IndexingSesameRDFStreamProcessor(     datasetReader, // input stream of statements     (s: SesameStatement) => None, // element to store in the matcher     List(new DefaultMatcher())) // process all statements proc.trigger proc.closeIndexWriter proc.closeIndexDirectory To search an index, standard Lucence queries can be used as in the example below: val dnbIndex = LuceneSearcher(FSDirectory.open(new File(""data/lucene.idx""))) dnbIndex.searchByParsedQuery(fac.createURI(gndo + ""preferredNameForThePerson"").toString(), ""some label"", 10) License This software is distributed under the terms of the Eclipse Public License 1.0, see http://www.eclipse.org/legal/epl-v10.html. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/jannvck/rdfp"	"RDF stream processing framework in Scala"	"true"
"Metrics and Monitoring"	"Kamon"	"http://kamon.io"	"Gathering metrics from applications built with Akka, Spray and Play! with support for user metrics as well."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"660"	"67"	"145"	"GitHub - kamon-io/Kamon: The Open Source tool for monitoring applications running on the JVM Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 67 Star 660 Fork 145 kamon-io/Kamon Code Issues 64 Pull requests 8 Pulse Graphs The Open Source tool for monitoring applications running on the JVM http://kamon.io 901 commits 26 branches 28 releases 37 contributors Scala 94.7% Java 5.2% Shell 0.1% Scala Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags errors-stage-1 errors histogram-experiments issue#271/fix-balancing-pool-metrics kamon-agent-new kamon-agent kamon-java8 kamon-jmh kamon-js kamon-weaver make-kamon-test-friendly master netty release-0.2 release-0.3_scala-2.11 release-akka-2.4 release-legacy-akka-2.2 tags-in-traces wip/cleanup-hdr-snapshots wip/decoupling-basic-apis-from-reporters wip/improve-trace-context-and-segments-api wip/influxdb-module wip/kamon-dashboard wip/newrelic-agent-rewrite wip/percentiles-and-histograms-for-newrelic wip/simple-tracing-implementation Nothing to show v0.6.1 v0.6.0 v0.5.2 v0.5.1 v0.5.0 v0.4.0 v0.4.0_akka-2.2 v0.3.5 v0.3.5_scala-2.11 v0.3.4 v0.3.4_scala-2.11 v0.3.3 v0.3.3_scala-2.11 v0.3.2 v0.3.2_scala-2.11 v0.3.1 v0.3.0 v0.2.5 v0.2.4 v0.2.3 v0.2.2 v0.2.1 v0.2.0 v0.0.15 v0.0.14 v0.0.13 v0.0.12 v0.0.11 Nothing to show New pull request Latest commit f21aec6 Jul 13, 2016 qkoshelev committed with dpsoft + added tracing metrics support for spm (#369) Permalink Failed to load latest commit information. kamon-akka-remote/src = core: minor refactor Jul 10, 2016 kamon-akka/src correctly differentiate between actors and routers, fixes #271 Mar 28, 2016 kamon-annotation/src Add tags for traces and closes #327 Apr 22, 2016 kamon-autoweave/src/main + kamon-autoweave: add support Attach on IBM J9 and closes #354 Jun 22, 2016 kamon-core/src = core: minor refactor Jul 10, 2016 kamon-datadog/src = statd,datadog: minor formatting changes. Jan 6, 2016 kamon-elasticsearch/src + kamon-elasticsearch: new integration module Mar 17, 2016 kamon-examples = fluentd-example: replace extension-id by extension-class in applica… Mar 6, 2016 kamon-fluentd/src = fluentd: change extension-id by extension-class Feb 7, 2016 kamon-influxdb/src = allow hostname override for influxdb statistics May 4, 2016 kamon-jdbc/src = all: some clean up in jdbc and annotation modules Nov 27, 2015 kamon-jmx/src/main Corrects key name error in reference configuration. Feb 5, 2016 kamon-log-reporter/src/main + log-reporter: include ExecutorServiceMetrics Mar 6, 2016 kamon-newrelic/src wip: merge from master and fix some test Nov 15, 2015 kamon-play-2.3.x/src + play: capture debug method in LoggerLikeInstrumentation Dec 17, 2015 kamon-play-2.4.x/src + play: capture debug method in LoggerLikeInstrumentation Dec 18, 2015 kamon-play-2.5.x/src + kamon-play-25: add play 2.5.x support Jun 15, 2016 kamon-playground/src/main introduce selective instrumentation for akka actors. Mar 14, 2016 kamon-riemann/src + add InfluxDB module May 2, 2016 kamon-scala/src + scala: add support for trace context propagation on Twitter futures (… Jul 8, 2016 kamon-spm/src + added tracing metrics support for spm (#369) Jul 13, 2016 kamon-spray/src + kamon-spray: Add tags in spray spray directives for tracing and clo… Apr 22, 2016 kamon-statsd/src + Kamon-core: introduce finishWithError(Throwable) for Traces and Seg… Jul 8, 2016 kamon-system-metrics/src = system-metrics: runSafe in UlimitMetrics and include a test Mar 3, 2016 kamon-testkit/src/main ! core: move the TraceContext manipulation API to the Tracer companio… Feb 15, 2015 project + added tracing metrics support for spm (#369) Jul 13, 2016 .gitignore ignore ensime-related files. Apr 26, 2016 .travis.yml ! build: filter most of the output from sbt test. Nov 23, 2015 CONTRIBUTING.md + github: contributing guidelines. Oct 12, 2014 LICENSE + site: added changelog and license views Apr 3, 2014 README.md Update README.md May 24, 2015 travis-test.sh ! build: remove even more output of dependency downloads. Nov 25, 2015 version.sbt set version to 0.6.2-SNAPSHOT. Apr 27, 2016 zersion.sbt Releasing 0.6.1 Apr 27, 2016 README.md Kamon Kamon is a set of tools for monitoring applications running on the JVM. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/kamon-io/Kamon"	"Gathering metrics from applications built with Akka, Spray and Play! with support for user metrics as well."	"true"
"Parsing"	"atto ★ 119 ⧗ 16"	"https://github.com/tpolecat/atto"	"Pure functional incremental text parsing library for Scala, based on Attoparsec."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"121"	"11"	"24"	"GitHub - tpolecat/atto: friendly little parsers Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 11 Star 121 Fork 24 tpolecat/atto Code Issues 7 Pull requests 2 Pulse Graphs friendly little parsers 164 commits 3 branches 13 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master pos series/0.5.x Nothing to show v0.5.0-M2 v0.5.0-M1 v0.4.2 v0.4.2-RC4 v0.4.2-RC3 v0.4.2-RC2 v0.4.2-RC1 v0.4.1 v0.4.0 v0.3 v0.2.1 v0.2 v0.1 Nothing to show New pull request Latest commit bb162ff Nov 6, 2015 tpolecat Merge pull request #25 from ClockworkConsulting/bump-spire … Bump spire to 0.11.0 Permalink Failed to load latest commit information. core/src single build file Aug 31, 2015 example/src/main single build file Sep 1, 2015 project single build file Sep 1, 2015 spire/src/main/scala/atto/parser/spire single build file Sep 1, 2015 stream/src/main/scala/atto single build file Sep 1, 2015 .gitignore add ensime cruft to .gitignore Aug 31, 2015 .travis.yml scala 2.11.6, 2.10.5; jdk 1.8 Apr 22, 2015 LICENSE Add MIT license file to the root dir. Apr 14, 2014 README.md Update README.md Sep 15, 2015 build.sbt Bump spire to 0.11.0 Nov 6, 2015 version.sbt Setting version to 0.4.3-SNAPSHOT Sep 15, 2015 README.md atto: everyday parsers atto is a compact, pure-functional, incremental text parsing library for Scala (if you're looking for binary parsing, please turn your attention to scodec). The atto API is non-invasive (nothing to extend!) and uses sensible and familiar abstractions. atto parsers are a fun and principled tool for everyday parsing. scala> int.sepBy1(spaceChar).parseOnly(""1 20 300"").option res0: Option[scalaz.NonEmptyList[Int]] = Some(NonEmptyList(1, 20, 300)) Current version is 0.4.2 and is available for Scala 2.10 and 2.11 with scalaz 7.1. What's New? Notable changes since 0.4.1 include: atto is now published on Sonatype, so the Bintray resolvers are no longer needed. New character parsers whitespace, horizontalWhitespace, oneOf, and noneOf. New text parsers skipWhitespace and token for dealing with tokens. New text parsers bracket, parens, squareBrackets, braces, envelopes, and bananas for dealing with bracketed text. Additional syntax for .token, .parens, .sepBy*, .many*, and .skipMany*. Constructions that combine multiple parsers such as discard* and either are now strict only in the initial parser, which makes it easier to write recursive parsers that would otherwise need to use delay. In addition to the above there were some logistical changes and a bug fix in optElem. For details on all changes please see the milestone. Many thanks to the generous folks who contributed to this release: Rúnar Óli Bjarnason Adelbert Chang Jonathan Ferguson Alberto Jácome Getting Started Add atto as a dependency in your build.sbt file. libraryDependencies += ""org.tpolecat"" %% ""atto-core""  % ""0.4.2"" if you are using Spire and want parsers for unsigned integral types you can also add atto-spire. Experimental integration with scalaz-stream is provided by atto-stream which can be added as above. This tiny library provides combinators to turn Parser[A] into Process1[String, A] with a few variations. There is a very basic example given here. Why atto? atto differs from stdlib parser combinators in a number of ways: You don't have to extend a trait or implement any methods. There is no tokenizer; the input type is always Char. Abstractions are better defined, which leads to simpler, more general code. Parser is a scalaz Monad for example, which gives us a lot of helpful operations for free. Parsers are incremental which means you can evaluate whether a prefix of your input is ""ok so far."" This can be helpful when working with streams or interactive UIs. It's not a big deal to construct and use atto parsers; use them in any situation where you might otherwise reach for regular expressions or raw string manipulation. Although atto is 50 times faster now than version 0.1, it's still not the fastest parsing lib on the block. If you're doing massive data processing you might look at a heavier library like Parboiled2, or even a hand-built parser like those used in the fastest JSON libs. But for ""everyday"" parsing where you have to turn user input into something useful, atto is a friendly little library to use. Documentation Behold: A wee REPL tutorial. A variety of tasty examples. Read the source! Perhaps start with the parser definitions. Contributors The core of atto originated in Edward Kmett's Scala port of Attoparsec. This library is an elaboration maintained by @tpolecat with contributions from some very helpful folks. Feedback and suggestions are always welcome. License Attoparsec, a Haskell library, is licensed under BSD-3 as specified here; the derivative work atto is provided under the MIT licence here. Both licenses appear in project metadata. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/tpolecat/atto"	"Pure functional incremental text parsing library for Scala, based on Attoparsec."	"true"
"Parsing"	"Fast Parse ★ 335 ⧗ 2"	"https://github.com/lihaoyi/fastparse"	"Fast to write, Fast running Parsers in Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"377"	"23"	"60"	"GitHub - lihaoyi/fastparse: Writing Fast Parsers Fast in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 23 Star 377 Fork 60 lihaoyi/fastparse Code Issues 15 Pull requests 4 Pulse Graphs Writing Fast Parsers Fast in Scala 395 commits 4 branches 8 releases Fetching contributors Scala 87.4% Python 12.6% Scala Python Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages low-level-intrinsics master test Nothing to show 0.3.7 0.3.5 0.3.4 0.3.3 0.2.1 0.2.0 0.1.1 0.1.0 Nothing to show New pull request Latest commit df35189 Jun 27, 2016 lihaoyi committed on GitHub Merge pull request #95 from jsyeo/jsyeo-typo … Fix typo Permalink Failed to load latest commit information. byteparse Moved byte and bmp tests to the separate module Jun 22, 2016 cssparse Name and style fixes Jun 19, 2016 demo/src/main/scala/demo Name and style fixes Jun 19, 2016 fastparse Minor fixes in tests Jun 24, 2016 perftests Added byte perfomance test Jun 22, 2016 project 0.3.7 Mar 7, 2016 pythonparse Name and style fixes Jun 19, 2016 readme Fix typo Jun 27, 2016 scalaparse Moved PerfTests to the separate module perftests, added benchmarks fo… Jun 20, 2016 utils/shared/src/main Name and style fixes Jun 19, 2016 .gitignore first commit in new repo Nov 29, 2014 .sbtopts reduce SBT memory max in the hopes of appeasing the travis gods Jun 1, 2015 .travis.yml Fixed travis configuration and changed Vector to ArrayBuffer in Chain Jun 24, 2016 README.md changed https to http in documentation link May 4, 2016 build.sbt Added byte perfomance test Jun 22, 2016 README.md FastParse This is where the code for the FastParse parsing library lives! If you want to use Fastparse, you probably will want to check out the documentation: Documentation This readme contains some developer docs, if you intend on working on the fastparse repo, not just using it as a library. Developer Docs The core of FastParse lives in the fastparse/ folder. It is cross-built ScalaJVM/Scala.js codebase, with almost everything shared between the two platforms in the fastparse/shared/ and minor differences in fastparse/js/ and fastparse/jvm/. The two subprojects scalaparse/ and pythonparse/ are FastParse parsers for those respective languages. These are both usable as standalone libraries, and also serve as extensive test-suites and use-cases for FastParse itself. Each of those projects clones & parses large quantities of code from Github as part of their own test suites. util/ contains some basic utilities that FastParse uses that aren't specific to parsers: bitsets, escaping, tries, etc.. readme/ contains the documentation site, which contains several live demos of FastParse parsers compiled to Scala.js. These all live in demo/. Common Commands sbt ~fastparseJVM/test runs the main testsuite. If you're hacking on FastParse, this is often where you want to go You can run the other suites for fastparseJS, scalaparseJVM, etc. if you wish, but I typically don't and leave that to CI unless I'm actively working on the sub-project You can use + to run it under different Scala versions, but again I usually don't bother +modules/test is the aggregate test-all command, and +modules/publishSigned is publish-all. Other things (compile, etc.) can also be run on modules readme/run builds the documentation site, which can then be found at readme/target/scalatex/index.html Contribution Guidelines If you're not sure if something is a bug or not, ask on Gitter first =) All code PRs should come with: a meaningful description, inline comments for important things, unit tests, and a green build Non-trivial changes, including bug fixes, should appear in the changelog. Feel free to add your name and link to your github profile! New features should be added to the relevant parts of the documentation To a large extent, FastParse is designed so that you can extend it in your own code without needing to modify the core. If you want to add features, be prepared to argue why it should be built-in and not just part of your own code. It's entirely possible your changes won't be merged, or will get ripped out later. This is also the case for my changes, as the Author! Even a rejected/reverted PR is valuable! It helps explore the solution space, and know what works and what doesn't. For every line in the repo, at least three lines were tried, committed, and reverted/refactored, and more than 10 were tried without committing. Feel free to send Proof-Of-Concept PRs that you don't intend to get merged. No binary or source compatibility is guaranteed between any releases. FastParse is still in the 0.x.y phase of development, which means it's still under rapid development and things do change. On the other hand, upgrading is usually trivial, and I don't expect existing functionality to go away License The MIT License (MIT) Copyright (c) 2014 Li Haoyi (haoyi.sg@gmail.com) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/fastparse"	"Fast to write, Fast running Parsers in Scala"	"true"
"Parsing"	"Parboiled2 ★ 432 ⧗ 0"	"https://github.com/sirthias/parboiled2"	"A Fast Parser Generator for Scala 2.10.3+."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"449"	"48"	"49"	"GitHub - sirthias/parboiled2: A macro-based PEG parser generator for Scala 2.10+ Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 48 Star 449 Fork 49 sirthias/parboiled2 Code Issues 28 Pull requests 4 Pulse Graphs A macro-based PEG parser generator for Scala 2.10+ http://parboiled2.org 361 commits 6 branches 14 releases 15 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: release-2.1 Switch branches/tags Branches Tags master release-2.1-js release-2.1 scala-2.10 wip/scala-2.11-possible-regression wip/value-class-rule Nothing to show v2.1.2 v2.1.1 v2.1.0_2.10 v2.1.0 v2.0.1_2.10 v2.0.1 v2.0.0_2.10 v2.0.0 v2.0.0-RC2_2.10 v2.0.0-RC2 v2.0.0-RC1_2.10 v2.0.0-RC1 v2.0-M2 v2.0-M1 Nothing to show New pull request Latest commit 2969eed Jun 22, 2016 sirthias committed on GitHub Merge pull request #168 from Primetalk/Primetalk-patch-fix-167 … Fix #167 (`andNot` implementation) Permalink Failed to load latest commit information. examples/src = Improve CsvParser example, closes #108 Feb 18, 2015 jsonBenchmark/src/main = Upgrade SBT and dependencies Sep 22, 2014 notes = Update implicit.ly notes for 2.0.1 Sep 5, 2014 parboiled-core/src Fix #167 (`andNot` implementation) Jun 22, 2016 parboiled/src = Fix most 2.11 compiler warnings Jun 3, 2014 project Scalajs: bump version to 0.6.8 Mar 29, 2016 scalaParser/src ! Another round of improvements to error reporting logic and implemen… Feb 20, 2015 .gitignore Initial commit Apr 18, 2013 CHANGELOG = Prepare for 2.1.3 release Apr 26, 2016 LICENSE Add CHANGELOG and LICENSE Dec 18, 2013 README.rst = Prepare for 2.1.3 release Apr 26, 2016 build.sbt = Prepare for 2.1.3 release Apr 26, 2016 README.rst parboiled2 – A Macro-Based PEG Parser Generator for Scala 2.10.3+ Contents of this Document Introduction Features Installation Example Quick Start How the Parser matches Input The Rule DSL Rule Types and the Value Stack Basic Character Matching Rule Combinators and Modifiers Parser Actions Additional Helpers Error Reporting The Error Collection Process Formatting Parse Errors Tweaking Error Reporting The atomic Marker The quiet Marker Naming Rules Manual Error Reporting Limiting Error Re-Runs Recovering from Parse Errors Advanced Techniques Common Mistakes Disregarding Order Choice Non-Termination when using Syntactic Predicates Unchecked Mutable State Handling Whitespace Parsing the whole Input Grammar Debugging Access to Parser Results Alternative DeliverySchemes Running the Examples Alternatives parboiled2 vs. parboiled 1.x parboiled2 vs. Scala Parser Combinators parboiled2 vs. Regular Expressions Roadmap Contributing Support References Credits License Introduction parboiled2 is a Scala 2.10.3+ library enabling lightweight and easy-to-use, yet powerful, fast and elegant parsing of arbitrary input text. It implements a macro-based parser generator for Parsing Expression Grammars (PEGs), which runs at compile time and translates a grammar rule definition (written in an internal Scala DSL) into corresponding JVM bytecode. PEGs are an alternative to Context-Free Grammars (CFGs) for formally specifying syntax, they make a good replacement for regular expressions and have some advantages over the ""traditional"" way of building parsers via CFGs (like not needing a separate lexer/scanner phase). parboiled2 is the successor of parboiled 1.x , which provides a similar capability (for Scala as well as Java) but does not actually generate a parser. Rather parboiled 1.x interprets a rule tree structure (which is also created via an internal DSL) against the input, which results in a much lower parsing performance. For more info on how parboiled 1.x and parboiled2 compare see parboiled2 vs. parboiled 1.x. You might also be interested in reading about parboiled2 vs. Scala Parser Combinators and parboiled2 vs. Regular Expressions. Features Concise, flexible and type-safe DSL for expressing parsing logic Full expressive power of Parsing Expression Grammars, for effectively dealing with most real-world parsing needs Excellent reporting of parse errors Parsing performance comparable to hand-written parsers Easy to learn and use (just one parsing phase (no lexer code required), rather small API) Light-weight enough to serve as a replacement for regular expressions (also strictly more powerful than regexes) Installation The artifacts for parboiled2 live on Maven Central and can be tied into your SBT-based Scala project like this: libraryDependencies += ""org.parboiled"" %% ""parboiled"" % ""2.1.3"" The latest released version is 2.1.3. It is available for Scala 2.10, Scala 2.11 as well as Scala.js 0.6. parboiled2 has only one single dependency that it will transitively pull into your classpath: shapeless (currently version 2.3.0). Note: If your project also uses ""io.spray"" %% ""spray-routing"" you'll need to change this to ""io.spray"" %% ""spray-routing-shapeless2"" in order for your project to continue to build since the ""regular"" spray builds use shapeless 1.x. Once on your classpath you can use this single import to bring everything you need into scope: import org.parboiled2._ There might be potentially newer snapshot builds available in the sonatype snapshots repository located at: https://oss.sonatype.org/content/repositories/snapshots/ You can find the latest ones here: https://oss.sonatype.org/content/repositories/snapshots/org/parboiled/parboiled_2.10/ (Scala 2.10) and https://oss.sonatype.org/content/repositories/snapshots/org/parboiled/parboiled_2.11/ (Scala 2.11) Example This is what a simple parboiled2 parser looks like: import org.parboiled2._  class Calculator(val input: ParserInput) extends Parser {   def InputLine = rule { Expression ~ EOI }    def Expression: Rule1[Int] = rule {     Term ~ zeroOrMore(       '+' ~ Term ~> ((_: Int) + _)     | '-' ~ Term ~> ((_: Int) - _))   }    def Term = rule {     Factor ~ zeroOrMore(       '*' ~ Factor ~> ((_: Int) * _)     | '/' ~ Factor ~> ((_: Int) / _))   }    def Factor = rule { Number | Parens }    def Parens = rule { '(' ~ Expression ~ ')' }    def Number = rule { capture(Digits) ~> (_.toInt) }    def Digits = rule { oneOrMore(CharPredicate.Digit) } }  new Calculator(""1+1"").InputLine.run() // evaluates to `scala.util.Success(2)` This implements a parser for simple integer expressions like 1+(2-3*4)/5 and runs the actual calculation in-phase with the parser. If you'd like to see it run and try it out yourself check out Running the Examples. Quick Start A parboiled2 parser is a class deriving from org.parboiled2.Parser, which defines one abstract member: def input: ParserInput holding the input for the parsing run. Usually it is best implemented as a val parameter in the constructor (as shown in the Example above). As you can see from this design you need to (re-)create a new parser instance for every parsing run (parser instances are very lightweight). The ""productions"" (or ""rules"") of your grammar are then defined as simple methods, which in most cases consist of a single call to the rule macro whose argument is a DSL expression defining what input the rule is to match and what actions to perform. In order to run your parser against a given input you create a new instance and call run() on the top-level rule, e.g: val parser = new MyParser(input) parser.topLevelRule.run() // by default returns a ``scala.util.Try`` For more info on what options you have with regard to accessing the results of a parsing run check out the section on Access to Parser Results. How the Parser matches Input PEG parsers are quite easy to understand as they work just like most people without a lot of background in parsing theory would build a parser ""by hand"": recursive-descent with backtracking. They have only one parsing phase (not two, like most parsers produced by traditional parser generators like ANTLR), do not require any look-ahead and perform quite well in most real-world scenarios (although they can exhibit exponential runtime for certain pathological languages and inputs). A PEG parser consists of a number of rules that logically form a ""tree"", with one ""root"" rule at the top calling zero or more lower-level rules, which can each call other rules and so on. Since rules can also call themselves or any of their parents the rule ""tree"" is not really a tree but rather a potentially cyclic directed graph, but in most cases the tree structure dominates, which is why its useful to think of it as a tree with potential cycles. When a rule is executed against the current position in an input buffer it applies its specific matching logic to the input, which can either succeed or fail. In the success case the parser advances the input position (the cursor) and potentially executes the next rule. Otherwise, when the rule fails, the cursor is reset and the parser backtracks in search of another parsing alternative that might succeed. For example consider this simple parboiled2 rule: def foo = rule { 'a' ~ ('b' ~ 'c' | 'b' ~ 'd') } When this rule is confronted with the input abd the parser matches the input in these steps: Rule foo starts executing, which calls its first sub-rule 'a'. The cursor is at position 0. Rule 'a' is executed against input position 0, matches (succeeds) and the cursor is advanced to position 1. Rule 'b' ~ 'c' | 'b' ~ 'd' starts executing, which calls its first sub-rule 'b' ~ 'c'. Rule 'b' ~ 'c' starts executing, which calls its first sub-rule 'b'. Rule 'b' is executed against input position 1, matches (succeeds) and the cursor is advanced to position 2. Rule 'c' is executed against input position 2 and mismatches (fails). Rule 'b' ~ 'c' | 'b' ~ 'd' notices that its first sub-rule has failed, resets the cursor to position 1 and calls its 2nd sub-rule 'b' ~ 'd'. Rule 'b' ~ 'd' starts executing, which calls its first sub-rule 'b'. Rule 'b' is executed against input position 1, matches and the cursor is advanced to position 2. Rule 'd' is executed against input position 2, matches and the cursor is advanced to position 3. Rule 'b' ~ 'd' completes successfully, as its last sub-rule has succeeded. Rule 'b' ~ 'c' | 'b' ~ 'd' completes successfully, as one of its sub-rules has succeeded. Rule foo completes execution successfully, as its last sub-rule has succeeded. The whole input ""abd"" was matched and the cursor is left at position 3 (after the last-matched character). The Rule DSL In order to work with parboiled2 effectively you should understand the core concepts behind its rule DSL, mainly the ""Value Stack"" and how parboiled2 encodes value stack operations in the Scala type system. Rule Types and the Value Stack Apart from the input buffer and the cursor the parser manages another important structure: the ""Value Stack"". The value stack is a simple stack construct that serves as temporary storage for your Parser Actions. In many cases it is used for constructing an AST during the parsing run but it can also be used for ""in-phase"" computations (like in the Example above) or for any other purpose. When a rule of a parboiled2 parser executes it performs any combination of the following three things: match input, i.e. advance the input cursor operate on the value stack, i.e. pop values off and/or push values to the value stack perform side-effects Matching input is done by calling Basic Character Matching rules, which do nothing but match input and advance the cursor. Value stack operations (and other potential side-effects) are performed by Parser Actions. It is important to understand that rules in parboiled2 (i.e. the rule methods in your parser class) do not directly return some custom value as a method result. Instead, all their consuming and producing values happens as side-effects to the value stack. Thereby the way that a rule interacts with value stack is encoded in the rule's type. This is the general definition of a parboiled2 rule: class Rule[-I <: HList, +O <: HList] This can look scary at first but is really quite simple. An HList is defined by shapeless and is essentially a type of list whose element number and element types are statically known at compile time. The I type parameter on Rule encodes what values (the number and types) the rule pops off the value stack and the O type parameter encodes what values (the number and types) the rule then pushes onto the value stack. Luckily, in most cases, you won't have to work with these types directly as they can either be inferred or you can use one of these predefined aliases: type Rule0 = RuleN[HNil] type Rule1[+T] = RuleN[T :: HNil] type Rule2[+A, +B] = RuleN[A :: B :: HNil] type RuleN[+L <: HList] = Rule[HNil, L] type PopRule[-L <: HList] = Rule[L, HNil] Here is what these type aliases denote: Rule0 A rule that neither pops off nor pushes to the value stack, i.e. has no effect on the value stack whatsoever. All Basic Character Matching rules are of this type. Rule1[+T] Pushes exactly one value of type T onto the value stack. After Rule0 this is the second-most frequently used rule type. Rule2[+A, +B] Pushes exactly two values of types A and B onto the value stack. RuleN[+L <: HList] Pushes a number of values onto the value stack, which correspond to the given L <: HList type parameter. PopRule[-L <: HList] Pops a number of values off the value stack (corresponding to the given L <: HList type parameter) and does not produce any new value itself. The rule DSL makes sure that the rule types are properly assembled and carried through your rule structure as you combine Basic Character Matching with Rule Combinators and Modifiers and Parser Actions, so as long as you don't write any logic that circumvents the value stack your parser will be completely type-safe and the compiler will be able to catch you if you make mistakes by combining rules in an unsound way. Basic Character Matching The following basic character matching rules are the only way to cause the parser to match actual input and ""make progress"". They are the ""atomic"" elements of the rule DSL which are then used by the Rule Combinators and Modifiers to form higher-level rules. implicit def ch(c: Char): Rule0 Char values can be directly used in the rule DSL and match themselves. There is one notable case where you will have to use the explicit ch wrapper: You cannot use the | operator directly on chars as it denotes the built-in Scala binary ""or"" operator defined on numeric types (Char is an unsigned 16-bit integer). So rather than saying 'a' | 'b' you will have to say ch('a') | 'b'. implicit def str(s: String): Rule0 String values can be directly used in the rule DSL and match themselves. implicit def predicate(p: CharPredicate): Rule0 You can use org.parboiled2.CharPredicate values directly in the rule DSL. CharPredicate is an efficient implementation of character sets and already comes with a number pre-defined character classes like CharPredicate.Digit or CharPredicate.LowerHexLetter. implicit def valueMap[T](m: Map[String, T]): R Values of type Map[String, T] can be directly used in the rule DSL and match any of the given map's keys and push the respective value upon a successful match. The resulting rule type depends on T: T R Unit Rule0 L <: HList RuleN[L] (pushes all values of L) T (otherwise) Rule1[T] (pushes only one value) def anyOf(chars: String): Rule0 This constructs a Rule0 which matches any of the given strings characters. def noneOf(chars: String): Rule0 This constructs a Rule0 which matches any single character except the ones in the given string and except EOI. def ignoreCase(c: Char): Rule0 Matches the given single character case insensitively. Note: The given character must be specified in lower-case! This requirement is currently NOT enforced! def ignoreCase(s: String): Rule0 Matches the given string of characters case insensitively. Note: The given string must be specified in all lower-case! This requirement is currently NOT enforced! def ANY: Rule0 Matches any character except EOI (end-of-input). def EOI: Char The EOI (end-of-input) character, which is a virtual character that the parser ""appends"" after the last character of the actual input. def MATCH: Rule0 Matches no character (i.e. doesn't cause the parser to make any progress) but succeeds always. It's the ""empty"" rule that is mostly used as a neutral element in rule composition. def MISMATCH[I <: HList, O <: HList]: Rule[I, O] A rule that always fails. Fits any rule signature. def MISMATCH0: Rule0 Same as MISMATCH but with a clearly defined type. Use it (rather then MISMATCH) if the call site doesn't clearly ""dictate"" a certain rule type and using MISMATCH therefore gives you a compiler error. Rule Combinators and Modifiers Rules can be freely combined/modified with these operations: a ~ b Two rules a and b can be combined with the ~ operator resulting in a rule that only matches if first a matches and then b matches. The computation of the resulting rule type is somewhat involved. Here is an illustration (using an abbreviated HList notation): a b a ~ b Rule[, A] Rule[, B] Rule[, A:B] Rule[A:B:C, D:E:F] Rule[F, G:H] Rule[A:B:C, D:E:G:H] Rule[A, B:C] Rule[D:B:C, E:F] Rule[D:A, E:F] Rule[A, B:C] Rule[D:C, E:F] Illegal if D != B a | b Two rules a and b can be combined with the | operator to form an ""ordered choice"" in PEG speak. The resulting rule tries to match a and succeeds if this succeeds. Otherwise the parser is reset and b is tried. This operator can only be used on compatible rules. &(a) Creates a ""positive syntactic predicate"", i.e. a rule that tests if the underlying rule matches but doesn't cause the parser to make any progress (i.e. match any input) itself. Also, all effects that the underlying rule might have had on the value stack are cleared out, the resulting rule type is therefore always Rule0, independently of the type of the underlying rule. Note that & not itself consuming any input can have surprising implications in repeating constructs, see Non-Termination when using Syntactic Predicates for more details. !a Creates a ""negative syntactic predicate"", i.e. a rule that matches only if the underlying one mismatches and vice versa. A syntactic predicate doesn't cause the parser to make any progress (i.e. match any input) and also clears out all effects that the underlying rule might have had on the value stack. The resulting rule type is therefore always Rule0, independently of the type of the underlying rule. Note that ! not itself consuming any input can have surprising implications in repeating constructs, see Non-Termination when using Syntactic Predicates for more details. optional(a) Runs its inner rule and succeeds even if the inner rule doesn't. The resulting rule type depends on the type of the inner rule: Type of a Type of optional(a) Rule0 Rule0 Rule1[T] Rule1[Option[T]] Rule[I, O <: I] Rule[I, O] The last case is a so-called ""reduction rule"", which leaves the value stack unchanged on a type level. This is an example of a reduction rule wrapped with optional: capture(CharPredicate.Digit) ~ optional(ch('h') ~> ((s: String) => s + ""hex"")) The inner rule of optional here has type Rule[String :: HNil, String :: HNil], i.e. it pops one String off the stack and pushes another one onto it, which means that the number of elements on the value stack as well as their types remain the same, even though the actual values might have changed. As a shortcut you can also use a.? instead of optional(a). zeroOrMore(a) Runs its inner rule until it fails, always succeeds. The resulting rule type depends on the type of the inner rule: Type of a Type of zeroOrMore(a) Rule0 Rule0 Rule1[T] Rule1[Seq[T]] Rule[I, O <: I] Rule[I, O] The last case is a so-called ""reduction rule"", which leaves the value stack unchanged on a type level. This is an example of a reduction rule wrapped with zeroOrMore: (factor :Rule1[Int]) ~ zeroOrMore('*' ~ factor ~> ((a: Int, b) => a * b)) The inner rule of zeroOrMore here has type Rule[Int :: HNil, Int :: HNil], i.e. it pops one Int off the stack and pushes another one onto it, which means that the number of elements on the value stack as well as their types remain the same, even though the actual values might have changed. As a shortcut you can also use a.* instead of zeroOrMore(a). oneOrMore(a) Runs its inner rule until it fails, succeeds if its inner rule succeeded at least once. The resulting rule type depends on the type of the inner rule: Type of a Type of oneOrMore(a) Rule0 Rule0 Rule1[T] Rule1[Seq[T]] Rule[I, O <: I] Rule[I, I] The last case is a so-called ""reduction rule"", which leaves the value stack unchanged on a type level. This is an example of a reduction rule wrapped with oneOrMore: (factor :Rule1[Int]) ~ oneOrMore('*' ~ factor ~> ((a: Int, b) => a * b)) The inner rule of oneOrMore here has type Rule[Int :: HNil, Int :: HNil], i.e. it pops one Int off the stack and pushes another one onto it, which means that the number of elements on the value stack as well as their types remain the same, even though the actual values might have changed. As a shortcut you can also use a.+ instead of oneOrMore(a). xxx.times(a) Repeats a rule a given number of times. xxx can be either a positive Int value or a range (<x> to <y>) whereby both <x> and <y> are positive Int values. The resulting rule type depends on the type of the inner rule: Type of a Type of xxx.times(a) Rule0 Rule0 Rule1[T] Rule1[Seq[T]] Rule[I, O <: I] Rule[I, O] The last case is a so-called ""reduction rule"", which leaves the value stack unchanged on a type level. This is an example of a reduction rule wrapped with oneOrMore: (factor :Rule1[Int]) ~ (1 to 5).times('*' ~ factor ~> ((a: Int, b) => a * b)) The inner rule here has type Rule[Int :: HNil, Int :: HNil], i.e. it pops one Int off the stack and pushes another one onto it, which means that the number of elements on the value stack as well as their types remain the same, even though the actual values might have changed. a.separatedBy(separator: Rule0) You can use a.separatedBy(b) to create a rule with efficient and automatic support for element separators if a is a rule produced by the zeroOrMore, oneOrMore or xxx.times modifier and b is a Rule0. The resulting rule has the same type as a but expects the individual repetition elements to be separated by a successful match of the separator rule. As a shortcut you can also use a.*(b) or (a * b) instead of zeroOrMore(a).separatedBy(b). The same shortcut also works for + (oneOrMore). a ~!~ b Same as ~ but with ""cut"" semantics, meaning that the parser will never backtrack across this boundary. If the rule being concatenated doesn't match a parse error will be triggered immediately. Usually you don't need to use this ""cut"" operator but in certain cases it can help in simplifying grammar construction. Parser Actions The Basic Character Matching rules and the Rule Combinators and Modifiers allow you to build recognizers for potentially complex languages, but usually your parser is supposed to do more than simply determine whether a given input conforms to the defined grammar. In order to run custom logic during parser execution, e.g. for creating custom objects (like an AST), you will have to add some ""actions"" to your rules. push(value) push(value) creates a rule that matches no input (but always succeeds, as a rule) and pushes the given value onto the value stack. Its rule type depends on the given value: Type of value Type of push(value) Unit Rule0 (identical to run in this case) L <: HList RuleN[L] (pushes all values of L) T (otherwise) Rule1[T] (pushes only one value) Also note that, due to the macro expansion the parboiled2 rule DSL is based on, the given value expression behaves like a call-by-name parameter even though it is not marked as one! This means that the argument expression to push is (re-)evaluated for every rule execution. capture(a) Wrapping a rule a with capture turns that rule into one that pushes an additional String instance onto the value stack (in addition to all values that a already pushes itself): the input text matched by a. For example capture(oneOrMore(CharPredicate.Digit)) has type Rule1[String] and pushes one value onto the value stack: the string of digit characters matched by oneOrMore(CharPredicate.Digit). Another example: capture(""foo"" ~ push(42)) has type Rule2[Int, String] and will match input ""foo"". After successful execution the value stack will have the String ""foo"" as its top element and 42 underneath. test(condition: Boolean): Rule0 test implements ""semantic predicates"". It creates a rule that matches no input and succeeds only if the given condition expression evaluates to true. Note that, due to the macro expansion the parboiled2 rule DSL is based on, the given argument behaves like a call-by-name parameter even though it is not marked as one! This means that the argument expression to test is (re-)evaluated for every rule execution, just as if test would have been defined as def test(condition: => Boolean): Rule0. a ~> (...) The ~> operator is the ""action operator"" and as such the most frequently used way to add custom logic to a rule. It can be applied to any rule and appends action logic to it. The argument to ~> is always a function, what functions are allowed and what the resulting rule type is depends on the type of a. The basic idea is that the input of the function is popped of the value stack and the result of the function is pushed back onto it. In its basic form the ~> operator therefore transforms the top elements of the value stack into some other object(s). Let's look at some examples: (foo: Rule1[Int]) ~> (i => i * 2) This results in a Rule1[Int] which multiplies the ""output"" of rule foo by 2. (foo: Rule2[Int, String]) ~> ((i, s) => s + i.toString) This results in a Rule1[String] which combines the two ""outputs"" of rule foo (an Int and a String) into one single String. (foo: Rule2[Int, String]) ~> (_.toDouble) This results in a Rule2[Int, Double]. As you can see the function argument to ~> doesn't always have to ""take"" the complete output of the rule its applied to. It can also take fewer or even more elements. Its parameters are simply matched left to right against the top of the value stack (the right-most parameter matching the top-level element). (foo: Rule1[String]) ~> ((i :Int, s) => s + i.toString) This results in a Rule[Int :: HNil, String :: HNil], i.e. a rule that pops one Int value off the stack and replaces it with a String. Note that, while the parameter types to the action function can be inferred if they can be matched against an ""output"" of the underlying rule, this is not the case for parameters that don't directly correspond to an underlying output. In these cases you need to add an explicit type annotation to the respective action function parameter(s). If an action function returns Unit it doesn't push anything on the stack. So this rule (foo: Rule1[String]) ~> (println(_)) has type Rule0. Also, an action function can also be a Function0, i.e. a function without any parameters: (foo: Rule1[String]) ~> (() => 42) This rule has type Rule2[String, Int] and is equivalent to this: (foo: Rule1[String]) ~ push(42) An action function can also produce more than one output by returning an HList instance: (foo: Rule1[String]) ~> (s => s.toInt :: 3.14 :: HNil) This has type Rule2[Int, Double]. One more very useful feature is special support for case class instance creation: case class Person(name: String, age: Int)  (foo: Rule2[String, Int]) ~> Person This has type Rule1[Person]. The top elements of the value stack are popped off and replaced by an instance of the case class if they match in number, order and types to the case class members. This is great for building AST-like structures! Check out the Calculator2 example to see this form in action. Note that there is one quirk: For some reason this notation stops working if you explicitly define a companion object for your case class. You'll have to write ~> (Person(_, _)) instead. And finally, there is one more very powerful action type: the action function can itself return a rule! If an action returns a rule this rule is immediately executed after the action application just as if it had been concatenated to the underlying rule with the ~ operator. You can therefore do things like (foo: Rule1[Int]) ~> (i => test(i % 2 == 0) ~ push(i)) which is a Rule1[Int] that only produces even integers and fails for all others. Or, somewhat unusual but still perfectly legal: capture(""x"") ~> (str(_)) which is a Rule0 that is identical to 'x' ~ 'x'. run(expression) run is the most versatile parser action. It can have several shapes, depending on the type of its argument expression. If the argument expression evaluates to a rule (i.e. has type R <: Rule[_, _]) the result type of run is this rule's type (i.e. R) and the produced rule is immediately executed. a function with 1 to 5 parameters these parameters are mapped against the top of the value stack, popped and the function executed. Thereby the function behaves just like an action function for the ~> operator, i.e. if it produces a Unit value this result is simply dropped. HList results are pushed onto the value stack (all their elements individually), rule results are immediately executed and other result values are pushed onto the value stack as a single element. The difference between using run and attaching an action function with the ~> operator is that in the latter case the compiler can usually infer the types of the function parameters (if they map to ""output"" values of the base rule) while with run you always have to explicitly attach type annotation to the function parameters. a function with one HList parameter the behavior is similar to the previous case with the difference that the elements of this parameter HList are mapped against the value stack top. This allows for consumption of an arbitrary number of value stack elements (Note: This feature of run is not yet currently implemented.) any other value the result type of run is an always succeeding Rule0. Since in this case it doesn't interact with the value stack and doesn't match any input all it can do is perform ""unchecked"" side effects. Note that by using run in this way you are leaving the ""safety-net"" that the value stack and the rule type system gives you! Make sure you understand what you are doing before using these kinds of run actions! Also note that, due to the macro expansion the parboiled2 rule DSL is based on, the given block behaves like a call-by-name parameter even though it is not marked as one! This means that the argument expression to run is (re-)evaluated for every rule execution. runSubParser(f: ParserInput ⇒ Rule[I, O]): Rule[I, O] This action allows creation of a sub parser and running of one of its rules as part of the current parsing process. The subparser will start parsing at the current input position and the outer parser (the one calling runSubParser) will continue where the sub-parser stopped. There are a few more members of the Parser class that are useful for writing efficient action logic: def cursor: Int The index of the next (yet unmatched) input character. Note: Might be equal to input.length if the cursor is currently behind the last input character! def cursorChar: Char The next (yet unmatched) input character, i.e. the one at the cursor index. Identical to if (cursor < input.length) input.charAt(cursor) else EOI but more efficient. def lastChar: Char Returns the last character that was matched, i.e. the one at index cursor - 1 and as such is equivalent to charAt(-1). Note that for performance optimization this method does not do a range check, i.e. depending on the ParserInput implementation you might get an exception when calling this method before any character was matched by the parser. def charAt(offset: Int): Char Returns the character at the input index with the given delta to the cursor and as such is equivalent to input.charAt(cursor + offset). Note that for performance optimization this method does not do a range check, i.e. depending on the ParserInput implementation you might get an exception if the computed index is out of bounds. def charAtRC(offset: Int): Char Same as charAt but range-checked. Returns the input character at the index with the given offset from the cursor. If this index is out of range the method returns EOI. You can use these to write efficient character-level logic like this: def hexDigit: Rule1[Int] = rule {   CharPredicate.HexAlpha ~ push(CharUtils.hexValue(lastChar)) } Additional Helpers Base64Parsing For parsing RFC2045 (Base64) encoded strings parboiled provides the Base64Parsing trait which you can mix into your Parser class. See its source for more info on what exactly it provides. parboiled also comes with the org.parboiled2.util.Base64 class which provides an efficient Base64 encoder/decoder for the standard as well as custom alphabets. DynamicRuleDispatch Sometimes an application cannot fully specify at compile-time which of a given set of rules is to be called at runtime. For example, a parser for parsing HTTP header values might need to select the right parser rule for a header name that is only known once the HTTP request has actually been read from the network. To prevent you from having to write a large (and not really efficient) match against the header name for separating out all the possible cases parboiled provides the DynamicRuleDispatch facility. Check out its test for more info on how to use it. StringBuilding For certain high-performance use-cases it is sometimes better to construct Strings that the parser is to produce/extract from the input in a char-by-char fashion. To support you in doing this parboiled provides the StringBuilding trait which you can mix into your Parser class. It provides convenient access to a single and mutable StringBuilder instance. As such it operates outside of the value stack and therefore without the full ""safety net"" that parboiled's DSL otherwise gives you. If you don't understand what this means you probably shouldn't be using the StringBuilding trait but resort to capture and ordinary parser actions instead. Error Reporting In many applications, especially with grammars that are not too complex, parboiled provides good error reports right out of the box, without any additional requirements on your part. However, there are cases where you want to have more control over how parse errors are created and/or formatted. This section gives an overview over how parse error reporting works in parboiled and how you can influence it. The Error Collection Process As described in the section about How the Parser matches Input above the parser consumes input by applying grammar rules and backtracking in the case of mismatches. As such rule mismatches are an integral part of the parsers operation and do not generally mean that there is something wrong with the input. Only when the root rule itself mismatches and the parser has no backtracking options remaining does it become clear that a parse error is present. At that point however, when the root rule mismatches, the information about where exactly the problematic input was and which of the many rule mismatches that the parser experienced during the run were the ""bad"" ones is already lost. parboiled overcomes this problem by simply re-running the failed parser, potentially many times, and ""watching"" it as it tries to consume the erroneous input. With every re-run parboiled learns a bit more about the position and nature of the error and when this analysis is complete a ParseError instance is constructed and handed to the application as the result of the parsing run, which can then use the error information on its level (e.g. for formatting it and displaying it to the user). Note that re-running the parser in the presence of parse errors does result in unsuccessful parsing runs being potentially much slower than successful ones. However, since in the vast majority of use cases failed runs constitute only a small minority of all parsing runs and the normal flow of application logic is disrupted anyway, this slow-down is normally quite acceptable, especially if it results in better error messages. See the section on Limiting Error Re-Runs if this is not true for your application. In principle the error reporting process looks like this: The grammar's root rule is run at maximum speed against the parser input. If this succeeds then all is well and the parsing result is immediately dispatched to the user. If the root rule did not match we know that there we have a parsing error. The parser is then run again to establish the ""principal error location"". The principal error location is the first character in the input that could not be matched by any rule during the parsing run. In order words, it is the maximum value that the parser's cursor member had during the parsing run. Once the error location is known the parser is run again. This time all rule mismatches against the input character at error location are recorded. These rule mismatches are used to determine what input the grammar ""expects"" at the error location but failed to see. For every such ""error rule mismatch"" the parser collects the ""rule trace"", i.e. the stack of rules that led to it. Currently this is done by throwing a special exception that bubbles up through the JVM call stack and records rule stack information on its way up. A consequence of this design is that the parser needs to be re-run once per ""error rule mismatch"". When all error rule traces have been collected all the relevant information about the parse error has been extracted and a ParseError instance can be constructed and dispatched to the user. Note: The real process contains a few more steps to properly deal with the atomic and quiet markers described below. However, knowledge of these additional steps is not important for understanding the basic approach for how ParseError instances are constructed. Formatting Parse Errors If a parsing runs fails and you receive a ParseError instance you can call the formatError method on your parser instance to get the error rendered into an error message string: val errorMsg = parser.formatError(error) The formatError message can also take an explicit ErrorFormatter as a second argument, which allows you to influence how exactly the error is to be rendered. For example, in order to also render the rule traces you can do: val errorMsg = parser.formatError(error, new ErrorFormatter(showTraces = true)) Look at the signature of the ErrorFormatter constructor for more information on what rendering options exist. If you want even more control over the error rendering process you can extend the ErrorFormatter and override its methods where you see fit. Tweaking Error Reporting While the error collection process described above yields all information required for a basic ""this character was not matched and these characters were expected instead"" information you sometimes want to have more control over what exactly is reported as ""found"" and as ""expected"". The atomic Marker Since PEG parsers are scanner-less (i.e. without an intermediate ""TOKEN-stream"") they operate directly on the input buffer's character level. As such, by default, parboiled reports all errors on this character level. For example, if you run the rule ""foo"" | ""fob"" | ""bar"" against input ""foxes"" you'll get this error message: Invalid input 'x', expected 'o' or 'b' (line 1, column 3): foxes   ^  While this error message is certainly correct, it might not be what you want to show your users, e.g. because foo, fob and bar are regarded as ""atomic"" keywords of your language, that should either be matched completely or not at all. In this case you can use the atomic marker to signal this to the parser. For example, running the rule atomic(""foo"") | atomic(""fob"") | atomic(""bar"") against input ""foxes"" yields this error message: Invalid input ""fox"", expected ""foo"", ""fob"" or ""bar"" (line 1, column 1): foxes ^  Of course you can use the atomic marker on any type of rule, not just string rules. It essentially moves the reported error position forward from the principal error position and lifts the level at which errors are reported from the character level to a rule level of your choice. The quiet Marker Another problem that more frequently occurs with parboiled's default error reporting is that the list of ""expected"" things becomes too long. Often the reason for this are rules that deal match input which can appear pretty much anywhere, like whitespace or comments. Consider this simple language: def Expr    = rule { oneOrMore(Id ~ Keyword ~ Id).separatedBy(',' ~ WS) ~ EOI } def Id      = rule { oneOrMore(CharPredicate.Alpha) ~ WS } def Keyword = rule { atomic((""has"" | ""is"") ~ WS) } def WS      = rule { zeroOrMore(anyOf("" \t \n"")) } When we run the Expr rule against input ""Tim has money, Tom Is poor"" we get this error: Invalid input 'I', expected [ \t \n] or Keyword (line 1, column 20): Tim has money, Tom Is poor                    ^  Again the list of ""expected"" things is technically correct but we don't want to bother the user with the information that whitespace is also allowed at the error location. The quiet marker let's us suppress a certain rule from the expected list if there are also non-quiet alternatives: def WS = rule { quiet(zeroOrMore(anyOf("" \t \n""))) } With that change the error message becomes: Invalid input 'I', expected Keyword (line 1, column 20): Tim has money, Tom Is poor                    ^  which is what we want. Naming Rules parboiled uses a somewhat involved logic to determine what exactly to report as ""mismatched"" and ""expected"" for a given parse error. Essentially the process looks like this: Compare all rule trace for the error and drop a potentially existing common prefix. This is done because, if all traces share a common prefix, this prefix can be regarded as the ""context"" of the error which is probably apparent to the user and as such doesn't need to be reported. For each trace (suffix), find the first frame that tried to start its match at the reported error position. The string representation of this frame (which might be an assigned name) is selected for ""expected"" reporting. Duplicate ""expected"" strings are removed. So, apart from placing atomic and quiet markers you can also influence what gets reported as ""expected"" by explicitly naming rules. One way to do this is to pick good names for the rule methods as they automatically attach their name to their rules. The names of val or def members that you use to reference CharPredicate instances also automatically name the respective rule. If you don't want to split out rules into their own methods you can also use the named modifier. With it you can attach an explicit name to any parser rule. For example, if you run the rule foo from this snippet: def foo = rule { ""aa"" | atomic(""aaa"").named(""threeAs"") | 'b' | 'B'.named(""bigB"") } against input x you'll get this error message: Invalid input 'x', expected 'a', threeAs, 'b' or bigB (line 1, column 1): x ^  Manual Error Reporting If you want to completely bypass parboiled's built-in error reporting logic you can do so by exclusively relying on the fail helper, which causes the parser to immediately and fatally terminate the parsing run with a single one-frame rule trace with a given ""expected"" message. For example, the rule ""foo"" | fail(""a true FOO"") will produce this error when run against x: Invalid input 'x', expected a true FOO (line 1, column 1): x ^  Limiting Error Re-Runs Really large grammars, especially ones with bugs as they commonly appear during development, can exhibit a very large number of rule traces (potentially thousands) and thus cause the parser to take longer than convenient to terminate an error parsing run. In order to mitigate this parboiled has a configurable limit on the maximum number of rule traces the parser will collect during a single error run. The default limit is 24, you can change it by overriding the errorTraceCollectionLimit method of the Parser class. Recovering from Parse Errors Currently parboiled only ever parses up to the very first parse error in the input. While this is all that's required for a large number of use cases there are applications that do require the ability to somehow recover from parse errors and continue parsing. Syntax highlighting in an interactive IDE-like environment is one such example. Future versions of parboiled might support parse error recovery. If your application would benefit from this feature please let us know in this github ticket. Advanced Techniques Meta-Rules Sometimes you might find yourself in a situation where you'd like to DRY up your grammar definition by factoring out common constructs from several rule definitions in a ""meta-rule"" that modifies/decorates other rules. Essentially you'd like to write something like this (illegal code!): def expression = rule { bracketed(ab) ~ bracketed(cd) } def ab = rule { ""ab"" } def cd = rule { ""cd"" } def bracketed(inner: Rule0) = rule { '[' ~ inner ~ ']' } In this hypothetical example bracketed is a meta-rule which takes another rule as parameter and calls it from within its own rule definition. Unfortunately enabling a syntax such as the one shown above it not directly possible with parboiled. When looking at how the parser generation in parboiled actually works the reason becomes clear. parboiled ""expands"" the rule definition that is passed as argument to the rule macro into actual Scala code. The rule methods themselves however remain what they are: instance methods on the parser class. And since you cannot simply pass a method name as argument to another method the calls bracketed(ab) and bracketed(cd) from above don't compile. However, there is a work-around which might be good enough for your meta-rule needs: def expression = rule { bracketed(ab) ~ bracketed(cd) } val ab = () ⇒ rule { ""ab"" } val cd = () ⇒ rule { ""cd"" } def bracketed(inner: () ⇒ Rule0) = rule { '[' ~ inner() ~ ']' } If you model the rules that you want to pass as arguments to other rules as Function0 instances you can pass them around. Assigning those function instances to val members avoids re-allocation during every execution of the expression rule which would come with a potentially significant performance cost. Common Mistakes Disregarding Order Choice There is one mistake that new users frequently make when starting out with writing PEG grammars: disregarding the ""ordered choice"" logic of the | operator. This operator always tries all alternatives in the order that they were defined and picks the first match. As a consequence earlier alternatives that are a prefix of later alternatives will always ""shadow"" the later ones, the later ones will never be able to match! For example in this simple rule def foo = rule { ""foo"" | ""foobar"" } ""foobar"" will never match. Reordering the alternatives to either ""factor out"" all common prefixes or putting the more specific alternatives first are the canonical solutions. If your parser is not behaving the way you expect it to watch out for this ""wrong ordering"" problem, which might be not that easy to spot in more complicated rule structures. Non-Termination when using Syntactic Predicates The syntactic predicate operators, & and !, don't themselves consume any input, so directly wrapping them with a repeating combinator (like zeroOrMore or oneOrMore) will lead to an infinite loop as the parser continuously runs the syntactic predicate against the very same input position without making any progress. If you use syntactic predicates in a loop make sure to actually consume input as well. For example: def foo = rule { capture(zeroOrMore( !',' )) } will never terminate, while def foo = rule { capture(zeroOrMore( !',' ~ ANY )) } will capture all input until it reaches a comma. Unchecked Mutable State parboiled2 parsers work with mutable state as a design choice for achieving good parsing performance. Matching input and operating on the value stack happen as side-effects to rule execution and mutate the parser state. However, as long as you confine yourself to the value stack and do not add parser actions that mutate custom parser members the rule DSL will protect you from making mistakes. It is important to understand that, in case of rule mismatch, the parser state (cursor and value stack) is reset to what it was before the rule execution was started. However, if you write rules that have side-effects beyond matching input and operating on the value stack than these side-effects cannot be automatically rolled-back! This means that you will have to make sure that you action logic ""cleans up after itself"" in the case of rule mismatches or is only used in locations where you know that rule execution can never fail. These techniques are considered advanced and are not recommended for beginners. The rule DSL is powerful enough to support even very complex parsing logic without the need to resort to custom mutable state, we consider the addition of mutable members as an optimization that should be well justified. Handling Whitespace One disadvantage of PEGs over lexer-based parser can be the handling of white space. In a ""traditional"" parser with a separate lexer (scanner) phase this lexer can simply skip all white space and only generate tokens for the actual parser to operate on. This can free the higher-level parser grammar from all white space treatment. Since PEGs do not have a lexer but directly operate on the raw input they have to deal with white space in the grammar itself. Language designers with little experience in PEGs can sometime be unsure of how to best handle white space in their grammar. The common and highly recommended pattern is to match white space always immediately after a terminal (a single character or string) but not in any other place. This helps with keeping your grammar rules properly structured and white space ""taken care of"" without it getting in the way. In order to reduce boilerplate in your grammar definition parboiled allows for cleanly factoring out whitespace matching logic into a dedicated rule. By defining a custom implicit conversion from String to Rule0 you can implicitly match whitespace after a string terminal: class FooParser(val input: ParserInput) extends Parser {   implicit def wspStr(s: String): Rule0 = rule {     str(s) ~ zeroOrMore(' ')   }    def foo = rule { ""foobar"" | ""foo"" } // implicitly matches trailing blanks   def fooNoWSP = rule { str(""foobar"") | str(""foo"") } // doesn't match trailing blanks } In this example all usages of a plain string literals in the parser rules will implicitly match trailing space characters. In order to not apply the implicit whitespace matching in this case simply say str(""foo"") instead of just ""foo"". Parsing the whole Input If you don't explicitly match EOI (the special end-of-input pseudo-character) in your grammar's root rule the parser will not produce an error if, at the end of a parsing run, there is still unmatched input left. This means that if the root rule matches only a prefix of the whole input the parser will report a successful parsing run, which might not be what you want. As an example, consider this very basic parser: class MyParser(val input: ParserInput) extends Parser {   def InputLine = rule { ""foo"" | ""bar"" } }  new MyParser(""foo"").InputLine.run()  // Success new MyParser(""foot"").InputLine.run()  // also Success!! In the second run of the parser, instead of failing with a ParseError as you might expect, it successfully parses the matching input foo and ignores the rest of the input. If this is not what you want you need to explicitly match EOI, for example as follows: def InputLine = rule { (""foo"" | ""bar"") ~ EOI } Grammar Debugging TODO (e.g., use parse.formatError(error, showTraces = true)) Access to Parser Results In order to run the top-level parser rule against a given input you create a new instance of your parser class and call run() on it, e.g: val parser = new MyParser(input) val result = parser.rootRule.run() By default the type of result in this snippet will be a Try[T] whereby T depends on the type of rootRule: Type of rootRule Type of rootRule.run() Rule0 Try[Unit] Rule1[T] Try[T] RuleN[L <: HList] (otherwise) Try[L] The contents of the value stack at the end of the rootRule execution constitute the result of the parsing run. Note that run() is not available on rules that are not of type RuleN[L <: HList]. If the parser is not able to match the input successfully it creates an instance of class ParseError , which is defined like this case class ParseError(position: Position, charCount: Int, traces: Seq[RuleTrace]) extends RuntimeException In such cases the Try is completed with a scala.util.Failure holding the ParseError. If other exceptions occur during the parsing run (e.g. because some parser action failed) these will also end up as a Try failure. parboiled2 has quite powerful error reporting facilities, which should help you (and your users) to easily understand why a particular input does not conform to the defined grammar and how this can be fixed. The formatError method available on the Parser class is of great utility here, as it can ""pretty print"" a parse error instance, to display something like this (excerpt from the ErrorReportingSpec): Invalid input 'x', expected 'f', Digit, hex or UpperAlpha (line 1, column 4): abcx    ^  4 rules mismatched at error location:   targetRule / | / ""fgh"" / 'f'   targetRule / | / Digit   targetRule / | / hex   targetRule / | / UpperAlpha  Alternative DeliverySchemes Apart from delivering your parser results as a Try[T] parboiled2 allows you to select another one of the pre-defined Parser.DeliveryScheme alternatives, or even define your own. They differ in how they wrap the three possible outcomes of a parsing run: parsing completed successfully, deliver a result of type T parsing failed with a ParseError parsing failed due to another exception This table compares the built-in Parser.DeliveryScheme alternatives (the first one being the default): Import Type of rootRule.run() Success ParseError Other Exceptions import Parser.DeliveryScheme.Try Try[T] Success Failure Failure import Parser.DeliveryScheme.Either Either[ParseError, T] Right Left thrown import Parser.DeliveryScheme.Throw T T thrown thrown Running the Examples Follow these steps to run the example parsers defined here on your own machine: Clone the parboiled2 repository: git clone git://github.com/sirthias/parboiled2.git  Change into the base directory: cd parboiled2  Run SBT: sbt ""project examples"" run  Alternatives parboiled2 vs. parboiled 1.x TODO (about one order of magnitude faster, more powerful DSL, improved error reporting, fewer dependencies (more lightweight), but Scala 2.10.3+ only, no error recovery (yet) and no Java version (ever)) parboiled2 vs. Scala Parser Combinators TODO (several hundred times (!) faster, better error reporting, more concise and elegant DSL, similarly powerful in terms of language class capabilities, but Scala 2.10.3+ only, 2 added dependencies (parboiled2 + shapeless)) parboiled2 vs. Regular Expressions TODO (much easier to read and maintain, more powerful (e.g. regexes do not support recursive structures), faster, but Scala 2.10.3+ only, 2 added dependencies (parboiled2 + shapeless)) Roadmap TODO Contributing TODO Support In most cases the parboiled2 mailing list is probably the best place for your needs with regard to support, feedback and general discussion. Note: Your first post after signup is going to be moderated (for spam protection), but we'll immediately give you full posting privileges if your message doesn't unmask you as a spammer. You can also use the gitter.im chat channel for parboiled2: References TODO Credits Much of parboiled2 was developed by Alexander Myltsev during GSoc 2013, a big thank you for his great work! Also, without the Macro Paradise made available by Eugene Burmako parboiled2 would probably still not be ready and its codebase would look a lot more messy. License parboiled2 is released under the Apache License 2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sirthias/parboiled2"	"A Fast Parser Generator for Scala 2.10.3+."	"true"
"Parsing"	"Scala Parser Combinators ★ 130 ⧗ 3"	"https://github.com/scala/scala-parser-combinators"	"Scala Standard Parser Combinator Library."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"156"	"32"	"50"	"GitHub - scala/scala-parser-combinators: Scala Standard Parser Combinator Library Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 32 Star 156 Fork 50 scala/scala-parser-combinators Code Issues 10 Pull requests 0 Pulse Graphs Scala Standard Parser Combinator Library 290 commits 2 branches 12 releases 26 contributors Scala 98.1% Shell 1.9% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 1.0.x Switch branches/tags Branches Tags 1.0.x master Nothing to show v1.0.4 v1.0.3 v1.0.3-RC2 v1.0.3-RC1 v1.0.2 v1.0.1 v1.0.0 v1.0.0-RC5 v1.0.0-RC4 v1.0.0-RC3 v1.0-RC2 v1.0-RC1 Nothing to show New pull request Latest commit 4d9cb2c Feb 13, 2016 gourlaysama Merge pull request #79 from gourlaysama/bump2 … bump scala, sbt and scala-module-plugin versions Permalink Failed to load latest commit information. admin cross-build with 2.11 and 2.12 May 11, 2015 project bump scala, sbt and scala-module-plugin versions Feb 12, 2016 src Fixes OffsetPosition.lineContents so that it doesn't grab a newline a… Feb 10, 2016 .gitignore SBT 0.13 build; license; groupId = org.scala-lang.modules Aug 15, 2013 .mailmap SBT 0.13 build; license; groupId = org.scala-lang.modules Aug 16, 2013 .travis.yml bump scala, sbt and scala-module-plugin versions Feb 12, 2016 LICENSE.md SBT 0.13 build; license; groupId = org.scala-lang.modules Aug 16, 2013 README.md [backport] add maintenance status on README, remove link to JIRA Sep 9, 2015 build.sbt bump scala, sbt and scala-module-plugin versions Feb 12, 2016 README.md scala-parser-combinators Scala Standard Parser Combinator Library This library is now community-maintained. If you are interested in helping please contact @gourlaysama or mention it on Gitter. As of Scala 2.11, this library is a separate jar that can be omitted from Scala projects that do not use Parser Combinators. Documentation Latest version Previous versions (included in the API docs for the Scala library until Scala 2.11) Adding an SBT dependency To depend on scala-parser-combinators in SBT, add something like this to your build.sbt: libraryDependencies += ""org.scala-lang.modules"" %% ""scala-parser-combinators"" % ""1.0.4""  (Assuming you're using a scalaVersion for which a scala-parser-combinators is published. The first 2.11 milestone for which this is true is 2.11.0-M4.) To support multiple Scala versions, see the example in https://github.com/scala/scala-module-dependency-sample. Contributing See the Scala Developer Guidelines for general contributing guidelines Have a look at existing issues Ask questions and discuss on Gitter Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala/scala-parser-combinators"	"Scala Standard Parser Combinator Library."	"true"
"Sbt plugins"	"pttrt ★ 2 ⧗ 120"	"https://github.com/Atry/pttrt"	"A sbt plugin, designed to pass data from compile-time to run-time."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"3"	"1"	"1"	"GitHub - Atry/pttrt: Pass Them To Run-Time Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 3 Fork 1 Atry/pttrt Code Issues 0 Pull requests 0 Pulse Graphs Pass Them To Run-Time 30 commits 1 branch 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 0.1.0 Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. pttrt-test pttrt .gitignore LICENSE NOTICE README.md README.md Pttrt Pttrt (/ˈpɪtrɪt/, or Pass Them To Run-time) is a sbt plugin, designed to pass data from compile-time to run-time. Motive Many times, my programs need some information, which is generated at compile-time and used at run-time. For example, Subversion's revision, JARs's dependencies, configuration files, and poor man's macro ;-), etc. It is really heavyweight to deal with these situation in sbt. You need to create a plugin for each type of files you generated, to define many SettingKeys in each plugin's .scala files, and to set values for each of these SettingKeys in your .sbt files. There are some plugins, like xsbt-reflect, also able to generate some infomation for run-time. But they cannot fit every variant cases I met. Another plugin, sbt-buildinfo has similar feature as Pttrt, but sbt-buildinfo cannot generate multiply files, preventing itself to be used for code generation from multiply separate plugins. I just want a lightweight and general-purpose way like Makefile: generated.properties:     echo my.base.dir=$(PWD) > $@  That's why I created Pttrt. Usage Step 1: Install Pttrt into your project Add the following line to your project/plugins.sbt: addSbtPlugin(""com.dongxiguo"" % ""pttrt"" % ""0.1.2"")  And add pttrtSettings to your build.sbt: pttrtSettings  Step 2: Export the data that will be passed to run-time For example, if you want to know the building version on run-time, you need to add following lines at build.sbt: pttrtSettings  version := ""1.2.3-SNAPSHOT""  PttrtKeys.pttrtData <+= version map { v =>   ""org.yourHost.yourProject.YourSingleton"" -> Map(""Version"" -> TypedExpression(v)) }  Step 3: Access the data on run-time Create PttrtExample.scala: object PttrtExample {   def main(args: Array[String]) {     println(""Building version is "" + org.yourHost.yourProject.YourSingleton.Version)   } }  Step 4: Run it! $ sbt > run-main PttrtExample  You will see: Building version is 1.2.3-SNAPSHOT  See https://github.com/Atry/pttrt/tree/master/pttrt-test for more example. Addition requirement Pttrt is for sbt 0.12 Any value being passed to run-time must be a primary type (Int, Double, Boolean, etc) or java.io.Serializable The value's type must be found in the classpath for both compile-time and run-time. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Atry/pttrt"	"A sbt plugin, designed to pass data from compile-time to run-time."	"true"
"Sbt plugins"	"sbt-api-mappings ★ 29 ⧗ 35"	"https://github.com/ThoughtWorksInc/sbt-api-mappings"	"A Sbt plugin that resolves external API links to common Scala libraries."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"30"	"23"	"2"	"GitHub - ThoughtWorksInc/sbt-api-mappings: A Sbt plugin that fills apiMappings for common Scala libraries. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 23 Star 30 Fork 2 ThoughtWorksInc/sbt-api-mappings Code Issues 0 Pull requests 0 Pulse Graphs A Sbt plugin that fills apiMappings for common Scala libraries. 56 commits 1 branch 5 releases Fetching contributors Scala 90.3% Shell 9.7% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.2.3 v0.2.2 v0.2.1 v0.2.0 v0.1.0 Nothing to show New pull request Latest commit 767d886 May 20, 2016 travis@localhost Setting version to 0.2.4-SNAPSHOT Permalink Failed to load latest commit information. project Upgrade sbt Apr 1, 2016 src/main/scala/com/thoughtworks/sbtApiMappings Support Apache Spark May 20, 2016 .gitignore Merge branch 'ci' of https://github.com/Atry/ci-template-for-sbt-libr… Apr 1, 2016 .travis.yml Run Travis CI on Docker Apr 1, 2016 LICENSE Initial commit Jul 11, 2015 NOTICE Initial commit Jul 10, 2015 README.md Update README.md Apr 15, 2016 build.sbt Merge branch 'ci' of https://github.com/Atry/ci-template-for-sbt-libr… Apr 1, 2016 ci.sbt Setup CI for deployment Feb 1, 2016 deploy.sh.disabled Setting version to 0.2.3 May 20, 2016 pubring.asc Setup CI for deployment Jan 31, 2016 version.sbt Setting version to 0.2.4-SNAPSHOT May 20, 2016 README.md sbt-api-mappings sbt-api-mappings is a Sbt plugin that fills apiMappings for common Scala libraries. Motivation Sometimes when you wrote ScalaDoc for your own classes, you may want to reference to documentation in some other libraries. For example: /**  * My own class, which works with [[scala.Option]] and [[scalaz.Monad]].  */ class MyClass(optionMonad: scalaz.Monad[Option]) Unfortunately when use run doc command in Sbt, you will receive a warning and the link would not be created. /path/to/MyClass.scala:3: Could not find any member to link for ""scala.Option"".  This plugin resolves the problem. Usage Step 1: Add the following lines in your project/plugins.sbt: addSbtPlugin(""com.thoughtworks.sbt-api-mappings"" % ""sbt-api-mappings"" % ""latest.release"") Step 2: Reload the Sbt configuration: > reload  Step 3: Generate your API documentation: > doc  Now, open the API documentation in your browser, and you will find the links to the scala.Option and scalaz.Monad's documentation in your MyClass page. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ThoughtWorksInc/sbt-api-mappings"	"A Sbt plugin that resolves external API links to common Scala libraries."	"true"
"Sbt plugins"	"sbt-buildinfo ★ 201 ⧗ 9"	"https://github.com/sbt/sbt-buildinfo"	"Generates Scala source from build definition."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"220"	"23"	"43"	"GitHub - sbt/sbt-buildinfo: I know this because build.sbt knows this. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 23 Star 220 Fork 43 sbt/sbt-buildinfo Code Issues 8 Pull requests 0 Pulse Graphs I know this because build.sbt knows this. 171 commits 2 branches 15 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master topic/sbt1.0.0-M4 Nothing to show v0.6.1 v0.6.0 0.5.0 0.4.0 0.3.2 0.3.1 0.3.0 0.2.5 0.2.4 0.2.3 0.2.2 0.2.1 0.2.0 0.1.1 0.1.0 Nothing to show New pull request Latest commit 7b8675d Jun 9, 2016 eed3si9n Merge pull request #94 from alexeyr/patch-1 … If generating resources, put them into resourceManaged Permalink Failed to load latest commit information. notes Adds `buildInfoValues` task, which exposes the set of `BuildInfoResul… May 2, 2016 project Updated repos for ls-sbt plugin. May 30, 2016 src If generating resources, put them into resourceManaged Jun 9, 2016 .gitignore add .gitignore. make BuildInfo private. remove unnecessary local vari… Aug 14, 2012 .travis.yml Add .travis.yml for continuous integration Jan 27, 2015 LICENSE MIT license Mar 3, 2012 Migration.md Migration guide Mar 22, 2015 README.markdown Clarified flexibility of `BuildInfoKey.map`. Jun 1, 2016 bintray.sbt publish to bintray Apr 11, 2015 build.sbt 0.6.1 Mar 13, 2016 scripted.sbt fix extra quotes #24 sbt 0.13 Nov 28, 2013 README.markdown sbt-buildinfo I know this because build.sbt knows this. sbt-buildinfo generates Scala source from your build definitions. Latest Stable For sbt 0.13.6+ add sbt-buildinfo as a dependency in project/buildinfo.sbt: addSbtPlugin(""com.eed3si9n"" % ""sbt-buildinfo"" % ""0.6.1"") For sbt 0.13.(x < 6), see 0.3.2. For sbt 0.12, see 0.2.5. Usage Add the following in your build.sbt: lazy val root = (project in file(""."")).   enablePlugins(BuildInfoPlugin).   settings(     buildInfoKeys := Seq[BuildInfoKey](name, version, scalaVersion, sbtVersion),     buildInfoPackage := ""hello""   ) When you reload the settings and compile, this generates the following: package hello  import java.io.File import java.lang._ import java.net.URL import scala._; import Predef._  /** This object was generated by sbt-buildinfo. */ case object BuildInfo {   /** The value is ""helloworld"". */   val name: String = ""helloworld""   /** The value is ""0.1-SNAPSHOT"". */   val version: String = ""0.1-SNAPSHOT""   /** The value is ""2.10.3"". */   val scalaVersion: String = ""2.10.3""   /** The value is ""0.13.2"". */   val sbtVersion: String = ""0.13.2""   override val toString: String = ""name: %s, version: %s, scalaVersion: %s, sbtVersion: %s"" format (name, version, scalaVersion, sbtVersion) } As this is generated source it will be found under target, specifically for a Scala 2.11 project under target/scala-2.11/src_managed/main/sbt-buildinfo Customize buildInfoKeys by adding whatever keys you want to have in BuildInfo. You can use BuildInfoKey.map to change the generated field name and value, add new fields with tuples, or add new fields with values computed at build-time. Note: BuildInfoKey.map can can handle both SettingKey[T] and TaskKey[T] types as arguments: buildInfoKeys ++= Seq[BuildInfoKey](   resolvers,   libraryDependencies in Test,   BuildInfoKey.map(name) { case (k, v) => ""project"" + k.capitalize -> v.capitalize },   ""custom"" -> 1234, // computed at project load time   BuildInfoKey.action(""buildTime"") {     System.currentTimeMillis   } // re-computed each time at compile ) This generates:   /** The value is Seq(""Sonatype Public: https://oss.sonatype.org/content/groups/public""). */   val resolvers: Seq[String] = Seq(""Sonatype Public: https://oss.sonatype.org/content/groups/public"")   /** The value is Seq(""org.scala-lang:scala-library:2.9.1"", ...). */   val test_libraryDependencies: Seq[String] = Seq(""org.scala-lang:scala-library:2.9.1"", ...)   /** The value is ""Helloworld"". */   val projectName = ""Helloworld""   /** The value is 1234. */   val custom = 1234   /** The value is 1346906092160L. */   val buildTime = 1346906092160L Tasks can be added only if they do not depend on sourceGenerators. Otherwise, it will cause an infinite loop. Here's how to change the generated object name: buildInfoObject := ""Info"" This changes the generated object name to object Info. Changing the object name is optional, but to avoid name clash with other jars, package name should be unique. Use buildInfoPackage key for this. buildInfoPackage := ""hello"" build number A build number can be generated as follows. Note that cross building against multiple Scala would each generate a new number. buildInfoKeys += buildInfoBuildNumber BuildInfoOption.ToMap Add the following option buildInfoOptions += BuildInfoOption.ToMap to generate toMap method: val toMap = Map[String, Any](   ""name"" -> name,   ""version"" -> version,   ""scalaVersion"" -> scalaVersion,   ""sbtVersion"" -> sbtVersion) BuildInfoOption.ToJson Add the following option buildInfoOptions += BuildInfoOption.ToJson to generate toJson method. BuildInfoOption.Traits Add the following option buildInfoOptions += BuildInfoOption.Traits(""TestTrait1"", ""TestTrait2"") to mixin traits to the generated object. BuildInfoOption.BuildTime Add the following option buildInfoOptions += BuildInfoOption.BuildTime to add timestamp values: /** The value is ""2015-07-30 03:30:16.849"". */ val builtAtString: String = ""2015-07-30 03:30:16.849"" /** The value is 1438227016849L. */ val builtAtMillis: Long = 1438227016849L Eclipse support If you use the sbteclipse plugin to generate projects for Eclipse, you need to tell sbteclipse that the generated BuildInfo.scala is a managed source, i.e., a generated source file. To do so, you can configure sbteclipse as follows: EclipseKeys.createSrc := EclipseCreateSrc.Default + EclipseCreateSrc.Managed This is explained in more detail in the sbtecliipse documentation. License MIT License Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sbt/sbt-buildinfo"	"Generates Scala source from build definition."	"true"
"Sbt plugins"	"sbt-classfinder ★ 2 ⧗ 113"	"https://github.com/ruippeixotog/sbt-classfinder"	"Retrieves runtime information about the classes and traits in a project."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"3"	"2"	"0"	"GitHub - ruippeixotog/sbt-classfinder: SBT plugin for retrieving runtime information about the classes and traits in a project Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 3 Fork 0 ruippeixotog/sbt-classfinder Code Issues 0 Pull requests 0 Pulse Graphs SBT plugin for retrieving runtime information about the classes and traits in a project 6 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. src/main/scala/net/ruippeixotog/sbt/classfinder .gitignore LICENSE README.md build.sbt README.md sbt-classfinder sbt-classfinder is a SBT plugin for retrieving runtime information about the classes and traits in a project and searching for specific classes according to some criteria, such as their superclasses or annotations. It allows one to modify the behavior and dependencies of other post-compile SBT tasks according to the compiled code or the classpath. Quick Start To use sbt-classfinder in an existing SBT project using SBT 0.13.5+, add the following dependency to your project/plugins.sbt: addSbtPlugin(""net.ruippeixotog"" % ""sbt-classfinder"" % ""0.1.1"") Tasks classFinderClasspath: determines the classpath to be searched. The scope of the classpath depends on the value of the classFinderScope setting; classFinder: returns a ClassFinder instance containing several utility methods for listing and finding classes in the specified classpath; allClassesInfo: an utility task for retrieving a stream with the information of all classes. Settings classFinderScope: defines what should and should not be searched in the build classpath. One of: Config: search the build products of the scoped SBT configuration, e.g. Compile classes for compile:classFinderScope and Test classes for test:classFinderScope; Build: search the build products of the whole SBT build, including other projects in multi-project builds; BuildAndDeps: search the full classpath, including external dependencies. Examples Run the main class marked with the annotation QuickRun: // build.sbt lazy val markedMain = TaskKey[String](""markedMain"")  lazy val runMarked = TaskKey[Unit](""runMarked"")  markedMain := {   val className = (classFinder in Compile).value.classesAnnotatedWith(""QuickRun"").head.name   if (className.endsWith(""$"")) className.dropRight(1) else className }  runMarked <<= std.FullInstance.flatten {   markedMain.map { mMain => (runMain in Compile).toTask("" "" + mMain) } } Make test run also subclasses of MyTest using a custom test executor: // build.sbt lazy val myTests = TaskKey[Seq[String]](""myTests"")  def myTestExecutor(testClass: String) = ???  myTests := (classFinder in Test).value.subtypesOf(""MyTest"").map(_.name).toSeq  test <<= myTests.map { testSeq => testSeq.map(myTestExecutor) } Copyright Copyright (c) 2015 Rui Gonçalves. See LICENSE for details. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ruippeixotog/sbt-classfinder"	"Retrieves runtime information about the classes and traits in a project."	"true"
"Sbt plugins"	"sbt-cppp ★ 3 ⧗ 16"	"https://github.com/Atry/sbt-cppp"	"A sbt plugin to support, especially in multi-project builds."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"3"	"3"	"3"	"GitHub - Atry/sbt-cppp: Cross-Project Protobuf Plugin for Sbt Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 3 Fork 3 Atry/sbt-cppp Code Issues 0 Pull requests 0 Pulse Graphs Cross-Project Protobuf Plugin for Sbt 32 commits 2 branches 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master patch-1 Nothing to show 0.1.2 Nothing to show New pull request Latest commit 7a0c889 Aug 5, 2015 Atry Update README.md Permalink Failed to load latest commit information. project Upgraded sbt version to 0.13.1 Jan 22, 2014 src/main/scala/com/dongxiguo/sbtCppp Renamed to internal class name to cppp. Jan 22, 2014 .gitignore Ignore only target directory. Jan 16, 2014 LICENSE Initial commit Jan 16, 2014 NOTICE Initial commit Jan 16, 2014 README.md Update README.md Aug 5, 2015 build.sbt Bump version. Jan 22, 2014 README.md sbt-cppp sbt-cppp (Sbt Cross-Project Protobuf Plugin) is a Sbt plugin to support Protocol Buffers, especially in multi-project builds. Features sbt-cppp compiles *.proto into .java files. In addition, sbt-cppp provides some features missed in sbt-protobuf or other protobuf plugins: Jar packaging from .proto files. Cross-project protoc include path dependency management in multi-project builds. Cross-library protoc include path dependency management by auto-unzipping .proto files from jar packages. Support for custom code generator to .proto files. Usage Step 1: Install sbt-cppp into your project Add the following line to your project/plugins.sbt: addSbtPlugin(""com.dongxiguo"" % ""sbt-cppp"" % ""0.1.4"")  And add protobufSettings and protobuf-java dependency to your build.sbt: protobufSettings  libraryDependencies += ""com.google.protobuf"" % ""protobuf-java"" % ""2.5.0""  Step 2: Install protoc into $PATH For windows, download at http://code.google.com/p/protobuf/downloads/detail?name=protoc-2.5.0-win32.zip. For most linux distributions, look for protobuf-compiler package. Step 3: Create your .proto files. Create src/protobuf/sample_proto.proto message SampleMessage {   optional int32 sample_field = 1; }  Step 4: Use the .proto files in your source files. Create src/main/scala/SampleMain.scala: object SampleMain {   def main(args: Array[String]) {     println(SampleMessage.newBuilder.setSampleField(123).build)   } }  Step 5: Run it! $ sbt > run-main SampleMain  Further information sbt-cppp is for sbt 0.12 or 0.13 If project-foo depends on project-bar, project-bar/src/protobuf/ will be added as a protoc include path when the plugin converts project-foo/src/protobuf/*.proto into .java files. If you want to generate .proto files by some tools (instead of creating them manually), put sourceGenerators in Protobuf += yourGenerator in your build.sbt. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Atry/sbt-cppp"	"A sbt plugin to support, especially in multi-project builds."	"true"
"Sbt plugins"	"Protocol Buffers"	"https://github.com/google/protobuf"	"A sbt plugin to support, especially in multi-project builds."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"10090"	"1052"	"3051"	"GitHub - google/protobuf: Protocol Buffers - Google's data interchange format Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1,052 Star 10,090 Fork 3,051 google/protobuf Code Issues 346 Pull requests 98 Wiki Pulse Graphs Protocol Buffers - Google's data interchange format https://developers.google.com/protocol-buffers/ 3,659 commits 12 branches 20 releases 169 contributors C++ 40.4% Java 17.2% Objective-C 12.1% C# 9.4% Protocol Buffer 7.2% C 5.2% Other 8.5% C++ Java Objective-C C# Protocol Buffer C Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 3.0.0-beta-3 3.0.0-beta-4 3.0.0-pre benchmarks gh-pages integrate_base integrate javalite master objc-framework-fix package-json rubypackagecap Nothing to show v3.0.0-beta-3.3 v3.0.0-beta-3.2 v3.0.0-beta-3.1 v3.0.0-beta-3 v3.0.0-beta-3-pre-1 v3.0.0-beta-2 v3.0.0-beta-1.1 v3.0.0-beta-1 v3.0.0-beta-1-bzl-fix v3.0.0-alpha-4.1 v3.0.0-alpha-4 v3.0.0-alpha-3.1 v3.0.0-alpha-3 v3.0.0-alpha-2 v3.0.0-alpha-1 v2.6.1 v2.6.1rc1 v2.6.0 v2.5.0 v2.4.1 Nothing to show New pull request Latest commit b99577c Jul 15, 2016 sergiocampama committed with thomasvl Exposes the currently registered extensions for a message and removes… … … the internal sortedExtensionsInUse Permalink Failed to load latest commit information. benchmarks Add the missing maintiner-clean entry for benchmarks May 6, 2016 cmake Fix a bad variable dereference causing <package>_FIND_VERSION_PRERELE… Jul 12, 2016 conformance Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 csharp Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 docs Add https://github.com/os72/protobuf-dynamic May 31, 2016 editors down-integrate internal changes May 21, 2015 examples Use ExternalProject_Add to build the examples in a stand-alone fashion. Jun 6, 2016 java Fix spelling in strings and comments Jul 3, 2016 javanano Fix spelling in strings and comments Jul 3, 2016 jenkins Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 js Fixed failing JS tests Jul 6, 2016 m4 Merge pull request #789 from motahan/solaris64_fix Jan 20, 2016 more_tests Add makefile for extended tests to be run before release. Jan 12, 2010 objectivec Exposes the currently registered extensions for a message and removes… Jul 15, 2016 php Fix spelling in strings and comments Jul 3, 2016 protoc-artifacts Merge pull request #1559 from google/beta-3 May 18, 2016 python Fix spelling in strings and comments Jul 3, 2016 ruby Fix spelling in strings and comments Jul 3, 2016 src Merge pull request #1735 from jskeet/attribute-placement Jul 7, 2016 util/python Remove hack for building Python support with Bazel. Feb 25, 2016 .gitignore Expand the OS X/Xcode gitignores May 19, 2016 .travis.yml Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 BUILD Bazel build: Keep generated sources and Python runtime in the same di… May 25, 2016 CHANGES.txt Update changes for lite May 16, 2016 CONTRIBUTORS.txt Add nano proto authors and update LICENSE file to include Android.mk. Nov 20, 2014 LICENSE Add support for POWER Linux Nov 3, 2015 Makefile.am Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 Protobuf.podspec Don't #import the .m files. Jul 7, 2016 README.md Fix typos in README.md Jan 25, 2016 WORKSPACE add java/util support based on java/util/pom.xml Mar 11, 2016 appveyor.bat Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 appveyor.yml Move to dotnet cli for building, and .NET Core (netstandard1.0) as ta… Jul 14, 2016 autogen.sh Add support for arguments. Oct 13, 2015 configure.ac Update the list of places where the version is stored. May 28, 2016 generate_descriptor_proto.sh Merge branch 'master' of github.com:google/protobuf Mar 30, 2016 gmock.BUILD Fix headers for gmock.BUILD Dec 1, 2015 post_process_dist.sh Add js to post_process_dist.sh. Dec 28, 2015 protobuf-lite.pc.in Uncomment conflict fields from pkg-config files. May 12, 2015 protobuf.bzl Bazel build: Keep generated sources and Python runtime in the same di… May 26, 2016 protobuf.pc.in Uncomment conflict fields from pkg-config files. May 12, 2015 six.BUILD Add srcs_version = ""PY2AND3"" in BUILD files Dec 3, 2015 tests.sh Uses head version of rvm to avoid shell_update_session not found error ( Jul 15, 2016 update_file_lists.sh Fix bugs on windows Aug 25, 2015 README.md Protocol Buffers - Google's data interchange format Copyright 2008 Google Inc. https://developers.google.com/protocol-buffers/ Overview Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find protobuf's documentation on the Google Developers site. This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language. Protocol Compiler Installation The protocol compiler is written in C++. If you are using C++, please follow the C++ Installation Instructions to install protoc along with the C++ runtime. For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page: https://github.com/google/protobuf/releases In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf. If you are looking for an old version that is not available in the release page, check out the maven repo here: http://repo1.maven.org/maven2/com/google/protobuf/protoc/ These pre-built binaries are only provided for released versions. If you want to use the github master version at HEAD, or you need to modify protobuf code, or you are using C++, it's recommended to build your own protoc binary from source. If you would like to build protoc binary from source, see the C++ Installation Instructions. Protobuf Runtime Installation Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language: Language Source C++ (include C++ runtime and protoc) src Java java Python python Objective-C objectivec C# csharp JavaNano javanano JavaScript js Ruby ruby Go golang/protobuf PHP TBD Usage The complete documentation for Protocol Buffers is available via the web at: https://developers.google.com/protocol-buffers/  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/google/protobuf"	"A sbt plugin to support, especially in multi-project builds."	"true"
"Sbt plugins"	"sbt-dependency-graph ★ 518 ⧗ 2"	"https://github.com/jrudolph/sbt-dependency-graph"	"Create a dependency graph for your project."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"572"	"28"	"58"	"GitHub - jrudolph/sbt-dependency-graph: sbt plugin to create a dependency graph for your project Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 28 Star 572 Fork 58 jrudolph/sbt-dependency-graph Code Issues 21 Pull requests 7 Pulse Graphs sbt plugin to create a dependency graph for your project 203 commits 6 branches 17 releases 14 contributors Scala 94.0% HTML 6.0% Scala HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.7 0.8 master sbt-0.12 sbt-0.13 w/sbt-1.0.0-M4-support Nothing to show v0.8.2 v0.8.1 v0.8.0 v0.8.0-beta1 v0.7.5 v0.7.4 v0.7.4-sbt-0.13 v0.7.3 v0.7.2 v0.7.0 v0.7.0-RC3 v0.7.0-RC2 v0.7.0-RC1 v0.6.0 v0.5.2 v0.5.1 v0.5 Nothing to show New pull request Latest commit 546e8e2 Jun 30, 2016 jrudolph committed on GitHub Merge pull request #108 from thetristan/generate-it-report … Generate reports for projects using IntegrationTest config Permalink Failed to load latest commit information. notes prepare 0.8.2 Feb 1, 2016 project update build to 0.13.8 Mar 30, 2015 src Return empty graph if no update report is found Jun 26, 2016 .gitignore initial version Nov 15, 2011 LICENSE readme and licensing Nov 15, 2011 NOTICE bump version to 0.8.0 and update README to reflect changes Nov 25, 2015 PUBLISHING update README to version 0.7.2 Mar 2, 2013 README.md Fix link to Compatibility notes Feb 16, 2016 build.sbt make it compile again with sbt 0.13.6 Nov 26, 2015 project.sbt next version Feb 1, 2016 publish.sbt remove credentials declaration Oct 24, 2012 README.md sbt-dependency-graph Visualize your project's dependencies. Preliminaries The plugin works best with sbt >= 0.13.6. See the compatibility notes to use this plugin with an older version of sbt. Usage Instructions Since sbt-dependency-graph is an informational tool rather than one that changes your build, you will more than likely wish to install it as a global plugin so that you can use it in any SBT project without the need to explicitly add it to each one. To do this, add the plugin dependency to ~/.sbt/0.13/plugins/plugins.sbt: addSbtPlugin(""net.virtual-void"" % ""sbt-dependency-graph"" % ""0.8.2"") To add the plugin only to a single project, put this line into project/plugins.sbt of your project, instead. This plugin is an auto-plugin which will be automatically enabled starting from sbt 0.13.5. Main Tasks dependencyTree: Shows an ASCII tree representation of the project's dependencies dependencyBrowseGraph: Opens a browser window with a visualization of the dependency graph (courtesy of graphlib-dot + dagre-d3). dependencyGraph: Shows an ASCII graph of the project's dependencies on the sbt console dependencyList: Shows a flat list of all transitive dependencies on the sbt console (sorted by organization and name) whatDependsOn <organization> <module> <revision>: Find out what depends on an artifact. Shows a reverse dependency tree for the selected module. dependencyLicenseInfo: show dependencies grouped by declared license dependencyStats: Shows a table with each module a row with (transitive) Jar sizes and number of dependencies dependencyGraphMl: Generates a .graphml file with the project's dependencies to target/dependencies-<config>.graphml. Use e.g. yEd to format the graph to your needs. dependencyDot: Generates a .dot file with the project's dependencies to target/dependencies-<config>.dot. Use graphviz to render it to your preferred graphic format. ivyReport: let's ivy generate the resolution report for you project. Use show ivyReport for the filename of the generated report All tasks can be scoped to a configuration to get the report for a specific configuration. test:dependencyGraph, for example, prints the dependencies in the test configuration. If you don't specify any configuration, compile is assumed as usual. Configuration settings filterScalaLibrary: Defines if the scala library should be excluded from the output of the dependency-* functions. If true, instead of showing the dependency ""[S]"" is appended to the artifact name. Set to false if you want the scala-library dependency to appear in the output. (default: true) dependencyGraphMLFile: a setting which allows configuring the output path of dependency-graph-ml. dependencyDotFile: a setting which allows configuring the output path of dependency-dot. dependencyDotHeader: a setting to customize the header of the dot file (e.g. to set your preferred node shapes). dependencyDotNodeLabel: defines the format of a node label (default set to [organisation]<BR/><B>[name]</B><BR/>[version]) E.g. in build.sbt you can change configuration settings like this: filterScalaLibrary := false // include scala library in output  dependencyDotFile := file(""dependencies.dot"") //render dot file to `./dependencies.dot` Known issues #19: There's an unfixed bug with graph generation for particular layouts. Workaround: Use dependency-tree instead of dependency-graph. #39: When using sbt-dependency-graph with sbt < 0.13.6. Compatibility notes sbt < 0.13.6: The plugin will fall back on the old ivy report XML backend which suffers from #39. sbt < 0.13.5: Old versions of sbt have no AutoPlugin support, you need to add net.virtualvoid.sbt.graph.DependencyGraphSettings.graphSettings to your build.sbt or (~/.sbt/0.13/user.sbt for global configuration) to enable the plugin. sbt <= 0.12.x: Old versions of sbt are not actively supported any more. Please use the old version from the 0.7 branch. License Published under the Apache License 2.0. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/jrudolph/sbt-dependency-graph"	"Create a dependency graph for your project."	"true"
"Sbt plugins"	"sbt-groll ★ 71 ⧗ 15"	"https://github.com/sbt/sbt-groll"	"sbt plugin to roll the Git history."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"79"	"7"	"4"	"GitHub - sbt/sbt-groll: sbt plugin to roll the Git history Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 7 Star 79 Fork 4 sbt/sbt-groll Code Issues 1 Pull requests 0 Pulse Graphs sbt plugin to roll the Git history 104 commits 1 branch 40 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v4.9.0 v4.8.3 v4.8.2 v4.8.1 v4.8.0 v4.7.0 v4.6.0 v4.5.0 v4.4.0 v4.3.0 v4.2.0 v4.1.0 v4.0.0 v3.2.1 v3.2.0 v3.1.4 v3.1.3 v3.1.2 v3.1.1 v3.1.0 v3.0.1 v3.0.0 v2.3.6 v2.3.1 v2.3.0 v2.2.0 v2.1.0 v2.0.0 v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.1 v1.1.0 v1.0.0 v0.4.0 v0.3.0 v0.2.0 v0.1.0 Nothing to show New pull request Latest commit caacb98 Feb 25, 2016 hseeberger Prepare release 4.9.0 Permalink Failed to load latest commit information. project Upgrade to sbt 0.13.11 Feb 25, 2016 src Merge pull request #18 from diversit/feature/help Nov 3, 2015 .gitignore Upgrade to sbt 0.13.8; also upgrade various plugins Mar 24, 2015 LICENSE Prepare release 4.3.0 Dec 3, 2014 NOTICE Upgrade to sbt 0.13.8; also upgrade various plugins Mar 24, 2015 README.md Prepare release 4.9.0 Feb 25, 2016 build.sbt Upgrade jgit to latest version Dec 18, 2015 shell-prompt.sbt Upgrade to sbt 0.13.8; also upgrade various plugins Mar 24, 2015 README.md sbt-groll Plugin for sbt to ""roll"" – view and navigate – the Git commit history. This turns out to be very useful for live coding and training sessions. As of version 2 sbt-groll is using JGit, i.e. no local Git installation is needed. Installing sbt-groll sbt-groll is a plugin for sbt 0.13.11 or higher. In order to install sbt, please refer to the sbt documentation. Please make sure that you are using a suitable version of sbt: As sbt-groll is a plugin for sbt, it is installed like any other sbt plugin, that is by mere configuration: just add sbt-groll to your global or local plugin definition. Global plugins are defined in ~/.sbt/<sbt_version>/plugins/plugins.sbt and local plugins are defined in project/plugins.sbt in your project. In order to add sbt-groll as a plugin, just add the below setting to the relevant plugin definition: addSbtPlugin(""de.heikoseeberger"" % ""sbt-groll"" % ""4.9.0"")  After adding the sbt-groll plugin like this, you should either start sbt or, if it was already started, reload the current session by executing the reload command. If everything worked, you should have the new groll command available. Using sbt-groll sbt-groll adds the groll command that provides various ways to view and navigate the Git history. Of course this means, that you can only use sbt-groll for projects which are using Git as version control system. If you navigate the Git history, i.e. move to some commit, sbt-groll reloads the sbt session if the build definition changes, i.e. any .sbt file in the project root directory or any .scala or .sbt file in the project/ directory. In order to use sbt-groll, just execute groll <arg_or_opt> in an sbt session, giving one of the arguments or options described below. Here are two examples: > groll show [info] == 0bf1f60 Exercise: Connect to a remote system> groll move=bc1ac93 > groll head [info] >> b97ef22 Exercise: HTTP server  Settings sbt-groll can be configured by the following settings: grollConfigFile: java.io.File – the configuration file for sbt-groll; ""~/.sbt-groll.conf"" by default grollHistoryRef: String – the ref (commit id, branch or tag) used for the Git history; ""master"" by default grollWorkingBranch: String – the working branch used by sbt-groll; ""groll"" by default Arguments/options The groll command must be followed by one of the following arguments or options: show – shows the current commit id and message, if current commit is in history list – shows the full commit history next – moves to the next commit prev – moves to the previous commit head – moves to the head of the commit history initial – moves to a commit with a message containing ""groll:initial"" or starting with ""Initial state"" or with a tag ""groll-initial"" move=<commit> – moves to the given commit push=<branch> – pushes the current commit via HTTPS to the ""origin-https"" remote repository (needs to be defined!) under the given branch version – shows the version of sbt-groll help - shows this help info. Contribution policy Contributions via GitHub pull requests are gladly accepted from their original author. Along with any pull requests, please state that the contribution is your original work and that you license the work to the project under the project's open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project's open source license and warrant that you have the legal authority to do so. License This code is open source software licensed under the Apache 2.0 License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sbt/sbt-groll"	"sbt plugin to roll the Git history."	"true"
"Sbt plugins"	"sbt-haxe ★ 7 ⧗ 57"	"https://github.com/qifun/sbt-haxe"	"A Sbt plugin to compile Haxe sources."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"7"	"11"	"4"	"GitHub - qifun/sbt-haxe Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 11 Star 7 Fork 4 qifun/sbt-haxe Code Issues 8 Pull requests 0 Pulse Graphs No description or website provided. 223 commits 1 branch 7 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 1.3.0 1.2.2 1.2.1 1.2.0 1.1.2 1.1.1 1.1.0 Nothing to show New pull request Latest commit 4fdef38 Sep 19, 2015 chank Merge pull request #68 from Atry/patch-2 … Create AllHaxePlugins.scala which enabled all the haxe plugins Permalink Failed to load latest commit information. project src/main/scala/com/qifun/sbtHaxe .gitignore .travis.yml LICENSE NOTICE README.md build.sbt version.sbt README.md sbt-haxe sbt-haxe is a Sbt plugin to compile Haxe sources in Java/Scala projects. Usage Step 1: Install sbt-haxe into your project Add the following line to your project/plugins.sbt: addSbtPlugin(""com.qifun"" % ""sbt-haxe"" % ""1.4.1"")  Step 2: Put your Haxe sources at src/haxe/yourPackage/YourHaxeClass.hx package yourPackage; import haxe.ds.Vector; class YourHaxeClass {   public static function main(args:Vector<String>)   {     trace(""Hello, World!"");   } } Step 3: Run it! $ sbt run [info] Loading global plugins from C:\Users\user\.sbt\0.13\plugins [info] Loading project definition from D:\Documents\sbt-haxe-test\project [info] Set current project to sbt-haxe-test (in build file:/D:/Documents/sbt-haxe-test/) [info] ""haxe"" ""-cp"" ""D:\Documents\sbt-haxe-test\src\haxe"" ""-cp"" ""D:\Documents\sbt-haxe-test\target\scala-2.10\src_managed\haxe"" ""-java-lib"" ""C:\Users\user\.sbt\boot\scala-2.10.3\lib\scala-library.jar"" ""-java"" ""D:\cygwin\tmp\sbt_97a26bd9"" ""-D"" ""no-compilation"" ""yourPackage.YourHaxeClass"" [info] Compiling 1 Java source to D:\Documents\sbt-haxe-test\target\scala-2.10\classes... [info] Running yourPackage.YourHaxeClass YourHaxeClass.hx:7: Hello, World! [success] Total time: 1 s, completed 2014-7-25 10:00:23  Targets supported Currently sbt-haxe supports all targets that haxe supported, but all of them are disabled by default except java. If you want to compile to specific target other than java, you need to enable it manually in build.sbt. And here's a sbt-haxe-sample project to show how to use them. JavaScript enablePlugins(HaxeJsPlugin) PHP enablePlugins(HaxePhpPlugin) Neko enablePlugins(HaxeNekoPlugin) C# enablePlugins(HaxeCSharpPlugin) Python enablePlugins(HaxePythonPlugin) C++ enablePlugins(HaxeCppPlugin) Flash enablePlugins(HaxeFlashPlugin) ActionScript 3 enablePlugins(HaxeAs3Plugin) Tasks and settings sbt-haxe provides following tasks and settings: haxe haxe:doc See src/main/scala/com/qifun/sbtHaxe/HaxePlugin.scala for more information. Dependencies sbt-haxe requires Sbt 0.13, Haxe 3.1, hxjava 3.1.0 and Dox 1.0.0. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/qifun/sbt-haxe"	"A Sbt plugin to compile Haxe sources."	"true"
"Sbt plugins"	"sbt-ide-settings ★ 22 ⧗ 9"	"https://github.com/Jetbrains/sbt-ide-settings"	"SBT plugin for tweaking various IDE settings"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"24"	"8"	"4"	"GitHub - JetBrains/sbt-ide-settings: SBT plugin for tweaking various IDE settings Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 8 Star 24 Fork 4 JetBrains/sbt-ide-settings Code Issues 3 Pull requests 1 Pulse Graphs SBT plugin for tweaking various IDE settings 26 commits 1 branch 3 releases 4 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.1.2 v0.1.1 v0.1.0 Nothing to show New pull request Latest commit 44825c2 Oct 23, 2015 dancingrobot84 Remove upcoming settings from README Permalink Failed to load latest commit information. project Add git versioning Feb 13, 2015 src/main/scala/sbtide Add `ideOutputDirectory` key Oct 23, 2015 .gitignore add .idea to gitignore Sep 25, 2014 LICENSE add license and readme files Oct 2, 2014 README.md Remove upcoming settings from README Oct 23, 2015 build.sbt Add git versioning Feb 13, 2015 README.md SBT plugin for tweaking various IDE settings This plugin provides several keys to be read by IDE while importing project. SBT 0.13.5 and up. Usage Add the following lines to project/plugins.sbt: resolvers += Resolver.url(""jetbrains-bintray"",  url(""http://dl.bintray.com/jetbrains/sbt-plugins/""))(Resolver.ivyStylePatterns)  addSbtPlugin(""org.jetbrains"" % ""sbt-ide-settings"" % ""<version>"") Tweak any settings you want Available settings ideExcludedDirectories :: Seq[File] List of directories to be marked as excluded in IDE. ideBasePackages :: Seq[String] List of packages to be used as base prefixes for chaining. Packages starting with one of these prefixes will be chained automatically in IDE. ideSkipProject :: Boolean Flag indicating that current subproject should be skipped from importing. ideOutputDirectory :: Option[File] Directory to use for production and test output instead of SBT's target directory. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Jetbrains/sbt-ide-settings"	"SBT plugin for tweaking various IDE settings"	"true"
"Sbt plugins"	"sbt-native-packager ★ 580 ⧗ 2"	"https://github.com/sbt/sbt-native-packager"	"Bundle up Scala software for native packaging systems, like deb, rpm, homebrew, msi.."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"641"	"52"	"239"	"GitHub - sbt/sbt-native-packager: SBT Native Packager Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 52 Star 641 Fork 239 sbt/sbt-native-packager Code Issues 70 Pull requests 7 Wiki Pulse Graphs SBT Native Packager http://www.scala-sbt.org/sbt-native-packager/ 1,171 commits 11 branches 70 releases Fetching contributors Scala 88.1% Shell 8.7% Python 2.2% Ruby 0.8% HTML 0.1% Groff 0.1% Scala Shell Python Ruby HTML Groff Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.6 0.7.x 0.8.x 1.1.x 633-multiple-apps fix-836-rpm-symlinks gh-pages master native-packager-logo wip/native-dll-support wip/windows-service Nothing to show v1.2.0-M3 v1.2.0-M2 v1.2.0-M1 v1.1.3 v1.1.2 v1.1.1 v1.1.0 v1.1.0-RC3 v1.1.0-RC2 v1.1.0-RC1 v1.1.0-M3 v1.1.0-M2 v1.1.0-M1 v1.0.6 v1.0.5 v1.0.5-RC1 v1.0.5-M3 v1.0.5-M2 v1.0.5-M1 v1.0.4 v1.0.4-RC1 v1.0.3 v1.0.2 v1.0.1 v1.0.0 v1.0.0-RC2 v1.0.0-RC1 v1.0.0-M5 v1.0.0-M4 v1.0.0-M3 v1.0.0-M2 v1.0.0-M1 v0.8.0 v0.8.0-RC2 v0.8.0-RC1 v0.8.0-M2 v0.8.0-M1 v0.7.7-RC1 v0.7.6 v0.7.5 v0.7.5-RC2 v0.7.5-RC1 v0.7.4 v0.7.3 v0.7.2 v0.7.2-RC2 v0.7.2-RC1 v0.7.1 v0.7.0 v0.7.0-RC3 v0.7.0-RC2 v0.7.0-RC1 v0.7.0-M3 v0.7.0-M2 v0.7.0-M1 v0.6.3 v0.6.2 v0.6.1 v0.6.0 v0.5.2 v0.5.1 v0.5.0 0.5.4 0.4.4 0.4.3 0.4.2 0.4.1 0.4 0.3.0 0.1.0 Nothing to show New pull request Latest commit 09c9c67 Jul 7, 2016 hedefalk committed with muuki88 Update introduction.rst (#837) … spelling Permalink Failed to load latest commit information. integration-tests-ansible/test-project-play-rpm Moving test project in integration and remove duplicated test projects Dec 19, 2015 project Wip/upgrade build (#794) May 22, 2016 src Update introduction.rst (#837) Jul 7, 2016 test-project-docker [codacy] avoid using mutable variables Jun 12, 2016 test-project-jdkpackager Initial work on using JDK's Ant task for generating packages. May 20, 2015 test-project-simple Extract Systemloaders into AutoPlugins (#785) May 22, 2016 test-project-windows Introduce new namespace and solve duplicate key issue (#802) Jun 9, 2016 .gitignore SystemD services now source /etc/default/{{app_name}} #737 Feb 22, 2016 .travis.yml Add a few docker tests for travis Apr 9, 2016 CONTRIBUTING.md Adding contribution.md Nov 2, 2014 LICENSE.md Added license for those who need it and want to use the code. Feb 25, 2012 README.md Add dwolla sbt plugin to related plugins list Jul 1, 2016 appveyor.yml FIX #502 Removing JAVA_OPTS Feb 23, 2015 build.sbt Wip/upgrade build (#794) May 22, 2016 sbt Travis: build on OS X Dec 13, 2015 version.sbt Setting version to 1.2.0-SNAPSHOT Jun 24, 2016 README.md SBT Native Packager Goal SBT native packager lets you build application packages in native formats. It offers different archetypes for common configurations, such as simple Java apps or server applications. Issues/Discussions Discussion/Questions: If you wish to ask questions about the native packager, we have a mailinglist and we're very active on Stack Overflow. You can either use the sbt tag or the sbt-native-packager tag. They also have far better search support for working around issues. Docs: Our docs are available online. If you'd like to help improve the docs, they're part of this repository in the src/sphinx directory. ScalaDocs are also available. The old documentation can be found here Issues/Feature Requests: Finally, any bugs or features you find you need, please report to our issue tracker. Please check the compatibility matrix to see if your system is able to produce the packages you want. Features Build native packages for different systems Universal zip,tar.gz, xz archives deb and rpm packages for Debian/RHEL based systems dmg for OSX msi for Windows docker images Provide archetypes for common use cases Java application with startscripts for linux/osx/windows Java server application with additional autostart configurations Systemd Systemv Upstart Java8 jdkpackager wrapper Optional JDeb integration for cross-platform Debian builds Optional Spotify docker client integration Installation Add the following to your project/plugins.sbt file: // for autoplugins addSbtPlugin(""com.typesafe.sbt"" % ""sbt-native-packager"" % ""1.1.1"") In your build.sbt enable the plugin you want. For example the JavaAppPackaging. enablePlugins(JavaAppPackaging) Or if you need a server with autostart support enablePlugins(JavaServerAppPackaging) Build If you have enabled one of the archetypes (app or server), you can build your application with sbt <config-scope>:packageBin Examples # universal zip sbt universal:packageBin  # debian package sbt debian:packageBin  # rpm package sbt rpm:packageBin  # docker image sbt docker:publishLocal Read more in the specific format documentation on how to configure and build your package. Documentation There's a complete ""getting started"" guide and more detailed topics available at the sbt-native-packager site. Please feel free to contribute documentation, or raise issues where you feel it may be lacking. Contributing Please read the contributing.md on how to build and test native-packager. Related SBT Plugins These are a list of plugins that either use sbt-native-packager, provide additional features or provide a richer API for a single packaging format. sbt-aether sbt-assembly sbt-bundle sbt-docker This is in addition to the built-in Docker Plugin from sbt-native. Both generate docker images. sbt-docker provides more customization abilities, while the DockerPlugin in this project integrates more directly with predefined archetypes. sbt-docker-containers - enhances sbt-native-packager's docker functionality. sbt-heroku sbt-newrelic sbt-packager sbt-package-courier sbt-typesafe-conductr Credits Josh Suereth for the initial developement Sascha Rinaldi for the native-packager logo Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sbt/sbt-native-packager"	"Bundle up Scala software for native packaging systems, like deb, rpm, homebrew, msi.."	"true"
"Sbt plugins"	"sbt-pack ★ 237 ⧗ 0"	"https://github.com/xerial/sbt-pack"	"A sbt plugin for creating distributable Scala packages."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"256"	"19"	"50"	"GitHub - xerial/sbt-pack: A sbt plugin for creating distributable Scala packages. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 19 Star 256 Fork 50 xerial/sbt-pack Code Issues 16 Pull requests 2 Pulse Graphs A sbt plugin for creating distributable Scala packages. 556 commits 9 branches 51 releases Fetching contributors Scala 63.5% Shell 29.4% HTML 7.1% Scala Shell HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags develop documentation exclude-jars feature/cross-build feature/homebrew feature/sbt-0.12.x feature/windows-bat master version-string Nothing to show 0.8.0 0.7.9 0.7.8 0.7.7 0.7.6 0.7.5 0.7.4 0.7.3 0.7.2 0.7.1 0.7.0 0.6.12 0.6.11 0.6.10 0.6.9 0.6.8 0.6.7 0.6.6 0.6.5 0.6.4 0.6.3 0.6.2 0.6.0 0.5.1 0.5.0 0.4.3 0.4.2 0.4.0 0.3.6 0.3.5 0.3.4 0.3.3 0.3.2 0.3.1 0.3.0 0.2.5 0.2.3 0.2.2 0.2.1 0.2 0.1.10 0.1.9 0.1.8 0.1.7 0.1.6 0.1.5 0.1.4 0.1.3 0.1.2 0.1.1 0.1 Nothing to show New pull request Latest commit c02348b May 9, 2016 xerial Update README.md Permalink Failed to load latest commit information. bin Allow spaces in finding plubin.sbt paths Jul 19, 2015 project 0.8.0 preparation. Remove code duplication May 6, 2016 src Setting version to 0.8.1-SNAPSHOT May 6, 2016 .gitignore 0.5.2 release Jul 14, 2014 .travis.yml Build master branch in Travis Jul 19, 2015 README.md Update README.md May 9, 2016 ReleaseNotes.md Update ReleaseNotes.md May 9, 2016 sbt Fixes #75: Allow to specify packDir outside target directory Jun 17, 2015 sonatype.sbt Removed custom branding Aug 19, 2015 version.sbt Setting version to 0.8.1-SNAPSHOT May 6, 2016 README.md sbt-pack plugin A sbt plugin for creating distributable Scala packages that include dependent jars and launch scripts. Features sbt pack creates a distributable package in target/pack folder. All dependent jars including scala-library.jar are collected in target/pack/lib folder. This process is much faster than creating a single-jar as in sbt-assembly or proguard plugins. Supporting multi-module projects. sbt pack-archive generates tar.gz archive that is ready to distribute. The archive name is target/{project name}-{version}.tar.gz sbt pack generates program launch scripts target/pack/bin/{program name} To run the program no need exists to install Scala, since it is included in the lib folder. Only java command needs to be found in the system. It also generates .bat launch scripts for Windows users. Generates a Makefile for program installation. Do cd target/pack; make install. Then you can run your program with ~/local/bin/{program name} You can install multiple versions of your program in the system. The above Makefile script uses a separate folder for each version (e.g., ~/local/{project name}/{project version}). The latest version is linked from ~/local/{project name}/current You can add other resources in src/pack folder. All resources in this folder will be copied to target/pack. Check duplicated classes in dependencies. Release Notes Usage Add sbt-pack plugin to your sbt configuration: project/plugins.sbt addSbtPlugin(""org.xerial.sbt"" % ""sbt-pack"" % ""0.8.0"")  // for sbt-0.13.x or higher  addSbtPlugin(""org.xerial.sbt"" % ""sbt-pack"" % ""0.2.5"")  // for sbt-0.12.x (New features will not be supported in this version.) Repository URL: http://repo1.maven.org/maven2/org/xerial/sbt/ Minimum configuration build.sbt // Automatically find def main(args:Array[String]) methods from classpath packAutoSettings  or // If you need to specify main classes manually, use packSettings and packMain packSettings  // [Optional] Creating `hello` command that calls org.mydomain.Hello#main(Array[String])  packMain := Map(""hello"" -> ""org.mydomain.Hello"")  Now you can use sbt pack command in your project. Full build configuration Import xerial.sbt.Pack.packAutoSettings into your project settings (Since version 0.6.2). sbt-pack finds main classes in your code and generates programs for them accordingly. The main classes must be Scala objects that define def main(args:Array[]) method. The program names are the main classes names, hyphenized. (For example, main class myprog.ExampleProg gives program name example-prog.) Alternatively, import xerial.sbt.Pack.packSettings instead of xerial.sbt.Pack.packAutoSettings. The main classes in your program will then not be guessed. Manually set the packMain variable, a mapping from your program names to their corresponding main classes (for example packMain := Map(""hello"" -> ""myprog.Hello"")). project/Build.scala import sbt._ import sbt.Keys._ import xerial.sbt.Pack._  object Build extends sbt.Build {    lazy val root = Project(     id = ""myprog"",     base = file("".""),     settings = Defaults.defaultSettings        ++ packAutoSettings // This settings add pack and pack-archive commands to sbt       ++ Seq(         // [Optional] If you used packSettings instead of packAutoSettings,          //  specify mappings from program name -> Main class (full package path)         // packMain := Map(""hello"" -> ""myprog.Hello""),         // Add custom settings here         // [Optional] JVM options of scripts (program name -> Seq(JVM option, ...))         packJvmOpts := Map(""hello"" -> Seq(""-Xmx512m"")),         // [Optional] Extra class paths to look when launching a program. You can use ${PROG_HOME} to specify the base directory         packExtraClasspath := Map(""hello"" -> Seq(""${PROG_HOME}/etc"")),          // [Optional] (Generate .bat files for Windows. The default value is true)         packGenerateWindowsBatFile := true,         // [Optional] jar file name format in pack/lib folder         //   ""default""   (project name)-(version).jar          //   ""full""      (organization name).(project name)-(version).jar         //   ""no-version"" (organization name).(project name).jar         //   ""original""  (Preserve original jar file names)         packJarNameConvention := ""default"",         // [Optional] Patterns of jar file names to exclude in pack         packExcludeJars := Seq(""scala-.*\\.jar""),         // [Optional] List full class paths in the launch scripts (default is false) (since 0.5.1)         packExpandedClasspath := false,         // [Optional] Resource directory mapping to be copied within target/pack. Default is Map(""{projectRoot}/src/pack"" -> """")          packResourceDir += (baseDirectory.value / ""web"" -> ""web-content"")       )      // To publish tar.gz, zip archives to the repository, add the following line     // ++ publishPackArchive     // Publish tar.gz archive. To publish another type of archive, use publishPackArchive(xxx) instead     // ++ publishPackArchiveTgz   ) } src/main/scala/Hello.scala package myprog  object Hello {   def main(args:Array[String]) = {     println(""Hello World!!"")   } } Command Examples Create a package $ sbt pack  Your program package will be generated in target/pack folder. Launch a command $ target/pack/bin/hello Hello World!!  Install the command Install the command to $(HOME)/local/bin: $ sbt packInstall  or $ cd target/pack; make install  To launch the command:     $ ~/local/bin/hello     Hello World!  Add the following configuration to your .bash_profile, .zsh_profile, etc. for the usability: export PATH=$(HOME)/local/bin:$PATH  Install the command to the system $ cd target/pack $ sudo make install PREFIX=""/usr/local"" $ /usr/local/bin/hello Hello World!  Create a tar.gz archive of your Scala program package $ sbt packArchive  Copy dependencies The packCopyDependencies task copies all the dependencies to the folder specified through the packCopyDependenciesTarget setting. By default, a symbolic link will be created. By setting packCopyDependenciesUseSymbolicLinks to false, the files will be copied instead of symlinking. A symbolic link is faster and uses less disk space. It can be used e.g. for copying dependencies of a webapp to WEB-INF/lib See an example project. Example projects See also examples folder in the source code. It contains several Scala project examples using sbt-pack. Use case scala-min: A minimal Scala project using sbt-pack: https://github.com/xerial/scala-min A minimal project to start writing Scala programs. For developers To test sbt-pack plugin, run $ ./sbt scripted  Run a single test project, e.g., src/sbt-test/sbt-pack/multi-module: $ ./sbt ""scripted sbt-pack/multi-module""  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/xerial/sbt-pack"	"A sbt plugin for creating distributable Scala packages."	"true"
"Sbt plugins"	"sbt-revolver ★ 420 ⧗ 1"	"https://github.com/spray/sbt-revolver"	"Fork & Stop processes from sbt."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"444"	"22"	"33"	"GitHub - spray/sbt-revolver: An SBT plugin for dangerously fast development turnaround in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 22 Star 444 Fork 33 spray/sbt-revolver Code Issues 8 Pull requests 2 Pulse Graphs An SBT plugin for dangerously fast development turnaround in Scala 132 commits 3 branches 10 releases 10 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags develop feature/sbt-1.0.0-M4-support master Nothing to show v0.8.0 v0.8.0-RC1 v0.7.2 v0.7.1 v0.7.1-sbt-0.13 v0.7.0 v0.6.2 v0.6.1 v0.6.0 v0.5.0 Nothing to show New pull request Latest commit 4078373 Mar 1, 2016 jrudolph Fix configuration examples in README, fixes #59 Permalink Failed to load latest commit information. notes publish 0.8.0 Dec 4, 2015 project update to sbt 0.13.9 Sep 5, 2015 src/main better error message if no main class was specified or detected Sep 5, 2015 .gitignore setup sbt project Dec 15, 2011 LICENSE add license Dec 15, 2011 README.md Fix configuration examples in README, fixes #59 Mar 1, 2016 build.sbt change to bintray publishing Sep 4, 2015 publish.sbt publish 0.8.0 Dec 4, 2015 README.md sbt-revolver is a plugin for SBT enabling a super-fast development turnaround for your Scala applications. It sports the following features: Starting and stopping your application in the background of your interactive SBT shell (in a forked JVM) Triggered restart: automatically restart your application as soon as some of its sources have been changed Even though sbt-revolver works great with spray on spray-can there is nothing spray-specific to it. It can be used with any Scala application as long as there is some object with a main method. Installation sbt-revolver requires SBT 0.13.x or greater. Add the following dependency to your project/plugins.sbt: addSbtPlugin(""io.spray"" % ""sbt-revolver"" % ""0.8.0"") sbt-revolver is an auto plugin, so you don't need any additional configuration in your build.sbt nor in Build.scala to make it work. In multi-module builds it will be enabled for each module. For older versions of sbt see version 0.7.2. Usage sbt-revolver defines three new commands (SBT tasks) in its own re configuration: re-start <args> --- <jvmArgs> starts your application in a forked JVM. The optionally specified (JVM) arguments are appended to the ones configured via the re-start-args/ java-options(for re-start) setting (see the ""Configuration"" section below). If the application is already running it is first stopped before being restarted. re-stop stops application. This is done by simply force-killing the forked JVM. Note, that this means that shutdown hooks are not run (see #20). re-status shows an informational message about the current running state of the application. Triggered Restart You can use ~re-start to go into ""triggered restart"" mode. Your application starts up and SBT watches for changes in your source (or resource) files. If a change is detected SBT recompiles the required classes and sbt-revolver automatically restarts your application. When you press <ENTER> SBT leaves ""triggered restart"" and returns to the normal prompt keeping your application running. Configuration The following SBT settings defined by sbt-revolver are of potential interest: re-start-args, a SettingKey[Seq[String]], which lets you define arguments that sbt-revolver should pass to your application on every start. Any arguments given to the re-start task directly will be appended to this setting. re-start::re-main-class, which lets you optionally define a main class to run in re-start independently of the one set for running the project normally. This value defaults to the value of compile:main-class(for run). If you don't specify a value here explicitly the same logic as for the normal run main class applies: If only one main class is found it one is chosen. Otherwise, the main-class chooser is shown to the user. re-start::java-options, a SettingKey[Seq[String]], which lets you define the options to pass to the forked JVM when starting your application re-start::base-directory, a SettingKey[File], which lets you customize the base directory independently from what run assumes. re-start::full-classpath, which lets you customize the full classpath path for running with re-start. re-jrebel-jar, a SettingKey[String], which lets you override the value of the JREBEL_PATH env variable. re-colors, a SettingKey[Seq[String]], which lets you change colors used to tag output from running processes. There are some pre-defined color schemes, see the example section below. re-log-tag, a SettingKey[String], which lets you change the log tag shown in front of log messages. Default is the project name. debug-settings, a SettingKey[Option[DebugSettings]] to specify remote debugger settings. There's a convenience helper Revolver.enableDebugging to simplify to enable debugging (see examples). Examples: To configure a 2 GB memory limit for your app when started with re-start: javaOptions in reStart += ""-Xmx2g""  To set a special main class for your app when started with re-start: mainClass in reStart := Some(""com.example.Main"")  To set fixed start arguments (than you can still append to with the re-start task): reStartArgs := Seq(""-x"")  To enable debugging with the specified options: Revolver.enableDebugging(port = 5050, suspend = true)  To change set of colors used to tag output from multiple processes: reColors := Seq(""blue"", ""green"", ""magenta"")  There are predefined color schemes to use with reColors: Revolver.noColors, Revolver.basicColors, Revolver.basicColorsAndUnderlined. Hot Reloading Note: JRebel support in sbt-revolver is not actively supported any more. If you have JRebel installed you can let sbt-revolver know where to find the jrebel.jar. You can do this either via the Revolver.jRebelJar setting directly in your SBT config or via a shell environment variable with the name JREBEL_PATH (which is the recommended way, since it doesn't pollute your SBT config with system-specific settings). For example, on OSX you would add the following line to your shell startup script: export JREBEL_PATH=/Applications/ZeroTurnaround/JRebel/jrebel.jar  With JRebel sbt-revolver supports hot reloading: Start your application with re-start. Enter ""triggered compilation"" with ~products. SBT watches for changes in your source (and resource) files. If a change is detected SBT recompiles the required classes and JRebel loads these classes right into your running application. Since your application is not restarted the time required to bring changes online is minimal (see the ""Understanding JRebel"" section below for more details). When you press <ENTER> SBT leaves triggered compilation and returns to the normal prompt keeping your application running. If you changed your application in a way that requires a full restart (see below) press <ENTER> to leave triggered compilation and re-start. Of course you always stop the application with re-stop. License sbt-revolver is licensed under APL 2.0. Patch Policy Feedback and contributions to the project, no matter what kind, are always very welcome. However, patches can only be accepted from their original author. Along with any patches, please state that the patch is your original work and that you license the work to the sbt-revolver project under the project’s open source license. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/spray/sbt-revolver"	"Fork & Stop processes from sbt."	"true"
"Sbt plugins"	"sbt-robovm ★ 107 ⧗ 23"	"https://github.com/roboscala/sbt-robovm"	"An sbt plugin for iOS development in Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"106"	"16"	"16"	"GitHub - roboscala/sbt-robovm: An sbt plugin for iOS development in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 16 Star 106 Fork 16 roboscala/sbt-robovm Code Issues 6 Pull requests 0 Wiki Pulse Graphs An sbt plugin for iOS development in Scala 184 commits 1 branch 0 releases Fetching contributors Scala 92.6% Java 7.4% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 654af2b Jan 16, 2016 ajhager Merge pull request #62 from roboscala/version1.12 … Update to RoboVM version 1.12.0 Permalink Failed to load latest commit information. project Do some small fixups Aug 24, 2015 src/main/scala Update to RoboVM version 1.12.0 Dec 20, 2015 .gitignore Prepare project to be published in sbt-plugins on bintray Aug 21, 2015 CONTRIBUTORS Add contributors file Oct 20, 2014 LICENSE Initial commit Sep 8, 2013 README.md Update to RoboVM version 1.12.0 Dec 20, 2015 build.sbt Update to RoboVM version 1.12.0 Dec 20, 2015 README.md sbt-robovm sbt-robovm is a plugin for the Scala build tool that aims to make it as simple as possible to compile Scala (and Java) code to binaries for iOS, linux, and OSX using RoboVM Setup Install Xcode 6 Install JDK 7 Install sbt See roboscala-samples for example on how to use and configure Add the Plugin First, add the plugin to your project by appending addSbtPlugin(""org.roboscala"" % ""sbt-robovm"" % ""1.12.0"") into the project/plugins.sbt file. The file name (not extension) may actually be different, but such is the convention. The plugin's version is in sync with the RoboVM version it uses, so it should always be clear which RoboVM is being used. Project Creation All you have to do to use the plugin, is to add iOSRoboVMSettings key (or nativeRoboVMSettings if you are creating a native project) to your build.sbt file. If you are creating a multi-project build, prepend that to your settings Seq: lazy val myproject = Project(id = ""myproject"", base = file(""myproject""), settings = iOSRoboVMSettings ++ Seq(     /* More settings */ )) Tasks There are different tasks defined for iOS and native console projects. Shared robovmLicense Allows you to enter your RoboVM license key to get access to premium features, such as line numbers in stack traces, debugger support and interface builder integration. iOS iphoneSim and ipadSim Build and run the app in a iPhone or iPad simulator, respectively. simulator Build and run the app on a simulator specified by the robovmSimulatorDevice setting. device Build and run the app on a connected device. It is possible to specify the order of preference of devices using the robovmPreferredDevices task. Otherwise, the plugin will attempt to connect to the last device it has used. ipa Create the .ipa archive for upload to the App Store or other distribution. simulatorDevices Print all installed simulator devices. Native native Build and run a native console application. Connecting the input to interactive apps is not implemented. Recommended workaround is to execute compiled binary (in target/robovm/) in separate Terminal window. nativeBuild Same as native, but does not execute the binary. Settings As with tasks, there are some settings that are only meaningful in iOS projects. Some settings are actually implemented as tasks. Shared robovmConfiguration Either[File,Elem] The most important key, specifies the configuration of your app, the robovm.xml file If you have a real file, set it to Left(file(""path-to-your/robovm.xml"")) If you want to specify it in-place, use the built-in scala support for XML literals: Right(<config> ... </config>) See examples in the sample repository robovmProperties Either[File, Map[String, String]] robovm.properties file contains key-value pairs substituted into robovm.xml and Info.plist If you have a real file with these, set this to Left(file(""path-to-your-file"")) If you wish to generate these in your build script: Right(Map(""some-key"" -> ""some-value"", ...)) By default, this contains set of useful values: app.name - Value of name sbt key app.executable - Name of executable derived from name sbt key app.mainclass - Fully specified main class of your application, either detected or specified in mainClass key robovmTarget64bit Boolean Whether to build 64bit executables for the device Default is false therefore you will need to set it to true if your device is newer and has a 64bit processor robovmHome Config.Home Return the home of RoboVM installation. By default, this task downloads RoboVM distribution into a local maven repository and unpacks it there, so there is no need to touch this unless you have a good reason to. robovmInputJars Seq[File] Jars and classes to feed into the RoboVM compiler By default, this returns fullClasspath, which is in most cases correct. You may want to override this if you want to modify the compiled classes first somehow (for example when using ProGuard). robovmVerbose Boolean Setting this to true will propagate RoboVM debug-level messages to info-level Useful when debugging RoboVM or plugin, otherwise not so much robovmDebug Boolean Whether to enable RoboVM debugger See Debugging section below first Needs commercial license, run robovmLicense task to enter yours Port can be specified with robovmDebugPort robovmDebugPort Int Port on which RoboVM debugger will listen (when enabled, see robovmDebug) iOS Only robovmProvisioningProfile Option[String] Specify provisioning profile to use when signing iOS code Profile can be specified by name, UUID, app prefix, etc. See Tips section robovmSigningIdentity Option[String] Specify signing identity to use when signing iOS code Signing identity can be specified by name, fingerprint, etc. See Tips section robovmSimulatorDevice Option[String] Name of device to be used in simulator task Use simulatorDevices task to list all installed devices robovmSkipSigning Option[Boolean] Setting this to Some(true/false) overrides default signing behavior and allows you to test without proper certificates and identities robovmPreferredDevices Seq[String] List of iOS device ID's listed in the priority in which you want to connect to them if multiple devices are connected robovmIBScope Scope Scope in which interfaceBuilder command operates. Defaults to ThisScope. Only reason to change this is if you have a custom configuration Debugging (licensed only) Line numbers will be enabled automatically when the license is entered (see robovmLicense task). To use the RoboVM debugger, prefix your task invocations with debug: (example: $ sbt myproject/debug:ipadSim). This sets the scope to the Debug configuration, in which the debugger is enabled and the debug port is set, by default to 5005. Running with the debugger enabled will allow you to connect to a running application with a java debugger. Using the debugger from IntelliJ IDEA 14 Create a new ""Remote"" Run/Debug configuration Top bar, right next to the ""Make"" button -> ""Edit configurations"" -> ""+"" -> ""Remote"" All settings can be left to their default values Run the project in debug mode from SBT (for example $ sbt debug:ipadSim) Make sure the configuration from step 1 is selected and press the ""Debug"" button IntelliJ will connect to the running application and you can start debugging like you are used to with standard Java debugging Application execution will pause before your main method and wait for the debugger to attach. Then it will continue normally. Interface Builder (licensed only) This plugin offers a basic integration with XCode's Interface Builder. There are some excellent tutorials on how to use IB with IntelliJ on RoboVM website. Getting familiar with them is recommended, since the workflow in sbt is similar. In the core of this feature is an interactive interfaceBuilder command. Run the command inside your iOS project, it will generate XCode project and open it in the Interface Builder. Then it will watch your code sources and when any of them change, it will recompile the project and update the XCode project accordingly. XCode will show new IBOutlets and IBActions very shortly after that. You will also notice, that the prompt in the sbt console will change to ""interfaceBuilder >"". That notes that you are in a special mode, where the interfaceBuilder command is still running, but you can still run any commands/tasks as usual, so you can, for example, run the ipadSimulator task to quickly view your changes on device. Pressing enter, without any command, will exit the interfaceBuilder mode and you will be back to standard sbt prompt. Because interfaceBuilder is a command and not a task (for technical reasons), it can not be scoped. Therefore, doing something like myProject/interfaceBuilder will not work. To work around this, use project myProject command first, to switch active project to that and then run interfaceBuilder. If you need even more granular scoping, use the robovmIBScope setting. Tips All paths in the configuration are relative to the base directory. During typical development, you usually end up with two pairs of signing identity and profile, one for development and one for distribution. It is possible to scope the robovmSigningIdentity/Profile keys to automatically use the distribution pair when building an ipa: robovmProvisioningProfile := Some(""name of development profile""), robovmSigningIdentity := Some(""name of development identity""), robovmProvisioningProfile in ipa := Some(""name of distribution profile""), robovmSigningIdentity in ipa := Some(""name of distribution identity"") You can download simulators for more iOS versions in Xcode. Xcode includes only the latest iOS simulator by default. The first time you try to compile a program, RoboVM must compile the Java and Scala standard libraries. This can take a few minutes, but the output of this process is cached. Subsequent compilations will be much faster. If you are having issues after installing Xcode, open Xcode and agree to the license or open a Terminal and run xcrun. Hacking on the plugin If you need to make modifications to the plugin itself, you can compile and install it locally: $ git clone git://github.com/roboscala/sbt-robovm.git $ cd sbt-robovm $ sbt publish-local When testing your changes, it is useful to publish locally with different version than what is officially used. That is because if you have already used the official version, your testing projects will most likely use that and not your modified version. To workaround that, change in sbt-robovm's build.sbt:     version := roboVMVersion.value, to     version := roboVMVersion.value + ""-YOUR_SUFFIX"", and in the project/plugins.sbt of your (testing) project, instead of standard installation: // Necessary only when testing with RoboVM snapshot build, such as 1.11.1-SNAPSHOT resolvers += Resolver.sonatypeRepo(""snapshots"")  addSbtPlugin(""org.roboscala"" % ""sbt-robovm"" % ""1.12.0-YOUR_SUFFIX"" changing()) Unless you need to use the SNAPSHOT version of RoboVM, it is easier to work with stable version, because for SNAPSHOT dependencies, sbt has to check for new version each run, which adds latency to testing. However, when working on the plugin, you want it to be ""redownloaded"" each time it changes, and the changing() in addSbtPlugin line does exactly that. But don't worry, thanks to it being installed/published locally, the latency of these checks is negligible. Contributing Reporting any issues you encounter helps. If you want to help improving the plugin, feel free to make a PR. Projects using the plugin libgdx-sbt-project.g8 (Uses older version) Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/roboscala/sbt-robovm"	"An sbt plugin for iOS development in Scala"	"true"
"Sbt plugins"	"sbt-scala-js-map ★ 7 ⧗ 18"	"https://github.com/ThoughtWorksInc/sbt-scala-js-map"	"A sbt plugin that configures source mapping for Scala.js projects hosted on Github"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"9"	"6"	"0"	"GitHub - ThoughtWorksInc/sbt-scala-js-map: A Sbt plugin that configures source mapping for Scala.js projects hosted on Github Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 6 Star 9 Fork 0 ThoughtWorksInc/sbt-scala-js-map Code Issues 0 Pull requests 0 Pulse Graphs A Sbt plugin that configures source mapping for Scala.js projects hosted on Github 32 commits 1 branch 3 releases Fetching contributors Scala 88.9% Shell 11.1% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v2.0.0 v1.0.1 v1.0.0 Nothing to show New pull request Latest commit 91419a3 Jun 11, 2016 travis@localhost Setting version to 2.0.1-SNAPSHOT Permalink Failed to load latest commit information. project src/main/scala/com/thoughtworks/sbtScalaJsMap .gitignore .travis.yml LICENSE NOTICE README.md build.sbt ci.sbt deploy.sh.disabled pubring.asc version.sbt README.md sbt-scala-js-map sbt-scala-js-map is a Sbt plugin that configures source mapping for Scala.js projects hosted on Github. Motivation I have some Scala.js libraries hosted on Github (Binding.scala and some private libraries). Then, other Scala.js applications would depend on these libraries. When I debug the Scala.js application in a browser, I want to see the Scala source files of the original libraries. However, by default, the generated *.js.map files maps the generated JavaScript to the absolute Scala source path where the original library compiled. For me, the path is on a Travis CI worker, like /home/travis/build/ThoughtWorksInc/.../Binding.scala. The path obviously does not exist my local computer that debugs the application. Too bad. This sbt plugin detects if a library is hosted on Github repository and let source map point to https://raw.githubusercontent.com/ instead of a local file path. Alternative options An alternative option is specifying Scala.js's relative source mappings flag. However, this approach enforce library users cloning the library's source files into their local file system before debugging their applications. On the other hand, libraries published with this plugin enable the library users to view the source (of correct revision), automatically. Usage Step 1: Add the dependencies in your Scala.js library's project/plugins.sbt addSbtPlugin(""com.thoughtworks.sbt-scala-js-map"" % ""sbt-scala-js-map"" % ""latest.release"") Step 2: Enable this plugin in your Scala.js library's build.sbt enablePlugins(ScalaJsMap) Step 3: Publish your Scala.js library Execute the release command if you have setup sbt-release correctly。 sbt release  Step 4: Debug it! Now switch your Scala.js application to the newly published Scala.js library, build it, browse your web page, and open the debugger in your browser. You will see the Scala source files hosted under https://raw.githubusercontent.com/ and you can set break points at code lines in these Scala files. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ThoughtWorksInc/sbt-scala-js-map"	"A sbt plugin that configures source mapping for Scala.js projects hosted on Github"	"true"
"Sbt plugins"	"sbt-sublime ★ 129 ⧗ 36"	"https://github.com/orrsella/sbt-sublime"	"Create Sublime Text projects with library dependencies sources"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"133"	"13"	"4"	"GitHub - orrsella/sbt-sublime: An sbt pluign for generating Sublime Text projects with library dependencies sources Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 13 Star 133 Fork 4 orrsella/sbt-sublime Code Issues 3 Pull requests 0 Pulse Graphs An sbt pluign for generating Sublime Text projects with library dependencies sources 74 commits 1 branch 4 releases 2 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v1.1.1 v1.1.0 v1.0.11 v1.0.10 Nothing to show New pull request Latest commit 5e6fa1e Sep 30, 2015 orrsella Update version to 1.1.1 Permalink Failed to load latest commit information. img Add example Jan 28, 2013 project Update to sbt version 0.13.9 Sep 1, 2015 src/main/scala/com/orrsella/sbtsublime Convert to AutoPlugin, small refactor for file/folder excludes Apr 13, 2015 .gitignore Fix sbt settings to allow overriding in full configuration (and not j… Oct 22, 2013 LICENSE Initial commit Jan 25, 2013 README.md Update version to 1.1.1 Sep 30, 2015 build.sbt Update spray-json version Sep 1, 2015 version.sbt Setting version to 1.1.2-SNAPSHOT Sep 1, 2015 README.md sbt-sublime An sbt (Simple Build Tool) plugin for generating Sublime Text 2 or 3 project files with library dependencies' sources. See example screenshot. The main goal of this plugin is to make dependency source files easily available in the project's Sublime window. This enables the ability to simultaneously code and browse all your dependencies' source code, similar to functionality that IntelliJ and other IDEs provide you with. This also means that external library source code plays nice with Sublime's excellent Goto Anything feature. It could also be useful for split editing. Don't remember a method's signature in scala.collection.immutable.List? Just hit CMD+T, enter ""List"" and the source is right in front of you (the scala-library is always a dependency so you have the entire Scala language code base a click away). See Functionality and Notes for more details, and this post for some background. Add Plugin To add sbt-sublime functionality to your project add the following to your project/plugins.sbt file: addSbtPlugin(""com.orrsella"" % ""sbt-sublime"" % ""1.1.1"") If you want to use it for more than one project, you can add it to your global plugins file, usually found at: ~/.sbt/plugins/plugins.sbt and then have it available for all sbt projects. See Using Plugins for additional information on sbt plugins. Requirements sbt 0.13.5+ (requires AutoPlugin) Scala 2.10.x Troubleshooting If you added the plugin globally but still don't have the gen-sublime command available, try: $ sbt > reload plugins > clean > reload return  Essentially, this enters the project project, cleans it, and returns back to your main project (remember that sbt is recursive – each project/ folder is an sbt project in itself!). Example As an example, here's the project file generated for Twitter's bijection project. The first folder, bijection, is the project's root as cloned from GitHub. The External Libraries folder is the generated external sources folder, showing all available dependencies' sources, readily available for you to browse or search (BTW, in case you're wondering, the theme I'm using is the immensely popular Soda Light): Usage To use sbt-sublime, simply enter the gen-sublime command in the sbt console to create the project file. When the command is done, open the new Sublime project created to see your own sources and external library sources. Functionality Creates a .sublime-project project file for your project. The default project file created will include the project's base directory and the special external library sources directory. If a project file already exists, the plugin will keep all existing settings in the file and only add the external sources directory. You don't have to worry about losing your Sublime project's settings. Automatically fetches sources available for all dependencies. Allows fetching all dependencies transitively – have access to the sources of all libraries that your own dependencies require. This can quickly escalate to a lot of source code, so the default behavior is to not fetch dependencies transitively, only your direct dependencies (see next section). Works with multi-project build configurations. In this scenario, external libraries will include the dependencies of all projects combined. Important: make sure to run the gen-sublime command on the root project. Otherwise, you'll create a Sublime project for the sub-project you ran the command on. Not the end of the world, but probably not what you meant to happen. Configuration The following custom sbt settings are used: sublimeExternalSourceDirectoryName – The name of the directory containing all external library source files. Default value: External Libraries. sublimeExternalSourceDirectoryParent – Where the external library sources directory will be located. Default value: sbt's target setting. If left unchanged, running the clean command will delete the sources folder. To have it persist, change it's parent away from the target folder. sublimeTransitive – Indicates whether dependencies should be added transitively (recursively) for all libraries (including the libraries that your own dependencies require – ""your dependencies' dependencies""). For large projects, this can amount to dozens of libraries pretty quickly, meaning that a lot of code will be searched and handled by Sublime. See if appropriate for your own project. Default value: false. sublimeProjectName – The name of the generated Sublime project file, not including the "".sublime-project"" extension. Default value: sbt's name setting, that is your project's name as you define it in build.sbt. sublimeProjectDir – Where the generated Sublime project file will be saved. Default value: sbt's baseDirectory setting, that is the root of your project. This can be set to anywhere on your machine, it doesn't have to be in the project's root directory (but would be convenient). If you already have a project file, or like to keep all project files together in some special folder, just point there. sublimeFileExcludePatterns – Optional file patterns to be excluded from the project tree. (See Projects for more details.) sublimeFolderExcludePatterns – Optional folder patterns to be excluded from the project tree. (See Projects for more details.) To change any/all of these settings (to these arbitrary alternative values), add the following to your build.sbt file: sublimeExternalSourceDirectoryName := ""ext-lib-src""  sublimeExternalSourceDirectoryParent <<= crossTarget  sublimeTransitive := true  sublimeProjectName := ""MySublProjectFile""  sublimeProjectDir := new java.io.File(""/Users/orr/Dev/Projects"")  sublimeFileExcludePatterns := Seq(""*.css"")  sublimeFolderExcludePatterns := Seq(""backup"", ""target"") Notes The external library sources directory is considered as artifacts and located by default in target, and so running the clean command will delete it. But don't worry – you can always re-run gen-sublime to get it back, or change sublimeExternalSourceDirectoryParent to have it reside out side of the target folder and not get deleted during clean. When running the gen-sublime command the existing library sources directory is deleted, and a new one is created. All library source files are intentionally marked as read-only so you won't be able to save changes to them. This is mainly to remind you that changing these sources has absolutely no effect on the libraries you're using! This is important – just because the sources are available doesn't mean they are used in compilation/runtime. These are merely extracted from the source jars for each dependency, as fetched by sbt. If you want to change and edit the external libraries you're using, this is not the way. Add them as an sbt project or manually to your own project as source files, to make any changes and compile. Again, this plugin only allows to quickly add the sources to the same Sublime window for convenience purposes only. Sbt doesn't compile anything in the sublimeExternalSourceDirectoryName folder! If you change any of the library dependencies or the specific settings detailed in Configuration, you'll need to reload the sbt project with the reload command, and then execute gen-sublime again. This will add/remove dependencies' sources accordingly, making sure the list in up-to-date. If you change the name of the external sources directory (sublimeExternalSourceDirectoryName), you might need to close and re-open the Sublime project for the change to take effect. All other Sublime project settings should remain intact when using the plugin, don't be afraid to tweak it if you want. Sources, as do dependencies, are usually appropriate for the scalaVersion you're using. Changing it and re-running the gen-sublime command will update sources accordingly. Consider adding the .sublime-project file to .gitignore and file_exclude_patterns (in sublime's preferences) to not commit and/or display the project file in Sublime, if it's saved to it's default location in the root folder. Feedback Any comments/suggestions? Let me know what you think – I'd love to hear from you. Send pull requests, issues or contact me: @orrsella and orrsella.com License This software is licensed under the Apache 2 license, quoted below. Copyright (c) 2013 Orr Sella Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/orrsella/sbt-sublime"	"Create Sublime Text projects with library dependencies sources"	"true"
"Sbt plugins"	"sbt-updates ★ 248 ⧗ 0"	"https://github.com/rtimush/sbt-updates"	"Shows sbt project's dependency updates."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"258"	"25"	"13"	"GitHub - rtimush/sbt-updates: SBT plugin that can check maven repositories for dependency updates Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 25 Star 258 Fork 13 rtimush/sbt-updates Code Issues 13 Pull requests 0 Pulse Graphs SBT plugin that can check maven repositories for dependency updates 117 commits 2 branches 11 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags ivy-resolution master Nothing to show 0.1.10 0.1.9 0.1.8 0.1.7 0.1.6 0.1.5 0.1.4 0.1.3 0.1.2 0.1.1 0.1.0 Nothing to show New pull request Latest commit 32e8982 Feb 23, 2016 rtimush Ignore resolvers with unsupported protocols Permalink Failed to load latest commit information. notes Release notes for 0.1.10 Oct 22, 2015 project Removed ls.implicit.ly integration Oct 22, 2015 src Ignore resolvers with unsupported protocols Feb 23, 2016 .gitignore Basic plugin functionality Oct 7, 2012 .travis.yml Faster travis builds Mar 21, 2015 LICENSE License text Oct 7, 2012 README.md Update the latest version on README Feb 10, 2016 build.sbt Publish snapshots to bintray Oct 20, 2015 publish.sbt Publish snapshots to bintray Oct 20, 2015 README.md sbt-updates Display your SBT project's dependency updates. Requirements SBT 0.13.5 and later Note: use version 0.1.0 for SBT 0.11.x, version 0.1.6 for SBT 0.12.x, version 0.1.7 for SBT 0.13.0-0.13.2. Installation Stable version Add the following line to one of these files: The project-specific file at project/sbt-updates.sbt Your global file at ~/.sbt/0.13/plugins/sbt-updates.sbt addSbtPlugin(""com.timushev.sbt"" % ""sbt-updates"" % ""0.1.10"")  Snapshot version Choose one of versions available on BinTray or the latest one. Add the following lines to one of these files: The project-specific file at project/sbt-updates.sbt Your global file at ~/.sbt/0.13/plugins/sbt-updates.sbt resolvers += Resolver.url(""rtimush/sbt-plugin-snapshots"", new URL(""https://dl.bintray.com/rtimush/sbt-plugin-snapshots/""))(Resolver.ivyStylePatterns) addSbtPlugin(""com.timushev.sbt"" % ""sbt-updates"" % ""0.1.9-6-g5a7705c"")  Note, that snapshots are not updated automatically. Tasks dependencyUpdates: show a list of project dependencies that can be updated, dependencyUpdatesReport: writes a list of project dependencies to a file. Settings dependencyUpdatesReportFile: report file location, target/dependency-updates.txt by default. dependencyUpdatesExclusions: filter matching dependencies that should be excluded from update reporting. dependencyUpdatesFailBuild: dependencyUpdates task will fail a build if updates found. dependencyAllowPreRelease: when enabled, pre-release dependencies will be reported as well. Exclusions You can exclude some modules from update checking: dependencyUpdatesExclusions := moduleFilter(organization = ""org.scala-lang"")  Example > dependencyUpdates [info] Found 3 dependency updates for test-project [info]   ch.qos.logback:logback-classic : 0.8   -> 0.8.1 -> 0.9.30 -> 1.0.13 [info]   org.scala-lang:scala-library   : 2.9.1 -> 2.9.3 -> 2.10.3 [info]   org.slf4j:slf4j-api            : 1.6.4 -> 1.6.6 -> 1.7.5  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/rtimush/sbt-updates"	"Shows sbt project's dependency updates."	"true"
"Sbt plugins"	"sbt-versions ★ 11 ⧗ 122"	"https://github.com/sksamuel/sbt-versions"	"Plugin that checks for updated versions of your project's dependencies."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"11"	"3"	"1"	"GitHub - sksamuel/sbt-versions: SBT Plugin for checking dependency versions Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 11 Fork 1 sksamuel/sbt-versions Code Issues 2 Pull requests 0 Pulse Graphs SBT Plugin for checking dependency versions 24 commits 1 branch 0 releases 1 contributor Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 2a47f18 Jul 29, 2014 sksamuel Update README.md Permalink Failed to load latest commit information. project Added plugins for deploy Jul 29, 2014 src/main/scala/com/sksamuel/sbt/versions Tweaked output Jul 29, 2014 .gitignore Added plugins for deploy Jul 29, 2014 LICENSE Initial commit Aug 5, 2013 README.md Update README.md Jul 29, 2014 build.sbt Bumped release ver Jul 29, 2014 README.md sbt-versions SBT plugin for checking for the availability of updated versions of your project's dependencies. How to use Add the plugin to your SBT by adding this to project/plugins.sbt addSbtPlugin(""com.sksamuel.sbt-versions"" % ""sbt-versions"" % ""0.2.0"")  Note that this is an auto plugin so only works with SBT 0.13.5 or higher. Then simply running sbt checkVersions will cause SBT to check all your declared dependencies against maven central. Any deps that have updated versions available will be highlighted in the console. Example Running this plugin on a project of mine gives: [info] [sbt-versions] 35 dependencies to check for [myproject] [info] -------------------------------------------------------------- [info] [sbt-versions] Update available org.scala-lang:scala-library:2.11.1 [latest: 2.11.2] [info] [sbt-versions] Update available org.scala-lang:scala-reflect:2.11.1 [latest: 2.11.2] [info] [sbt-versions] Update available org.scala-lang:scala-compiler:2.11.1 [latest: 2.11.2] [info] [sbt-versions] Update available net.sf.uadetector:uadetector-resources:2014.04 [latest: 2014.06] [info] [sbt-versions] Update available com.fasterxml.jackson.core:jackson-core:2.4.1 [latest: 2.4.1.1] [info] [sbt-versions] Update available com.fasterxml.jackson.core:jackson-databind:2.4.1 [latest: 2.4.1.3] [info] [sbt-versions] Update available org.codehaus.woodstox:woodstox-core-asl:4.3.0 [latest: 4.4.0] [info] [sbt-versions] Update available com.amazonaws:aws-java-sdk:1.7.10 [latest: 1.8.6] [info] [sbt-versions] Update available joda-time:joda-time:2.3 [latest: 2.4] [info] [sbt-versions] Update available com.sendgrid:sendgrid-java:1.0.0 [latest: 1.1.0] [info] [sbt-versions] Update available com.sendgrid:smtpapi-java:0.0.2 [latest: 0.1.1] [info] [sbt-versions] Update available com.stripe:stripe-java:1.14.0 [latest: 1.15.1] [info] [sbt-versions] Update available org.apache.commons:commons-email:1.3.2 [latest: 1.3.3]  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sksamuel/sbt-versions"	"Plugin that checks for updated versions of your project's dependencies."	"true"
"Sbt plugins"	"sbt-view ★ 4 ⧗ 51"	"https://github.com/nestorpersist/sbt-view"	"View ScalaDoc/JavaDoc in browser window."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"5"	"1"	"1"	"GitHub - nestorpersist/sbt-view: SBT Plugin for Viewing ScalaDoc and JavaDoc Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 1 Star 5 Fork 1 nestorpersist/sbt-view Code Issues 0 Pull requests 0 Pulse Graphs SBT Plugin for Viewing ScalaDoc and JavaDoc 16 commits 1 branch 0 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Fetching latest commit… Cannot retrieve the latest commit at this time. Permalink Failed to load latest commit information. license project src/main/scala/com/persist .gitignore README.md build.sbt README.md Scaladoc/JavaDoc Viewer SBT Plugin Opens a browser window with Javadoc/Scaladoc for the current project or its dependencies. In project/plugins.sbt add addSbtPlugin(""com.persist"" % ""sbt-view"" % ""1.0.1"") In build.sbt add viewSettings Make sure that your SBT dependencies include the javadoc jar. Here is an example ""com.foo"" %% ""foopackage"" % ""1.2.3"" withJavadoc() View Scaladoc for current project. Calls doc if none there. view Finds an item in the classpath where all words are matched. Unpack the javadoc jar (if not already done) in the ivy2 cache and then view it. view WORD1 WORD2 ... The plugin is based on a plugin developed by Whitepages https://github.com/whitepages/WP_Sbt_Plugins Development of this plugin was supported by 47 Degrees. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/nestorpersist/sbt-view"	"View ScalaDoc/JavaDoc in browser window."	"true"
"Sbt plugins"	"sbteclipse ★ 540 ⧗ 0"	"https://github.com/typesafehub/sbteclipse"	"Create Eclipse project definitions from sbt builds."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"576"	"49"	"118"	"GitHub - typesafehub/sbteclipse: Plugin for sbt to create Eclipse project definitions Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 49 Star 576 Fork 118 typesafehub/sbteclipse Code Issues 65 Pull requests 1 Wiki Pulse Graphs Plugin for sbt to create Eclipse project definitions 261 commits 6 branches 45 releases 22 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 1.x dotta-issue/dont-add-scala-nature-for-java-projects master revert-274-issue/239-support-for-multiple-scala-versions sbt-0.11 travis Nothing to show v4.0.0 v4.0.0-RC2 v4.0.0-RC1 v3.0.0 v2.5.0 v2.4.0 v2.3.0 v2.2.0 v2.2.0-RC2 v2.2.0-RC1 v2.1.2 v2.1.1 v2.1.0 v2.1.0-RC1 v2.1.0-RC1-0.12.0-Beta2 v2.1.0-M2 v2.1.0-M2-0.11.3 v2.1.0-M1 v2.0.0 v2.0.0-RC1 v2.0.0-M3 v2.0.0-M2 v2.0.0-M1 v2.0.0-0.12.0 v2.0.0-0.11.3 v1.5.0 v1.4.0 v1.4.0-RC4 v1.4.0-RC3 v1.4-RC2 v1.4-RC1 v1.3-RC3 v1.3-RC2 v1.3-RC1 v1.2 v1.1 v1.0.1 v1.0 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Nothing to show New pull request Latest commit f16a09b Jul 4, 2016 benmccann Make compatible with the latest scalaz Permalink Failed to load latest commit information. notes Added notes for 2.1.0-M2 release. Apr 21, 2012 project Allow overriding setting defaults at build-level May 2, 2016 src Make compatible with the latest scalaz Jul 4, 2016 .gitignore closes #34: Use relative path for local libraries Dec 17, 2011 .travis.yml Use Oracle JDK7 on Travis Jan 20, 2016 LICENSE-2.0.txt Consolidate license files Jul 25, 2015 README.rst Update README with info about latest version Jun 21, 2015 README.rst sbteclipse Plugin for sbt to create Eclipse project definitions. Please see the Documentation for information about installing and using sbteclipse. Information about contribution policy and license can be found below. For sbt 0.13 and up Add sbteclipse to your plugin definition file (or create one if doesn't exist). You can use either: the global file (for version 0.13 and up) at ~/.sbt/0.13/plugins/plugins.sbt the project-specific file at PROJECT_DIR/project/plugins.sbt For the latest version: addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""4.0.0"")  In sbt use the command eclipse to create Eclipse project files > eclipse  In Eclipse use the Import Wizard to import Existing Projects into Workspace Contribution policy Contributions via GitHub pull requests are gladly accepted from their original author. Before we can accept pull requests, you will need to agree to the Typesafe Contributor License Agreement online, using your GitHub account - it takes 30 seconds. License This code is open source software licensed under the Apache 2.0 License. Feel free to use it accordingly. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/typesafehub/sbteclipse"	"Create Eclipse project definitions from sbt builds."	"true"
"Sbt plugins"	"ScalaKata2 ★ 36 ⧗ 1"	"https://github.com/MasseGuillaume/ScalaKata2"	"Scala playground & Documentation tool."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"54"	"5"	"10"	"GitHub - MasseGuillaume/ScalaKata2: Interactive Playground Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 5 Star 54 Fork 10 MasseGuillaume/ScalaKata2 Code Issues 3 Pull requests 0 Pulse Graphs Interactive Playground http://scalakata.com 112 commits 3 branches 10 releases Fetching contributors Scala 91.2% CSS 8.6% Shell 0.2% Scala CSS Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags fix-35 master share Nothing to show 1.1.5 1.1.4 1.1.3 1.1.2 1.0.10 1.0.9 1.0.6 1.0.5 1.0.4 1.0.0 Nothing to show New pull request Latest commit 71ee9f3 Jul 15, 2016 MasseGuillaume synchronize Permalink Failed to load latest commit information. annotation/src annother shot at gracefully handling macro errors May 18, 2016 codemirror render console output, println :-( & unsused import May 18, 2016 evaluation/src synchronize Jul 15, 2016 misc bump to 1.1.5 Jul 15, 2016 model/src/main/scala/com.scalakata Implement server side for room list. Jun 21, 2016 project udpate scala.js version Jun 21, 2016 sbtScalaKata/src/main/scala/com.scalakata fix timeout parsing error in plugin Jul 15, 2016 webapp create a room from the room list Jul 15, 2016 .travis.yml bugfix pasing large code snippet in room Apr 24, 2016 README.md bump to 1.1.5 Jul 15, 2016 appveyor.yml collaborative editing /room/id Mar 18, 2016 build.sbt bump to 1.1.5 Jul 15, 2016 README.md ScalaKata Distributions Sbt Plugin Add the following line to project/plugins.sbt addSbtPlugin(""com.scalakata"" % ""sbt-scalakata"" % ""1.1.5"") And add the following line to build.sbt enablePlugins(ScalaKataPlugin) Docker container sudo docker run -p 7331:7331 --name scalakata masseguillaume/scalakata:v1.1.5 open your browser at http://localhost:7331 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/MasseGuillaume/ScalaKata2"	"Scala playground & Documentation tool."	"true"
"Sbt plugins"	"tut ★ 227 ⧗ 3"	"https://github.com/tpolecat/tut"	"Tool for writing documentation with typechecked examples."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"257"	"15"	"31"	"GitHub - tpolecat/tut: doc/tutorial generator for scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 257 Fork 31 tpolecat/tut Code Issues 35 Pull requests 2 Pulse Graphs doc/tutorial generator for scala 210 commits 1 branch 10 releases Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.4.2 v0.4.1 v0.4.0 v0.3.2 v0.3.1 v0.3.0 v0.2.2 v0.2.1 v0.2 v0.1 Nothing to show New pull request Latest commit 9664dba May 16, 2016 tpolecat Merge pull request #110 from OlivierBlanvillain/patch-1 … Change error reporting to use canonical path Permalink Failed to load latest commit information. core Change error reporting to use canonical path May 16, 2016 plugin Fix setting reference May 15, 2016 project bintray 0.3.0 Feb 23, 2016 tests Added Tests May 15, 2016 .gitignore initial checkin Oct 25, 2013 .travis.yml Update SBT and tests for Scala 2.12.0-M3 Feb 17, 2016 CHANGELOG.md v0.4.2 Feb 23, 2016 LICENSE Initial commit Oct 25, 2013 README.md v0.4.2 Feb 24, 2016 build.sbt Fix setting reference May 15, 2016 tut.jpg margin on image Jun 25, 2015 README.md tut tut is a very simple documentation tool for Scala that reads Markdown files and interprets Scala code in tut sheds, allowing you to write documentation that is typechecked and run as part of your build. The current version is 0.4.2 (changelog here) which runs on Scala 2.10, 2.11, and 2.12 (as of 2.12.0-M3). Projects using tut include doobie and cats. If you're using it and would like be added to the list, please submit a PR! tut is a Typelevel project. This means we embrace pure, typeful, functional programming, and provide a safe and friendly environment for teaching, learning, and contributing as described in the Typelevel Code of Conduct. Quick Start 1. Add the following to project/plugins.sbt: addSbtPlugin(""org.tpolecat"" % ""tut-plugin"" % ""0.4.2"") 2. And add the following to build.sbt: tutSettings 3. Write a tutorial in src/main/tut/Foo.md: Here is how you add numbers: ```tut 1 + 1 ```  4. At the sbt> prompt type tut, then look at the output in target/<scala-version>/tut/Foo.md: Here is how you add numbers: ```scala scala> 1 + 1 res0: Int = 2     ```  Commands tut adds the following commands: Command Explanation tut Moves the contents of tutSourceDirectory into tutTargetDirectory, interpreting code in tut sheds in any file whose name matches tutNameFilter (other files are copied but not interpreted). tutOnly Does the same thing as tut but only for the specified path under tutSourceDirectory. Note that tab completion works for this command Interpretation obeys the following particulars: Each file is interpreted with an independent REPL session. Definitions earlier in the file are available later in the file. Each REPL has the same classpath as your build's Test configuration, and by default also has the same scalac options and compiler plugins. By default any error in interpretation (compilation failure or runtime exception) will cause the tut command to fail. If this command is part of your CI configuration then your build will fail. Yay! tut captures output from the REPL, as well as anything your code writes to standard output (System.out). ANSI escapes are removed from this output, so colorized console output will show up as plaintext. In modes that show REPL prompts (see below) blank lines in between statements are discarded and a single blank line is introduced between prompts, as in the normal REPL. Otherwise blank lines are neither introduced nor discarded. This is a change from prior versions. Modifiers By default tut will interpret code in tut sheds as if it had been pasted into a Scala REPL. However sometimes you might want a definition without REPL noise, or might want to demonstrate non-compiling code (which would normally cause the build to fail). For these occasions tut provides a number of modifiers that you can add to the shed declaration. For instance, ```tut:silent import com.woozle.fnord._ ```  will produce the following output, suppressing REPL noise: ```scala import com.woozle.fnord._ ```  The following modifiers are supported. Note that you can use multiples if you like; for example you could use tut:silent:fail to show code that doesn't compile, without showing the compilation error. Modifier Explanation :fail Code in the shed must throw an exception or fail to compile. Successful interpretation will cause a buid failure. :nofail Code in the shed might throw an exception or fail to compile. Such failure will not cause a build failure. Note that this modifier is deprecated in favor of :fail. :silent Suppresses REPL prompts and output; under this modifier the input and output text are identical. :plain Output will not have scala syntax highlighting. :invisible Suppresses all output. This is not recommended since the point of tut is to provide code that the user can type in and expect to work. But in rare cases you might want one of these at the bottom of your file to clean up filesystem mess that your code left behind. :book Output will be suitable for copy and paste into the REPL. That is, there are no REPL prompts or margins, and output from the REPL is commented. :reset Resets the REPL state prior to evaluating the code block. Use this option with care, as it has no visible indication and can be confusing to readers who are following along in their own REPLs. Settings tut also adds the following sbt settings, all of which have reasonable defaults. It is unlikely that you will need to change any of them, but here you go. Setting Explanation Default Value tutSourceDirectory Location of tut source files. sourceDirectory.value / ""main"" / ""tut"" tutNameFilter Regex specifying files that should be interpreted. Names ending in .md .txt .htm .html tutTargetDirectory Destination for tut output. crossTarget.value / ""tut"" tutScalacOptions Compiler options that will be passed to the tut REPL. Same as Test configuration. tutPluginJars List of compiler plugin jarfiles to be passed to the tut REPL. Same as Test configuration. Integration with sbt-site Tut is designed to work seamlessly with sbt-site so that your checked tutorials can be incorporated into your website. Add the following to project/plugins.sbt in your project to add SBT shell commands: addSbtPlugin(""com.typesafe.sbt"" % ""sbt-site"" % ""0.8.1"") Then in your build sbt, link the files generated by tut to your site generation: project(""name"").settings(site.addMappingsToSiteDir(tut, ""tut"")) When the makeSite task is run in sbt, the typechecked tutorials from src/main/tut will be incorporated with the site generated by sbt-site in target/site. Running the Tests There are a set of test markdown files and corresponding expected markdown output from tut. Run these tests from sbt with: publishLocal tests/scripted  Complaints and other Feedback Feedback of any kind is always appreciated. Issues and PR's are welcome, or just find me on Twitter or #scala on FreeNode or on gitter. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/tpolecat/tut"	"Tool for writing documentation with typechecked examples."	"true"
"Sbt plugins"	"xsbt-web-plugin ★ 312 ⧗ 17"	"https://github.com/earldouglas/xsbt-web-plugin"	"Build enterprise J2EE Web applications in Scala."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"315"	"27"	"88"	"GitHub - earldouglas/xsbt-web-plugin: Build J2EE Web applications in Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 27 Star 315 Fork 88 earldouglas/xsbt-web-plugin Code Issues 16 Pull requests 2 Wiki Pulse Graphs Build J2EE Web applications in Scala. http://earldouglas.com/projects/xsbt-web-plugin/ 419 commits 3 branches 28 releases 21 contributors Scala 92.6% HTML 6.1% CSS 1.3% Scala HTML CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master scala Nothing to show 2.1.0 2.0.5 2.0.4 2.0.3 2.0.2 2.0.1 2.0.0 1.2.0-M1 1.1.1 1.1.0 1.0.0 1.0.0-M7 1.0.0-M6 1.0.0-M5 1.0.0-M4 1.0.0-M3 1.0.0-M2 1.0.0-M1 0.9.1 0.7.0 0.6.0 0.5.0 0.4.2 0.4.1 0.4.0 0.3.0 0.2.13 0.2.12 Nothing to show New pull request Latest commit 9a289ec Mar 22, 2016 earldouglas Merge pull request #305 from joeytsaiexpedia/patch-2 … Update 2.1.md Permalink Failed to load latest commit information. docs Update 2.1.md Mar 22, 2016 notes Add 2.1.0 notes Nov 21, 2015 project Tidy up, update Travis Scala version Jul 3, 2015 src Tidy up a little Nov 15, 2015 .travis.yml Allow Travis to run on non-legacy infrastructure Nov 22, 2015 LICENSE Rewrite from scratch Jul 19, 2014 README.md Add link to 2.1 docs Nov 16, 2015 build.sbt Update to Scala 2.10.6 Nov 19, 2015 README.md xsbt-web-plugin xsbt-web-plugin is an sbt extension for building J2EE Web applications in Scala and Java. It is best suited for projects that: Deploy to common cloud platforms (e.g. Google App Engine, Heroku, Elastic Beanstalk, Jelastic) Deploy to production J2EE environments (e.g. Tomcat, Jetty, GlassFish, WebSphere) Incorporate J2EE libraries (e.g. JSP, JSF, EJB) Utilize J2EE technologies (e.g. Servlet, Filter, JNDI) Have a specific need to be packaged as a .war file Reference manual Documentation is available by minor release version. Version numbers follow the Specified Versioning guidelines. Version 2.1 (current) Version 2.0 Version 1.1 Version 1.0 Version 0.9 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/earldouglas/xsbt-web-plugin"	"Build enterprise J2EE Web applications in Scala."	"true"
"XML / HTML"	"scala-scraper ★ 137 ⧗ 0"	"https://github.com/ruippeixotog/scala-scraper"	"A library for scraping content from HTML pages."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"182"	"17"	"26"	"GitHub - ruippeixotog/scala-scraper: A Scala library for scraping content from HTML pages Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 17 Star 182 Fork 26 ruippeixotog/scala-scraper Code Issues 2 Pull requests 0 Pulse Graphs A Scala library for scraping content from HTML pages 141 commits 1 branch 4 releases 4 contributors Scala 95.9% HTML 4.1% Scala HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v1.0.0 v0.1.2 v0.1.1 v0.1 Nothing to show New pull request Latest commit 3a4f1e5 Jul 2, 2016 ruippeixotog Add support for missing name and value attributes in `formData` extra… … …ctor Permalink Failed to load latest commit information. project Update SBT and use http4s 0.13.0a Mar 21, 2016 src Add support for missing name and value attributes in `formData` extra… Jul 2, 2016 .gitignore Initial import Oct 12, 2014 .travis.yml Update sbt-scoverage and sbt-coveralls Feb 29, 2016 LICENSE Update copyright years Apr 4, 2016 README.md Release version 1.0.0 Apr 5, 2016 build.sbt Add a clearCookies() method to Browser Jul 2, 2016 README.md Scala Scraper A library providing a DSL for loading and extracting content from HTML pages. Take a look at Examples.scala and at the unit specs for usage examples or keep reading for more thorough documentation. Quick Start To use Scala Scraper in an existing SBT project with Scala 2.11.x, add the following dependency to your build.sbt: libraryDependencies += ""net.ruippeixotog"" %% ""scala-scraper"" % ""1.0.0"" If you are using an older version of this library, see this document for the version you're using: 0.1, 0.1.1, 0.1.2. An implementation of the Browser trait, such as JsoupBrowser, can be used to fetch HTML from the web or to parse a local HTML file or a string. import net.ruippeixotog.scalascraper.browser.JsoupBrowser  val browser = JsoupBrowser() val doc = browser.get(""http://example.com/"") val doc2 = browser.parseFile(""file.html"") The returned object is a Document, which already provides several methods for manipulating and querying HTML elements. For simple use cases, it can be enough. For others, this library improves the content extracting process by providing a powerful DSL. First of all, the DSL methods and conversions must be imported: import net.ruippeixotog.scalascraper.dsl.DSL._ import net.ruippeixotog.scalascraper.dsl.DSL.Extract._ import net.ruippeixotog.scalascraper.dsl.DSL.Parse._ Content can then be extracted using the >> extraction operator and CSS queries: import net.ruippeixotog.scalascraper.model.Element  // Extract the text inside the first h1 element val title: String = doc >> text(""h1"")  // Extract the elements with class ""item"" val items: List[Element] = doc >> elementList("".item"")  // From each item, extract the text of the h3 element inside val itemTitles: List[String] = items.map(_ >> text(""h3""))  // From the meta element with ""viewport"" as its attribute name, extract the // text in the content attribute val viewport: String = doc >> attr(""content"")(""meta[name=viewport]"") If the element may or may not be in the page, the >?> tries to extract the content and returns it wrapped in an Option: // Extract the element with id ""optional"" if it exists, return `None` if it // doesn't: val title: Option[Element] = doc >?> element(""#optional"") With only these two operators, some useful things can already be achieved: // Go to a news website and extract the hyperlink inside the h1 element if it // exists. Follow that link and print both the article title and its short // description (inside "".lead"") for {   headline <- browser.get(""http://observador.pt"") >?> element(""h1 a"")   headlineDesc = browser.get(headline.attr(""href"")) >> text("".lead"") } println(""== "" + headline.text + "" ==\n"" + headlineDesc) In the next two sections the core classes used by this library are presented. They are followed by a description of the full capabilities of the DSL, including the ability to parse content after extracting, validating the contents of a page and defining custom extractors or validators. Core Model The library represents HTML documents and their elements by Document and Element objects, simple interfaces containing several methods for retrieving information and navigating throughout the DOM. Browser implementations are the entrypoints for obtaining Document instances. They implement most notably get, post, parseFile and parseString methods for retrieving documents from different sources. Depending on the browser used, Document and Element instances may have different semantics, mainly on their immutability guarantees. Browsers The library currently provides two built-in implementations of Browser: JsoupBrowser is backed by jsoup, a Java HTML parser library. JsoupBrowser provides powerful and efficient document querying, but it doesn't run JavaScript in the pages. As such, it is limited to working strictly with the HTML send in the page source; HtmlUnitBrowser is based on HtmlUnit, a GUI-less browser for Java programs. HtmlUnitBrowser simulates thoroughly a web browser, executing JavaScript code in the pages besides parsing and modelling its HTML content. It supports several compatibility modes, allowing it to emulate browsers such as Internet Explorer. Due to its speed and maturity, JsoupBrowser is the recommended browser to use when JavaScript execution is not needed. More information about each browser and its semantics can be obtained in each implementations' Scaladoc. Content Extraction The >> and >?> operators shown above accept an HtmlExtractor as their right argument, which has a very simple interface: trait HtmlExtractor[+A] {   def extract(doc: Elements): A } One can always create a custom extractor by implementing HtmlExtractor. However, the DSL provides several useful constructors for HtmlExtractor instances. In general, you can use the extractor factory method: doc >> extractor(<cssQuery>, <contentExtractor>, <contentParser>) Where the arguments are: cssQuery: the CSS query used to select the elements to be processed; contentExtractor: defines the content to be extracted from the selected elements, e.g. the element objects themselves, their text, a specific attribute, form data; contentParser: an optional parser for the data extracted in the step above, such as parsing numbers and dates or using regexes. The DSL provides several contentExtractor and contentParser instances, which were imported before with DSL.Extract._ and DSL.Parse._. The full list can be seen in the ContentExtractors and ContentParsers objects inside HtmlExtractor.scala. Some usage examples: // Extract the date from the ""#date-taken"" element doc >> extractor(""#date-taken"", text, asDate(""yyyy-MM-dd""))  // Extract the text of all "".price"" elements and parse each of them as a number doc >> extractor(""section .price"", texts, seq(asDouble))  // Extract an element ""#card"" and do no parsing (the default parsing behavior) doc >> extractor(""#card"", element, asIs[Element]) The DSL also provides implicit conversions to write more succinctly the most common extractor types: Writing <contentExtractor>(<cssQuery>) is taken as extractor(<cssQuery>, <contentExtractor>, asIs); Writing <cssQuery> is taken as extractor(<cssQuery>, texts, asIs). That way, one can write the expressions in the Quick Start section, as well as: // Extract the elements with class ""article"" doc >> elements("".article"")  // Extract the text inside each p element doc >> texts(""p"")  // Exactly the same as the extractor above doc >> ""p"" Content Validation While scraping web pages, it is a common use case to validate if a page effectively has the expected structure. This library provides special support for creating and applying validations. A HtmlValidator has the following signature: trait HtmlValidator[+R] {   def matches(doc: Elements): Boolean   def result: Option[R] } As with extractors, the DSL provides the validator constructor and the ~/~ operator for applying a validation to a document: doc ~/~ validator(<extractor>)(<matcher>) Where the arguments are: extractor: an extractor as defined in the previous section; matcher: a function mapping the extracted content to a boolean indicating if the document is valid. The result of a validation is a Validated[A, R] instance, where A is the type of the document and R is the result type of the validation (which will be explained later). A Validated can be either a VSuccess(a: A) or a VFailure(res: R). Some validation examples: import net.ruippeixotog.scalascraper.util.Validated._  // Check if the title of the page is ""My Page"" doc ~/~ validator(text(""title""))(_ == ""My Page"") match {   case VSuccess(_) => println(""Correct!"")   case VFailure(_) => println(""Wrong title"") }  // Check if there are at least 3 items doc ~/~ validator("".item"")(_.size >= 3)  // Check if the text in "".desc"" contains the word ""blue"" doc ~/~ validator(text("".desc""))(_.contains(""blue"")) When a document fails a validation, it may be useful to identify the problem by pattern-matching it against common scraping pitfalls, such as a login page that appears unexpectedly because of an expired cookie, dynamic content that disappeared or server-side errors. Validators can be also used to match ""error"" pages instead of expected pages: val succ = validator(text(""title""))(_ == ""My Page"") val errors = Seq(   validator(text("".msg""))(_.contains(""sign in"")) withResult ""Not logged in"",   validator("".item"")(_.size < 3) withResult ""Too few items"",   validator(text(""h1""))(_.contains(""500"")) withResult ""Internal Server Error"")  doc ~/~ (succ, errors) match {   case VSuccess(_) => println(""yey"")   case VFailure(msg) => println(s""Error: $msg"") } For validators matching errors, withResult should be used most times. It returns a new validator holding a result value which will be returned wrapped in a VFailure if that particular error ever occurs. Other DSL Features As shown before in the Quick Start section, one can try if an extractor works in a page and obtain the extracted content wrapped in an Option: // Try to extract an element with id ""title"", return `None` if none exist doc >?> element(""#title"") Note that when using >?> with content extractors that return sequences, such as texts and elements, None will never be returned (Some(Seq()) will be returned instead). If you want to use multiple extractors in a single document or element, you can pass tuples or triples to >>: // Extract the text of the title element and all the form elements doc >> (text(""title""), elements(""form"")) The extraction operators work on List, Option, Either, Validated and other instances for which a Scalaz Functor instance is provided. The extraction occurs by mapping over the functors: // Extract the titles of all documents in the list List(doc1, doc2) >> text(""title"")  // Extract the title if the document is a `Some` Option(doc) >> text(""title"") You can apply other extractors and validators to the result of an extraction, which is particularly powerful combined with the last feature shown above: // From the ""#menu"" element, extract the text in the "".curr"" element inside doc >> element(""#menu"") >> text("".curr"")  // Same as above, but in a scenario where ""#menu"" can be absent doc >?> element(""#menu"") >> text("".curr"")  // Same as above, but check if the ""#menu"" has any section before extracting // the text doc >?> element(""#menu"") ~/~ validator(""section"")(_.nonEmpty) >> text("".curr"")  // Extract the links inside all the "".article"" elements doc >> elementList("".article"") >> attr(""href"")(""a"") This library also provides a Functor for HtmlExtractor, which makes it possible to map over extractors and create chained extractors that can be passed around and stored like objects: // An extractor for the links inside all the "".article"" elements val linksExtractor = elementList("".article"") >> attr(""href"")(""a"") doc >> linksExtractor  // An extractor for the number of links val linkCountExtractor = linksExtractor.map(_.length) doc >> linkCountExtractor Just remember that you can only apply extraction operators >> and >?> to documents, elements or functors ""containing"" them, which means that the following is a compile-time error: // The `texts` extractor extracts a list of strings and extractors cannot be // applied to strings doc >> texts("".article"") >> attr(""href"")(""a"") Finally, if you prefer not using operators for the sake of code legibility, you can use full method names: // `extract` is the same as `>>` doc extract text(""title"")  // `tryExtract` is the same as `>?>` doc tryExtract element(""#optional"") Integration with Typesafe Config Matchers and validators can be loaded from a Typesafe config using the methods matcherAt, validatorAt and validatorsAt of the DSL. Take a look at the examples.conf config used in the examples and at the application.conf used in tests to see how they can be used. NOTE: this feature is in a beta stage. Please expect API changes in future releases. Working Behind an HTTP/HTTPS Proxy If you are behind an HTTP proxy, you can configure Browser implementations to make connections through it by setting the Java system properties http.proxyHost, https.proxyHost, http.proxyPort and https.proxyPort. Scala Scraper provides a ProxyUtils object that facilitates that configuration: ProxyUtils.setProxy(""localhost"", 3128) val browser = JsoupBrowser() // HTTP requests and scraping operations... ProxyUtils.removeProxy() JsoupBrowser uses internally java.net.HttpURLConnection. Configuring those JVM-wide system properties will affect not only Browser instances, but all requests done using HttpURLConnection directly or indirectly. HtmlUnitBrowser was implementated so that it reads the same system properties for configuration, but once the browser is created they will be used on every request done by the instance, regardless of the properties' values at the time of the request. NOTE: this feature is in a beta stage. Please expect API changes in future releases. Copyright Copyright (c) 2014-2016 Rui Gonçalves. See LICENSE for details. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/ruippeixotog/scala-scraper"	"A library for scraping content from HTML pages."	"true"
"XML / HTML"	"Scalatags ★ 384 ⧗ 5"	"https://github.com/lihaoyi/scalatags"	"Write html as scala code and have your ide syntax check it."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"404"	"36"	"71"	"GitHub - lihaoyi/scalatags: ScalaTags is a small XML/HTML construction library for Scala. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 36 Star 404 Fork 71 lihaoyi/scalatags Code Issues 7 Pull requests 4 Pulse Graphs ScalaTags is a small XML/HTML construction library for Scala. 430 commits 8 branches 7 releases 29 contributors Scala 99.2% HTML 0.8% Scala HTML Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 2.11 cake gh-pages master new rewrite stylesheets test Nothing to show v0.5.2 v0.4.2 v0.4.1 v0.4.0 v0.2.5-M1 0.5.4 0.5.3 Nothing to show New pull request Latest commit 023b748 Jul 9, 2016 lihaoyi tweak-comment Permalink Failed to load latest commit information. example/src/main move constants into their own file, camelCase svg tags, merge example… Oct 24, 2015 project Convert all `.tag` `.voidTag` `.attr` `.style` etc .extension methods… Jul 9, 2016 readme fix-typo-readme Apr 23, 2016 scalatags tweak-comment Jul 9, 2016 .gitignore doesn't work Jan 28, 2015 .travis.yml broader test matrix Apr 19, 2016 build.sbt Upgrade scala-js-dom dependency to 0.9.1 Jul 6, 2016 readme.md fix travis badge Oct 24, 2015 readme.md Scalatags Documentation Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/scalatags"	"Write html as scala code and have your ide syntax check it."	"true"
"JavaScript"	"js-scala ★ 150 ⧗ 29"	"https://github.com/js-scala/js-scala"	"JavaScript as an embedded DSL in Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"151"	"23"	"18"	"GitHub - js-scala/js-scala: js.scala: JavaScript as an embedded DSL in Scala Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 23 Star 151 Fork 18 js-scala/js-scala Code Issues 6 Pull requests 0 Wiki Pulse Graphs js.scala: JavaScript as an embedded DSL in Scala 495 commits 4 branches 0 releases Fetching contributors TeX 58.9% JavaScript 15.5% Scala 15.4% HTML 9.9% Other 0.3% TeX JavaScript Scala HTML Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags adt-annotation feature/polymorphism master reified-classes Nothing to show Nothing to show New pull request Latest commit 0b57718 Jun 30, 2015 julienrf Merge pull request #25 from timxzl/develop … catch up with Scala 2.11.2 version (the most recent scala virtualized version) Permalink Failed to load latest commit information. core/src catch up with Scala 2.11.2 style, fix errors and reduce warnings Jun 29, 2015 examples catch up with Scala 2.11.2 style, fix errors and reduce warnings Jun 29, 2015 papers minor rewording Apr 7, 2014 project catch up with Scala 2.11.2 style, fix errors and reduce warnings Jun 29, 2015 test-out compile with up to date LMS Aug 29, 2014 .gitignore add *.swp and .DS_Store to .gitignore Jun 29, 2015 README.md more in the readme Jan 18, 2014 README.md js-scala: JavaScript as an embedded DSL in Scala js-scala is a Scala library providing composable JavaScript code generators as embedded DSLs. Generate (optimized) JavaScript code from Scala-like code: import scala.js.language.JS trait JavaScriptGreet extends JS {    def greet(name: Rep[String]): Rep[Unit] = {     println(""Hello, "" + name + ""!"")   }  } greet is a JavaScript program generator producing a program that prints a message in the console: function greet(x0) {   var x1 = ""Hello, ""+x0;   var x2 = x1+""!"";   var x3 = console.log(x2); } Note: some language units also support server-side code generation (see the Quick start section below), allowing code sharing between client and server sides. Publications and talks ECOOP 2012 paper (PDF) and slides (PDF) Scala Days 2012 talk mloc-js'13 talk (slides) GPCE'13 paper (PDF) and slides Setup Setup virtualization-lms-core: $ git clone git@github.com:TiarkRompf/virtualization-lms-core.git $ cd virtualization-lms-core $ sbt publish-local Clone the project and manage it using sbt: $ cd .. $ git clone git@github.com:js-scala/js-scala.git $ cd js-scala $ sbt Run the tests: > test Publish it (if you want to use it in your project): > publish-local Run the examples: > project examples > run Use it in your project Add a dependency on js-scala 0.4-SNAPSHOT libraryDependencies += ""EPFL"" %% ""js-scala"" % ""0.4-SNAPSHOT"" Use Scala 2.10.2-RC1 and set the Scala organization to org.scala-lang.virtualized scalaVersion := ""2.10.2-RC1"" scalaOrganization := ""org.scala-lang.virtualized"" Set the -Yvirtualize compiler option scalacOptions += ""-Yvirtualize"" Further projects play-js-validation uses this DSL to enable form validation code in Play 2.0 to be written once and checked on both client and server sides. forest uses this DSL to enable HTML templates to be written once and shared between client and server sides, both for initial rendering and automatic updating. Quick start First, be sure to be familiar with LMS tutorials. Write a program generator Write your program generator in a trait extending the language units you want to use: import scala.js.language.JS // JavaScript-like language unit import scala.js.language.dom.Dom // DOM API  trait Program extends JS with Dom {    def printClicks() = {     for (target <- document.find(""#target"")) {       target.on(Click) { click =>         println(""click at ("" + click.offsetX + "", "" + click.offsetY + "")"")       }     }   }  } The Program trait contains a method printClicks that describes a JavaScript program searching for an element with id “target”, attaching it a “click” event listener and printing the mouse coordinates on each click. Generate JavaScript code To generate a JavaScript program from the above program description, write a Scala application instantiating your program generator and creating a code generator with the corresponding language unit implementations: import scala.js.exp.JSExp // JS language unit implementation import scala.js.exp.dom.DomExp // Dom language unit implementation import scala.js.gen.js.GenJS // JS JavaScript code generator import scala.js.gen.js.dom.GenDom // Dom JavaScript code generator  object Generator extends App {   // instantiate the program generator with language unit implementations   val program = new Program with JSExp with DomExp   // create a code generator for these language units and pass it the program as a parameter   val gen = new GenJS with GenDom { val IR: program.type = program }    // emit the code of the program described by the printClicks method   gen.emitExecution(program.printClicks(), new java.io.PrintWriter(""printclicks.js"")) } The above code generator will print the following JavaScript program in file printclicks.js: (function () {   var x0 = document.querySelector(""#target"");   if (x0 !== null) {     var x1 = x0;     x1.addEventListener('click', function (x2) {       var x3 = x2.offsetX;       var x4 = ""click at (""+x3;       var x5 = x4+"", "";       var x6 = x2.offsetY;       var x7 = x5+x6;       var x8 = x7+"")"";       var x9 = console.log(x8);     }, false);   }   var x13 = undefined; } Go a bit further Some language unit implementations also have a trait defining optimizations. When such a trait exists it is named like the language unit implementation trait with the additional suffix Opt. For instance, there is a DomExpOpt trait for the Dom language unit. If you use it in the above example instead of DomExp, it will generate document.getElementById(""target"") instead of document.querySelector(""#target""). Some language units also have Scala code generators, allowing code using them to be shared by the client-side and the server-side (see next section for more details on this). The browser API is not exactly the same as the native API. It tries to be as most type safe as possible and sometimes uses shorter names (e.g. on instead of addEventListener). js-scala modules layout The modules defined by js-scala generally stick to the following conventions: language units are defined under the scala.js.language package (though some language units are already provided by LMS under the scala.virtualization.lms.common package) ; implementations are defined under the scala.js.exp package. A language unit Foo is implemented by a module named FooExp (another module defining optimizations can also be defined under the name FooExpOpt) ; code generators are defined under the scala.js.gen.js and scala.js.gen.scala packages, for JavaScript and Scala, respectively. A code generator for a language unit Foo has name GenFoo. A language unit is a trait usually named according to a concept (e.g. Arrays, Adts, etc.) or to a type suffixed by “Ops” (e.g. OptionOps, ListOps, etc.). Mix such traits in your code to import their vocabulary. This vocabulary can consist of top-level functions (e.g. fun, provided by the Functions language unit, or array, provided by Arrays), or methods implicitly added to Rep[X] values where “X” is (generally) the name of the language unit without the trailing Ops (e.g. the ElementOps trait implicitly adds methods on Rep[Element] values, the BooleanOps trait implicitly adds methods on Rep[Boolean] values). The JsScala language unit defines a core language supporting the following concepts or types: if, while, ==, var, tuples (using Scala syntax) ; “primitive types” (Int, Long, Double, Float, Boolean), String, List ; functions (using the term fun instead of Scala’s def) ; println. This trait has code generators for both Scala and JavaScript, meaning that you can generate Scala and JavaScript programs from the same code. The JS trait extends JsScala with JavaScript specific language units: arrays, dynamic typing, regexps. The Dom trait provides Web browsers APIs (e.g. the window and document objects, a selector API, etc.). Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/js-scala/js-scala"	"JavaScript as an embedded DSL in Scala"	"true"
"JavaScript"	"scala-js-fiddle"	"http://www.scala-js-fiddle.com/"	"() - Browser-based Scala.js playground"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Scala-Js-Fiddle Loading gist... Loading Scala-Js-Fiddle This takes a while the first time. Please be patient =)"	"null"	"null"	"() - Browser-based Scala.js playground"	"true"
"JavaScript"	"repo"	"https://github.com/lihaoyi/scala-js-fiddle"	"() - Browser-based Scala.js playground"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"76"	"16"	"31"	"GitHub - lihaoyi/scala-js-fiddle: Source code for Scala.jsFiddle Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 16 Star 76 Fork 31 lihaoyi/scala-js-fiddle Code Issues 12 Pull requests 4 Pulse Graphs Source code for Scala.jsFiddle http://www.scala-js-fiddle.com 151 commits 1 branch 0 releases Fetching contributors Scala 97.8% CSS 2.2% Scala CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit 8ccf45b Feb 5, 2015 lihaoyi tweak css Permalink Failed to load latest commit information. client/src/main/scala/fiddle tweak things Feb 4, 2015 page/src/main/scala/fiddle tweak things Feb 5, 2015 project tweak things Feb 5, 2015 server/src tweak css Feb 5, 2015 shared tweak things Feb 5, 2015 .gitignore so close, but still dumping compiled javascript files to the filesyst… Feb 23, 2014 Capstanfile added new stuff Apr 27, 2014 Procfile Revert ""move POST after server starting"" Jan 9, 2015 javamains added new stuff Apr 27, 2014 readme.md It's alive! Jun 21, 2014 system.properties moved system properties Feb 25, 2014 readme.md Scala-Js-Fiddle Source code for www.scala-js-fiddle.com. To develop, run: sbt ""~; server/re-start""  You can also run sbt stage; ./server/target/start  To stage and run without SBT, sbt assembly; java -jar server/target/scala-2.10/server-assembly-0.1-SNAPSHOT.jar  To package as a fat jar and run, or capstan build -p vmw; capstan run -p vmw  To bundle as an image using OSv and run it under VMware. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/scala-js-fiddle"	"() - Browser-based Scala.js playground"	"true"
"JavaScript"	"Scala.js"	"http://www.scala-js.org/"	"() - Scala to JavaScript compiler"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2409"	"154"	"219"	"GitHub - scala-js/scala-js: Scala.js, the Scala to JavaScript compiler Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 154 Star 2,409 Fork 219 scala-js/scala-js Code Issues 86 Pull requests 7 Pulse Graphs Scala.js, the Scala to JavaScript compiler https://www.scala-js.org/ 3,459 commits 1 branch 40 releases 45 contributors Scala 99.8% Other 0.2% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.6.10 v0.6.9 v0.6.8 v0.6.7 v0.6.6 v0.6.5 v0.6.5-scala-2.12.0-M3 v0.6.4 v0.6.3 v0.6.2 v0.6.1 v0.6.0 v0.6.0-RC2 v0.6.0-RC1 v0.6.0-M3 v0.6.0-M2 v0.6.0-M1 v0.5.6 v0.5.5 v0.5.4 v0.5.3 v0.5.2 v0.5.1 v0.5.0 v0.5.0-RC2 v0.5.0-RC1 v0.5.0-M3 v0.5.0-M2 v0.5.0-M1 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0 v0.3 v0.2.1 v0.2.1-for-scala-2.11 v0.2 v0.2-for-scala-2.11 v0.1 Nothing to show New pull request Latest commit 8bc6b43 Jul 15, 2016 nicolasstucki committed on GitHub Merge pull request #2531 from sjrd/backport-named-classtag-classes … Fix #2521: Update overrides of ClassTag and Manifest for 2.12. Permalink Failed to load latest commit information. assets Fix #1970: Fix links to JavaDoc Oct 28, 2015 ci Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 cli/src/main Merge pull request #2181 from sjrd/analyzer-error-handling Jan 20, 2016 compiler/src Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 examples Fix #2361: Remove Jasmine from testingExample Jun 13, 2016 ir/src Towards 0.6.11. Jun 16, 2016 javalanglib/src/main/scala/java/lang Stack-allocate RuntimeLongs and inline their operations. Jul 8, 2016 javalib-ex-test-suite/src/test/scala/scala/scalajs/testsuite Port javalibExTestSuite to JUnit. Apr 27, 2016 javalib-ex/src/main/scala/java Add Scalastyle with a basic configuration. Jun 1, 2015 javalib/src/main/scala Implement java.util.EventObject. Jul 14, 2016 js-envs-test-kit/src/main/scala/org/scalajs/jsenv/test Fix #2376: PhantomJSEnv does not escape JS code May 10, 2016 js-envs-test-suite/src/test/scala/org/scalajs/jsenv/test Fix #2375: Move Node and Phantom CustomInitFilesTest to jsEnvTestSuite. May 3, 2016 js-envs/src/main/scala/org/scalajs/jsenv Fix #2376: PhantomJSEnv does not escape JS code May 10, 2016 junit-plugin/src/main Add junit bootstrapper classes to the currentRun.symSource map Dec 16, 2015 junit-runtime/src/main/scala Fix #2369: Fix JUnit class ignored count. May 26, 2016 junit-test Fix #2347: Add JUnit logger tests. May 26, 2016 library-aux/src/main/scala/scala/runtime Fix #2345: Bring hash implem in Statics up to date wrt BoxesRunTime. Apr 22, 2016 library/src/main/scala/scala/scalajs Fix #2513: Annotate `@JSExport` with `@field @getter @setter`. Jul 11, 2016 no-ir-check-test/src/test/scala/org/scalajs/testsuite/noircheck Port noIrCheckTest to JUnit. Apr 26, 2016 partest-suite/src/test/resources/scala/tools/partest/scalajs Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 partest/src/main/scala/scala/tools Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 project Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 sbt-plugin-test Fix #2494: Store the location of source map files as attributes. Jul 6, 2016 sbt-plugin/src/main Fix #2494: Store the location of source map files as attributes. Jul 6, 2016 scala-test-suite/src/test/resources Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 scalalib Fix #2521: Update overrides of ClassTag and Manifest for 2.12. Jul 14, 2016 scripts Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 stubs/src/main/scala Add Scalastyle with a basic configuration. Jun 1, 2015 test-adapter/src/main/scala/org/scalajs/sbttestadapter Fix #1709: Provide an HTML runner for tests Jun 8, 2016 test-interface/src/main/scala Fix #2480: Handle framework bugs in HTML runner Jul 11, 2016 test-suite Implement java.util.EventObject. Jul 14, 2016 tools Fix #2474: Check conflicting top-level exports Jul 13, 2016 .gitignore Improve the story for setting up an Eclipse development environment. Dec 11, 2015 CODINGSTYLE.md Fix #1644: Document the coding style. Jun 17, 2015 CONTRIBUTING.md Fix #1644: Document the coding style. Jun 17, 2015 DEVELOPING.md Remove JasmineTestFramework Jun 13, 2016 LICENSE LICENCE: Update copyright year. Sep 17, 2014 README.md Update, clean, and reorganize the Readme files. Jan 28, 2015 TESTING Fix #1709: Provide an HTML runner for tests Jun 8, 2016 build.sbt Remove JasmineTestFramework Jun 13, 2016 scalastyle-config.xml Fix #1805: Anchor scalastyle regex for class/object names Jul 17, 2015 README.md This is the repository for Scala.js, the Scala to JavaScript compiler. Report an issue Developer documentation Contributing guidelines License Scala.js is distributed under the Scala License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-js/scala-js"	"() - Scala to JavaScript compiler"	"true"
"JavaScript"	"repo"	"https://github.com/scala-js/scala-js"	"() - Scala to JavaScript compiler"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2409"	"154"	"219"	"GitHub - scala-js/scala-js: Scala.js, the Scala to JavaScript compiler Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 154 Star 2,409 Fork 219 scala-js/scala-js Code Issues 86 Pull requests 7 Pulse Graphs Scala.js, the Scala to JavaScript compiler https://www.scala-js.org/ 3,459 commits 1 branch 40 releases 45 contributors Scala 99.8% Other 0.2% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.6.10 v0.6.9 v0.6.8 v0.6.7 v0.6.6 v0.6.5 v0.6.5-scala-2.12.0-M3 v0.6.4 v0.6.3 v0.6.2 v0.6.1 v0.6.0 v0.6.0-RC2 v0.6.0-RC1 v0.6.0-M3 v0.6.0-M2 v0.6.0-M1 v0.5.6 v0.5.5 v0.5.4 v0.5.3 v0.5.2 v0.5.1 v0.5.0 v0.5.0-RC2 v0.5.0-RC1 v0.5.0-M3 v0.5.0-M2 v0.5.0-M1 v0.4.4 v0.4.3 v0.4.2 v0.4.1 v0.4.0 v0.3 v0.2.1 v0.2.1-for-scala-2.11 v0.2 v0.2-for-scala-2.11 v0.1 Nothing to show New pull request Latest commit 8bc6b43 Jul 15, 2016 nicolasstucki committed on GitHub Merge pull request #2531 from sjrd/backport-named-classtag-classes … Fix #2521: Update overrides of ClassTag and Manifest for 2.12. Permalink Failed to load latest commit information. assets Fix #1970: Fix links to JavaDoc Oct 28, 2015 ci Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 cli/src/main Merge pull request #2181 from sjrd/analyzer-error-handling Jan 20, 2016 compiler/src Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 examples Fix #2361: Remove Jasmine from testingExample Jun 13, 2016 ir/src Towards 0.6.11. Jun 16, 2016 javalanglib/src/main/scala/java/lang Stack-allocate RuntimeLongs and inline their operations. Jul 8, 2016 javalib-ex-test-suite/src/test/scala/scala/scalajs/testsuite Port javalibExTestSuite to JUnit. Apr 27, 2016 javalib-ex/src/main/scala/java Add Scalastyle with a basic configuration. Jun 1, 2015 javalib/src/main/scala Implement java.util.EventObject. Jul 14, 2016 js-envs-test-kit/src/main/scala/org/scalajs/jsenv/test Fix #2376: PhantomJSEnv does not escape JS code May 10, 2016 js-envs-test-suite/src/test/scala/org/scalajs/jsenv/test Fix #2375: Move Node and Phantom CustomInitFilesTest to jsEnvTestSuite. May 3, 2016 js-envs/src/main/scala/org/scalajs/jsenv Fix #2376: PhantomJSEnv does not escape JS code May 10, 2016 junit-plugin/src/main Add junit bootstrapper classes to the currentRun.symSource map Dec 16, 2015 junit-runtime/src/main/scala Fix #2369: Fix JUnit class ignored count. May 26, 2016 junit-test Fix #2347: Add JUnit logger tests. May 26, 2016 library-aux/src/main/scala/scala/runtime Fix #2345: Bring hash implem in Statics up to date wrt BoxesRunTime. Apr 22, 2016 library/src/main/scala/scala/scalajs Fix #2513: Annotate `@JSExport` with `@field @getter @setter`. Jul 11, 2016 no-ir-check-test/src/test/scala/org/scalajs/testsuite/noircheck Port noIrCheckTest to JUnit. Apr 26, 2016 partest-suite/src/test/resources/scala/tools/partest/scalajs Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 partest/src/main/scala/scala/tools Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 project Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 sbt-plugin-test Fix #2494: Store the location of source map files as attributes. Jul 6, 2016 sbt-plugin/src/main Fix #2494: Store the location of source map files as attributes. Jul 6, 2016 scala-test-suite/src/test/resources Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 scalalib Fix #2521: Update overrides of ClassTag and Manifest for 2.12. Jul 14, 2016 scripts Fix #2496: Upgrade to 2.12.0-M5. Jul 13, 2016 stubs/src/main/scala Add Scalastyle with a basic configuration. Jun 1, 2015 test-adapter/src/main/scala/org/scalajs/sbttestadapter Fix #1709: Provide an HTML runner for tests Jun 8, 2016 test-interface/src/main/scala Fix #2480: Handle framework bugs in HTML runner Jul 11, 2016 test-suite Implement java.util.EventObject. Jul 14, 2016 tools Fix #2474: Check conflicting top-level exports Jul 13, 2016 .gitignore Improve the story for setting up an Eclipse development environment. Dec 11, 2015 CODINGSTYLE.md Fix #1644: Document the coding style. Jun 17, 2015 CONTRIBUTING.md Fix #1644: Document the coding style. Jun 17, 2015 DEVELOPING.md Remove JasmineTestFramework Jun 13, 2016 LICENSE LICENCE: Update copyright year. Sep 17, 2014 README.md Update, clean, and reorganize the Readme files. Jan 28, 2015 TESTING Fix #1709: Provide an HTML runner for tests Jun 8, 2016 build.sbt Remove JasmineTestFramework Jun 13, 2016 scalastyle-config.xml Fix #1805: Anchor scalastyle regex for class/object names Jul 17, 2015 README.md This is the repository for Scala.js, the Scala to JavaScript compiler. Report an issue Developer documentation Contributing guidelines License Scala.js is distributed under the Scala License. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-js/scala-js"	"() - Scala to JavaScript compiler"	"true"
"Tools"	"Abide ★ 209 ⧗ 7"	"https://github.com/scala/scala-abide"	"Library for quick scala code checking and validation"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"220"	"39"	"23"	"GitHub - scala/scala-abide: Library for quick scala code checking and validation. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 39 Star 220 Fork 23 scala/scala-abide Code Issues 29 Pull requests 9 Pulse Graphs Library for quick scala code checking and validation. 80 commits 1 branch 0 releases 7 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show Nothing to show New pull request Latest commit ad5116b Mar 25, 2015 dragos Merge pull request #51 from rtfpessoa/presenter-extension … Allow presenter extensions Permalink Failed to load latest commit information. abide/src add presenter extensions Mar 24, 2015 macros/src Move sources to the proper directory path according to their package … Sep 29, 2014 project add presenter extensions Mar 24, 2015 rules Merge branch 'master' into pr/38 Feb 16, 2015 sbt-plugin/src/main/scala/scala/tools/abide add presenter extensions Mar 24, 2015 wiki add presenter extensions Mar 24, 2015 .gitignore Move sources to the proper directory path according to their package … Sep 29, 2014 .travis.yml Added travis integration Jul 28, 2014 README.md add presenter extensions Mar 24, 2015 README.md Abide : Lint tooling for Scala Abide aims to provide users with a simple framework for lint-like rule creation and verification. Abide rules are designed as small, self-contained units of logic which abstract concerns such as traversal and optimization away from the rule writter. Using the tool Abide is only available for sbt (and command line) for now, but will be ported to a Scala-IDE plugin and possibly to maven/gradle/etc as well. To add abide verification to an sbt project, three different options are available : Important! At this moment, abide has not been released. You need to run sbt publish-local in a local checkout of the abide repository. Sbt Plugin Activate the sbt-abide plugin in both scala 2.10 and 2.11 projects by extending your project/plugin.sbt file with addSbtPlugin(""com.typesafe"" % ""sbt-abide"" % ""0.1-SNAPSHOT"") abide requires Sbt version 0.13.5 or later. Make sure you have the following line in project/build.properties sbt.version=0.13.5  Now you need to choose the rule libraries by adding the required jars to your dependencies in your project definitions (eg. build.sbt). Notice the abide configuration at the end of the line. libraryDependencies += ""com.typesafe"" %% ""abide-core"" % ""0.1-SNAPSHOT"" % ""abide"" One can also use sbt projects as rule libraries by using dependsOn(rules % ""abide"") in the project definition. This allows one to define project-specific rules! This mode can run on scala 2.10 projects by using the compiler -Xsource:2.10 flag (automatically managed by the plugin), however one must force the use of the abide libraries version built against scala 2.11! You can do that by specifying the full cross-compiled name, instead of relying on the %% operator: libraryDependencies += ""com.typesafe"" % ""abide-core_2.11"" % ""0.1-SNAPSHOT"" % ""abide"" Compiler Plugin Abide can be activated as a compiler plugin in scala 2.11 projects by extending the sbt build file with libraryDependencies += compilerPlugin(""com.typesafe"" %% ""abide"" % ""0.1-SNAPSHOT"") scalacOptions ++= Seq(   ""-P:abide:abidecp:<some.rules.classpath>"",   ""-P:abide:ruleClass:<some.rule.Class>"",   ""-P:abide:analyzerClass:<some.analyzer.generator.Module>"",   ""-P:abide:presenterClass:<some.presenter.generator.Module>"",   ...) or simply add these options to your call to scalac to enable abide during your standard compilation run. Since sbt configurations are not available to compiler plugins, the abidecp argument is required to specify the classpath where the abide rules are located. In turn, the actual paths to these rules are specified through the ruleClass argument (which will typically appear multiple times, once per rule) and plugin analyzer generators will appear in analyzerClass arguments (also multiple instances). While slightly more complexe than the following alternative, this mode provides integration capabilities for non-sbt build tools like eclipse or maven. Command line The abide compiler plugin can also be used directly on the command line by adding the plugin to your scalac command and using the options described in the compiler plugin as cli arguments: scalac -Xplugin:<path/to/abide.jar>                            \        -P:abide:abidecp:<some.rules.classpath>                 \        -P:abide:ruleClass:<some.rule.Class>                    \        -P:abide:analyzerClass:<some.analyzer.generator.Module> \        -P:abide:presenterClass:<some.presenter.generator.Module> \        ...  Note that this feature, as in the compiler plugin case, can only be used on scala 2.11 projects. Existing plugins The abide framework comes with a few pre-made rule packages that can be selectively enabled as discussed in the previous section. The list of available packages along with the associated ivy dependency are: rules/core provided by ""com.typesafe"" %% ""abide-core"" % ""0.1-SNAPSHOT"" rules/extra provided by ""com.typesafe"" %% ""abide-extra"" % ""0.1-SNAPSHOT"" rules/akka provided by ""com.typesafe"" %% ""abide-akka"" % ""0.1-SNAPSHOT"" Extending Abide Setting aside bugfixes and basic feature modification, abide components are generally loosely coupled and self-contained to enable simple extensions to the framework. Such extensions will typically fall into one of three different categories and will vary in complexity. From simplest (and most useful extension point) to most involved: rule extension The abide framework initially ships with few rules but provides a powerful extension point to accomodate user-defined rule verification. These rules can either be defined locally (using the Project(...).dependsOn(rules % ""abide"") construction) or shared over github by submitting new rules as pull requests to this repository. directive extension Abide rules all share a minimal amount of context, namely the universe instance which defines the compiler AST cake. This context is passed to rules through the val context : Context constructor parameter that each rule must define. Directives will typically be mixed in to the shared context object and cache computation results when these can be shared between rules. analyzer extension When rule application can benefit from some global traversal logic, or partial tree analysis, these computations are performed in an Analyzer subtype. These analyzers are typically non-trivial and will be integrated into the main abide deliverable to then be shared by all rules. However, analyzers can also be provided alongside rules in plugin libraries. presenter extension The abide framework initially ships with one presenter but provides a powerful extension point to accomodate user-defined presenters. These presenters can either be defined locally (using the Project(...).dependsOn(rules % ""abide"") construction) or shared over github by submitting new presenters as pull requests to this repository. The provided extension mechanism uses a plugin architecture based on an xml description that specifies plugin capabilities. This description sits at the base of the resources directory, in abide-plugin.xml and has the following structure: <plugin>   <rule class=""some.rule.Class"" />   <rule class=""some.other.rule.Class"" />   <analyzer class=""some.analyzer.generator.Class"" />   <presenter class=""some.presenter.generator.Class"" /> </plugin> Directives don't need to (and shouldn't) be specified in the plugin description as they will be statically referenced in the source code (and they can't be dynamically subsumed like analyzers). Further Work The abide compiler plugin needs to manage rule enabling and disabling. This should be implemented in scala.tools.abide.compiler.AbidePlugin in the component.apply method by replacing the analyzer filter (first argument of scala gen(_ => true)(unit.body)) by an actual filter. Extending build tool support. Abide plugins for eclipse, gradle, maven, etc would be a nice feature extension to the framework. Such extensions should be relatively easy to implement by using the compiler plugin and manually injecting -P:abide:... command line arguments to configure the framework. Abide could also be run on the result of a presentation compiler run, and scala.tools.abide.compiler.AbidePlugin would be a good place to start for loading rules, contexts, generators, etc. One can also look at the tests to witness abide verification in action without the compiler plugin. Adding new Presenter types. For now, abide can only output warnigns as compiler warnings, but it would be nice, for example, to be able to output interactive HTML5 reports. Enabling cross-unit rules by writing a new analyzer. The base would resemble that of the FusingTraversalAnalyzer but would additionnaly require keeping track of the result of previous traversals for incremental compilation integration. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala/scala-abide"	"Library for quick scala code checking and validation"	"true"
"Tools"	"Codacy"	"https://www.codacy.com/"	"Automated Code Reviews for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Home | Codacy Toggle navigation Product Pricing Enterprise Login Sign up Review less, merge faster Check code style, security, duplication, complexity and coverage on every change while tracking code quality throughout your sprints.Sign up with GitHub Or sign up with a different account Loved by Developers Codacy saves hours in code review and code quality monitoring, from small teams to big companies. Save time in Code reviews Automatically identify new static analysis issues, code coverage, code duplication and code complexity evolution in every commit and pull request. Learn More Integrated in your Workflow Codacy is flexible and adapts to your code review process. Codacy pushes results as comments in your pull requests or as notifications in Slack or Hipchat channels. Codacy also plays nice with your Continuous Integration tools and serves as an ideal complement to your unit tests. See all Features and Integrations Track your project quality evolution Get a code quality glance at your project and track its quality evolution over time. Our dashboard answers three main questions: _What is the state of your projects code quality? _How is it evolving throughout time? _What are the hotspots in my code? Learn more What our customers say Akara Sucharitakul Principal MTS at Paypal ""We came to rely on Codacy to analyze every commit and every pull request to catch code quality issues, manage code coverage, or even new bugs before we even accept the pull request, saving us a huge amount of churn and cost."" Raúl Raja CTO at 47 Degrees ""Codacy has probably saved us over two hours a day of valuable engineering time.""Read more Xavier Fernández CTO at Expansiva Engineering ""It helps a lot because it reduces a lot of the time of code reviews while increasing the time we spend coding.The code reviews are now about code structure and its internal logic, and not about code conventions, making the reviews less repetitive and more interesting.""Read more Jules Ivanic Colisweb Lead Developer and Technical Architect ""Code quality is an everyday job; it’s the responsibility of every member of the team and Codacy is a very good assistant in that everyday job.""Read more Elton Minetto CEO at Coderockr ""With Codacy our code review process became fast and precise, especially thanks to the integration with GitHub and Slack. Now our team can receive an analysis in each commit and improve the code right away.""Read more Product Product Features Why Codacy Customers Pricing Enterprise Company About Us Security Open Source Help Documentation Getting Started Request a Demo Privacy Policy Terms and Conditions Get in touch Contact Us Blog Twitter GitHub © 2016 - Codacy Automated Code Review"	"null"	"null"	"Automated Code Reviews for Scala"	"true"
"Tools"	"Fastring ★ 73 ⧗ 4"	"https://github.com/Atry/fastring"	"Extremely fast string formatting"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"74"	"15"	"6"	"GitHub - Atry/fastring: Extremely fast string formatting Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 15 Star 74 Fork 6 Atry/fastring Code Issues 2 Pull requests 0 Pulse Graphs Extremely fast string formatting 83 commits 1 branch 6 releases Fetching contributors Scala 97.4% Shell 2.6% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show 0.2.4 0.2.2 0.2.1 0.2.0 0.1.1 0.1.0 Nothing to show New pull request Latest commit 43ab046 Apr 28, 2016 Atry Run CI for 2.11.8 and 2.12.0-M4 Permalink Failed to load latest commit information. benchmark/src/main/scala/com/dongxiguo/fastring/benchmark Add license declaration Nov 28, 2012 project Upgrade Scala and Sbt versions Mar 3, 2016 src Added fastraw""""""don't escape \ ${interpolation}"""""" syntax. Sep 10, 2013 .gitignore Merge branch 'ci' of https://github.com/Atry/ci-template-for-sbt-libr… Mar 3, 2016 .travis.yml Upgrade Scala and Sbt versions Mar 3, 2016 LICENSE Add copyright notice. Nov 27, 2012 NOTICE Add copyright notice. Nov 26, 2012 README.md Update README.md Apr 15, 2016 build.sbt Run CI for 2.11.8 and 2.12.0-M4 Apr 28, 2016 ci.sbt Setup CI for deployment Feb 1, 2016 deploy.sh.disabled Setup CI for deployment Jan 31, 2016 pubring.asc Setup CI for deployment Jan 31, 2016 version.sbt Upgrade Scala and Sbt versions Mar 3, 2016 README.md Fastring Fastring is a string formatting library for Scala. Fastring is also designed to be a template engine, and it is an excellent replacement of JSP, Scalate or FreeMarker. It's simple to use Fastring uses string interpolation syntax. For example, if you are writing a CGI page: import com.dongxiguo.fastring.Fastring.Implicits._ def printHtml(link: java.net.URL) {   val fastHtml = fast""<html><body><a href='$link'>Click Me!</a></body></html>""   print(fastHtml) }  It's extremely fast I made a benchmark. I used 4 different ways to create a 545-characters string. Fastring (fast""Concat with $something"" syntax); String concatenation (s""Concat with $something"" syntax); Handwritten StringBuilder (stringBuilder ++= ""Build from "" ++= something syntax); java.util.Formatter (f""Format with $something"" syntax). This is the result from my Intel i5-3450 computer: Fastring import com.dongxiguo.fastring.Fastring.Implicits._ def fast(a: Int) =   fast""head ${     (for (j <- 0 until 10 view) yield {       fast""baz$j $a foo ${         (for (i <- 0 until 4 view) yield {           fast""$a i=$i""         }).mkFastring("","")       } bar\n""     }).mkFastring(""<hr/>"")   } tail""  fast(0).toString Took 669 nanoseconds to generate a 545-characters string. (Simple and fast) String concatenation def s(a: Int) =   s""head ${     (for (j <- 0 until 10 view) yield {       s""baz$j $a foo ${         (for (i <- 0 until 4 view) yield {           s""$a i=$i""         }).mkString("","")       } bar\n""     }).mkString(""<hr/>"")   } tail""  s(0) Took 1738 nanoseconds to generate a 545-characters string. (Simple but slow) Handwritten StringBuilder def sb(sb: StringBuilder, a: Int) {   sb ++= ""head ""   var first = true   for (j <- 0 until 10 view) {     if (first) {       first = false     } else {       sb ++= """"<hr/>""""     }     sb ++=       ""baz"" ++= j.toString ++=       "" "" ++= a.toString ++= "" foo "";     {       var first = true       for (i <- 0 until 4 view) {         if (first) {           first = false         } else {           sb ++= "",""         }         sb ++= a.toString         sb ++= "" i=""         sb ++= i.toString       }     }     sb ++= "" bar\n""   }   sb ++= "" tail""   sb }  val s = new StringBuilder sb(s, 0) s.toString Took 537 nanoseconds to generate a 545-characters string. (Fast but too trivial) java.util.Formatter def f(a: Int) =     f""head ${       (for (j <- 0 until 10 view) yield {         f""baz$j $a foo ${           (for (i <- 0 until 4 view) yield {             f""$a i=$i""           }).mkString("","")         } bar\n""       }).mkString(""<hr/>"")     } tail""  f(0) Took 7436 nanoseconds to generate a 545-characters string. (Simple but extremely slow) Fastring is so fast because it is lazily evaluated. It avoids coping content for nested String Interpolation. Thus, Fastring is very suitable to generate complex text content(e.g. HTML, JSON). For example, in the previous benchmark for Fastring, the most of time was spent on invoking toString. You can avoid these overhead if you do not need a whole string. For example: // Faster than: print(fast""My lazy string from $something"") fast""My lazy string from $something"".foreach(print)  You can invoke foreach because Fastring is just a Traversable[String]. Utilities There is a mkFastring method for Seq: // Enable mkFastring method import com.dongxiguo.fastring.Fastring.Implicits._  // Got Fastring(""Seq.mkFastring: Hello, world"") fast""Seq.mkFastring: ${Seq(""Hello"", ""world"").mkFastring("", "")}""  // Also works, but slower: // Got Fastring(""Seq.mkString: Hello, world"") fast""Seq.mkString: ${Seq(""Hello"", ""world"").mkString("", "")}""  And a filled method for Byte, Short, Int and Long: // Enable filled method import com.dongxiguo.fastring.Fastring.Implicits._  // Got Fastring(""Int.filled:   123"") fast""Int.filled: ${123.filled(5)}""  // Got Fastring(""Int.filled: 00123"") fast""Int.filled: ${123.filled(5, '0')}""  Installation Put these lines in your build.sbt if you use Sbt: libraryDependencies += ""com.dongxiguo"" %% ""fastring"" % ""latest.release""  See http://mvnrepository.com/artifact/com.dongxiguo/fastring_2.11/0.2.4 if you use Maven or other build systems. Note that Fastring requires Scala 2.10 or 2.11. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/Atry/fastring"	"Extremely fast string formatting"	"true"
"Tools"	"Gitbucket ★ 5478 ⧗ 0"	"https://github.com/gitbucket/gitbucket"	"The easily installable GitHub clone powered by Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"5799"	"548"	"705"	"GitHub - gitbucket/gitbucket: A Git platform powered by Scala with easy installation, high extensibility & github API compatibility Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 548 Star 5,799 Fork 705 gitbucket/gitbucket Code Issues 189 Pull requests 16 Wiki Pulse Graphs A Git platform powered by Scala with easy installation, high extensibility & github API compatibility https://gitbucket.github.io/gitbucket-news/ 3,122 commits 12 branches 49 releases 91 contributors Scala 62.2% HTML 30.7% CSS 3.4% JavaScript 2.2% Java 0.8% Shell 0.7% Scala HTML CSS JavaScript Java Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags elasticsearch h2-server-mode h2_pagestore maintenance/1.11 maintenance/2.2 maintenance/3.10_h2-1.4.180 maintenance/3.10 master quill scalajs slick-3.0-merged slick-3.0 Nothing to show 4.2.1 4.2 4.1 4.0 3.14 3.13 3.12 3.11 3.10.1 3.10.1_h2-1.4.180 3.10 3.10_h2-1.4.180 3.9 3.8 3.7 3.6 3.5 3.4 3.3 3.2 3.1.1 3.1 3.0 2.8 2.7 2.6 2.5 2.4.1 2.4 2.3 2.2.1 2.2 2.1 2.0 1.13 1.12 1.11.1 1.11 1.10 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1.0 Nothing to show New pull request Latest commit 1b85d51 Jul 14, 2016 takezoe Purge Emoji support because it will be provided as plugin Permalink Failed to load latest commit information. .github correct path to CONTRIBUTING file, fixes #1128 Mar 1, 2016 contrib Update url in docs Oct 17, 2015 doc Fix doc Jul 2, 2016 project Bump sbt version to 0.13.11 Jun 19, 2016 release Bump wagon-ssh Apr 2, 2016 src Purge Emoji support because it will be provided as plugin Jul 14, 2016 .gitignore Ignore ensime files Jul 14, 2015 .travis.yml Stop PostgreSQL before Travis test May 28, 2016 LICENSE (refs #30)Add LICENSE file Jul 18, 2013 README.md Update README.md Jul 4, 2016 build.sbt (refs #1238) Add new extension point to supply assets by plugin Jul 11, 2016 command.bat Initial Import. Apr 11, 2013 sbt-launch-0.13.9.jar (refs #1083)Bump sbt launcher Feb 3, 2016 sbt.bat remove -XX:MaxPermSize from java option May 24, 2016 sbt.sh remove -XX:MaxPermSize from java option May 24, 2016 README.md GitBucket GitBucket is a Git platform powered by Scala offering: easy installation high extensibility by plugins API compatibility with Github Features The current version of GitBucket provides a basic features below: Public / Private Git repository (http and ssh access) Repository viewer and online file editing Wiki Issues / Pull request Email notification Simple user and group management with LDAP integration Plug-in system If you want to try the development version of GitBucket, see Developer's Guide. Installation GitBucket requires Java8. You have to install beforehand when it's not installed. Download latest gitbucket.war from the release page. Deploy it to the Servlet 3.0 container such as Tomcat 7.x, Jetty 8.x, GlassFish 3.x or higher. Access http://[hostname]:[port]/gitbucket/ using your web browser and logged-in with root / root. or you can start GitBucket by java -jar gitbucket.war without servlet container. In this case, GitBucket URL is http://[hostname]:8080/. You can specify following options. --port=[NUMBER] --prefix=[CONTEXTPATH] --host=[HOSTNAME] --gitbucket.home=[DATA_DIR] To upgrade GitBucket, only replace gitbucket.war after stop GitBucket. All GitBucket data is stored in HOME/.gitbucket. So if you want to back up GitBucket data, copy this directory to the other disk. About installation on Mac or Windows Server (with IIS), configuration of Apache or Nginx and also integration with other tools or services such as Jenkins or Slack, see Wiki. Plug-ins GitBucket has the plug-in system to extend GitBucket from outside of GitBucket. Some plug-ins are available now: gitbucket-gist-plugin gitbucket-announce-plugin gitbucket-h2-backup-plugin gitbucket-desktopnotify-plugin gitbucket-commitgraphs-plugin gitbucket-asciidoctor-plugin gitbucket-network-plugin You can find community plugins other than them at gitbucket community plugins. Support If you have any question about GitBucket, send it to gitter room before raise an issue. Make sure check whether there is a same question or request in the past. When raise a new issue, write subject in English at least. We can also support in Japaneses other than English at gitter room for Japanese. First priority of GitBucket is easy installation and API compatibility with GitHub, so we might reject if your request is against it. Release Notes 4.2.1 - 3 Jul 2016 Fix migration bug This is hotfix for a critical bug in migration. If you are new installation, use 4.2.0. But if you have an exisiting installation and it had been updated to 4.0 from 3.x, you must update to 4.2.1. 4.2 - 2 Jul 2016 New UI based on AdminLTE git gc Issues and Wiki have been possible to be disabled SMTP configuration test mail 4.1 - 4 Jun 2016 Generic ssh user Improve branch protection UI Default value of pull request title 4.0 - 30 Apr 2016 MySQL and PostgreSQL support Data export and import Migration system has been switched to solidbase Note: You can upgrade to GitBucket 4.0 from 3.14. If your GitBucket is 3.13 or before, you have to upgrade 3.14 at first. 3.14 - 30 Apr 2016 File attachment and search for wiki pages New extension points to add menus Content-Type of webhooks has been choosable 3.13 - 1 Apr 2016 Refresh user interface for wide screen Add pull_request key in list issues API for pull requests Add X-Hub-Signature security to webhooks Provide SHA-256 checksum for gitbucket.war 3.12 - 27 Feb 2016 New GitHub UI Improve mobile view Improve printing style Individual URL for pull request tabs SSH host configuration is separated from HTTP base URL 3.11 - 30 Jan 2016 Upgrade Scalatra to 2.4 Sidebar and Footer for Wiki Branch protection and receive hook extension point for plug-in Limit recent updated repositories list Issue actions look-alike GitHub Web API for labels Requires Java 8 3.10 - 30 Dec 2015 Move to Bootstrap3 New URL for raw contents (raw/master/doc/activity.md instead of blob/master/doc/activity.md?raw=true) Update xsbt-web-plugin Update H2 database 3.9 - 5 Dec 2015 GFM inline breaks support in Markdown WebHook on create review comment is available WebHook event trigger is selectable 3.8 - 31 Oct 2015 Moved to GitHub organization Omit diff view for large differences Repository creation API Render url as link in repository description Expand attachable file types 3.7 - 3 Oct 2015 Markdown processor has been switched to markedj from pegdown Clone in desktop button Providing MD5 and SHA-1 checksum for gitbucket.war has started 3.6 - 30 Aug 2015 User interface Improvements: Especially, commit list, issues and pull request have been updated largely. Installed plugins list has been available at the system administration console. Pages and repository list in the sidebar have been limited and more pages and repositories link is available. More reference link notation in Markdown has been supported. 3.5 - 1 Aug 2015 Octicons has been applied Global header has been enhanced. Now it's further similar to GitHub. Default compare / pull request target has been changed to the parent repository A lot of updates for gitbucket-gist-plugin 3.4 - 27 Jun 2015 Declarative style plug-in definition New extension point to add markup render go-import support 3.3 - 31 May 2015 Rich graphical diff for images File finder is available in the repository viewer Blame is displayed at the source viewer Remain user data and repositories even if user is disabled Mobile view improvement 3.2 - 3 May 2015 Directory history button Compare / pull request button Limit of activity log 3.1.1 - 4 Apr 2015 Rolled back H2 version to avoid version compatibility issue Plug-ins became possible to access ServletContext 3.1 - 28 Mar 2015 Web APIs for Jenkins github pull-request builder Improved diff view Bump Scalatra to 2.3.1, sbt to 0.13.8 3.0 - 3 Mar 2015 New plug-in system is available Connection pooling by c3p0 New branch UI Compare between specified commit ids 2.8 - 1 Feb 2015 New logo and icons New system setting options to control visibility Comment on side-by-side diff Information message on sign-in page Fork repository by group account 2.7 - 29 Dec 2014 Comment for commit and diff Fix security issue in markdown rendering Some bug fix and improvements 2.6 - 24 Nov 2014 Search box at issues and pull requests Information from administrator Pull request UI has been updated Move to TravisCI from Buildhive Some bug fix and improvements 2.5 - 4 Nov 2014 New Dashboard Change datetime format Create branch from Web UI Task list in Markdown Some bug fix and improvements 2.4.1 - 6 Oct 2014 Bug fix 2.4 - 6 Oct 2014 New UI is applied to Issues and Pull requests Side-by-side diff is available Fix relative path problem in Markdown links and images Plugin System is disabled in default Some bug fix and improvements 2.3 - 1 Sep 2014 Scala based plugin system Embedded Jetty war extraction directory moved to GITBUCKET_HOME/tmp Some bug fix and improvements 2.2.1 - 5 Aug 2014 Bug fix 2.2 - 4 Aug 2014 Plug-in system is available Move to Scala 2.11, Scalatra 2.3 and Slick 2.1 tar.gz export for repository contents LDAP authentication improvement (mail address became optional) Show news feed of a private repository to members Some bug fix and improvements 2.1 - 6 Jul 2014 Upgrade to Slick 2.0 from 1.9 Base part of the plug-in system is merged Many bug fix and improvements 2.0 - 31 May 2014 Modern Github UI Preview in AceEditor Select lines by clicking line number in blob view 1.13 - 29 Apr 2014 Direct file editing in the repository viewer using AceEditor File attachment for issues Atom feed of user activity Fix some bugs 1.12 - 29 Mar 2014 SSH repository access is available Allow users can create and management their groups Git submodule support Close issues via commit messages Show repository description below the name on repository page Fix presentation of the source viewer Upgrade to sbt 0.13 Fix some bugs 1.11.1 - 06 Mar 2014 Bug fix 1.11 - 01 Mar 2014 Base URL for redirection, notification and repository URL box is configurable Remove --https option because it's possible to substitute in the base url Headline anchor is available for Markdown contents such as Wiki page Improve H2 connectivity Label is available for pull requests not only issues Delete branch button is added Repository icons are updated Select lines of source code by URL hash like #L10 or #L10-L15 in repository viewer Display reference to issue from others in comment list Fix some bugs 1.10 - 01 Feb 2014 Rename repository Transfer repository owner Change default data directory to HOME/.gitbucket from HOME/gitbucket to avoid problem like #243, but if data directory already exist at HOME/gitbucket, it continues being used. Add LDAP display name attribute Response performance improvement Fix some bugs 1.9 - 28 Dec 2013 Display GITBUCKET_HOME on the system settings page Fix some bugs 1.8 - 30 Nov 2013 Add user and group deletion Improve pull request performance Pull request synchronization (when source repository is updated after pull request, it's applied to the pull request) LDAP StartTLS support Enable hard wrapping in Markdown Add new some options to specify the data directory. See details in Wiki. Fix some bugs 1.7 - 26 Oct 2013 Support working on Java6 in embedded Jetty mode Add --host option to bind specified host name in embedded Jetty mode Add --https=true option to force https scheme when using embedded Jetty mode at the back of https proxy Add full name as user property Change link color for absent Wiki pages Add ZIP download button to the repository viewer tab Improve ZIP exporting performance Expand issue and comment textarea for long text automatically Add conflict detection in Wiki Add reverting wiki page from history Match committer to user name by email address Mail notification sender is customizable Add link to changeset in refs comment for issues Fix some bugs 1.6 - 1 Oct 2013 Web hook Performance improvement for pull request Executable war file Specify suitable Content-Type for downloaded files in the repository viewer Fix some bugs 1.5 - 4 Sep 2013 Fork and pull request LDAP authentication Mail notification Add an option to turn off the gravatar support Add the branch tab in the repository viewer Encoding auto detection for the file content in the repository viewer Add favicon, header logo and icons for the timeline Specify data directory via environment variable GITBUCKET_HOME Fix some bugs 1.4 - 31 Jul 2013 Group management Repository search for code and issues Display user related issues on the dashboard Display participants avatar of issues on the issue page Performance improvement for repository viewer Alert by milestone due date H2 database administration console Fix some bugs 1.3 - 18 Jul 2013 Batch updating for issues Display assigned user on issue list User icon and Gravatar support Convert @xxxx to link to the account page Add copy to clipboard button for git clone URL Allow multi-byte characters as wiki page name Allow to create the empty repository Fix some bugs 1.2 - 09 Jul 2013 Add activity timeline Bugfix for Git 1.8.1.5 or later Allow multi-byte characters as label Fix some bugs 1.1 - 05 Jul 2013 Fix some bugs Upgrade to JGit 3.0 1.0 - 04 Jul 2013 This is a first public release Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/gitbucket/gitbucket"	"The easily installable GitHub clone powered by Scala"	"true"
"Tools"	"sbt"	"http://www.scala-sbt.org/"	"() - The interactive build tool for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2670"	"168"	"501"	"GitHub - sbt/sbt: sbt, the interactive build tool Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 168 Star 2,670 Fork 501 sbt/sbt Code Issues 509 Pull requests 23 Wiki Pulse Graphs sbt, the interactive build tool http://scala-sbt.org 4,696 commits 19 branches 127 releases 141 contributors Scala 97.7% Java 2.0% Other 0.3% Scala Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 1.0.x Switch branches/tags Branches Tags 0.7 0.10 0.12 0.13 0.13.12 1.0.x backport/internal-tracking fix/1616 wip/cablecar wip/derived-settings wip/ivy-parallel-artifact-download wip/josh-interface-specific-sources-clean wip/multi-vm-tests wip/no-setting-bind wip/old-1.0 wip/performance-work wip/scripted-plugin-test-suite wip/snapshot-classifier wip/unify-javadoc Nothing to show v1.0.0-M4 v1.0.0-M3 v1.0.0-M2 v1.0.0-M1 v0.99.0-reboot v0.13.12-RC1 v0.13.12-M1 v0.13.11 v0.13.10 v0.13.10-RC3 v0.13.10-RC2 v0.13.10-RC1 v0.13.9 v0.13.9-RC3 v0.13.9-RC2 v0.13.9-RC1 v0.13.9-M4 v0.13.9-M3 v0.13.9-M2 v0.13.9-M1 v0.13.8 v0.13.8-RC1 v0.13.8-M6 v0.13.8-M5 v0.13.8-M4 v0.13.8-M3 v0.13.8-M2 v0.13.8-M1 v0.13.7 v0.13.7-RC4 v0.13.7-RC3 v0.13.7-RC2 v0.13.7-RC1 v0.13.7-M3 v0.13.7-M2 v0.13.7-M1 v0.13.6 v0.13.6-RC3 v0.13.6-RC2 v0.13.6-RC1 v0.13.6-MSERVER-4 v0.13.6-MSERVER-3 v0.13.6-MSERVER-2 v0.13.6-MSERVER-1 v0.13.6-M2 v0.13.6-M1 v0.13.5 v0.13.5-RC4 v0.13.5-RC3 v0.13.5-RC2 v0.13.5-RC1 v0.13.5-M4 v0.13.5-M3 v0.13.5-M2 v0.13.5-M1 v0.13.2 v0.13.2-RC3 v0.13.2-RC2 v0.13.2-RC1 v0.13.2-MSERVER-1 v0.13.2-M3 v0.13.2-M2 v0.13.2-M1 v0.13.1 v0.13.1-RC5 v0.13.1-RC4 v0.13.1-RC3 v0.13.1-RC2 v0.13.1-RC1 v0.13.1-M1 v0.13.0 v0.13.0-for-scala-2.11.0-M5 v0.13.0-RC5 v0.13.0-RC4 v0.13.0-RC3 v0.13.0-RC2 v0.13.0-RC1 v0.13.0-M2 v0.13.0-M1 v0.13.0-Beta2 v0.12.4 v0.12.4-RC3 v0.12.4-RC2 v0.12.4-RC1 v0.12.4-M1 v0.12.3 v0.12.3-RC2 v0.12.3-RC1 v0.12.2 v0.12.2-RC2 v0.12.2-RC1 v0.12.1 v0.12.1-RC2 v0.12.1-RC1 v0.12.0 v0.12.0-RC4 v0.12.0-RC3 v0.12.0-RC2 v0.12.0-RC1 v0.12.0-M2 Nothing to show New pull request Latest commit 5abccae Jul 14, 2016 eed3si9n committed on GitHub Merge pull request #2673 from dwijnand/leave-update-options-alone … Don't redefine updateOptions to ignore ThisBuild Permalink Failed to load latest commit information. launch Include new repositories to grab sbt May 8, 2016 licenses move remaining pieces of sbt subproject to sbt_pending and fix notices Sep 21, 2010 main-actions/src Remove unused vals/defs Jul 12, 2016 main-command/src/main/scala Remove unused vals/defs Jul 12, 2016 main-settings/src Remove unused vals/defs Jul 12, 2016 main Merge pull request #2673 from dwijnand/leave-update-options-alone Jul 14, 2016 notes Typo Jun 24, 2016 project Remove unused vals/defs Jul 12, 2016 run Remove unused vals/defs Jul 12, 2016 sbt/src Merge pull request #2673 from dwijnand/leave-update-options-alone Jul 15, 2016 scripted Remove unused vals/defs Jul 12, 2016 src/main/conscript rename conscripted app from sbt to xsbt May 8, 2016 tasks-standard Remove unused imports Jul 12, 2016 tasks Remove unused imports Jul 12, 2016 testing Remove unused imports Jul 12, 2016 .gitattributes Added .gitattributes file. Apr 25, 2013 .gitignore .gitignore cleaned Oct 3, 2014 .travis.yml Update mima usage in Travis Jul 7, 2016 CONTRIBUTING.md CONTRIBUTING.md: Fix error in scripted test command May 2, 2016 ISSUE_TEMPLATE.md Add ""Clean history"" to contributing with links Feb 19, 2016 LICENSE Update CONTRIBUTING.md Apr 14, 2014 MIGRATION.md Update scripted tests Jun 21, 2016 NOTICE Update CONTRIBUTING.md Apr 14, 2014 PULL_REQUEST_TEMPLATE.md Add ""Clean history"" to contributing with links Feb 19, 2016 README.md readme May 8, 2016 build.sbt Don't use test artifacts from other projects May 12, 2016 README.md sbt sbt is a build tool for Scala, Java, and more. For general documentation, see http://www.scala-sbt.org/. sbt 1.0.x This is the 1.0.x series of sbt. The source code of sbt is split across several Github repositories, including this one. sbt/io hosts sbt.io module. sbt/util hosts a collection of internally used modules. sbt/librarymanagement hosts sbt.librarymanagement module that wraps Ivy. sbt/zinc hosts Zinc, an incremental compiler for Scala. sbt/sbt, this repository hosts modules that implements the build tool. Other links Setup: Describes getting started with the latest binary release. FAQ: Explains how to get help and more. sbt/sbt-zero-seven: hosts sbt 0.7.7 and earlier versions Issues and Pull Requests Please read CONTRIBUTING carefully before opening a GitHub Issue. The short version: try searching or asking on StackOverflow and sbt-dev. license See LICENSE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sbt/sbt"	"() - The interactive build tool for Scala"	"true"
"Tools"	"repo"	"https://github.com/sbt/sbt"	"() - The interactive build tool for Scala"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"2670"	"168"	"501"	"GitHub - sbt/sbt: sbt, the interactive build tool Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 168 Star 2,670 Fork 501 sbt/sbt Code Issues 509 Pull requests 23 Wiki Pulse Graphs sbt, the interactive build tool http://scala-sbt.org 4,696 commits 19 branches 127 releases 141 contributors Scala 97.7% Java 2.0% Other 0.3% Scala Java Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: 1.0.x Switch branches/tags Branches Tags 0.7 0.10 0.12 0.13 0.13.12 1.0.x backport/internal-tracking fix/1616 wip/cablecar wip/derived-settings wip/ivy-parallel-artifact-download wip/josh-interface-specific-sources-clean wip/multi-vm-tests wip/no-setting-bind wip/old-1.0 wip/performance-work wip/scripted-plugin-test-suite wip/snapshot-classifier wip/unify-javadoc Nothing to show v1.0.0-M4 v1.0.0-M3 v1.0.0-M2 v1.0.0-M1 v0.99.0-reboot v0.13.12-RC1 v0.13.12-M1 v0.13.11 v0.13.10 v0.13.10-RC3 v0.13.10-RC2 v0.13.10-RC1 v0.13.9 v0.13.9-RC3 v0.13.9-RC2 v0.13.9-RC1 v0.13.9-M4 v0.13.9-M3 v0.13.9-M2 v0.13.9-M1 v0.13.8 v0.13.8-RC1 v0.13.8-M6 v0.13.8-M5 v0.13.8-M4 v0.13.8-M3 v0.13.8-M2 v0.13.8-M1 v0.13.7 v0.13.7-RC4 v0.13.7-RC3 v0.13.7-RC2 v0.13.7-RC1 v0.13.7-M3 v0.13.7-M2 v0.13.7-M1 v0.13.6 v0.13.6-RC3 v0.13.6-RC2 v0.13.6-RC1 v0.13.6-MSERVER-4 v0.13.6-MSERVER-3 v0.13.6-MSERVER-2 v0.13.6-MSERVER-1 v0.13.6-M2 v0.13.6-M1 v0.13.5 v0.13.5-RC4 v0.13.5-RC3 v0.13.5-RC2 v0.13.5-RC1 v0.13.5-M4 v0.13.5-M3 v0.13.5-M2 v0.13.5-M1 v0.13.2 v0.13.2-RC3 v0.13.2-RC2 v0.13.2-RC1 v0.13.2-MSERVER-1 v0.13.2-M3 v0.13.2-M2 v0.13.2-M1 v0.13.1 v0.13.1-RC5 v0.13.1-RC4 v0.13.1-RC3 v0.13.1-RC2 v0.13.1-RC1 v0.13.1-M1 v0.13.0 v0.13.0-for-scala-2.11.0-M5 v0.13.0-RC5 v0.13.0-RC4 v0.13.0-RC3 v0.13.0-RC2 v0.13.0-RC1 v0.13.0-M2 v0.13.0-M1 v0.13.0-Beta2 v0.12.4 v0.12.4-RC3 v0.12.4-RC2 v0.12.4-RC1 v0.12.4-M1 v0.12.3 v0.12.3-RC2 v0.12.3-RC1 v0.12.2 v0.12.2-RC2 v0.12.2-RC1 v0.12.1 v0.12.1-RC2 v0.12.1-RC1 v0.12.0 v0.12.0-RC4 v0.12.0-RC3 v0.12.0-RC2 v0.12.0-RC1 v0.12.0-M2 Nothing to show New pull request Latest commit 5abccae Jul 14, 2016 eed3si9n committed on GitHub Merge pull request #2673 from dwijnand/leave-update-options-alone … Don't redefine updateOptions to ignore ThisBuild Permalink Failed to load latest commit information. launch Include new repositories to grab sbt May 8, 2016 licenses move remaining pieces of sbt subproject to sbt_pending and fix notices Sep 21, 2010 main-actions/src Remove unused vals/defs Jul 12, 2016 main-command/src/main/scala Remove unused vals/defs Jul 12, 2016 main-settings/src Remove unused vals/defs Jul 12, 2016 main Merge pull request #2673 from dwijnand/leave-update-options-alone Jul 14, 2016 notes Typo Jun 24, 2016 project Remove unused vals/defs Jul 12, 2016 run Remove unused vals/defs Jul 12, 2016 sbt/src Merge pull request #2673 from dwijnand/leave-update-options-alone Jul 15, 2016 scripted Remove unused vals/defs Jul 12, 2016 src/main/conscript rename conscripted app from sbt to xsbt May 8, 2016 tasks-standard Remove unused imports Jul 12, 2016 tasks Remove unused imports Jul 12, 2016 testing Remove unused imports Jul 12, 2016 .gitattributes Added .gitattributes file. Apr 25, 2013 .gitignore .gitignore cleaned Oct 3, 2014 .travis.yml Update mima usage in Travis Jul 7, 2016 CONTRIBUTING.md CONTRIBUTING.md: Fix error in scripted test command May 2, 2016 ISSUE_TEMPLATE.md Add ""Clean history"" to contributing with links Feb 19, 2016 LICENSE Update CONTRIBUTING.md Apr 14, 2014 MIGRATION.md Update scripted tests Jun 21, 2016 NOTICE Update CONTRIBUTING.md Apr 14, 2014 PULL_REQUEST_TEMPLATE.md Add ""Clean history"" to contributing with links Feb 19, 2016 README.md readme May 8, 2016 build.sbt Don't use test artifacts from other projects May 12, 2016 README.md sbt sbt is a build tool for Scala, Java, and more. For general documentation, see http://www.scala-sbt.org/. sbt 1.0.x This is the 1.0.x series of sbt. The source code of sbt is split across several Github repositories, including this one. sbt/io hosts sbt.io module. sbt/util hosts a collection of internally used modules. sbt/librarymanagement hosts sbt.librarymanagement module that wraps Ivy. sbt/zinc hosts Zinc, an incremental compiler for Scala. sbt/sbt, this repository hosts modules that implements the build tool. Other links Setup: Describes getting started with the latest binary release. FAQ: Explains how to get help and more. sbt/sbt-zero-seven: hosts sbt 0.7.7 and earlier versions Issues and Pull Requests Please read CONTRIBUTING carefully before opening a GitHub Issue. The short version: try searching or asking on StackOverflow and sbt-dev. license See LICENSE. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sbt/sbt"	"() - The interactive build tool for Scala"	"true"
"Tools"	"scala-trace-debug ★ 39 ⧗ 0"	"https://github.com/JohnReedLOL/scala-trace-debug"	"Multithreaded print debug tool"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"43"	"2"	"1"	"GitHub - JohnReedLOL/scala-trace-debug: Locates print/log statements, aids in print debugging. Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 2 Star 43 Fork 1 JohnReedLOL/scala-trace-debug Code Issues 3 Pull requests 1 Pulse Graphs Locates print/log statements, aids in print debugging. http://johnreedlol.github.io/scala-trace-debug/ 326 commits 7 branches 0 releases 1 contributor Scala 67.5% XSLT 18.8% Java 9.6% CSS 4.1% Scala XSLT Java CSS Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.1.0-legacy 3.0 experimental gh-pages master refactor trimed Nothing to show Nothing to show New pull request Latest commit ec037c1 Jul 3, 2016 JohnReedLOL committed on GitHub Added video demo Permalink Failed to load latest commit information. project java-1.7.0-openjdk-amd64 Jun 12, 2016 src optimized implicits with Value Classes May 21, 2016 target java-1.7.0-openjdk-amd64 Jun 12, 2016 .gitattributes Log.find now works with Java arrays Apr 21, 2016 .gitignore bumped up to 0.2.2 Apr 17, 2016 Attribution Forgot MasGui :-) Jun 9, 2016 Build.scala made function names more Java friendly Apr 26, 2016 CHANGE_LOG.md java-1.7.0-openjdk-amd64 Jun 12, 2016 LICENSE changed the license to apache Mar 24, 2016 README.md Added video demo Jul 3, 2016 USE_WITH_IDE.md Update USE_WITH_IDE.md Apr 22, 2016 build.sbt java-1.7.0-openjdk-amd64 Jun 12, 2016 scalastyle-config.xml style check Apr 26, 2016 README.md scala-trace-debug Make multithreaded print debugging easier than ever with scala trace debug. Provides human-friendly prints, traces, assertions, container printing, source code printing, and log output. Table of Contents Finding log statements Getting started Video Demo Java Examples Scala Examples Requirements Instructions Logger Incorporation Bonus Features Performance Finding your log statements: ^ Just add ""Pos"" ^ Getting started: 1. Copy this into your SBT ""build.sbt"" file: resolvers += ""johnreed2 bintray"" at ""http://dl.bintray.com/content/johnreed2/maven""  libraryDependencies += ""scala.trace"" %% ""scala-trace-debug"" % ""2.2.17"" Note: If you get: NoClassDefFoundError: scala/reflect/runtime/package ... Caused by: java.lang.ClassNotFoundException Add: libraryDependencies += ""org.scala-lang"" % ""scala-reflect"" % scalaVersion.value 2. Use this Maven dependency: <dependency>   <groupId>scala.trace</groupId>   <artifactId>scala-trace-debug_2.11</artifactId>   <version>2.2.14</version>   <type>pom</type> </dependency> 3. Copy-paste a jar file located in the target folder. Java users need to add this dependency to the maven build. Copy-pasting the jar works too. All the functions that a Java user can call are in here. Video Demo: https://www.youtube.com/watch?v=nL_11tu3Vr0&feature=youtu.be&t=49m00s Java Examples: ^ Note that all my stack traces are off by one. This only happens when the methods are called from Java. To get around this, specify ""2"" for last parameter (2 lines of stack trace). ^ If you just want to copy-paste, Java example is here. Master Shutoff Switch (Java Capable): If you set the environment variable ENABLE_TRACE_DEBUG to false, it will disable all printing and assertions. A system property may also be used. ""The system property takes precedence over the environment variable"". The preprocessor will also replace all calls to Log.find and Pos with an empty String at compile time. Runtime Switches (Java Capable): Debug.traceErrOn/Off() Debug.traceOutOn/Off() Debug.fatalAssertOn/Off() Debug.nonFatalAssertOn/Off() // assertNonFatal = check Debug.setElementsPerRow() // For container printing Scala Examples: Without logger: ^ Note that this is an old screenshot. The import name was changed to scala.trace. ^ With logger: ^ The left side in parenthesis is the name of a variable; the right side (after ""->"") is the contents. ^ If you just want to copy-paste, Scala example is here. Requirements: Scala 2.10.4 or higher (or Java 8+) Any IDE or text editor that supports stack trace highlighting Instructions (for IntelliJ IDE): Add the library dependency or grab the jar file from the target/scala-2.11 folder. import scala.trace._ Place some calls to scala trace debug and click the green 'Debug' (Shift+F9) button and follow the stack traces in the console. Use the IntelliJ console arrows to navigate up and down the stack traces. Logger Incorporation: Log.find is designed to be used with a logger. Does not incur the overhead of a full stack trace. Debug methods can be called from Java code and without a logger. All calls to Debug.trace, Debug.assert, etc. return a String that can be passed into a logger. SDebug stands for ""Scala Debug"". It provides special debug methods that are only available in Scala (macros, source code printing, etc). You can disable printing to standard out and standard error via Debug.disableEverything_!. Debug methods will still return a String that you can pass into a logger. Container Printing: ^ Note the jar file name, scalatest_2.11, in the stack trace. ^ ^ Container printing works for any Scala container. To pass in Java containers, import scala.collection.JavaConversions._ ^ Cheat Sheet / Examples: Methods available through implicit conversion Example functions (old): http://pastebin.com/2e1JN1De ^ For more examples, see Main.scala, which you can run with sbt test:run Method Chaining: Add-on methods available through implicit conversion return the object they were called upon so that you can use them inside an expression or chain them together. import scala.trace.implicitlyTraceable ... val foo = true if( foo.trace ) { ... }  import scala.trace.implicitlyPrintable ... val foobar = ""foo"".trace().concat(""bar"").println() // Chaining.  More features: Desugared macro expression tracing: ^ Useful if you have a line like ""object method object param"" and you can't find where the dot and parenthesis go ^ ^ Note that this is an old screenshot. These macro methods now use SDebug (Scala Debug) instead of Debug. ^ Code tracing and assertions: ^ Useful if you do not want to repeat the name of a variable in a print statement. ^ ^ Note that this is an old screenshot. These macro methods now use SDebug (Scala Debug) instead of Debug. ^ Output formatting (beta): ^ Useful if you have a giant ""wall"" of text in std output and you want to inset line breaks programatically. ^ Use in practice: For use in practice, see this link To only add prints, import scala.trace.implicitlyPrintable To only add traces, import scala.trace.implicitlyTraceable If only add asserts, import scala.trace.implicitlyAssertable To add prints, traces, and asserts, import scala.trace._ Performance: No overhead for no stack trace. ""foo"".trace(0) // no call to Thread.currentThread.getStackTrace() Note that calls to Log.find are faster than calls to Debug.trace, but Log.find is limited to one line. Code layout: Currently all the actual printing is done in Printer.scala, all the implicit conversions are in package.scala, and all the calls to the ""Debug"" object are in Debug.scala Links (Old): See ScalaDoc in source code for in detail documentation. See also: http://stackoverflow.com/questions/36194905/how-can-we-trace-expressions-print-statements-with-line-numbers-in-scala/36194986#36194986 http://stackoverflow.com/questions/4272797/debugging-functional-code-in-scala/36287172#36287172 Old version of this library: https://www.reddit.com/r/scala/comments/4aeqvh/debug_trace_library_needs_users_review/ Less old version of this library: https://www.reddit.com/r/scala/comments/4fap0r/making_debugging_easier/ Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/JohnReedLOL/scala-trace-debug"	"Multithreaded print debug tool"	"true"
"Tools"	"Scalariform ★ 122 ⧗ 22"	"https://github.com/daniel-trinh/scalariform"	"Scala source code formatter"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"121"	"11"	"119"	"GitHub - daniel-trinh/scalariform: Scala source code formatter Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 11 Star 121 Fork 119 daniel-trinh/scalariform forked from scala-ide/scalariform Code Issues 0 Pull requests 5 Wiki Pulse Graphs Scala source code formatter http://mdr.github.com/scalariform/ 514 commits 35 branches 9 releases Fetching contributors Scala 98.0% XSLT 1.4% Other 0.6% Scala XSLT Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags 0.0.5 0.0.6 0.0.7 0.0.8 0.0.9 0.1.0 0.1.1 0.1.2 0.1.3 0.1.4 0.1.5-SNAPSHOT 0.1.5 0.1.6-SNAPSHOT 0.1.8 2.12-support 60-parse-error-on-case-expressions alignMethodCalls_Issue53 alignParams_Issue61 alignParams_Issue62_WIP annotation_next_line_integration better_align_parameters daniel-trinh/40-refactor-fqn daniel-trinh/49-cli-to-maven daniel-trinh/52-prepare-0.1.6 forexpander gh-pages issue7_issue8_issue9 issue73_integration lexer-perf master origin readme_updates sbt10 update_travis_for_2.11 wip Nothing to show 0.1.7 0.1.6 0.0.7-tag 0.0.6-tag 0.0.5 0.0.4 0.0.3 0.0.2 0.0.1 Nothing to show New pull request Pull request Compare This branch is 2 commits ahead, 105 commits behind scala-ide:master. Latest commit 3ba7738 Mar 1, 2016 daniel-trinh Update README.rst Permalink Failed to load latest commit information. cli correct checkFile exit status Aug 25, 2015 docs Update versions for 0.1.5-SNAPSHOT Apr 24, 2013 misc remove unused import Dec 8, 2015 notes remove changelist for #62 Jan 17, 2014 project update version Dec 10, 2015 scalariform.feature update changelog, readme, and pom files for 0.1.8 Dec 8, 2015 scalariform.update update changelog, readme, and pom files for 0.1.8 Dec 9, 2015 scalariform update changelog, readme, and pom files for 0.1.8 Dec 9, 2015 scripts tweak version script list May 7, 2012 .ensime Add .ensime Jan 26, 2011 .gitignore remove .idea files Feb 18, 2014 .travis.yml add scala 2.11 to travis Dec 7, 2015 CHANGELOG update changelog, readme, and pom files for 0.1.8 Dec 9, 2015 CONTRIBUTORS Add option to support CompactControlReadability style -- thanks to by… Jul 18, 2011 LICENCE First commit Apr 11, 2010 README.rst Update README.rst Mar 1, 2016 formatterPreferences.properties build cleanup, upgrade sbt-scalariform Dec 6, 2015 pom.xml update changelog, readme, and pom files for 0.1.8 Dec 9, 2015 README.rst NOTICE: We've moved! Scalariform is now officially maintained by scala-ide. All further developments will be happening in their branch. Scalariform Scalariform is a code formatter for Scala. It's available as a library, a stand-alone command line tool, or via integrations with various editors and build tools (listed below). The plan is to add preferences and features as and when people ask for them, so please do raise a Github issue if it doesn't format your code the way you'd like it, and I'll see what I can do. Scalariform is licenced under The MIT Licence. Installing with Homebrew (for OS X users) Mac OS X users can install the scalariform CLI tool using the Homebrew package manager. brew install scalariform  Or, if you would like to install the latest development release: brew install --HEAD scalariform  Packaging an executable JAR If you would like to package scalariform for use on the command line with java -jar, clone the repo and perform the following simple steps: sbt ""project cli"" ""assembly""  sbt will build one jar with all the dependencies and put it in cli/target/scala-$your_scala_version/cli-assembly-$scalariform_version.jar  You can copy this to a location in your path and execute it as follows: java -jar /home/me/bin/cli-assembly-$scalariform_version.jar -f -q +compactControlReadability +alignParameters +alignSingleLineCaseStatements +doubleIndentClassDeclaration +preserveDanglingCloseParenthesis +rewriteArrowSymbols +preserveSpaceBeforeArguments --stdout ~/myproject/src/main/scala/Stuff.scala > Stuff.scala  Integration with sbt A version for sbt >= 0.13.x has been written by Peter Vlugter: https://github.com/daniel-trinh/sbt-scalariform Please see https://github.com/sbt/sbt-scalariform for older versions of sbt. Usage within a project Have a use for the scalariform source code directly? You can use it as a build dependency: ""org.scalariform"" %% ""scalariform"" % ""0.1.8""  Integration with Eclipse Scala IDE for Eclipse uses Scalariform for code formatting: Right click in the editor -> Source -> Format Press Ctrl-Shift-F If you select some lines, only those will be formatted. You can also configure formatting to be run as a save action (Window -> Preferences -> Java -> Editor -> Save Actions). To set preferences, go to either Window -> Preferences -> Scala -> Editor -> Formatter Project -> Properties -> Scala Formatter Integration with Emacs/ENSIME ""ENSIME uses the Scalariform library to format Scala sources. Type C-c C-v f to format the current buffer."" http://aemon.com/file_dump/ensime_manual.html#tth_sEc4.8 Integration with jEdit See ScalaSidekick by Stefan Ettrup: Run Plugins -> scalaSidekickPlugin -> Format Scala File Integration with Maven There is a Maven plugin to run Scalariform contributed by Adam Crain on scala-tools. Usage: <plugin>   <groupId>org.scalariform</groupId>   <artifactId>scalariform-maven-plugin</artifactId>   <version>0.1.4</version>   <executions>     <execution>       <phase>process-sources</phase>       <goals>         <goal>format</goal>       </goals>       <configuration>         <rewriteArrowSymbols>true</rewriteArrowSymbols>       </configuration>     </execution>   </executions> </plugin>  Integration with TextMate See Mads Jensen's Scala TextMate bundle: http://github.com/mads379/scala.tmbundle Reformat using Ctrl-Shift-H. Use with Vim While there is no specific Vim integration at present, you can use Scalariform as an external formatter for the gq command by adding the following to .vimrc au BufEnter *.scala setl formatprg=java\ -jar\ /home/me/bin/scalariform.jar\ -f\ -q\ +compactControlReadability\ +alignParameters\ +alignSingleLineCaseStatements\ +doubleIndentClassDeclaration\ +preserveDanglingCloseParenthesis\ +rewriteArrowSymbols\ +preserveSpaceBeforeArguments\ --stdin\ --stdout  Or, if you don't like escaping spaces, you can set up a mapping: map ,st :%!java -jar /home/me/bin/scalariform.jar -f -q +compactControlReadability +alignParameters +alignSingleLineCaseStatements +doubleIndentClassDeclaration +preserveDanglingCloseParenthesis +rewriteArrowSymbols +preserveSpaceBeforeArguments --stdin --stdout <CR>  You can create your own executable scalariform.jar by following the instructions at the top of this file, in ""Packaging an executable JAR."" Command line tool https://github.com/mdr/scalariform/wiki/Command-line-tool Library https://github.com/mdr/scalariform/wiki/Library Preferences alignParameters Default: false Align class/function parameters (modifiers and name, type, and defaults) in three columns. For example, if false, then: class Person(name: String,   age: Int = 24,   birthdate: Date,   astrologicalSign: String = ""libra"",   shoeSize: Int,   favoriteColor: java.awt.Color )  If true, then: class Person(   name:             String,   age:              Int            = 24,   birthdate:        Date,   astrologicalSign: String         = ""libra"",   shoeSize:         Int,   favoriteColor:    java.awt.Color )  This will also place the ""implicit"" keyword in parameters on its own line, whenever the parameter being formatted contains a newline: For example, if false, then: def formatBirthDate(   implicit birthdate: Date = Date(""11/11/11""),   birthtime: Time ): DateTime  If true, then: def formatBirthDate(   implicit   birthdate: Date = Date(""11/11/11""),   birthtime: Time ): DateTime  This option is disabled if indentWithTabs is true. alignArguments Default: false Aligns multi-line arguments For example, if false, then: Cake(candles = 10,   frostingFlavor = Vanilla,   layerFlavor = Chocolate,   iceCream = true )  If true, then: Cake(   candles        = 10,   frostingFlavor = Vanilla,   layerFlavor    = Chocolate,   iceCream       = true )  This option is disabled if indentWithTabs is true. alignSingleLineCaseStatements Default: false Align the arrows of consecutive single-line case statements. For example, if true, then: a match {   case b => 1   case ccc => 2   case dd => 3 }  Is reformatted as: a match {   case b   => 1   case ccc => 2   case dd  => 3 }  This option is disabled if indentWithTabs is true. alignSingleLineCaseStatements.maxArrowIndent Default: 40 When alignSingleLineCaseStatements is true, this is a limit on the number of spaces that can be inserted before an arrow to align it with other case statements. This can be used to avoid very large gaps, e.g.: a match {   case Some(wibble, wobble) if wibble + wibble > wobble * wibble => 1   case ccc                                                       => 2 }  compactControlReadability Default: false When compactControlReadability is true, then if/else and try/catch/finally control structures will be formatted using Compact Control Readability style if (x == y) {   foo() } else if (y == z) {   bar() } else {   baz() }  try {   foo() } catch {   case _ => bar() } finally {   baz() }  compactStringConcatenation Default: false Omit spaces when formatting a '+' operator on String literals. For example, if false, then: ""Hello "" + name + ""!""  If true, then: ""Hello ""+name+""!""  The Scala Style Guide recommends that operators, ""should always be invoked using infix notation with spaces separated the target"". doubleIndentClassDeclaration Default: false With this set to true, class (and trait / object) declarations will be formatted as recommended by the Scala Style Guide. That is, if the declaration section spans multiple lines, it will be formatted so that either the parameter section or the inheritance section is doubly indented. This provides a visual distinction from the members of the class. For example: class Person(   name: String,   age: Int,   birthdate: Date,   astrologicalSign: String,   shoeSize: Int,   favoriteColor: java.awt.Color)     extends Entity     with Logging     with Identifiable     with Serializable {   def firstMethod = ... }  Or: class Person(     name: String,     age: Int,     birthdate: Date,     astrologicalSign: String,     shoeSize: Int,     favoriteColor: java.awt.Color) {   def firstMethod = ... }  formatXml Default: true Format embedded XML literals; if false they will be left untouched. indentLocalDefs Default: false If true, indent local methods an extra level, with the intention of distinguishing them from other statements. For example,: class A {   def find(...) = {     val x = ...       def find0() = {         ...       }     find0(...)   } }  indentPackageBlocks Default: true Whether to indent package blocks. For example, if true: package foo {   package bar {     class Baz   } }  Else if false: package foo { package bar { class Baz } }  indentSpaces Default: 2 The number of spaces to use for each level of indentation. This option is ignored if indentWithTabs is true. indentWithTabs Default: false Use a tab for each level of indentation. When set to true, this ignores any setting given for indentSpaces. In addition, for the moment, alignSingleLineCaseStatements, alignArguments, and alignParameters options are not supported when indenting with tabs, and XML indentation is handled differently. multilineScaladocCommentsStartOnFirstLine Default: false If true, start a multi-line Scaladoc comment body on same line as the opening comment delimiter: /** This method applies f to each  *  element of the given list.  */  If false, start the comment body on a separate line below the opening delimiter: /**  * This method applies f to each  * element of the given list.  */  placeScaladocAsterisksBeneathSecondAsterisk Default: false If true, Scaladoc asterisks will be placed beneath the second asterisk: /** Wibble   * wobble   */ class A  Otherwise, if false, beneath the first asterisk: /** Wibble  *  wobble  */ class A  preserveSpaceBeforeArguments Default: false If true, the formatter will keep an existing space before a parenthesis argument. For example: stack.pop() should equal (2)  Otherwise, if false, spaces before arguments will always be removed. danglingCloseParenthesis Default: Force If Force, any closing parentheses will be set to dangle. For example: Box(   contents: List[Thing])  becomes: Box(   contents: List[Thing] )  If Prevent, all dangling parenthesis are collapsed. For example: Box(   contents: List[Thing] )  becomes: Box(   contents: List[Thing])  If Preserve, scalariform will try to match what unformatted source code is already doing per parenthesis, either forcing or preventing. rewriteArrowSymbols Default: false Replace arrow tokens with their unicode equivalents: => with ⇒, and <- with ←. For example: for (n <- 1 to 10) n % 2 match {   case 0 => println(""even"")   case 1 => println(""odd"") }  is formatted as: for (n ← 1 to 10) n % 2 match {   case 0 ⇒ println(""even"")   case 1 ⇒ println(""odd"") }  spaceBeforeColon Default: false Whether to ensure a space before colon. For example, if false, then: def add(a: Int, b: Int): Int = a + b  If true, then: def add(a : Int, b : Int) : Int = a + b  spaceInsideBrackets Default: false Whether to use a space inside type brackets. For example, if true, then: Array[ String ]  If false, then: Array[String]  spaceInsideParentheses Default: false Whether to use a space inside non-empty parentheses. For example, if true, then: def main( args : Array[String] )  If false, then: def main(args : Array[String])  spacesWithinPatternBinders Default: true Whether to add a space around the @ token in pattern binders. For example, if true,: case elem @ Multi(values @ _*) =>  If false,: case elem@Multi(values@_*) =>  spacesAroundMultiImports Default: false Whether or not to add spaces around multi-imports. For example, if false, then: import a.{b,c,d} import foo.{bar => baz}  If true, then: import a.{ b, c, d } import foo.{ bar => baz }  Older versions of Scalariform used true, but the standard Scala formatting requires false. See the examples given in ""Chapter 13 - Packages and Imports."", page 244 of Programming in Scala 2nd ed. (2010) by Odersky, Spoon and Venners. Scala Style Guide Scalariform is compatible with the Scala Style Guide in the sense that, given the right preference settings, source code that is initially compliant with the Style Guide will not become uncompliant after formatting. In a number of cases, running the formatter will make uncompliant source more compliant. Preference Value Default? alignParameters false   compactStringConcatenation false   doubleIndentClassDeclaration true No indentSpaces 2   placeScaladocAsterisksBeneathSecondAsterisk true No preserveSpaceBeforeArguments false   rewriteArrowSymbols false   spaceBeforeColon false   spaceInsideBrackets false   spaceInsideParentheses false   spacesAroundMultiImports false   Source Directives As well as global preferences, formatting can be tweaked at the source level through comments. format: [ON|OFF] Disables the formatter for selective portions of a source file: // format: OFF    <-- this directive disables formatting from this point class AsciiDSL {   n ¦- ""1"" -+ { n: Node =>           n ¦- ""i""           n ¦- ""ii""           n ¦- ""iii""           n ¦- ""iv""           n ¦- ""v""   }   n ¦- ""2""   n ¦- ""3"" -+ { n: Node =>           n ¦- ""i""           n ¦- ""ii"" -+ { n: Node =>                    n ¦- ""a""                    n ¦- ""b""                    n ¦- ""c""           }           n ¦- ""iii""           n ¦- ""iv""           n ¦- ""v""   }   // format: ON   <-- formatter resumes from this point   ... } // (see: http://dev.day.com/microsling/content/blogs/main/scalajcr2.html)  format: [+|-]<preferenceName> Sets a preference for the entirety of the source file, overriding the global formatting settings: // format: +preserveSpaceBeforeArguments class StackSpec extends FlatSpec with ShouldMatchers {   // ...   stack.pop() should equal (2) }  Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/daniel-trinh/scalariform"	"Scala source code formatter"	"true"
"Tools"	"Scalastyle ★ 361 ⧗ 0"	"https://github.com/scalastyle/scalastyle"	"Scala style checker."	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"390"	"32"	"121"	"GitHub - scalastyle/scalastyle: scalastyle Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 32 Star 390 Fork 121 scalastyle/scalastyle Code Issues 48 Pull requests 4 Wiki Pulse Graphs scalastyle 522 commits 2 branches 10 releases 41 contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master plugin Nothing to show v0.8.0 v0.7.0 v0.6.0 scalastyle_2.10-0.5.0 scalastyle_2.10-0.4.0 scalastyle_2.10-0.3.2 scalastyle_2.10-0.3.1 scalastyle_2.10-0.3.0 scalastyle_2.9.2-0.2.0 scalastyle_2.9.1-0.1.0 Nothing to show New pull request Latest commit 129957e Jul 5, 2016 matthewfarwell committed on GitHub Merge pull request #201 from shanielh/master … ScalaDocChecker: Add ignoreTokenTypes option Permalink Failed to load latest commit information. lib Add an option to ignore reporting on entire lines that are whitespace… Dec 8, 2015 project Finish enabling codecov and update README Oct 20, 2015 src ScalaDocChecker: Add ignoreTokenTypes option Jun 16, 2016 .gitignore removed .DS_store from .gitignore and improved documentation for excl… Sep 10, 2015 .scalastyle Added various rules May 17, 2013 .travis.yml Finish enabling codecov and update README Oct 20, 2015 LICENCE.txt Added licence text to files Mar 8, 2012 README.md Finish enabling codecov and update README Oct 20, 2015 build.sbt Added -Dscalastyle.publish-ivy-only for community-builds Feb 28, 2016 release-notes.txt Update release notes for 0.3.0 May 16, 2013 version.sbt Prepare for next iteration Dec 1, 2015 README.md Scalastyle - Scala style checker Scalastyle examines your Scala code and indicates potential problems with it. Full documentation is currently available on the Scalastyle website Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scalastyle/scalastyle"	"Scala style checker."	"true"
"Tools"	"Scalatex ★ 184 ⧗ 4"	"https://github.com/lihaoyi/Scalatex"	"Programmable, Typesafe Document Generation"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"203"	"17"	"16"	"GitHub - lihaoyi/Scalatex: Programmable, Typesafe Document Generation Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 17 Star 203 Fork 16 lihaoyi/Scalatex Code Issues 8 Pull requests 0 Pulse Graphs Programmable, Typesafe Document Generation 145 commits 2 branches 1 release Fetching contributors Scala 100.0% Scala Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags gh-pages master Nothing to show 0.3.0 Nothing to show New pull request Latest commit 9d6bdab Jun 24, 2016 lihaoyi committed on GitHub Merge pull request #28 from williamho/fix-typos … Fix typos and capitalization Permalink Failed to load latest commit information. api/src Merge pull request #20 from jopasserat/master Dec 5, 2015 project 0.3.5 Dec 5, 2015 readme Fix typos and capitalization Jun 3, 2016 scalatexSbtPlugin/src/main/scala/scalatex also call fixPath on the output; windows build fixed Nov 22, 2015 scrollspy/src/main/scala/scalatex/scrollspy Update dependencies Oct 31, 2015 site/src fix build 2 Dec 6, 2015 .gitignore Added Eclipse project files to .gitignore Sep 22, 2015 .travis.yml Add `@lnk` function, and `--validate` flag to `run` which validates a… Oct 31, 2015 build.sbt 0.3.4 Nov 10, 2015 readme.md tweak travis Oct 25, 2015 readme.md Scalatex Documentation Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/lihaoyi/Scalatex"	"Programmable, Typesafe Document Generation"	"true"
"Tools"	"Scapegoat ★ 133 ⧗ 4"	"https://github.com/sksamuel/scapegoat"	"Scala compiler plugin for static code analysis"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"142"	"14"	"24"	"GitHub - sksamuel/scapegoat: Scala compiler plugin for static code analysis Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 14 Star 142 Fork 24 sksamuel/scapegoat Code Issues 29 Pull requests 1 Pulse Graphs Scala compiler plugin for static code analysis 638 commits 2 branches 31 releases 17 contributors Scala 99.8% Java 0.2% Scala Java Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master patch-1 Nothing to show v1.2.1 v1.2.0 v1.1.1 v1.1.0 1.0.0 0.94.7 0.94.6 0.94.4 0.94.3 0.94.2 0.94.1 0.94.0 0.93.2 0.93.1 0.92.2 0.92.1 0.92.0 0.90.17 0.90.14 0.90.12 0.90.11 0.90.10 0.90.9 0.90.8 0.90.5 0.90.4 0.90.3 0.90.1 0.90.0 0.5.0 0.3.0 Nothing to show New pull request Latest commit 4d3bca6 Mar 7, 2016 sksamuel added label for 2.11 Permalink Failed to load latest commit information. project Added sbt release plugin Dec 24, 2015 src Added checking for arrays for reverse.tail.reverse; check if Traversable Mar 3, 2016 .gitignore Add target to the gitignore May 21, 2015 .travis.yml Update .travis.yml Nov 29, 2015 LICENSE Initial commit Nov 21, 2013 README.md added label for 2.11 Mar 7, 2016 build.sbt Fixed pom Dec 24, 2015 screenshot1.png Improved snippet in AsInstanceOf Jul 28, 2014 version.sbt Setting version to 1.2.2-SNAPSHOT Mar 6, 2016 README.md Scapegoat Scapegoat is a Scala static code analyzer, what is more colloquially known as a code lint tool or linter. Scapegoat works in a similar vein to Java's FindBugs or checkstyle, or Scala's Scalastyle. A static code analyzer is a tool that flag suspicious language usage in code. This can include behavior likely to lead or bugs, non idiomatic usage of a language, or just code that doesn't conform to specified style guidelines. What's the difference between this project and Scalastyle (or others)? Scalastyle is a similar linting tool which focuses mostly on enforcing style/code standards. There's no problems running multiple analysis tools on the same codebase. In fact it could be beneficial as the total set of possible warnings is the union of the inspections of all the enabled tools. The worst case is that the same warnings might be generated by multiple tools. Usage Scapegoat is developed as a scala compiler plugin, which can then be used inside your build tool. Latest Release: 1.2.0 See: sbt-scapegoat for SBT integration. Reports Here is sample output from the console during the build for a project with warnings/errors: [warning] [scapegoat] Unused method parameter - org.ensime.util.ClassIterator.scala:46 [warning] [scapegoat] Unused method parameter - org.ensime.util.ClassIterator.scala:137 [warning] [scapegoat] Use of var - org.ensime.util.ClassIterator.scala:22 [warning] [scapegoat] Use of var - org.ensime.util.ClassIterator.scala:157 [   info] [scapegoat]: Inspecting compilation unit [FileUtil.scala] [warning] [scapegoat] Empty if statement - org.ensime.util.FileUtil.scala:157 [warning] [scapegoat] Expression as statement - org.ensime.util.FileUtil.scala:180   And if you prefer a prettier report, here is a screen shot of the type of HTML report scapegoat generates: Configuration For instructions on suppressing warnings by file, by inspection or by line see the sbt-scapegoat README. False positives Please note that scapegoat is a new project. While it's been tested on some common open source projects, there is still a good chance you'll find false positives. Please open up issues if you run into these so we can fix them. Changelog 0.94.0 - Fixed some more false positives. Added MethodNames inspection, StripMarginOnRegex inspection, and VariableShadowing inspection (the latter being a work in progress, please report feedback). 0.93.2 - Fixed false positives. 0.93.1 - #67 fixed var args in duplicate map check, #66 ignoring methods returning nothing when checking for unused params, #69 fixed extended classes false pos, #73 Removed incorrect inspection, #64 updated supression to use tree.symbol.isSynthetic instead of mods.synth, Merge pull request #77 from paulp/psp, Give access to the inspection logic through the sbt console, #76 Improve the contains test. 0.93.0 - Added ability to define multiple traversers that run in seperate phases of the compiler, #58 Updated supression to work on objects and classes, #60 handling case objects in suspcious match on class object, Allow all inspections to be disabled, other fixes 0.92.2 - Added debug option, Made summary optional and disabled in tests, Improved var could be val #54, Split null inspections into assignment and invocation #53, Bumped count on operators to > 2, loads of fixes, loads of verboseness removed. 0.92.1 - Fixed a load of false positives. 0.92.0 - Added swallowed exception inspection, Added public finalizer inspection, Added use expm1(x) instead of exp(x) - 1 inspection, Added use log1p(x) instead of log(x + 1) inspection, Added use log10(x) instead of log(x)/log(10) inspection, Added use cbrt inspection 0.91.0 - Updated logging format to include less [scapegoat] everywhere, Addded scala.math and java.StrictMath to useSqrt, Added ignored files patterns option, Added wildcard import inspection, Added comparison to empty set inspection 0.90.17 - Added looks like interpolated string inspection, Added SuspiciousMatchOnClassObject inspection, Updated varuse to not warn on vars in actors #46, Added comparison to empty list inspection, #37 Changed emptyinterpolated string to error, #37 Fixed warning on max parameters 0.90.14 - Bunch of bug fixes for false positives. No new inspections. 0.90.13 - Fixed NPE in VarClosure inspection, Added Object Names inspection, Added classnames inspection, Added avoid to minus one inspection. 0.90.12 - New inspections: unnecessary override, duplicate import, pointless type bounds, max parameters, var closure, method returning any. Updated repeated case body to ignore bodies with two or less statements #28. Removed false positives on getter/setter #27. 0.90.11 - Added empty for inspection, AnyUse inspection, Added ArrayEquals inspection, Added double negation inspection, Disabled expession as statement inspection by default, Added avoid operator overload inspection, #25 improving repeated case bodies, Added lonely sealed trait. Added postInspection call to inspections 0.90.10 - Added type shadowing inspection, var could be val inspection, unreachable catch inspection and unnecessary toString inspection 0.90.09 - Added new inspections: bounded by final type, empty while block, prefer vector empty, finalizer without super, impossible option size condition, filter dot head, repeated case body. Added infos to HTML output header 0.90.8 - Fixed erroneous partial functions inspection. Added inspection for empty case classe. Changed levels in output to lowercase. Added console output option. Fixed seq empty on non empty seq. Changed return usage to info. Fixed odd issue with empty tree. Changed unused parameter in override to be info. Ignoring all synthetic method added to case classes. Fixed while(true) being detected by ConstantIf Inspections There are currently 107 inspections. An overview list is given, followed by a more detailed description of each inspection after the list (todo: finish rest of detailed descriptions) Name Brief Description ArrayEquals Checks for comparison of arrays using == which will always return false ArraysInFormat Checks for arrays passed to String.format ArraysToString Checks for explicit toString calls on arrays AvoidOperatorOverload Checks for mental symbolic method names AvoidSizeEqualsZero Traversable.size can be slow for some data structure, prefer .isEmpty AvoidSizeNotEqualsZero Traversable.size can be slow for some data structure, prefer .nonEmpty AvoidToMinusOne Checks for loops that use x to n-1 instead of x until n AsInstanceOf Checks for use of asInstanceOf BigDecimalDoubleConstructor Checks for use of BigDecimal(double) which can be unsafe BoundedByFinalType Looks for types with upper bounds of a final type BrokenOddness checks for a % 2 == 1 for oddness because this fails on negative numbers CatchNpe Checks for try blocks that catch null pointer exceptions CatchException Checks for try blocks that catch Exception CatchFatal Checks for try blocks that catch fatal exceptions: VirtualMachineError, ThreadDeath, InterruptedException, LinkageError, ControlThrowable CatchThrowable Checks for try blocks that catch Throwable ClassNames Ensures class names adhere to the style guidelines CollectionNamingConfusion Checks for variables that are confusingly named CollectionNegativeIndex Checks for negative access on a sequence eg list.get(-1) CollectionPromotionToAny Checks for collection operations that promote the collection to Any ComparingFloatingPointTypes Checks for equality checks on floating point types ComparingUnrelatedTypes Checks for equality comparisons that cannot succeed ComparisonToEmptyList Checks for code like a == List() or a == Nil ComparisonToEmptySet Checks for code like a == Set() or a == Set.empty ComparisonWithSelf Checks for equality checks with itself ConstantIf Checks for code where the if condition compiles to a constant DivideByOne Checks for divide by one, which always returns the original value DoubleNegation Checks for code like !(!b) DuplicateImport Checks for import statements that import the same selector DuplicateMapKey Checks for duplicate key names in Map literals DuplicateSetValue Checks for duplicate values in set literals EitherGet Checks for use of .get on Left or Right EmptyCaseClass Checks for case classes like case class Faceman() EmptyCatchBlock Checks for swallowing exceptions EmptyFor Checks for empty for loops EmptyIfBlock Checks for empty if blocks EmptyInterpolatedString Looks for interpolated strings that have no arguments EmptyMethod Looks for empty methods EmptySynchronizedBlock Looks for empty synchronized blocks EmptyTryBlock Looks for empty try blocks EmptyWhileBlock Looks for empty while loops ExistsSimplifableToContains exists(x => x == b) replaceable with contains(b) FilterDotHead .filter(x => ).head can be replaced with find(x => ) match { .. } FilterDotHeadOption .filter(x =>).headOption can be replaced with find(x => ) FilterDotIsEmpty .filter(x => Bool).isEmpty can be replaced with !exists(x => Bool) FilterOptionAndGet .filter(_.isDefined).map(_.get) can be replaced with flatten FilterDotSize .filter(x => Bool).size can be replaced more concisely with with count(x => Bool) FinalizerWithoutSuper Checks for overriden finalizers that do not call super FindDotIsDefined find(x => Bool).isDefined can be replaced with exist(x => Bool) IllegalFormatString Looks for invalid format strings IncorrectlyNamedExceptions Checks for exceptions that are not called *Exception and vice versa IncorrectNumberOfArgsToFormat Checks for wrong number of arguments to String.format InvalidRegex Checks for invalid regex literals ImpossibleOptionSizeCondition Checks for code like option.size > 2 which can never be true IsInstanceOf Checks for use of isInstanceOf JavaConversionsUse Checks for use of implicit Java conversions ListAppend Checks for List :+ which is O(n) ListSize Checks for List.size which is O(n). LooksLikeInterpolatedString Finds strings that look like they should be interpolated but are not LonelySealedTrait Checks for sealed traits which have no implementation MaxParameters Checks for methods that have over 10 parameters MethodNames Warns on method names that don't adhere to the Scala style guidelines MethodReturningAny Checks for defs that are defined or inferred to return Any ModOne Checks for x % 1 which will always return 0 NanComparison Checks for x == Double.NaN which will always fail NegationIsEmpty !Traversable.isEmpty can be replaced with Traversable.nonEmpty NegationNonEmpty !Traversable.nonEmpty can be replaced with Traversable.isEmpty NullUse Checks for use of null ObjectNames Ensures object names adhere to the Scala style guidelines OptionGet Checks for Option.get OptionSize Checks for Option.size ParameterlessMethodReturnsUnit Checks for def foo : Unit PartialFunctionInsteadOfMatch Warns when you could use a partial function directly instead of a match block PointlessTypeBounds Finds type bounds of the form [A <: Any] or [A >: Nothing] PreferSeqEmpty Checks for Seq() when could use Seq.empty PreferSetEmpty Checks for Set() when could use Set.empty PreferVectorEmpty Checks for Vector() when could use Vector.empty ProductWithSerializableInferred Checks for vals that have Product with Serializable as their inferred type PublicFinalizer Checks for overriden finalizes that are public RedundantFinalizer Checks for empty finalizers. RepeatedCaseBody Checks for case statements which have the same body SimplifyBooleanExpression b == false can be simplified to !b StripMarginOnRegex Checks for .stripMargin on regex strings that contain a pipe SubstringZero Checks for String.substring(0) SuspiciousMatchOnClassObject Finds code where matching is taking place on class literals SwallowedException Finds catch blocks that don't handle caught exceptions SwapSortFilter sort.filter can be replaced with filter.sort for performance TraversableHead Looks for unsafe usage of Traversable.head TryGet Checks for use of Try.get TypeShadowing Checks for shadowed type parameters in methods UnnecessaryIf Checks for code like if (expr) true else false UnneccessryOverride Checks for code that overrides parent method but simply calls super UnnecessaryReturnUse Checks for use of return keyword in blocks UnnecessaryToInt Checks for unnecessary toInt on instances of Int UnnecessaryToString Checks for unnecessary toString on instances of String UnreachableCatch Checks for catch clauses that cannot be reached UnsafeContains Checks for List.contains(value) for invalid types UnusedMethodParameter Checks for unused method parameters UseCbrt Checks for use of math.pow for calculating math.cbrt UseExpM1 Checks for use of math.exp(x) - 1 instead of math.expm1(x) UseLog10 Checks for use of math.log(x)/math.log(10) instead of math.log10(x) UseLog1P Checks for use of math.log(x + 1) instead of math.log1p(x) UseSqrt Checks for use of math.pow for calculating math.sqrt VarClosure Finds closures that reference var VarCouldBeVal Checks for vars that could be declared as vals VariableShadowing Warns for variables that shadow variables or parameters in an outer scope with the same name VarUse Checks for use of var WhileTrue Checks for code that uses a while(true) or do { } while(true) block. WildcardImport Checks for wildcard imports ZeroNumerator Checks for dividing by 0 by a number, eg 0 / x which will always return 0 Arrays to string Checks for explicit toString calls on arrays. Since toString on an array does not perform a deep toString, like say scala's List, this is usually a mistake. ComparingUnrelatedTypes Checks for equality comparisons that cannot succeed because the types are unrelated. Eg ""string"" == BigDecimal(1.0). The scala compiler has a less strict version of this inspection. ConstantIf Checks for if statements where the condition is always true or false. Not only checks for the boolean literals, but also any expression that the compiler is able to turn into a constant value. Eg, if (0 < 1) then else that IllegalFormatString Checks for a format string that is not invalid, such as invalid conversions, invalid flags, etc. Eg, ""% s"", ""%qs"", %.-4f"" IncorrectNumberOfArgsToFormat Checks for an incorrect number of arguments to String.format. Eg, ""%s %s %f"".format(""need"", ""three"") flags an error because the format string specifies 3 parameters but the call only provides 2. InvalidRegex Checks for invalid regex literals that would fail at compile time. Either dangling metacharacters, or unclosed escape characters, etc that kind of thing. List size Checks for .size on an instance of List. Eg, val a = List(1,2,3); a.size Rationale List.size is O(n) so for performance reasons if .size is needed on a list that could be large, consider using an alternative with O(1), eg Array, Vector or ListBuffer. Redundant finalizer Checks for empty finalizers. This is redundant code and should be removed. Eg, override def finalize : Unit = { } PreferSetEmpty Indicates where code using Set() could be replaced with Set.empty. Set() instantiates a new instance each time it is invoked, whereas Set.emtpy returns a pre-instantiated instance. UnnecessaryReturnUse Checks for use of return in a function or method. Since the final expression of a block is always the return value, using return is unnecessary. Eg, def foo = { println(""hello""); return 12; } UnreachableCatch Checks for catch clauses that cannot be reached. This means the exception is dead and if you want that exception to take precedence you should move up further up the case list. UnsafeContains Checks for List.contains(value) for invalid types. The method for contains accepts any types. This inspection finds situations when you have a list of type A and you are checking for contains on type B which cannot hold. While true Checks for code that uses a while(true) or do { } while(true) block. Rationale: This type of code is usually not meant for production as it will not return normally. If you need to loop until interrupted then consider using a flag. Other static analysis tools: ScalaStyle (Scala) - https://github.com/scalastyle/scalastyle/wiki Linter (Scala) - https://github.com/HairyFotr/linter WartRemover (Scala) - https://github.com/puffnfresh/wartremover Findbugs (JVM) - http://findbugs.sourceforge.net/bugDescriptions.html Fb-contrib (JVM) - http://fb-contrib.sourceforge.net/ CheckStyle (Java) - http://checkstyle.sourceforge.net/availablechecks.html PMD (Java) - http://pmd.sourceforge.net/pmd-5.0.3/rules/index.html Error-prone (Java) - https://github.com/google/error-prone CodeNarc (Groovy) - http://codenarc.sourceforge.net/codenarc-rule-index.html PVS-Studio (C++) - http://www.viva64.com/en/d/ Coverity (C++) - http://www.slideshare.net/Coverity/static-analysis-primer-22874326 (6,7) CppCheck (C++) - http://cppcheck.sourceforge.net/ OCLint (C++/ObjC) - http://docs.oclint.org/en/dev/rules/index.html JSHint (Javascript) - http://jshint.com/ JavascriptLint (Javascript) - http://www.javascriptlint.com/ ClosureLinter (Javascript) - https://developers.google.com/closure/utilities/ Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/sksamuel/scapegoat"	"Scala compiler plugin for static code analysis"	"true"
"Tools"	"Scaps"	"http://scala-search.org/"	"() - A search engine for Scala libraries"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"Scaps: Scala API Search Scaps scala-library:2.11.7 scalajs-dom_sjs0.6_2.11:0.8.0 scalajs-library_2.11:0.6.2 scalaz-core_2.11:7.1.1 Scaps: Scala API Search Scaps is a search engine for discovering functionality in Scala libraries (or in other words, a Hoogle for Scala). You can use both type signatures and keywords in your search queries. Example Queries max: Int - An integer value with `max` in it's name or doc comment. max: (Int, Int) => Int - A function taking two ints and returning Int. max: Int => Int => Int - Same query as above but in curried form. Ordering[String] - Implementations of the `Ordering` typeclass for strings. List[A] => Int => Option[A] - A generic query which uses a type parameter `A`. All type identifiers consisting of a single character are treated as type parameters. List => Int => Option - The identical query as above but with omitted type parameters. &~ - Searches for symbolic operators are also possible. About Scaps Scaps is an offspring of a master's thesis by Lukas Wegmann at the University of Applied Science Rapperswil (HSR). by Lukas Wegmann | version 0.5-SNAPSHOT | About"	"null"	"null"	"() - A search engine for Scala libraries"	"true"
"Tools"	"repo"	"https://github.com/scala-search/scaps"	"() - A search engine for Scala libraries"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"20"	"3"	"2"	"GitHub - scala-search/scaps: Scala API Search Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 3 Star 20 Fork 2 scala-search/scaps Code Issues 9 Pull requests 0 Pulse Graphs Scala API Search http://scala-search.org 437 commits 1 branch 9 releases Fetching contributors Scala 99.7% Other 0.3% Scala Other Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: master Switch branches/tags Branches Tags master Nothing to show v0.6 v0.5 v0.4 v0.3 v0.2 v0.1 maFinal evaluation ecoop Nothing to show New pull request Latest commit c1d9ba8 Jun 2, 2016 Luegg Fix distance and improve frequency calculation Permalink Failed to load latest commit information. api/shared/src Transitive views lookup May 18, 2016 core/src Fix distance and improve frequency calculation Jun 2, 2016 demoEnvironment Update to Scala 2.11.7 May 20, 2016 evaluation/src Fix distance and improve frequency calculation Jun 2, 2016 nucleus/src Integrate nucleus Apr 4, 2016 project Update to Scala 2.11.7 May 20, 2016 sbtPlugin/src/main/scala/scaps/sbtPlugin Make scaps-sbt config more robust May 18, 2016 scala/src Fix weird extraction errors in 2.11.8 May 20, 2016 webservice/src/main upgrade upickle Mar 2, 2016 webserviceShared/shared/src/main/scala/scaps/webservice/ui relicensing to MPL Feb 22, 2016 webserviceUI/src/main/scala/scaps/webservice/ui upgrade upickle Mar 2, 2016 .gitignore add scapsApi sketch Feb 15, 2016 .travis.yml use JDK 8 on travis Oct 7, 2015 LICENSE relicensing to MPL Feb 22, 2016 README.md update readme Feb 22, 2016 build.sbt publish to maven Mar 4, 2016 codeMetrics.sh small fixes May 1, 2015 evaluation.sh additional evaluation runs and improved reporting Nov 20, 2015 runBenchmark.sh use type refs for lower and upper bounds of type params; addresses #19 Dec 15, 2015 version.sbt Setting version to 0.7-SNAPSHOT May 20, 2016 README.md Scaps: Scala API Search Scaps is a search engine for discovering functionality in Scala libraries. You can use both type signatures and keywords to search for definitions. License Scaps is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with Scaps, You can obtain one at http://mozilla.org/MPL/2.0/. Development Benchmark Changes to the search algorithm can be evaluated with the test collections defined in evaluation/src/main/resources/. To benchmark the core library with the standard parametrization, run sbt ’evaluation/run-main scaps.evaluation.Benchmark’ Publish and Install the SBT Plug-in The SBT plug-in has to be published to a maven repository before it can be used in an SBT project. To publish all artifacts including the plug-in to the local Ivy repository, run: sbt publishLocal The plug-in can now be used by creating a new SBT project on the same machine that also hosts the web service and including the following line in the project/plugins.sbt file: addSbtPlugin(""org.scala-search"" % ""scaps-sbt"" % <scapsVersion>) Additionally, the libraries to index have to be included in the project's build.sbt file as a library dependency. For example, to index scalaz, add libraryDependencies += ""org.scalaz"" %% ""scalaz-core"" % ""7.1.1"" The Scala Standard Library is a library dependency per default and does not need to be added in order to be indexed. If the Scaps control API is not exposed at the default ports, the correct hostname can bet set by using scapsControlHost in Scaps := ""localhost:9000"" to set the location of the Control API. Finally, an index job can be started by running sbt scaps:run An example project, that demonstrates the required project structure is also given in the demoEnvironment directory. Run the Web Service The simplest way to start an instance of the Scaps web service is by using SBT: sbt 'webservice/run' This will bind the User API and the Control API to the interfaces and ports as configured in webservice/src/main/resources/application.conf. By default, the User API (including the Web UI) is bound to all interfaces on the port 8080 and the Control API is bound to the localhost interface on the port 8081. For executing the Service in a productive deployment, using SBT is not advised. Instead, the service should be packaged in a standalone, executable jar (see http://www.scala-sbt.org/sbt-native-packager/): sbt 'webservice/debian:packageBin' An example configuration for a productive deployment is given in webservice/src/main/resources/application-prod.conf. Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scala-search/scaps"	"() - A search engine for Scala libraries"	"true"
"Tools"	"Scoverage"	"https://github.com/scoverage"	"Scala Code Coverage tool"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"scoverage · GitHub Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This organization scoverage scalaland http://scoverage.org Repositories People 4 Filters Sources Forks Scala 232 61 sbt-scoverage sbt plugin for scoverage Updated Jul 12, 2016 Scala 68 29 sbt-coveralls Sbt plugin for uploading Scala code coverage to coveralls Updated Jun 28, 2016 Scala 225 57 scalac-scoverage-plugin Scoverage Scala Code Coverage Core Libs Updated Jun 22, 2016 Scala 1 3 scoverage-maven-samples scoverage-maven-samples Updated Mar 31, 2016 Java 11 14 scoverage-maven-plugin Maven plugin for scoverage Updated Feb 23, 2016 Groovy 17 7 gradle-scoverage A plugin to enable the use of Scoverage in a gradle Scala project Updated Jan 16, 2016 Scala 16 14 sbt-scoverage-samples Sample project for scoverage in SBT and Gradle Updated Mar 18, 2015 Scala 1 2 gradle-scoverage-sample Updated Feb 7, 2015 0 0 scoverage.github.io Updated Nov 26, 2014 CSS 2 1 scoverage.org Updated Jul 20, 2014 1 0 scoverage-activator Typesafe Activator template for Scoverage, a code coverage tool for Scala Updated May 21, 2014 Java 1 7 scoverage-plugin forked from jenkinsci/scoverage-plugin Jenkins plugin for scoverage Updated Mar 22, 2014 Scala 8 19 sonar-scoverage-plugin forked from RadoBuransky/sonar-scoverage-plugin Sonar plugin for Scala statement coverage tool Updated Feb 12, 2014 4 People Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/scoverage"	"Scala Code Coverage tool"	"true"
"Tools"	"Wartremover ★ 467 ⧗ 2"	"https://github.com/puffnfresh/wartremover"	"Wartremover a flexible Scala code linting tool"	"null"	"null"	"null"	"null"	"null"	"null"	"null"	"526"	"37"	"49"	"GitHub - puffnfresh/wartremover: Flexible Scala code linting tool Skip to content Personal Open source Business Explore Sign up Sign in Pricing Blog Support Search GitHub This repository Watch 37 Star 526 Fork 49 puffnfresh/wartremover Code Issues 73 Pull requests 7 Pulse Graphs Flexible Scala code linting tool 297 commits 5 branches 16 releases 25 contributors Scala 85.8% Shell 14.2% Scala Shell Clone or download Use SSH Clone with HTTPS Use Git or checkout with SVN using the web URL. Use HTTPS Clone with SSH Use an SSH key and passphrase from account. Open in Desktop Download ZIP Find file Branch: latest-release Switch branches/tags Branches Tags debug latest-release master newInterface revert-245-ExcludesAsTaskKey Nothing to show v1.0.1 v1.0.0 v0.14 v0.13 v0.12 v0.11 v0.10 v0.9 v0.8 v0.7 v0.6 v0.5 v0.4 v0.3 v0.2 v0.1 Nothing to show New pull request Latest commit 8294364 Jul 10, 2016 puffnfresh Revert ""Switched wartremoverExcluded to a task key"" … This reverts commit a0bf3b1.  Should have been merged to master. Permalink Failed to load latest commit information. core/src Fix #242 ImplicitConversion wart triggers for implicit evidence. (#243) Jun 27, 2016 project Upgrade to sbt 0.13.9 Dec 19, 2015 sbt-plugin/src/main/scala/wartremover Revert ""Switched wartremoverExcluded to a task key"" Jul 10, 2016 .gitignore Create sbt-wartremover sbt plugin Aug 1, 2014 .travis.yml Move Travis to Docker Dec 20, 2015 OTHER-WAYS.md Update READMEs. Jul 3, 2016 README.md Update READMEs. Jul 3, 2016 build.sbt Use GPG. Jun 25, 2016 sbt Complete rewrite Sep 15, 2013 version.sbt Setting version to 1.0.2-SNAPSHOT Jul 3, 2016 wartremover Fix the `wartremover` shell script Aug 1, 2014 README.md WartRemover WartRemover is a flexible Scala code linting tool. Usage Add the following to your project/plugins.sbt: addSbtPlugin(""org.wartremover"" % ""sbt-wartremover"" % ""1.0.1"") Now, you can proceed to configure the linter in your build.sbt. By default, all errors and warnings are turned off. To turn on all checks that are currently considered stable, use: wartremoverErrors ++= Warts.unsafe To turn on all available errors (some have false positives), use: wartremoverErrors ++= Warts.all Similarly, to just issue warnings instead of errors for all built-in warts, you can use: wartremoverWarnings ++= Warts.all    // or Warts.unsafe You can also use scopes, e.g. to turn on all warts only for compilation (and not for the tests nor the sbt console), use: wartremoverErrors in (Compile, compile) ++= Warts.all To choose warts more selectively, use any of the following: wartremoverErrors ++= Warts.allBut(Wart.Any, Wart.Nothing, Wart.Serializable)  wartremoverWarnings += Wart.Nothing  wartremoverWarnings ++= Seq(Wart.Any, Wart.Serializable) To exclude a file from all checks, use: wartremoverExcluded += baseDirectory.value / ""src"" / ""main"" / ""scala"" / ""SomeFile.scala"" To exclude a specific piece of code from one or more checks, use the SuppressWarnings annotation: @SuppressWarnings(Array(""org.wartremover.warts.Var"", ""org.wartremover.warts.Null"")) var foo = null Finally, if you want to add your custom WartTraverser, provide its classpath first: wartremoverClasspaths += ""some-url""  wartremoverErrors += Wart.custom(""org.your.custom.WartTraverser"") See also other ways of using WartRemover for information on how to use it as a command-line tool, a macro or a compiler plugin, while providing all the scalac options manually. Note - the WartRemover SBT plugin sets scalac options - make sure you're not overwriting those by having a scalacOptions := ... setting in your SBT settings. Use scalacOptions ++= ... instead. Warts Here is a list of built-in warts under the org.wartremover.warts package. Any Any is the top type; it is the supertype of every other type. The Scala compiler loves to infer Any as a generic type, but that is almost always incorrect. Explicit type arguments should be used instead. // Won't compile: Inferred type containing Any val any = List(1, true, ""three"") Any2StringAdd Scala has an implicit that converts anything to a String if the right hand side of + is a String. // Won't compile: Scala inserted an any2stringadd call println({} + ""test"") AsInstanceOf asInstanceOf is unsafe in isolation and violates parametricity when guarded by isInstanceOf. Refactor so that the desired type is proven statically. // Won't compile: asInstanceOf is disabled x.asInstanceOf[String] DefaultArguments Scala allows methods to have default arguments, which make it hard to use methods as functions. // Won't compile: Function has default arguments def x(y: Int = 0) EitherProjectionPartial scala.util.Either.LeftProjection and scala.util.Either.RightProjection have a get method which will throw if the value doesn't match the projection. The program should be refactored to use scala.util.Either.LeftProjection#toOption and scala.util.Either.RightProjection#toOption to explicitly handle both the Some and None cases. Enumeration Scala's Enumeration can cause performance problems due to its reliance on reflection. Additionally, the lack of exhaustive match checks and partial methods can lead to runtime errors. Instead of Enumeration, a sealed abstract class extended by case objects should be used instead. Equals Scala's Any type provides an == method which is not type-safe. Using this method allows obviously incorrect code like 5 == ""5"" to compile. A better version which forbids equality checks across types (which always fail) is easily defined: @SuppressWarnings(Array(""org.wartremover.warts.Equals"")) implicit final class AnyOps[A](self: A) {    def ===(other: A): Boolean = self == other } ExplicitImplicitTypes Scala has trouble correctly resolving implicits when some of them lack explicit result types. To avoid this, all implicits should have explicit type ascriptions. FinalCaseClass Scala's case classes provide a useful implementation of logicless data types. Extending a case class can break this functionality in surprising ways. This can be avoided by always making them final. // Won't compile: case classes must be final case class Foo() ImplicitConversion Implicit conversions weaken type safety and always can be replaced by explicit conversions. // Won't compile: implicit conversion is disabled implicit def int2Array(i: Int) = Array.fill(i)(""madness"") IsInstanceOf isInstanceOf violates parametricity. Refactor so that the type is established statically. // Won't compile: isInstanceOf is disabled x.isInstanceOf[String] JavaConversions The standard library provides implicits conversions to and from Java types in scala.collection.JavaConversions. This can make code difficult to understand and read about. The explicit conversions provided by scala.collection.JavaConverters should be used instead. // Won't compile: scala.collection.JavaConversions is disabled import scala.collection.JavaConversions._ val scalaMap: Map[String, String] = Map() val javaMap: java.util.Map[String, String] = scalaMap LeakingSealed Descendants of a sealed type must be final or sealed. Otherwise this type can be extended in another file through its descendant. file 1: // Won't compile: Descendants of a sealed type must be final or sealed sealed trait t class c extends t  file 2: class d extends c ListOps scala.collection.immutable.List has: head, tail, init, last, reduce, reduceLeft and reduceRight methods, all of which will throw if the list is empty. The program should be refactored to use: List#headOption, List#drop(1), List#dropRight(1), List#lastOption, List#reduceOption or List#fold, List#reduceLeftOption or List#foldLeft and List#reduceRightOption or List#foldRight respectively, to explicitly handle both the populated and empty List. MutableDataStructures The standard library provides mutable collections. Mutation breaks equational reasoning. // Won't compile: scala.collection.mutable package is disabled import scala.collection.mutable.ListBuffer val mutList = ListBuffer() NoNeedForMonad Sometimes an additional power of Monad is not needed, and Applicative is enough. This issues a warning in such cases (not an error, since using a Monad instance might still be a conscious decision) scala> for {      | x <- List(1,2,3)      | y <- List(2,3,4)      | } yield x * y <console>:19: warning: No need for Monad here (Applicative should suffice).  > ""If the extra power provided by Monad isn’t needed, it’s usually a good idea to use Applicative instead.""  Typeclassopedia (http://www.haskell.org/haskellwiki/Typeclassopedia)  Apart from a cleaner code, using Applicatives instead of Monads can in general case result in a more parallel code.  For more context, please refer to the aforementioned Typeclassopedia, http://comonad.com/reader/2012/abstracting-with-applicatives/, or http://www.serpentine.com/blog/2008/02/06/the-basics-of-applicative-functors-put-to-practical-work/               x <- List(1,2,3)                 ^ res0: List[Int] = List(2, 3, 4, 4, 6, 8, 6, 9, 12)  scala> for {      | x <- List(1,2,3)      | y <- x to 3      | } yield x * y res1: List[Int] = List(1, 2, 3, 4, 6, 9) NonUnitStatements Scala allows statements to return any type. Statements should only return Unit (this ensures that they're really intended to be statements). // Won't compile: Statements must return Unit 10 false Nothing Nothing is a special bottom type; it is a subtype of every other type. The Scala compiler loves to infer Nothing as a generic type but that is almost always incorrect. Explicit type arguments should be used instead. // Won't compile: Inferred type containing Nothing val nothing = ??? val nothingList = List.empty Null null is a special value that inhabits all reference types. It breaks type safety. // Won't compile: null is disabled val s: String = null Option2Iterable Scala inserts an implicit conversion from Option to Iterable. This can hide bugs and creates surprising situations like Some(1) zip Some(2) returning an Iterable[(Int, Int)]. OptionPartial scala.Option has a get method which will throw if the value is None. The program should be refactored to use scala.Option#fold to explicitly handle both the Some and None cases. Overloading Method overloading may lead to confusion and usually can be avoided. // Won't compile: Overloading is disabled class c {   def equals(x: Int) = {} } Product Product is a type common to many structures; it is the supertype of case classes and tuples. The Scala compiler loves to infer Product as a generic type, but that is almost always incorrect. Explicit type arguments should be used instead. // Won't compile: Inferred type containing Product val any = List((1, 2, 3), (1, 2)) Return return breaks referential transparency. Refactor to terminate computations in a safe way. // Won't compile: return is disabled def foo(n:Int): Int = return n + 1 def foo(ns: List[Int]): Any = ns.map(n => return n + 1) Serializable Serializable is a type common to many structures. The Scala compiler loves to infer Serializable as a generic type, but that is almost always incorrect. Explicit type arguments should be used instead. // Won't compile: Inferred type containing Serializable val any = List((1, 2, 3), (1, 2)) Throw throw implies partiality. Encode exceptions/errors as return values instead using Either. ToString Scala creates a toString method automatically for all classes. Since toString is based on the class name, any rename can potentially introduce bugs. This is especially pernicious for case objects. toString should be explicitly overridden wherever used. case object Foo { override val toString = ""Foo"" } TryPartial scala.util.Try has a get method which will throw if the value is a Failure. The program should be refactored to use scala.util.Try#map and scala.util.Try#getOrElse to explicitly handle both the Success and Failure cases. Unsafe Checks for the following warts: Any Any2StringAdd AsInstanceOf EitherProjectionPartial IsInstanceOf ListOps NonUnitStatements Null OptionPartial Product Return Serializable Throw TryPartial Var Var Mutation breaks equational reasoning. // Won't compile: var is disabled var x = 100 While while loop usually indicates low-level code. If performance is not an issue, it can be replaced. // Won't compile: while is disabled while(i < 10) {   i += 1   ... } Writing Wart Rules A wart rule has to be an object that extends WartTraverser. The object only needs an apply method which takes a WartUniverse and returns a WartUniverse#universe#Traverser. The WartUniverse has error and warning methods, which both take (WartUniverse#universe#Position, String). They are side-effecting methods for adding errors and warnings. Most traversers will want a super.traverse call to be able to recursively continue. import org.wartremover.{WartTraverser, WartUniverse}  object Unimplemented extends WartTraverser {   def apply(u: WartUniverse): u.Traverser = {     import u.universe._     import scala.reflect.NameTransformer      val notImplementedName: TermName = NameTransformer.encode(""???"")     val notImplemented: Symbol = typeOf[Predef.type].member(notImplementedName)     require(notImplemented != NoSymbol)     new Traverser {       override def traverse(tree: Tree) {         tree match {           case rt: RefTree if rt.symbol == notImplemented =>             u.error(tree.pos, ""There was something left unimplemented"")           case _ =>         }         super.traverse(tree)       }     }   } } Reporting Issues It's very useful to get the tree expanded by the Scala compiler, rather than the original source. Adding the -Xprint:typer flag to the Scala compiler will show code like the following: // println(""Hello world"") package $line4 {   object $read extends scala.AnyRef {     def <init>(): $line4.$read.type = {       $read.super.<init>();       ()     };     object $iw extends scala.AnyRef {       def <init>(): type = {         $iw.super.<init>();         ()       };       object $iw extends scala.AnyRef {         def <init>(): type = {           $iw.super.<init>();           ()         };         private[this] val res1: Unit = scala.this.Predef.println(""Hello world"");         <stable> <accessor> def res1: Unit = $iw.this.res1       }     }   } } Adding the generated code to an issue is very useful for debugging. License The Apache Software License, Version 2.0 Contact GitHub API Training Shop Blog About © 2016 GitHub, Inc. Terms Privacy Security Status Help Something went wrong with that request. Please try again. You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."	"null"	"https://github.com/puffnfresh/wartremover"	"Wartremover a flexible Scala code linting tool"	"true"
